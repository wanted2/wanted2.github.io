<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="https://wanted2.github.io/assets/images/favicon.ico">

<title>Seq2Seq và kiến trúc Encoder-Decoder | AiFi</title>

 
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-2CDTCF0EP6" crossorigin="anonymous"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-2CDTCF0EP6');
        </script>
    


<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Seq2Seq và kiến trúc Encoder-Decoder | AiFi</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Seq2Seq và kiến trúc Encoder-Decoder" />
<meta name="author" content="tuan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Seq2Seq [51, 51, 51] là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, [51]), Question-Answering (QA, [51]), Visual Question Answering (VQA, [51, 51]), Text Summarization (TS, [51, 51]) và Video-To-Text (VTT, [51]). Bài toán Image Captioning thì cũng có thể ứng dụng seq2seq nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào seq2seq như trong bài Semantic Attention (SA) [51] hay Densecap [51]. Nên nhìn chung là seq2seq là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng [51].1 Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn, hôm nay đọc tiếp cái sequence-to-sequence (Seq2Seq) cũng thấy có cả 30k-40k cũng có. Thế nên là cái Seq2Seq này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái Seq2Seq này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm). Chứ đọc làm sao mà hết được? Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp. Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. Có bài như bài gốc Transformer (Attention is all you need, [51]) mới ra đời từ 2017 mà đã hơn 35k trích dẫn! &#8617;" />
<meta property="og:description" content="Seq2Seq [51, 51, 51] là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, [51]), Question-Answering (QA, [51]), Visual Question Answering (VQA, [51, 51]), Text Summarization (TS, [51, 51]) và Video-To-Text (VTT, [51]). Bài toán Image Captioning thì cũng có thể ứng dụng seq2seq nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào seq2seq như trong bài Semantic Attention (SA) [51] hay Densecap [51]. Nên nhìn chung là seq2seq là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng [51].1 Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn, hôm nay đọc tiếp cái sequence-to-sequence (Seq2Seq) cũng thấy có cả 30k-40k cũng có. Thế nên là cái Seq2Seq này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái Seq2Seq này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm). Chứ đọc làm sao mà hết được? Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp. Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. Có bài như bài gốc Transformer (Attention is all you need, [51]) mới ra đời từ 2017 mà đã hơn 35k trích dẫn! &#8617;" />
<meta property="og:site_name" content="AiFi" />
<meta property="og:image" content="/https://wanted2.github.io/assets/images/san01.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-01T00:00:00+09:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="/https://wanted2.github.io/assets/images/san01.jpg" />
<meta property="twitter:title" content="Seq2Seq và kiến trúc Encoder-Decoder" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"tuan"},"dateModified":"2022-02-01T00:00:00+09:00","datePublished":"2022-02-01T00:00:00+09:00","description":"Seq2Seq [51, 51, 51] là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, [51]), Question-Answering (QA, [51]), Visual Question Answering (VQA, [51, 51]), Text Summarization (TS, [51, 51]) và Video-To-Text (VTT, [51]). Bài toán Image Captioning thì cũng có thể ứng dụng seq2seq nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào seq2seq như trong bài Semantic Attention (SA) [51] hay Densecap [51]. Nên nhìn chung là seq2seq là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng [51].1 Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn, hôm nay đọc tiếp cái sequence-to-sequence (Seq2Seq) cũng thấy có cả 30k-40k cũng có. Thế nên là cái Seq2Seq này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái Seq2Seq này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm). Chứ đọc làm sao mà hết được? Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp. Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. Có bài như bài gốc Transformer (Attention is all you need, [51]) mới ra đời từ 2017 mà đã hơn 35k trích dẫn! &#8617;","headline":"Seq2Seq và kiến trúc Encoder-Decoder","image":"/https://wanted2.github.io/assets/images/san01.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"/https://wanted2.github.io/seq2seq/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/https://wanted2.github.io/assets/images/favicon.ico"},"name":"tuan"},"url":"/https://wanted2.github.io/seq2seq/"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="https://wanted2.github.io/assets/css/screen.css" rel="stylesheet">

<link href="https://wanted2.github.io/assets/css/main.css" rel="stylesheet">

<script src="https://wanted2.github.io/assets/js/jquery.min.js"></script>
<script src="https://kit.fontawesome.com/d0b91d895e.js" crossorigin="anonymous"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
    }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" crossorigin="anonymous"></script>
<script src="https://d3js.org/d3.v4.js" crossorigin="anonymous"></script>
<!-- <script src="https://bl.ocks.org/mbostock/raw/4061502/0a200ddf998aa75dfdb1ff32e16b680a15e5cb01/box.js" crossorigin="anonymous"></script> -->
</head>


<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="https://wanted2.github.io/">
    <img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="https://wanted2.github.io/">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/about">About</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/projects">Projects</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-wordpress/"><i class="fab fa-wordpress-simple"></i> WP Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-ghost/"><i class="fab fa-snapchat-ghost"></i> Ghost Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/wowthemesnet/mediumish-theme-jekyll"><i class="fab fa-github"></i> Fork on Github</a>
                </li> -->

                <!-- <script src="https://wanted2.github.io/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="https://wanted2.github.io/assets/js/lunrsearchengine.js"></script> -->

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">AiFi</h1>
    <p class="lead">
        An AI Engineer's blog
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Seq2Seq và kiến trúc Encoder-Decoder&url=https://wanted2.github.io/seq2seq/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=https://wanted2.github.io/seq2seq/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=https://wanted2.github.io/seq2seq/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="mailto:?subject=Seq2Seq và kiến trúc Encoder-Decoder&body=https://wanted2.github.io/seq2seq/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fas fa-envelope"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
    
    <div class="sep">

    </div>
    <ul>
        <li class="small">
        10838
     words</li>
        <li class="small">60 minutes</li>
    </ul>
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-2 col-lg-2 text-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="https://wanted2.github.io/assets/images/favicon.png" alt="AiFi">
                        
                    </div>
                    <div class="col-xs-12 col-md-10 col-lg-10 text-right">
                        <a target="_blank" class="link-dark" href="">AiFi</a>
                        <!-- <a target="_blank" href="" class="btn follow">Follow</a> -->
                        <!-- LikeBtn.com BEGIN -->
                        <span class="likebtn-wrapper" 
                            data-site_id="61cfccd36fd08b2d68c1929e"
                            data-theme="custom" 
                            data-icon_l_url="/assets/images/OK_EM.png" 
                            data-icon_l_url_v="/assets/images/OK_EM_clicked.png" 
                            data-identifier="/seq2seq/" 
                            data-show_like_label="false" 
                            data-like_enabled="false" 
                            data-dislike_enabled="false" 
                            data-icon_dislike_show="false" 
                            data-voting_cancelable="false" 
                            data-counter_show="true"
                            data-counter_frmt="comma"></span>
                        <script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
                        <!-- LikeBtn.com END -->
                        <span class="author-description"></span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Seq2Seq và kiến trúc Encoder-Decoder</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid lazyimg" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=" data-src="https://wanted2.github.io/assets/images/san01.jpg" alt="Seq2Seq và kiến trúc Encoder-Decoder">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                    
                    <div class="toc mt-4 mb-4 lead">
                        <h3 class="font-weight-bold">Summary</h3>
                        <ul>
  <li><a href="#giới-thiệu-model-seq2seq">Giới thiệu model Seq2Seq</a>
    <ul>
      <li><a href="#recurrent-neural-nets-long-short-term-memory-và-gated-recurrent-units">Recurrent Neural Nets, Long-Short Term Memory và Gated Recurrent Units</a></li>
      <li><a href="#kiến-trúc-seq2seq-s2s">Kiến trúc Seq2Seq (S2S)</a></li>
      <li><a href="#transformers-và-bert">Transformers và BERT</a>
        <ul>
          <li><a href="#transformer-13">Transformer <a class="citation" href="#vaswani2017attention">[13]</a></a></li>
          <li><a href="#bert">BERT</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#ứng-dụng">Ứng dụng</a>
    <ul>
      <li><a href="#cách-tiếp-cận">Cách tiếp cận</a>
        <ul>
          <li><a href="#các-nguồn-tài-nguyên">Các nguồn tài nguyên</a></li>
          <li><a href="#đôi-lời-về-google-colab">Đôi lời về Google Colab</a>
            <ul>
              <li><a href="#chú-ý-về-train-song-song">Chú ý về train song song</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#neural-machine-translation-nmt">Neural Machine Translation (NMT)</a></li>
      <li><a href="#text-summarization">Text Summarization</a></li>
      <li><a href="#qa-và-vqa">QA và VQA</a></li>
      <li><a href="#video-to-text-và-image-captioning">Video-to-Text và Image Captioning</a></li>
    </ul>
  </li>
  <li><a href="#kết-luận">Kết luận</a>
    <ul>
      <li><a href="#về-seq2seq-trong-nlp">Về <code class="language-plaintext highlighter-rouge">seq2seq</code> trong NLP</a></li>
      <li><a href="#về-chuyện-tài-nguyên-training">Về chuyện tài nguyên training</a></li>
      <li><a href="#về-các-task-vision-language">Về các task vision-language</a></li>
    </ul>
  </li>
  <li><a href="#tài-liệu-tham-khảo">Tài liệu tham khảo</a></li>
</ul>
                    </div>
                
                <!-- End Toc -->
                <p><code class="language-plaintext highlighter-rouge">Seq2Seq</code> <a class="citation" href="#kalchbrenner2013recurrent">[1, 2, 3]</a> là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, <a class="citation" href="#sutskever2014sequence">[2]</a>), Question-Answering (QA, <a class="citation" href="#rajpurkar2016squad">[4]</a>), Visual Question Answering (VQA, <a class="citation" href="#antol2015vqa">[5, 6]</a>), Text Summarization (TS, <a class="citation" href="#rush2015neural">[7, 8]</a>) và Video-To-Text (VTT, <a class="citation" href="#venugopalan2015sequence">[9]</a>).
Bài toán Image Captioning thì cũng có thể ứng dụng <code class="language-plaintext highlighter-rouge">seq2seq</code> nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào <code class="language-plaintext highlighter-rouge">seq2seq</code> như trong bài <em>Semantic Attention (SA) <a class="citation" href="#you2016image">[10]</a></em> hay <em>Densecap <a class="citation" href="#johnson2016densecap">[11]</a></em>.
Nên nhìn chung là <code class="language-plaintext highlighter-rouge">seq2seq</code> là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng <a class="citation" href="#luong2015multi">[12]</a>.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<!--more-->

<h1 id="giới-thiệu-model-seq2seq">Giới thiệu model Seq2Seq</h1>

<p>Nỗi lòng người làm thày mà hướng dẫn sinh viên thì tùy level mà kỳ vọng thì nó cũng khác nhau, và với những level cao như postdoc là luôn có sự mong muốn nhất định.
Đó là phải vượt qua <code class="language-plaintext highlighter-rouge">thử thách (challenges)</code> của thày.
Thì cái challenge đây có hai nghĩa là cuộc thi và thử thách, thì nói thực là cuộc thi không cần đâu, chỉ cần vượt qua thử thách tối thiểu của thày trong cái kỹ năng làm nghiên cứu thôi là <code class="language-plaintext highlighter-rouge">ok em</code> rồi.</p>

<p>Trong thực tế kinh nghiệm thì tôi thấy mấy cái thử thách cũng không phức tạp lắm đâu, đặc biệt là trong lĩnh vực NLP+Vision này thì tôi thấy cũng chỉ có vài bài như Image Captioning, Video-to-Text hay VQA.
Code thì nhan nhản trong cộng đồng nghiên cứu (đặc thù của ngạch nghiên cứu là kế thừa, vì người ta công bố mà mình không dùng thì phí), rồi tài liệu thì mảng NLP với Vision các ông cũng publish trên ArXiV với mấy hội nghị open, chứ nào có giấu giếm gì nhau?
Thế mà không hiểu vì sao vẫn không vượt qua được cho thày?
Xem lại rốt cuộc là tôi nghĩ thì có lẽ nên thêm câu hỏi sau vào bộ dữ liệu QA:</p>

<blockquote>
  <p>Q: Không làm muốn có ăn thì ăn gì bây giờ?
A: ??? (câu hỏi tự luận nha)
Context: Hãy xem video của Prof. Huan Rose</p>
</blockquote>

<p>Thì nhìn chung là <code class="language-plaintext highlighter-rouge">seq2seq</code> là một model thử thách như vậy.
Với những bài đặc biệt chỉ của NLP như NMT, Text Summarization hay QA thì <code class="language-plaintext highlighter-rouge">seq2seq</code> đã mở ra hẳn cả một mảng riêng mà về sau còn có thêm mấy cái pretraining encoder/decoder như BERT, AlBERT, RoBERT, BART, … mà chủ yếu là representation learning.</p>

<blockquote>
  <p>Mô hình <code class="language-plaintext highlighter-rouge">seq2seq</code> đơn giản chỉ gồm một chuỗi input $\mathbb{x}_1, \mathbb{x}_2, \ldots, \mathbb{x}_m$. 
Chuỗi này đương nhiên là biểu diễn vector (embedding) sau khi đã preprocessing qua tokenizer và thay thế từ không có trong vocabulảy bởi <code class="language-plaintext highlighter-rouge">UNK</code>.
Sau đó là <strong>encoder</strong> với các trạng thái $\mathbb{h}_1, \mathbb{h}_2, \ldots, \mathbb{h}_m$ cũng như biểu diễn đầu ra của encoder là $\mathbb{z}_1, \mathbb{z}_2, \ldots, \mathbb{z}_m$.
Encoder chính là một mạng trí tuệ nhân tạo dạng Long-Short Term Memory (LSTM) mà chúng ta sẽ nói sau.
Bây giờ thì chúng ta cần aggregate chuỗi biểu diễn đầu ra của encoder thành biến vector \(\mathbb{c}_t=\sum_{i=1}^m\alpha_i^t\mathbb{z}_i\).
Ở mỗi bước \(t\) thì <strong>decoder</strong> LSTM sẽ nhận input là output của bước \(t-1\) là \(\mathbb{g}_{t-1}\) và biến context \(\mathbb{c}_t\) để đưa ra output \(\mathbb{g}_t\).</p>

\[\mathbb{g}_t=\mbox{LSTM}\left(\mathbb{g}_{t-1},\mathbb{c}_t\right)\]

  <p>ứng với mỗi \(\mathbb{g}_t\) sẽ được giải mã thành một ký tự trong ngôn ngữ đầu ra.
Quá trình này kết thúc khi ký tự <code class="language-plaintext highlighter-rouge">EOS</code> được giải mã ra.
Chuỗi đầu vào \(\mathbb{x}_i\) và chuỗi đầu ra của decoder \(\mathbb{g}_j\) có thể có độ dài khác nhau.</p>
</blockquote>

<p>Trong bài toán NMT, thì chuỗi đầu vào có thể là tiếng Anh và chuỗi đầu ra là tiếng Nhật.
Nhưng ngược lại thì sẽ cần tokenizer bằng tiếng Nhật.
Trong bài toán Text Summarization (TS), thì cả đầu ra và đầu vào đều sẽ cùng ngôn ngữ, nhưng đầu ra sẽ là một chuỗi ngắn gọn xúc tích hơn.
Cái gọi là <code class="language-plaintext highlighter-rouge">ngắn gọn, xúc tích hơn</code> sẽ được định nghĩa và học thông qua dữ liệu.
Thì NMT có bộ WMT2014 <a class="citation" href="#bojar2014findings">[14]</a>, WMT2017 <a class="citation" href="#bojar2017findings">[15]</a>, còn TS thì có bộ DUC <a class="citation" href="#paul2004introduc">[16]</a>.
Còn bài toán QA thì chuỗi đầu vào là một câu hỏi còn đầu ra là 1 câu trả lời.
Cái hay của QA là đôi khi có thêm chuỗi context (hay gọi là gợi ý), ví dụ như hỏi <code class="language-plaintext highlighter-rouge">Bạn sống ở đâu?</code> thì có thêm context là <code class="language-plaintext highlighter-rouge">Tôi sống ở Nhật</code> thì máy sẽ trả lời luôn là <code class="language-plaintext highlighter-rouge">Nhật</code>.
QA thì có bộ SQuAD v1 <a class="citation" href="#rajpurkar2016squad">[4]</a> và v2 <a class="citation" href="#rajpurkar2018know">[17]</a> với hơn trăm ngàn bộ câu hỏi (nghe như luyện thì TOEIC).
Metrics đánh giá thì có cái BLUE-4 score, CIDEr, ROGUE <a class="citation" href="#lin2004rouge">[18]</a> là có thể dùng để đánh giá chuỗi đầu ra có phù hợp không.
Chúng ta sẽ đi sâu thêm vào từng chi tiết sau.</p>

<p>Các task trong NLP thì là như vậy, dữ liệu, code và metrics hầu như có sẵn.
Thì cũng là kết nối với Vision là khoảng CVPR tầm 2015-2016 gì đó có mấy cái Workshops nói về làm sao để có thể tích hợp nhiều modal (multi-modal) để đưa ra những giải pháp tốt hơn.
Mọc ra trước mắt lúc đó thì có 3 tasks nằm trong định hướng: Image Captioning, Video-to-Text và VQA.
Nói chung nghe lúc đó có vẻ mới, nhưng chỉ có task là mới thôi chứ còn những cái để thực thi những task ấy các sếp promote các task ấy lên họ đã chuẩn bị hết rồi.
Technique thì có sẵn <code class="language-plaintext highlighter-rouge">seq2seq</code>, CNN, RNN, LSTM, Faster R-CNN để extract attributes dưới dạng object detections, …
Metrics thì vì output vẫn là chuỗi text nên lại xài lại BLUE-4, CIDEr rồi ROGUE thôi.
Nên coi như nền tảng rất sẵn rồi, chỉ nhảy vào làm thí nghiệm và viết paper rồi … ăn!</p>

<p><em>Vậy tại sao vẫn cứ không đến nơi đến chốn được?</em></p>

<ol>
  <li>Tôi nghĩ vấn đề đầu tiên lúc làm Image Captioning là thiếu <code class="language-plaintext highlighter-rouge">alignment</code>.</li>
  <li>Lúc làm cái Video-to-Text thì cái visual semantic embedding (VSE) là không có. Mà nói thẳng ra thì cái ấy chính mình <code class="language-plaintext highlighter-rouge">chủ động</code> nghĩ ra mà làm chứ?</li>
  <li>CÒn cái VQA thì lúc ấy nói thật là hai cái captioning với VTT nó đã <code class="language-plaintext highlighter-rouge">bết bát</code> sẵn rồi thì sẽ rất khó vì bản thân VQA tuy chỉ thay cái context là text bởi hình ảnh, nhưng chất lượng detector, rồi alignment mà hai bước trên chưa làm tốt thì sang đến VQA coi như … vỡ trận.</li>
</ol>

<p>Tuy nhiên, nếu ở vào vai trò anh Postdoc mà làm cái mảng này tôi vẫn sẽ đề xuất luồng làm việc <code class="language-plaintext highlighter-rouge">Image Captioning --&gt; Video-to-Text --&gt; VQA</code>.
Bởi luồng làm việc này nó theo chiều hướng tích lùy dần know-how để làm việc ngày càng tốt hơn.
Thứ hai, là vì nó có một vài điểm trigger giữa chừng nên nếu làm postdoc các bạn có thể submit bài báo tại các thời điểm ấy như là làm Image Captioning xong thì 1 bài, …
<strong>Nhưng rốt cuộc cái quan trọng nhất vẫn là phải có làm có ăn.</strong>
Còn không muốn làm thì hỏi giáo sư Huấn để biết phải làm gì.</p>

<h2 id="recurrent-neural-nets-long-short-term-memory-và-gated-recurrent-units">Recurrent Neural Nets, Long-Short Term Memory và Gated Recurrent Units</h2>

<p>Thôi nói chung là cái trường hợp postdoc ở trên là một ví dụ điển hình thôi, mà trường hợp ấy tôi nghĩ đã được giáo sư Huấn chỉ bảo tận tình rồi nên không cần lo lắng nữa.
Chúng ta quay lại với chủ để chính của hôm nay là giới thiệu về <code class="language-plaintext highlighter-rouge">seq2seq</code>.
Để implement được <code class="language-plaintext highlighter-rouge">seq2seq</code> thì chúng ta cần 1 mô hình nhận chuỗi và output đầu ra cũng là một chuỗi khác, và quan trọng hơn là có thể huấn luyện bằng thuật toán Gradient Descent, cụ thể hơn là <em>Stochastic Gradient Descent (SGD, <a class="citation" href="#bottou2007tradeoffs">[19]</a>)</em>.
Thì cái học củ SGD là mình chỉ random sample một phần của dữ liệu học để tính được gradient và update weight của model.
Chúng ta sẽ có <em>learning rate</em> mà nhỏ thì chậm, còn cao quá thì mô hình sẽ khó hội tụ.</p>

<p>Lời giải thì đơn giản nhất là có mô hình <em>Recurrent Neural Nets (RNN, <a class="citation" href="#rumelhart1986learning">[20]</a>, <a class="citation" href="#goodfellow2016deep">[21]</a>)</em> mà mô hình và công thức forward pass như bên dưới:</p>

<p><img src="/assets/images/rnn.svg" alt="rnn" /></p>

<p>Công thức rất rõ ràng nên có thể sử dụng <code class="language-plaintext highlighter-rouge">Numpy</code> để implement forward pass khá đơn giản với vài dòng code chơi thôi.
Cái khó khăn là khi đã tính xong kết quả các biến \(u_t, o_t, h_t, x_t\) và hàm loss \(\mathcal{L}\) thì <strong>làm sao update được các weight \(\mathbf{V}, \mathbf{W}, \mathbf{b}, \mathbf{c}\)</strong>?</p>

<p>Thì có hệ thống các công thức phía dưới:</p>

<p><img src="/assets/images/bptt.svg" alt="bptt" /></p>

<p>Tuy hệ thống công thức này phức tạp hơn, nhưng vẫn có thể implement bằng <code class="language-plaintext highlighter-rouge">Numpy</code> và thực hiện train với SGD (tầm khoảng vài chục dòng code nữa).
Nhìn lại thì công thức và cả phương thức implement RNN là cũng rất <strong>rõ ràng rồi</strong>, vì vậy, nếu không thể implement được thì … thày cũng chịu không thể giải thích tại sao được?</p>

<p>Nhược điểm của RNN là vấn đề ghi nhớ <em>long-term dependencies</em>: ví dụ chúng ta có 1 gradient \(\mathbf{g}\) và giả sử là RNN không có non-linear activations nào thì chúng ta có thể giả sử thêm là cứ sau mỗi time step thì Jacobian matrix là \(\mathbf{J}\), vậy sau \(n\) bước thì gradient của chúng ta sẽ biến thành \(\mathbf{J}^n\mathbf{g}\) và nếu nó có một giá trị riêng \(\lambda\) mà giá trị khác \(\pm 1\), thì sẽ dẫn đến vấn đề là gradient triệt tiêu hoặc tiến ra vô cùng sau hữu hạn bước tính.
Để khắc phục nhược điểm này thì một giải pháp được đưa ra là đưa <em>self-loop</em> vào cùng các <em>non-linear gates</em> như forget gate, output/input gates để control giá trị.
Một kiến trúc đã làm được việc đó là <em>Long-Short Term Memory (LSTM, <a class="citation" href="#hochreiter1997long">[22, 23]</a>)</em>.
Mỗi cổng đều chứa <em>sigmoid</em> activation để đưa giá trị về khoảng \((0,1)\) dẫn đến các giá trị output ở các cổng có thể bị shut-off.</p>

<p><img src="/assets/images/lstm.svg" alt="lstm" /></p>

<p>Việc thêm gates và non-linear activation (<code class="language-plaintext highlighter-rouge">sigmoid</code>) để control giá trị là hợp lý với LSTM.
Tuy nhiên, nếu nhìn vào cái đám công thức thì trong LSTM khi update và forget của \(\mathbf{h}_{t+1}\) là đồng thời thực hiện song song việc forget và quyết định có update hay không.
<strong>Gated Recurrent Unit (GRU, <a class="citation" href="#cho2014properties">[24]</a>)</strong> thực hiện chia ra, tức là thay vì 1 cổng là làm luôn cả hai chức năng, thì tạo 2 cổng <code class="language-plaintext highlighter-rouge">update</code> và <code class="language-plaintext highlighter-rouge">reset</code> cho từng mục đích ấy.</p>

<blockquote>
  <p>Nhìn chung, công thức toán của các kiến trúc RNN/LSTM/GRU đều không phức tạp.
Nếu có thời gian, các bạn có thể tự thực hiện implement các kiến t
rúc trên bằng <code class="language-plaintext highlighter-rouge">numpy</code>.
Forward pass thì chắc là đơn giản hơn, tuy nhiên backward pass với SGD thì sẽ đòi hỏi một chút công sức (vài chục dòng code nữa).</p>
</blockquote>

<p>Nói một chút về hàm loss \(\mathcal{L}\) khi train NMT chẳng hạn, vì chúng ta biết chuỗi <strong>đúng</strong> \(\mathbf{y}=(y_1,y_2,\ldots,y_n)\) nên ở mỗi step \(0\leq t\leq n-1\), ta sẽ xem xét vector output \(\mathbf{u}_t\) (là softmax) và lấy xác suất của vị trí thứ \(y_t\), và tất nhiên lấy log nghịch đảo như mọi khi:</p>

\[\mathcal{L}=\sum_t-\log u_{y_t}\]

<p>Khi testing, chúng ta sẽ sử dụng <strong>beam search</strong> để tìm ra các chuỗi phù hợp (sẽ giải thích thêm ở phần sau).</p>

<h2 id="kiến-trúc-seq2seq-s2s">Kiến trúc Seq2Seq (S2S)</h2>

<p><img src="/assets/images/san01.png" alt="seq2seq" /></p>

<p>Training <code class="language-plaintext highlighter-rouge">seq2seq</code> đòi hỏi cần có GPU phù hợp và tận dụng xử lý song song.
1 đặc điểm không thể tránh khỏi của bộ dữ liệu là độ dài các chuỗi không đồng bộ, nên có thể tạo ra giải pháp là <strong>fill thêm ký tự trống vào để cho tất cả các chuỗi cùng độ dài</strong>.
Việc này sẽ đòi hỏi thêm khá nhiều memory, vì vậy, một giải pháp khá phù hợp là chia ra thành các <code class="language-plaintext highlighter-rouge">maxi-batches</code> rồi trong từng <code class="language-plaintext highlighter-rouge">maxi-batch</code> thì lại chia tiếp thành các <code class="language-plaintext highlighter-rouge">mini-batch</code>.
Trong mỗi <code class="language-plaintext highlighter-rouge">mini-batch</code> như hình dưới thì mới thực hiện fill ký tự trống.</p>

<p><img src="/assets/images/rnn-mini-batches.png" alt="rnn-mini-batches" /></p>

<p>Training <code class="language-plaintext highlighter-rouge">seq2seq</code> thường mất 5-15 epochs (1 lượt qua toàn bộ corpus).
Bạn cũng nên thiết lập một tiêu chí dừng lại khi validation error rate không cải tiến thêm.
Training lâu thêm không những không tạo thêm cải tiến mà có thể dẫn tới overfitting.</p>

<p><strong>Testing <code class="language-plaintext highlighter-rouge">seq2seq</code></strong>. Giả sử chúng ta có một corpus 50,000 từ.
Vậy trong vector \(\mathbf{u}_t\in\mathbb{R}^{50,000}\), ta sẽ chọn phần tử có xác suất cao nhất (dựa trên embedding logits) để output ra ở bước thứ \(t\).
Giải pháp này tức là <strong>chỉ chọn cái tốt nhất ở mỗi bước</strong>.
Vấn đề ở đây là nếu chỉ chọn cái tốt nhất độc lập ở từng bước thì khi đến một thời điểm \(t'\), câu dịch trở lên không đúng thì không thể quay lại sửa từ đầu được.
Do đó, cách tốt hơn là dùng <strong>Beam Search</strong>:</p>

<ul>
  <li>Ở bước \(t=0\) chúng ta chọn một beam gồm \(n\) top words \(p(w_0^1), p(w_0^2), \ldots, p(w_0^n)\).</li>
  <li>Ở thời điểm tiếp theo \(t=1\), ta lại chọn tiếp \(n\) top words \(p(w_1^1), p(w_1^2), \ldots, p(w_1^n)\) và làm tiếp:
    <ul>
      <li>nhân xác xuất của cụm 2 từ liên tiếp \(p(w_0^{i_0})p(w_1^{i_1}), 1\leq i_0, i_1\leq n\).</li>
      <li>Chọn top \(n\) cặp từ \((i_0,i_1)\) giữ lại.</li>
    </ul>
  </li>
  <li>Ở thời điểm \(t=2\), ta lại chọn tiếp top \(n\) words \(p(w_2^1), p(w_2^2), \ldots, p(w_2^n)\), và lại nhân xác suất để tìm ra top \(n\) triplets \((i_0, i_1, i_2)\) để giữ lại trong beam.</li>
  <li>Cứ tiếp tục như vậy đến khi gặp <code class="language-plaintext highlighter-rouge">EOS</code> thì bỏ câu dịch đó ra khỏi beam và giảm size của beam đi 1.</li>
  <li>Khi kích thước của beam giảm xuống còn 0 thì dừng beam search.</li>
</ul>

<p>Một đoạn code demo của <code class="language-plaintext highlighter-rouge">beam search</code>:</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/6397ad4648f7c891ccd41513bb8206e5.js"> </script>

<p>Ngoài ra, nếu có thời gian các bạn có thể tìm hiểu thêm các thủ thuật để tăng hiệu suất khi inference như fusion (ensembling), reranking, hoặc khi train như tăng kích thước vocabularies, training data, back translation, round-trip training, guided alignment training, .v.v… (<a class="citation" href="#koehn2009statistical">[25]</a>).</p>

<blockquote>
  <p>Tất nhiên, cũng cần chú ý là implement những cái này ở local cũng chỉ để học hỏi thôi nhé.
<strong>Chứ còn triển khai vận hành thực sự thì nó có những solutions có sẵn chạy ầm ầm trên AWS/Azure/GCP rồi.</strong>
Mà có khi nó còn thành dịch vụ đem bán khắp nơi rồi ấy chứ (có cả tiếng Việt luôn nhé).</p>
</blockquote>

<h2 id="transformers-và-bert">Transformers và BERT</h2>

<h3 id="transformer-13">Transformer <a class="citation" href="#vaswani2017attention">[13]</a></h3>

<p><img src="/assets/images/transformer.png" style="float: right; margin: 10px; width: 25%;" />
Deep Networks nhưng không có RNNs hay CNNs gì cả!
Transformer cũng có thể coi là một dạng <code class="language-plaintext highlighter-rouge">seq2seq</code> nhưng không chỉ gồm các tầng FC, embedding, …
Điểm khác biệt là context (hay là attention) đã được chuyển thành tầng đề xuất <code class="language-plaintext highlighter-rouge">self-attention</code>.
Nhìn chung, Transformer hay cả các kiến trúc BERT về sau cũng kế thừa tương đối nhiều tính chất của <code class="language-plaintext highlighter-rouge">seq2seq</code>.
Bản thân Transformer cũng là <code class="language-plaintext highlighter-rouge">seq2seq</code> nhưng bỏ đi RNN/CNN.
<img src="/assets/images/mha.png" style="float: left; margin: 10px; width: 15%;" />
Trong kiến trúc thì ngoài residual connection chỉ có điểm đáng chú ý là <code class="language-plaintext highlighter-rouge">multi-head attention</code>.
Như hình vẽ bên trái, có 3 vector mới được đưa ra là \(K, V, Q\), tương ứng cho <code class="language-plaintext highlighter-rouge">keys, values, queries</code>.
Thì công thức attention (không có mask) sẽ như sau:</p>

\[\mbox{Attention}\left(Q,K,V\right)=\mbox{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V\]

<p>trong đó, \(d_k\) là số chiều của vector query \(Q\) và key \(K\).
Trong trường hợp dùng mask (ở decoder chả hạn) thì chủ yếu là để kết hợp với position embedding để hạn chế ảnh hưởng của các token trước thời điểm $t$ chả hạn.</p>

<p>Nói chung lý thuyết của Transformer cũng không có gì quá phức tạp.
Bản thân Transformer cũng đã được implement chi tiết trong thư viện <code class="language-plaintext highlighter-rouge">tensor2tensor</code> <a class="citation" href="#vaswani2018tensor2tensor">[26]</a> nên như tôi đã nói, cái mảng này thực ra tài liệu tài nguyên thì vô cùng dồi dào.
Ngoài ra một thư viện nữa là HuggingFace <a class="citation" href="#wolf2020transformers">[27]</a> với model zoo dồi dào cũng rất đáng xem.
<strong>Thế là lại nói lại câu chuyện postdoc ở trên: <em>không hiểu tại sao lại không làm được?</em></strong></p>

<blockquote>
  <p>Thì tôi nghĩ là ngoài 3 yếu tố đã nói ở mục đầu, thì quá tập trung vào competition! 
Từ <code class="language-plaintext highlighter-rouge">challenge</code> mà người thày nói cũng có nghĩa là cuộc thi (competition) nhưng cũng có nghĩa là thử thách.
Mà vấn đề là cái nghĩa sau nó sẽ quan trọng hơn.
Bởi mảng nghiên cứu là giao thoa giữa Vision và NLP, nên là nếu chỉ tập trung vào 1 competition nhất định của vision mà không cập nhật bên NLP thì có vẻ là tầng nghĩa sau (thử thách) là sẽ bị lose track.
Đấy thế nên là cũng nhắc lại lời người thày: “<strong>Thực ra nhiệm vụ của người làm thày là giảm bớt số lượng những con bò trên thế giới xuống</strong>”.
Tôi nghĩ đúng là để biến một con bò thành người cũng khá vất vả, mà thất bại mãi mới thành công một phát cũng là chuyện thường!</p>
</blockquote>

<blockquote>
  <p>Chốt là cuộc thi, competition là không quan trọng đâu.
Cái quan trọng với postdoc là rèn luyện được một cái tác phong nghiên cứu tốt để có thể hướng tới lâu dài.
<strong>Chứ mất công giành được vị trí trong 1 competition trước mắt, nhưng lại mất vị trí tenure cả đời thì luận về TRÍ là thua rồi!</strong>
Đã xác định gắn bó lâu dài thì không cần thi cử gì cho mất thời gian ra, lo mà tìm ra cống hiến để đời và gắn bó lâu dài thì hơn.</p>
</blockquote>

<p>Mà nhìn lại tôi nghĩ nguyên nhân sâu xa dẫn tới sự vụ hậu quả postdoc kể trên là <strong>thiếu kỹ năng quản lý dự án (Project Management)</strong>.
Triệu chứng bệnh rõ ràng nhất ở đây là</p>
<ul>
  <li>Việc không quản lý được các tri thức về thử thách, dẫn đến bị đánh lạc hướng, sa đà vào các competition. Đây chính là triệu chứng bệnh <strong>thiếu kỹ năng quản lý scope dự án.</strong></li>
  <li>Việc không control được những lời khuyên từ bên NLP chứng tỏ có dấu hiệu <strong>yếu về quản lý stakeholder</strong>.</li>
  <li>Việc ưu tiên competition trước việc nhận ra thách thức thực sự của mình chính là <strong>yếu về quản lý action plan, time và schedule</strong>.</li>
</ul>

<p>Nhìn chung việc thiếu kỹ năng quản lý dự án sẽ không bao giờ đưa người postdoc gia nhập nhóm (dưới) 5% thành công được.
<strong>Mà ví dụ khoảng làm postdoc được 2-3 năm mà người thày nhìn vào thấy đủ thứ bệnh, chỗ nào cũng yếu thế này thì rất là khó để không cho … bật bãi!</strong>
Ở Mỹ chả hạn, làm nghiên cứu muốn tồn tại lâu dài phải có kỹ năng quản lý dự án nghiên cứu. Thế mà chạy postdoc 2 năm mà không ngộ ra chân lý, không thể hiện tiềm năng làm quản lý lãnh đạo, thì không bật bãi chắc chỉ có ở thiên đường thôi!</p>

<h3 id="bert">BERT</h3>
<p><strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers (<strong>BERT</strong>, <a class="citation" href="#devlin2019bert">[28]</a>) lại là 1 kiến trúc mới gần đây (từ 2018?).
Pretraining kiến trúc trên 1 task unsupervised (kiểu tự học), rồi dùng lại kiến trúc đó trong 1 task supervised khác là tư tưởng chính của BERT.
Các phiên bản cải tiến của BERT có thể kể đến AlBERT <a class="citation" href="#lan2019albert">[29]</a>, RoBERTa <a class="citation" href="#liu2019roberta">[30]</a>, BART <a class="citation" href="#lewis2020bart">[31]</a>, XlNet <a class="citation" href="#yang2019xlnet">[32]</a>, XLM <a class="citation" href="#conneau2019cross">[33]</a> hay gần đây là ELECTRA <a class="citation" href="#clark2020electra">[34]</a> đều kế thừa tinh thần này.
Trong quá khứ thì việc sử dụng pretraining để tạo ra một xuất phát điểm tốt hơn cho model là ý tưởng đã được khai thác <a class="citation" href="#hinton2006fast">[35]</a>.
Ví dụ như Hinton thì đã pretrain deep belief nets và gọi là <em>greedy layer-wise unsupervised pretraining</em>.
Thủ thuật này là cần thiết khi train các model lớn:</p>

<blockquote>
  <p>to train the first layer in isolation, then extract all features from the first layer only once, then train the second layer in isolation given those features, and so on. <a class="citation" href="#goodfellow2016deep">[21]</a>.</p>
</blockquote>

<p>Ý tưởng của kiểu pretraining này là ta có một model rất lớn \(A\), việc train \(A\) từ đầu (random weights) sẽ rất khó.
Do vậy, chúng ta sẽ tìm một task unsupervised \(u\) (vì vậy không cần labels), để train \(A\) trước.
Sau đó, khi vào train cho task chính \(T\) (có labels) thì weights của \(A\) đã được khởi tạo bởi việc học \(u\) sẽ khiến training \(A\) cho \(T\) trở nên nhanh chóng hơn.
Trên thực tế có khá nhiều task bên computer vision đã sử dụng ý tưởng này và đưa ra khá nhiều kết quả thuyết phục.</p>

<p>Với bên NLP, thì BERT lựa chọn task pretraining (gọi là <em>pretext</em>) là masked language modeling (MLM).
Ví dụ như trong câu</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>Original: I have a pen &lt;EOS&gt;
Masked: I &lt;MASK&gt; a pen &lt;EOS&gt;
</pre></td></tr></tbody></table></code></pre></div></div>
<p>thì từ câu <code class="language-plaintext highlighter-rouge">Masked</code>, nhiệm vụ của pretext là phải <em>phục hồi</em> lại các từ bị <code class="language-plaintext highlighter-rouge">&lt;MASK&gt;</code>.
Nguyên tắc pretext này nhìn chung giống với <em>denoising auto-encoders</em> <a class="citation" href="#goodfellow2016deep">[21]</a>.
Vì việc tạo bộ dữ liệu masking là dễ dàng và có thể tự làm được (viết script để ngẫu nhiên thay 1 token bởi <code class="language-plaintext highlighter-rouge">&lt;MASK&gt;</code>) nên task này có thể xếp vào hạng mục <strong>unsupervised learning</strong> hoặc <strong>self-supervised learning (SSL)</strong>.
Lợi thế là người train có thể tạo ra bộ dữ liệu lớn tự động để máy học pretext mà không cần gắn nhãn!
Một pretext khác là <em>đoán câu tiếp theo (Next Sentence Prediction, NSP)</em>.
Ví dụ:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>I told the principal that I would like to revolutionize this university if he give me something to do.
Next Sentence: He sent me to the Psychology department.
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Trong kết quả của BERT thì có vẻ pretext NSP giúp cải thiện độ chính xác của task QA.
Tất nhiên, ngoài dùng pretext thì còn những kiểu pretraining khác như sử dụng feature đã pretrain (ELMo, <a class="citation" href="#peters2018deep">[36]</a>) hoặc fine-tune toàn bộ pretrain parameters (OpenAI GPT, <a class="citation" href="#radford2018improving">[37]</a>).
Gần đây có model BART <a class="citation" href="#lewis2020bart">[31]</a> tích hợp cả BERT và GPT để tạo ra một cơ chế pretext có thể mô phỏng hàm noise bất kỳ.</p>

<p>Hầu hết những phương pháp kể trên đều có thể tìm thấy tại thư viện model HuggingFace <a class="citation" href="#wolf2020transformers">[27]</a>.</p>

<h1 id="ứng-dụng">Ứng dụng</h1>

<h2 id="cách-tiếp-cận">Cách tiếp cận</h2>
<h3 id="các-nguồn-tài-nguyên">Các nguồn tài nguyên</h3>
<p>Như đã nói ở trên, mảng này thì code kiếc, tài liệu, tài nguyên thì vô cùng <strong>sẵn</strong> có và dồi dào.
Thế nên thôi mình cứ <strong>ăn “sẵn”</strong> đi cho nó nhanh chứ nấu làm gì mất thời gian.</p>

<blockquote>
  <p>Nó cũng kiểu như đồ ăn Tết ấy mà: 
Hì hục nấu mất cả buổi mà ăn thì chắc được mấy miếng là chán.</p>
</blockquote>

<p>Thế nên tôi mới bảo rồi, những cái này mà mở khóa học truyền bá tri thức phổ cập thì <strong>liệu có khách không? ai học cho mà dạy?</strong></p>
<blockquote>
  <p>Tôi nghĩ chả ai học đâu, giờ người ta ăn sẵn hết.
Có dạy “nấu” thì cũng phải cái gì mà nó kiểu sẵn sẵn mà nấu nhanh ăn luôn.</p>
</blockquote>

<p>Thì tại sao nói mảng này đồ ăn sẵn nhiều thì dưới đây là một số nguồn mà các bạn có thể tận dụng:</p>

<ul>
  <li><strong>Code</strong> thì HuggingFace, tensorflow, Github</li>
  <li><strong>Model Zoo</strong> thì HuggingFace thôi.
    <ul>
      <li><a href="https://huggingface.co/">https://huggingface.co/</a></li>
      <li>Cứ tải về mà hì hục “nấu”.</li>
      <li>Nấu xong chắc cũng chỉ chạm đũa vài miếng là ngấy thôi nhưng nếu thích nấu thì nấu thôi.</li>
    </ul>
  </li>
  <li><strong>Tài liệu</strong> tutorials thì cứ search YouTube là ra hết mà.
    <ul>
      <li><a href="https://www.youtube.com/c/HuggingFace">Kênh hướng dẫn của HuggingFace</a></li>
      <li><a href="https://www.youtube.com/watch?v=G5lmya6eKtc">The Future of Natural Language Processing</a></li>
    </ul>
  </li>
</ul>

<iframe width="100%" height="315" src="https://www.youtube.com/embed/G5lmya6eKtc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<ul>
  <li>Nguồn <strong>tài nguyên tính toán</strong>:
    <ul>
      <li>Đầu tiên cũng chả cần mua GPU/TPU riêng đâu.</li>
      <li>Để làm demo bạn cứ xài tạm Colab ấy.
        <ul>
          <li><a href="https://colab.research.google.com/">https://colab.research.google.com/</a></li>
          <li><a href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL">Một playlist hướng dẫn dùng Colab với Tensorflow</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="đôi-lời-về-google-colab">Đôi lời về Google Colab</h3>

<blockquote>
  <p><a href="https://research.google.com/colaboratory/faq.html">FAQ</a> <strong>What is Colabotory?</strong></p>
</blockquote>

<blockquote>
  <p>Colaboratory, or “Colab” for short, is a product from Google Research. Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning, data analysis and education. More technically, Colab is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources including GPUs.</p>
</blockquote>

<p>Google Colab nhìn chung là 1 dịch vụ Notebook cung cấp sẵn môi trường để chạy các tác vụ liên quan tới machine learning.
Với Colab, bạn có thể định nghĩa form các biến chương trình như hình bên dưới, rồi tạo lập mạng trí tuệ nhân tạo, thiết lập môi trường GPU/TPU để training hoặc inference.</p>

<p><img src="/assets/images/nmt-colab-01.png" alt="colab form" /></p>

<p>Sau khi đã làm quen với môi trường Google Colab thì bạn đã all-set to go!</p>

<h4 id="chú-ý-về-train-song-song">Chú ý về train song song</h4>
<p>Một chú ý nhỏ mang tính hô trợ trong quản lý time thôi là nếu bạn chọn Colab Free plan thì GPU/TPU tốc độ khá thấp.
Ngoài ra, bộ dữ liệu hạng vừa như WMT 2014 thì cũng có 4,468,840 cặp câu dịch (bộ training set).
Mà Colab Free thì tốc độ training sẽ rơi vào tầm 18-20 FPS với model AlBERT, như vậy để train hết 1 epoch với WMT sẽ mất tầm 62-70 (h), tức là khoảng 3 ngày/epoch (liên tục).
Mà để train model NMT thì sẽ mất 5-15 epochs nên sẽ phải mất tầm <strong>15-45 ngày train liên tục</strong>.
Tuy nhiên, Colab Free có giới hạn là 1 session chỉ được 12h liên tục, nên bạn sẽ phải break ra tầm 30-90 sessions.
Ngoài ra đóng browser là mất luôn đấy chứ nó lại không chạy ở background đâu.</p>

<p>Bài viết này chỉ sử dụng có sẵn với viết script để demo inference hoặc show training vài step nó như thế nào thôi, nên chúng tôi cũng chỉ cần Colab Free là đủ.
Tuy nhiên, nếu mà các bạn định làm nghiêm túc kiểu hì hục nấu cả buổi thì tôi nghĩ là nếu dùng GPU thì dưới 8 GPUs là hì hục lâu phết đấy!
Có mấy giải pháp:</p>

<ul>
  <li><strong>Nâng cấp nên Colab Pro (10USD/tháng) và Pro Plus (50USD/tháng).</strong> Tuy nhiên, dù lên Pro Plus thì 1 session cũng chỉ được 24h liên tục. Được cái là cho phép chạy background nên sẽ dễ thở hơn. Có điều là 24h thì lại phải save ra Google Drive, rồi khởi động lại training <strong>bằng tay</strong>. GPU/TPU sẽ nhanh hơn tầm vài lần nên có thể sẽ chỉ mất vài tuần để train thôi. <strong>Hì hục</strong> nó là thế đấy!</li>
  <li><strong>Tự chế ra 1 hệ thống multi-GPU</strong>: kiếm kinh phí mua tầm 24 cái GPUs P100 thì có khi 1 epoch mất 1h thôi, thì train trong ngày sẽ xong. Giá cả thì <strong>Tesla P100 SXM2 16GB</strong> tầm 10,000 USD/cái, 24 cái thì tầm 240,000 USD thôi! Muốn làm big data mà chỉ code thôi là không ăn thua đâu! Bởi vận hành triển khai mỗi tháng hóa đơn đã vài chục ngàn đô, tiền upfront mua GPU cũng đã 240,000 USD thì code hầu như là phần ít giá trị nhất! Code nói chung … rẻ mạt!</li>
  <li><strong>Thuê EC2</strong>: thì có vài lựa chọn là <strong>on-demand</strong> và <strong>reserved</strong>.
    <ul>
      <li>reserved thì 1 tháng 30 ngày 720 h là mình phải trả hết 720 h. Thì rẻ nhất là <code class="language-plaintext highlighter-rouge">g5.xlarge</code> cũng mỗi giờ 0.63 đô. Như vậy là mỗi tháng \(0.63 \times 720 = 453.6\) USD/tháng. Tuy nhiên, <code class="language-plaintext highlighter-rouge">g5.xlarge</code> chỉ có 1 GPU và tính năng thì xấp xỉ Colab Free, nên giả sử train 15 epochs mất 45 ngày liên tục, thì sẽ tốn là \(0.63 \times 45 \times 24 = 680.4\) USD cho lượt train này.
        <ul>
          <li>Nếu thuê GPU tốt hẳn đi là <code class="language-plaintext highlighter-rouge">g5.48xlarge</code> coi như 8 GPUs thì nhanh hơn tầm 8 lần thì giá reserved sẽ là \(10.26 \times 45 \times 24 /8 = 1385.1\) USD cho lượt train. Tuy nhiên, vì reserved nên mình thuê theo tháng thì ngoài lượt train này, hàng tháng mình tổng phải trả là \(10.26 \times 720 = 7387.2\) USD/tháng. Tức là train được tầm 6 lượt WMT 2014.</li>
        </ul>
      </li>
      <li>Thuê <strong>on-demand</strong> thì mình không trả hàng tháng, dùng phát nào xong trả phát ấy thôi. Thì ví dụ, <code class="language-plaintext highlighter-rouge">g5.48xlarge</code> thì sẽ mất \(16.29\times 45 \times 24 /8 = 2199.5\) USD cho mỗi lượt train này kéo dài tầm gần 1 tuần.</li>
      <li>Thuê <a href="https://aws.amazon.com/ec2/spot/pricing/"><strong>Spot</strong></a> thì có thể thuê loại <code class="language-plaintext highlighter-rouge">g5g.16xlarge</code> thì có 2 Tensor core và mất 1.1112 USD/h. Nhìn chung Spot giá sẽ mềm hơn <strong>on-demand</strong> và cùng hạng với reserved. Tuy nhiên, vì là spot nên lúc nào mà tự dưng bị interupt là phải có kế hoạch ứng phó trước:
        <blockquote>
          <p>Spot Instances are a cost-effective choice if you can be flexible about when your applications run and <strong>if your applications can be interrupted</strong></p>
        </blockquote>
      </li>
    </ul>
  </li>
</ul>

<p><em>Như các bạn đã thấy ở trên: cùng mức GPU <code class="language-plaintext highlighter-rouge">g5.xlarge</code> thì Colab Free còn thuê EC2 mất 453.6 USD. Vậy sự khác biệt là gì? Điểm khác chính yếu là thuê EC2 thì bạn không phải lo cái <strong>session 12 h</strong> (sau 12h phải khởi động lại bằng tay và save dữ liệu vào Google Drive).</em>
<strong>Tức là chỉ mỗi cái limit 12h ấy thôi đã trị giá 453.6 USD rồi.</strong></p>

<p>Một điểm đáng lưu ý thứ 2 là giới R&amp;D cạnh tranh khá khốc liệt, nên trên thứ vô thưởng vô phạt kiểu này thì kéo dài 45 ngày và free cũng được.
Nhưng cái gì mà có giá trị kinh tế một chút là cạnh tranh ác liệt: <strong>train là phải xong trong ngày, chứ đợi cả tháng sau có đứa nó publish mất thì sao?</strong>
Thế nên tôi nghĩ nếu thực sự phải cạnh tranh thì chắc lại 240,000 USD hoặc <code class="language-plaintext highlighter-rouge">g5.48xlarge</code> thôi.
Và nếu chọn con đường cạnh tranh này thì các tập đoàn lớn, có sẵn hàng chục cái GPU server trong tay muốn huy động lúc nào cũng <code class="language-plaintext highlighter-rouge">ok em</code> thì chắc chắn là thắng thế.
Nên cạnh tranh vào cái con đường kiểu đọ thông số GPU này có lẽ là chỉ dành cho tập đoàn lớn thôi.</p>

<p>Thứ 3 nữa là tự chế 240,000 USD thì mới là DIY và <code class="language-plaintext highlighter-rouge">self-supervised</code> chứ còn thuê với Colab thì đâu còn là <code class="language-plaintext highlighter-rouge">self</code> nữa nên rốt cuộc là chắc cũng quay lại 240,000 USD nếu thực sự là <code class="language-plaintext highlighter-rouge">self</code>.</p>

<h2 id="neural-machine-translation-nmt">Neural Machine Translation (NMT)</h2>

<p>NMT là 1 bài toán NLP khá điển hình. Lần này chúng ta sẽ chạy thử 1 vài mô hình trên HuggingFace để dịch tiếng Đức sang tiếng Anh.
Đầu tiên, là với mớ lý thuyết phía trên, chúng ta sẽ xem thử cách thức fine-tune model AlBERT cho task NMT.
Tôi dùng bộ dữ liệu WMT 2014 để show:</p>
<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/fab8ab5644bd1a122dd3ad8880d283f9.js"> </script>

<p>Nhìn chung, cứ setup đúng các thông số, tokenizer, hàm loss, optimizer, .v.v… thì hầu như quá trình fine-tune mô hình kiểu BERT diễn ra rất trôi chảy.
Vấn đề tài nguyên để kết thúc quá trình train thì như thảo luận ở trên, các bạn có thể tự lựa chọn giải pháp phù hợp túi tiền.</p>

<p>Nói chung nấu cả buổi nhưng ăn thì nhanh thôi. 
Để demo chức năng dịch Đức-Anh thì tôi xài luôn model có sẵn <a href="https://huggingface.co/google/bert2bert_L-24_wmt_de_en"><code class="language-plaintext highlighter-rouge">google/bert2bert_L-24_wmt_de_en</code></a>.</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/88d0506002341d151a8e5ad745f3364f.js"> </script>

<p>Tôi cũng sử dụng <code class="language-plaintext highlighter-rouge">sacrebleu</code> để show được điểm số BLEU của vài câu dịch.</p>

<h2 id="text-summarization">Text Summarization</h2>
<p>Bài toán <code class="language-plaintext highlighter-rouge">Text Summarization</code> (TS), yêu cầu đưa ra một đoạn <code class="language-plaintext highlighter-rouge">tóm tắt</code> ngắn gọn của 1 văn bản dài.
Có khá nhiều bộ dữ liệu hỗ trợ việc training một model như vậy <a class="citation" href="#metatext2022summary">[38]</a>.
Trong bài này, tôi chọn 1 bộ nhỏ vừa là bộ WikiHow <a class="citation" href="#koupaee2018wikihow">[39]</a> với tầm 240,000 cặp bài báo-tóm tắt.</p>

<p>Về mặt model, thì có khá nhiều hệ thống state-of-the-art và là <code class="language-plaintext highlighter-rouge">seq2seq</code> cho abstractive TS như NAM <a class="citation" href="#rush2015neural">[7]</a>, Encoder-Decoder RNN <a class="citation" href="#nallapati2016abstractive">[40]</a> hay BART <a class="citation" href="#lewis2020bart">[31]</a>.</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/08fb46dd6f03981287c5d3155bf7d6fa.js"> </script>

<p>Nhìn chung, tuy WikiHow số lượng cặp dữ liệu ít hơn WMT14 (240,000 vs. 4,500,000) nhưng mỗi cặp dữ liệu lại là hẳn 1 bài dài, nên kết cục là cùng 1 GPU Colab Free thì thời gian train vẫn mất 49h/epoch.
Tức là cứ 1 epoch 2 ngày, và train khoảng 15 epochs thì mất tầm 30 ngày (hay 1 tháng, tính cả nghỉ lễ, T7/CN).</p>

<p><img src="/assets/images/ts_training.png" alt="ts" /></p>

<h2 id="qa-và-vqa">QA và VQA</h2>

<p>Nếu bạn đã làm bài thi TOEIC reading and listening thì cũng dễ giải thích bài QA và VQA thôi.
Task QA là 1 bộ phận trong task lớn Reading Comprehension (RC): cho 1 bài báo (có thể kèm hình ảnh), người/máy sẽ đọc rồi trả lời 1 câu hỏi liên quan tới bài viết đó.
<em>Bài báo</em> làm reference được gọi là <em>context</em>, một ví dụ điển hình:</p>

<blockquote>
  <p>Context: I live in Japan.
Question: Where do I live?
Answer: Japan.</p>
</blockquote>

<p><img src="https://tweetqa.github.io/image/example.png" style="float: left; margin: 10px; width: 50%;" />
Dạng đơn giản nhất là như vậy chỉ cần đọc kỹ và <strong>paraphrasing</strong> lại cụm từ có sẵn trong context thôi.
Đây là dạng <strong>extractive QA</strong> tức là chỉ đơn giản skim bài viết và quote lại thôi.
Bộ dữ liệu đại diện cho kiểu extractive QA này là SQuAD v1 <a class="citation" href="#rajpurkar2016squad">[4]</a> và v2 <a class="citation" href="#rajpurkar2018know">[17]</a>.
Tuy nhiên, nếu bạn làm bài thi TOEIC bạn sẽ hiểu là có cả những câu hỏi mà không chỉ paraphrasing mà phải suy luận.
Đây gọi là <strong>abstractive QA</strong>, điển hình cho dạng này là bộ TweetQA <a class="citation" href="#xiong2019tweetqa">[41]</a>.
Như ở ví dụ bên tay trái, bạn có thể thấy cần phải để ý chi tiết và suy luận thì mới trả lời được.
Ví dụ bên dưới là 1 model Abstractive QA dựa trên AlBERT:</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/f3def206dd7f578e723a90d3336d04da.js"> </script>

<p>Để đánh giá, có thể dùng các metric như ROUGE-L hoặc METEOR.</p>

<p><img src="/assets/images/how-and-why.png" style="float: left; margin: 10px; width: 40%;" />
<strong>V</strong>isual <strong>Q</strong>uestion <strong>A</strong>nswering (VQA, <a class="citation" href="#antol2015vqa">[5, 6]</a>) thay context bởi 1 hình ảnh hoặc video.
Tức là giống câu hỏi bài nghe thứ nhất trong thi TOEIC (nhưng TOEIC thì không có xem video).
Thì nói chung cũng đa dạng đấy!
Vì câu hỏi thì cũng tùy vào mức độ hình ảnh mà đánh đố hay không đánh đố.
Thì làm sao để thấy được sự khác biệt với QA của bên NLP thì chắc là các bạn xem hình ảnh bên trái (lấy từ <a class="citation" href="#VCRdataset">[42, 43]</a>) thì sẽ hiểu ngay.
Mà tôi nghĩ cũng chỉ nhìn vào mỗi cái hình này là cũng hiểu được là <strong>how and why</strong> như thế nào rồi đấy.
Dưới đây là bức tranh lớn trong mảng VQA (tập hợp qua các bài trên 300 citations).
Tuy chỉ là một mảng nhỏ, nhưng cái scene này nhìn cũng là 1 graph phức tạp phết!</p>

<p><img src="/assets/images/vqa-author-paper-graphs.png" alt="vqa" /></p>

<p>Cũng không nên quên là cách tiếp cận của nhân vật chính postdoc trong bài này là <code class="language-plaintext highlighter-rouge">seq2seq</code>, mà nói chung là mảng giao giữa với vision và language nên tôi nghĩ là tiếp cận theo cách nào cũng sẽ đi đến cái chỗ nó như vậy thôi.
Vậy chúng ta lại cứ bám sát vào <code class="language-plaintext highlighter-rouge">seq2seq</code> nhé (mà hậu duệ là Transformers, BERT là những cái tôi thấy có vẻ cũng bắt đầu lấn sân sang Vision gần đây rồi).</p>

<blockquote>
  <p>Điểm khác biệt lớn nhất giữa vision và language chính là <strong>ORDER</strong>.
Nếu như đơn vị trong language là words được sắp xếp theo order thành phrase và sentence, words được extract từ documents nhờ <code class="language-plaintext highlighter-rouge">tokenizer</code>.
Trong hình ảnh, có một sự mapping với language, và cái hướng này cũng có mấy nhóm cũng rất mạnh đang làm chủ công nghệ này.
Ví dụ, word, phrase &lt;-&gt; visual concepts
còn <code class="language-plaintext highlighter-rouge">tokenizer</code> &lt;-&gt; classifier, object detectors, visual relation detectors, .v.v…
Nói chung, bên vision cũng có đủ những “vũ khí” tương xứng để extract tokens và feed vào <code class="language-plaintext highlighter-rouge">seq2seq</code> models.
Tuy nhiên, vấn đề chính là order: cùng trong một hình ảnh, có thể extract ra 2 vật thể, nhưng khi xếp vào 1 câu văn thì thứ tự nào cũng có khả năng cả.
Nó khác language là <strong>ORDER</strong> đã được input sẵn, còn vision thì không.</p>
</blockquote>

<p>Vấn đề <code class="language-plaintext highlighter-rouge">order</code> cũng không phải mới mà được xem xét ngay từ 2015 (<code class="language-plaintext highlighter-rouge">seq2seq</code> for sets, <a class="citation" href="#vinyals2015order">[44]</a>).
Tuy nhiên, họ cũng chỉ dừng ở mức chọn 1 thứ tự nhất định để bắt đầu và optimize thứ tự đó trong quá trình training.
Đó cũng là tinh thần của hầu hết các ứng dụng vision-language về sau có dùng <code class="language-plaintext highlighter-rouge">seq2seq</code> và attention như image captioning, Video-To-Text (VTT) và VQA.</p>

<p>Một cách giải quyết khác chính là tìm một <strong>representation</strong> tốt hơn cho <code class="language-plaintext highlighter-rouge">set</code>.
Vì <code class="language-plaintext highlighter-rouge">set</code> trong vision không phải <code class="language-plaintext highlighter-rouge">sequence</code> trong NLP nên vấn đề sẽ xảy ra khi dùng representation mới là <code class="language-plaintext highlighter-rouge">seq2seq</code> sẽ phải thay đổi để phù hợp với representation mới.
Trong những publication gần đây, một <code class="language-plaintext highlighter-rouge">set</code> representation phù hợp là <code class="language-plaintext highlighter-rouge">graph</code>.
Vấn đề là <code class="language-plaintext highlighter-rouge">graph</code> không phải <code class="language-plaintext highlighter-rouge">sequence</code> nên <code class="language-plaintext highlighter-rouge">seq2seq</code> cũng thay thế bằng graph neural networks (GNN).</p>

<blockquote>
  <p>Là người thày, có thể không trực tiếp bắt tay vào làm, nhưng phải định hướng.
Vì vậy không thể định hướng một hướng đi “cụt”, chỉ làm 1 đời postdoc là hết.
Định hướng, đôi khi dù 10 đời postdoc không làm gì thì cũng không thể hết việc được, mới là định hướng tốt.
Thậm chí, nếu postdoc mà chịu khó làm thì còn sinh sôi nảy nở ra làm mãi không hết.
Vision-Language (VL) nhìn chung là 1 định hướng lâu dài như vậy.</p>
</blockquote>

<blockquote>
  <p>Làm người trò, thì khi bắt đầu dự án không nên quá thụ động.
Việc representation có thể phải thay đổi giữa chừng (từ <code class="language-plaintext highlighter-rouge">sequence</code> sang <code class="language-plaintext highlighter-rouge">graph</code>) là chuyện nếu không nhìn ra ngay từ đầu,
có thể dẫn đến dự án bị đình chỉ giữa chừng.
Làm postdoc như vậy là yếu về <strong>quản lý rủi ro</strong>.
Nhìn chung vẫn là nghiệp vụ quản lý dự án (Project Management) vẫn còn chưa mạnh.</p>
</blockquote>

<p>Nhìn chung, mảng Vision-Language (VL) này tôi thấy còn nhiều dư địa cả về lý thuyết lẫn ứng dụng:</p>

<ul>
  <li>Về lý thuyết, cái representation vẫn còn chỗ làm. Ứng với mỗi representation có thể sẽ phải thu thập dataset và gắn nhãn riêng. Xây dựng model mới hẳn cho <code class="language-plaintext highlighter-rouge">graph</code> và <code class="language-plaintext highlighter-rouge">seq2seq</code>.</li>
  <li>Về ứng dụng, VQA hiện tại vẫn chỉ là <code class="language-plaintext highlighter-rouge">closed form</code>: xem 1 hình ảnh và extract concepts rồi dùng kỹ thuật QA để trả lời nội dung trong ảnh, tức là chưa phải dạng suy luận nhiều. VQA ở dạng <code class="language-plaintext highlighter-rouge">open</code> hơn sẽ thay context không chỉ bởi 1 hình ảnh hay 1 video, mà bởi 1 <code class="language-plaintext highlighter-rouge">multimedia database</code>, cũng như trong trò chơi tỷ phú, mỗi câu hỏi không chỉ đòi hỏi phải xem 1 context nhất định mà phải <strong>search</strong> trong 1 database knowledge mới có thể trả lời được.
    <ul>
      <li>Một bước đơn giản hơn chỉ là thay context hình ảnh bởi context video, và câu hỏi là về 1 details nào đó: ví dụ show cho máy một video nấu ăn, và hỏi <code class="language-plaintext highlighter-rouge">Tại sao lại cho hành vào phi trước?</code>.</li>
      <li>Thay vì giới hạn <code class="language-plaintext highlighter-rouge">luật chơi</code> rằng máy chỉ được xem context là 1 bài báo, 1 hình ảnh, 1 video cụ thể, thì nhưu IBM Watson hay Siri, cho phép máy kết nối Internet và có thể query trong cả 1 multimedia database lớn, thì QA sẽ như thế nào?</li>
    </ul>
  </li>
</ul>

<p>Tuy vậy, với công nghệ hiện tại cũng đủ để làm 1 demo nho nhỏ dạng: <code class="language-plaintext highlighter-rouge">cho máy xem 1 hình ảnh, và đặt 1 vài câu hỏi xung quanh nội dung hình ảnh</code> và không cho máy kết nối Internet, thì máy cũng có thể trả lời ở mức độ nhất định. Tức là có thể lấy điểm bài nghe số 1 của TOEIC!</p>

<p>Đánh giá về tiềm năng bài toán VQA, thì tôi thấy cái mảng này nó cũng dồi dào, code kiếc cũng thừa mứa trên Github, kiểu như HuggingFace ý: classifier thì có TIMM <a class="citation" href="#rw2019timm">[45]</a>, detector thì có Detectron2 <a class="citation" href="#wu2019detectron2">[46]</a> hay MMDetection <a class="citation" href="#mmdetection">[47]</a>, segmentations thì có <a class="citation" href="#Yakubovskiy:2019">[48]</a>, visual relationship detector (VRD) thì cũng tràn lan vì các tác giả cũng công bố hết lên Github.
Thế nên là những cái low-level detector kiểu visual <code class="language-plaintext highlighter-rouge">tokenizer</code> là sẵn có, mà người ta công bố mà mình không dùng thì cũng phí.
Mà vấn đề là của người ta chất lượng nó cao.
Dữ liệu thì cũng công bố sẵn có hết, bộ dữ liệu VQA hay commonsense hay Visual Genome <a class="citation" href="#krishna2017visual">[49]</a> thì nhìn chung cũng tầm triệu câu hỏi, với vài trăm ngàn hình ảnh.
<strong>Đấy vấn đề với quy mô dữ liệu này thì lại quay về bài toán <code class="language-plaintext highlighter-rouge">240,000</code> USD thoai!</strong>
Đấy, mấy cái mảng này nó toàn kiểu code kiếc, dữ liệu thì bằng cho không trên Github rồi, nhưng vấn đề là để chạy được thì hàng trăm ngàn đô.
Các bạn thấy có những nhóm nó chịu khó train để ra model công bố thì toàn 8 GPUs mà cũng hì hục train 1 tuần.
Có cái train ImageNet trong 1 giờ <a class="citation" href="#goyal2017accurate">[50]</a> thì lại là họ dùng những <code class="language-plaintext highlighter-rouge">256 GPUs</code> mà họ có được nguồn tài nguyên ấy là nhóm của … Facebook!
Nên tôi nghĩ khi làm postdoc mà không kiểu có estimate trước thời gian train là lại … giữa chừng đứt gánh vì … không có tiền!
Mà không có tiền thì đấy như ví dụ postdoc của chúng ta là lặng lẽ giải tán rồi!</p>
<blockquote>
  <p>Chứ không có tiền thì thày cũng không có, trò cũng không có thì nhìn nhau … cười à?</p>
</blockquote>

<blockquote>
  <p>Mà vì tài nguyên cái mảng này nó tràn lan trên Github nên có bảo tôi code thì lấy code làm công cũng không được vì cùng cái code trên mạng Internet nó nhan nhản ra, <strong>code nói chung là rẻ mạt!</strong>
<em>Cái quan trọng là vượt qua cái khó khăn như 240,000 USD và ra được sản phẩm thoai!</em></p>
</blockquote>

<h2 id="video-to-text-và-image-captioning">Video-to-Text và Image Captioning</h2>

<blockquote>
  <p>Training on videos used standard SGD with momentum set to 0.9 in all cases, with synchronous parallelization across <strong>32 GPUs</strong> for all models except the 3D ConvNets which receive a large number of input frames and hence require more GPUs to form large batches – we used <strong>64 GPUs</strong> for these. We trained models on on Kinetics for <strong>110k steps</strong>, with a 10x reduction of learning rate when validation loss saturated. We tuned the learning rate hyperparameter on the validation set of Kinetics. Models were trained for up to <strong>5k steps</strong> on UCF-101 and HMDB-51 using a similar learning rate adaptation procedure as for Kinetics but using just <strong>16 GPUs</strong>. (trích dẫn <a class="citation" href="#carreira2017quo">[51]</a>)</p>
</blockquote>

<p>Vấn đề gì trong đoạn văn trên?</p>
<ul>
  <li>Thứ nhất, UCF-101 và HMDB-51 là những bộ dữ liệu nhỏ nhưng họ cũng phải dùng tới <strong>16 GPUs</strong> để train. Thì gọi cho 16 GPUs sẽ nhanh lên tầm 1s/iteration thì train 5k steps mất tầm 1h-2h. Thì với bộ dữ liệu nhỏ như vậy là ổn.</li>
  <li>Tuy nhiên, với bộ dữ liệu lớn như Kinetics thì họ phải inputs hàng triệu frames, và để nhanh thì bắt buộc dùng tới 32, thậm chí 64 GPUs để train. Gọi cho là dùng <strong>64 GPUs</strong> thì sẽ train nhanh đi thì 1s/iteration thì 110k steps cũng mất 1-2 ngày.</li>
  <li>Nhưng nên nhớ họ có 64 GPUs nhé, chứ còn chỉ có 1-2 GPUs thì xác định mất 1-2 ngày x 30 lần tức là train mất <strong>1-2 tháng</strong> (không ngủ nghỉ gì cả, T7/CN nghỉ lễ cũng train).</li>
</ul>

<p>Bài toán Video-To-Text hay Video Captioning, Video-Text Retrieval đòi hỏi cần có những biểu diễn phù hợp cho cả vision lẫn language.
Về mặt vision thì có mấy cái hệ biểu diễn C3D/I3D như trên.
Nhìn chung có thể dùng <code class="language-plaintext highlighter-rouge">seq2seq</code> hoặc BERT để learn được những biểu diễn như vậy và input vào <code class="language-plaintext highlighter-rouge">seq2seq</code> để xuất ra captions.</p>
<blockquote>
  <p>Nhưng nhìn chung, độ khả thi của đề xuất này vẫn phụ thuộc con số <code class="language-plaintext highlighter-rouge">16-32-64</code> GPUs ở trên. Thày nghèo với thày giàu là có phân biệt đấy.
Nói chung làm hình ảnh thì dưới 8 GPUs không nên theo.
Nhưng video thì dưới 16 GPUs thì cũng không nên train triếc mất thời gian.
Mà thày nào dưới 16 GPUs thì cũng không nên theo nếu bạn định <code class="language-plaintext highlighter-rouge">train</code>.</p>
</blockquote>

<p>Các bạn cũng cần hiểu, đó là nguồn tài nguyên trong một lab nghiên cứu luôn là mua chung cho cả lab, chứ không phải cho 1 anh postdoc cụ thể nào.
Ví dụ postdoc muốn xin dùng hệ thống 8 GPUs của thày tầm 1 ngày 2 ngày thì còn ổn.
Chứ postdoc mà muốn chiếm hẳn 1 tuần để train model video captioning thì tự dưng nó nảy sinh ra rất nhiều vấn đề khác.
Nên là nếu bạn chọn vào chỗ <code class="language-plaintext highlighter-rouge">nghèo</code> quá ấy, thì liệu cơm gắp mắm, nhưng tốt nhất đừng có <code class="language-plaintext highlighter-rouge">train</code> gì cả.
Bây giờ làm việc bên mảng vision này, đặc biệt là video, thì tự dưng lại phải ngồi build cả 1 hệ thống GPU server <code class="language-plaintext highlighter-rouge">16-32-64-128-256</code> GPUs để cho đệ tử nó dùng.
Nên là trước khi bắt đầu dự án, việc lên kế hoạch mua sắm, build server các cái là phải chuẩn bị từ trước hết.</p>

<blockquote>
  <p>Chứ không chạy đến giữa chừng lại ngậm ngùi giải tán vì … không có GPUs (mà thực ra là không có tiền) để chạy thì còn ra cái thể thống cống rãnh gì nữa?</p>
</blockquote>

<p>Image Captioning thì cũng dùng RNN/LSTM và Attention từ lâu rồi.
Nhìn chung, cái mảng này vẫn là <strong>cuộc chơi của người giàu</strong>.
Mà độ giàu của ông thày là số lượng GPUs mà lab thày sở hữu.
Bây giờ, postdoc có ý tưởng, postdoc có đề tài, postdoc lên kế hoạch, postdoc tính toán tỷ mỷ số lượng tài nguyên cần thiết.
Thế bây giờ, postdoc là người chọn thày chứ không phải thày chọn postdoc.
Thì quay lại trường hợp postdoc của chúng ta:</p>
<blockquote>
  <p><strong>Tại sao lại chọn chỗ nghèo như vậy?</strong></p>
</blockquote>

<p>Chưa đi ra đến chợ mà đã hết tiền như thế thì làm thế nào cạnh tranh nổi trong giới postdoc?
Mà đã thấy không có tiền thì đổi đề tài cho nó phù hợp, làm cái task khác nó không cần tiền đi.
Tất nhiên là người nghèo lại làm task không có tiền thì lại thành … <code class="language-plaintext highlighter-rouge">nghèo bền vững</code> thôi!
Nói chung là ngay từ bước <strong>quản lý tài nguyên, một phần trong nghiệp vụ quản lý dự án</strong> là đã có vấn đề.</p>

<h1 id="kết-luận">Kết luận</h1>

<h2 id="về-seq2seq-trong-nlp">Về <code class="language-plaintext highlighter-rouge">seq2seq</code> trong NLP</h2>
<p>Nhìn chung về mấy task bên NLP, thì tất nhiên bài viết này cũng chỉ là 1 phần (bao gồm task NMT/TS/QA với mấy cái tự học) còn nhiều task khác như POS Tagging, semantic roles, …
Tuy nhiên, nhìn chung những tiến bộ trong mảng NLP đã được tóm tắt, và có vẻ cũng không khó khăn lắm đâu.</p>
<blockquote>
  <p>Có làm thì sẽ có ăn thôi, còn không làm mà đòi có ăn thì …</p>
</blockquote>

<iframe width="100%" height="315" src="https://www.youtube.com/embed/peTwj6WK3vo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>Cũng xoáy kha khá cái trường hợp postdoc, nhưng tôi nghĩ là cứ chuyển hết cho Prof. Huan Rose chỉ bảo là ok thôi.</p>

<p>Nói chung phần kỹ thuật thì không có nhiều điểm khó, nhưng cái khó nhất chắc vẫn là cái 240,000 USD thôi. 
Mà cái đó thì chắc chắn không phải là cái anh em kỹ thuật có thể giải quyết nổi.</p>
<blockquote>
  <p>Nên nếu không có tiền thì … giải tán thôi, chứ biết làm thế nào bây giờ?
Cái này có đố cũng chịu!
Như bình thường thì có thể đổi sang làm những cái bài toán mà nó <code class="language-plaintext highlighter-rouge">train phát xong luôn, dùng luôn</code>, những bài toán mà nó có thể chạy chỉ cần CPU, thậm chí chỉ mấy cái device nhỏ nhỏ cũng chạy được ấy.
Nên tôi nghĩ cũng không cần xoáy nhiều vào cái đám big data nữa:</p>
  <ol>
    <li>Xin vô mấy cái tập đoàn lớn đùng ấy mà làm, nó có nhiều người dùng nó sẵn sàng chi tiền tấn ra mà mua. Ví dụ, các anh em thử nghĩ xem mấy cái papers nhiều citation của cái mảng này làm thí nghiệm toàn hàng trăm GPU, TPU thì tác giả toàn nhóm của những công ty như thế nào? phải Google, Facebook, … chứ như anh em <strong>chân đất mắt toét</strong> thì chờ nó ra model rồi dùng lại, chứ mất công hì hục nấu nướng làm gì cho mất thời gian.</li>
    <li>Chuyển sang làm mấy cái thiết bị nhỏ nhỏ mà làm. Mà chấm dứt suy nghĩ về GPU với mấy cái model to đùng, nấu thì lâu mà ăn thì chóng đi.</li>
  </ol>
</blockquote>

<h2 id="về-chuyện-tài-nguyên-training">Về chuyện tài nguyên training</h2>
<p>Các bạn có thể thấy điển tích <code class="language-plaintext highlighter-rouge">240,000 USD</code> xuất hiện khá nhiều trong bài viết.
Nói chung mảng này lúc bắt đầu làm postdoc hay dự án nghiên cứu mà không nhìn trước được những khó khăn điển tích này thì kỹ năng quản lý dự án là kém.
<strong>Chứ chạy đến giữa postdoc lại ngậm ngùi giải tán vì … không có tiền thì còn ra thể thống gì?</strong></p>

<p>Thứ hai là nếu bạn đã xác định đi con đường train triếc mất thời gian này thì điều tra kỹ về lab hoặc nhóm mình vào:</p>
<blockquote>
  <p>Nếu thày có <code class="language-plaintext highlighter-rouge">ít hơn 8 GPUs</code> (mà phải là GPU xịn khỏe nhé chứ GPU đểu thì cũng nhiều mà) thì đừng vào lab ấy làm gì, vì cơ sở hạ tầng của lab không có và lúc làm bạn phải tự lo tự chạy đấy. Trong trường hợp thuê thì thày phải có ngân sách và chịu chi.</p>
</blockquote>

<p>Thứ ba là tốt nhất là nên chọn mấy cái đề tài <code class="language-plaintext highlighter-rouge">không cần train</code>, hoặc có train thì cũng tí là xong.
Đấy mấy cái Random Forest, mấy cái model nhẹ nhàng train vèo phát xong.
Rồi mình làm mấy cái deploy lên thiết bị nhỏ nhẹ, vừa khỏe vừa có tiền, cả nhà đều vui :))</p>

<blockquote>
  <p>Cách đây ít lâu, tôi có làm phỏng vấn tuyển dụng, có bạn trẻ “thị uy” với tôi là từng làm việc với hệ thống 24 GPUs.
Tôi cũng thú nhận là chỗ chúng tôi <code class="language-plaintext highlighter-rouge">không train</code> gì hết, chỉ vào làm luôn inference nhỏ nhẹ, vừa khỏe vừa vui thôi.
Nên nếu bạn mà định kiểu dành hàng tuần ngồi train thì chúng tôi không đáp ứng được.</p>
</blockquote>

<p>Thế nên chốt lại là tốt nhất là nên chọn hướng đi ít train thôi.
Còn nếu chọn con đường train nhiều tốn kém thì <strong>PHẢI CÓ TIỀN</strong> vì đây là cuộc chơi của người giàu!</p>

<h2 id="về-các-task-vision-language">Về các task vision-language</h2>
<p>Thật thú vị là cùng thời điểm khoảng 2015-2016, bên vision cũng xuất hiện 1 loạt task kế thừa <code class="language-plaintext highlighter-rouge">seq2seq</code> và đến bây giờ vẫn phát triển song song và lấy những công nghệ tiên tiến nhất của bên NLP như Transformers hay BERT về để ứng dụng.
Thì họ cũng làm lên một nhánh vision-language learning (VL learning), với những task thú vị như VQA, VTT, image captioning, video/image-text retrieval.</p>

<p>Nhìn chung là thú vị, nhưng có một số vấn đề như representation, thì đặc điểm của <code class="language-plaintext highlighter-rouge">set</code> nên có những biểu diễn mới như <code class="language-plaintext highlighter-rouge">graph</code> hay <code class="language-plaintext highlighter-rouge">3D</code> (C3D/I3D).
Nếu làm một kế hoạch nghiêm chỉnh cũng có thể làm ra một cái postdoc tốt nhưng không hiểu sao ngày ấy lại không đi đến đâu cả.
Mà cuối cùng ấy, tôi nghĩ có nhiều nguyên nhân như phân tích ở trên, nhưng một nguyên nhân tuy nói ra hơi <code class="language-plaintext highlighter-rouge">phô</code> nhưng mà <code class="language-plaintext highlighter-rouge">thật</code>:</p>

<blockquote>
  <p>Không có GPUs!</p>
</blockquote>

<p>Cái này cũng thể hiện sự yếu kém về mặt nghiệp vụ quản lý dự án (liên quan tới quản lý scope, tài nguyên và kế hoạch).</p>

<p>Nhìn chung cũng là một bài học kinh nghiệm quý báu.
Tôi nghĩ vẫn có nhiều điều đáng lưu ý để thế hệ sau không vấp phải.</p>

<h1 id="tài-liệu-tham-khảo">Tài liệu tham khảo</h1>

<ol class="bibliography"><li><span id="kalchbrenner2013recurrent">Kalchbrenner, N. and Blunsom, P. 2013. Recurrent continuous translation models. <i>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA. Association for Computational Linguistics</i> (2013).</span><a class="details" href="https://wanted2.github.io/bibliography/kalchbrenner2013recurrent/">Details</a></li>
<li><span id="sutskever2014sequence">Sutskever, I., Vinyals, O. and Le, Q.V.V. 2014. Sequence to sequence learning with neural networks. <i>Advances in Neural Information Processing Systems</i> (2014), 3104–3112.</span><a class="details" href="https://wanted2.github.io/bibliography/sutskever2014sequence/">Details</a></li>
<li><span id="cho2014learning">Cho, K., Merrienboer, B. van, Gulcehre, C., Bougares, F., Schwenk, H. and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. <i>arXiv preprint arXiv:1406.1078</i>. (2014).</span><a class="details" href="https://wanted2.github.io/bibliography/cho2014learning/">Details</a></li>
<li><span id="rajpurkar2016squad">Rajpurkar, P., Zhang, J., Lopyrev, K. and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. <i>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</i> (2016), 2383–2392.</span><a class="details" href="https://wanted2.github.io/bibliography/rajpurkar2016squad/">Details</a></li>
<li><span id="antol2015vqa">Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L. and Parikh, D. 2015. Vqa: Visual question answering. <i>Proceedings of the IEEE international conference on computer vision</i> (2015), 2425–2433.</span><a class="details" href="https://wanted2.github.io/bibliography/antol2015vqa/">Details</a></li>
<li><span id="goyal2017making">Goyal, Y., Khot, T., Summers-Stay, D., Batra, D. and Parikh, D. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (2017), 6904–6913.</span><a class="details" href="https://wanted2.github.io/bibliography/goyal2017making/">Details</a></li>
<li><span id="rush2015neural">Rush, A.M., Chopra, S. and Weston, J. 2015. A Neural Attention Model for Abstractive Sentence Summarization. <i>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</i> (2015), 379–389.</span><a class="details" href="https://wanted2.github.io/bibliography/rush2015neural/">Details</a></li>
<li><span id="ranzato2015sequence">Ranzato, M.A., Chopra, S., Auli, M. and Zaremba, W. 2015. Sequence level training with recurrent neural networks. <i>arXiv preprint arXiv:1511.06732</i>. (2015).</span><a class="details" href="https://wanted2.github.io/bibliography/ranzato2015sequence/">Details</a></li>
<li><span id="venugopalan2015sequence">Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T. and Saenko, K. 2015. Sequence to sequence-video to text. <i>Proceedings of the IEEE international conference on computer vision</i> (2015), 4534–4542.</span><a class="details" href="https://wanted2.github.io/bibliography/venugopalan2015sequence/">Details</a></li>
<li><span id="you2016image">You, Q., Jin, H., Wang, Z., Fang, C. and Luo, J. 2016. Image captioning with semantic attention. <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (2016), 4651–4659.</span><a class="details" href="https://wanted2.github.io/bibliography/you2016image/">Details</a></li>
<li><span id="johnson2016densecap">Johnson, J., Karpathy, A. and Fei-Fei, L. 2016. Densecap: Fully convolutional localization networks for dense captioning. <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (2016), 4565–4574.</span><a class="details" href="https://wanted2.github.io/bibliography/johnson2016densecap/">Details</a></li>
<li><span id="luong2015multi">Luong, M.-T., Le, Q.V., Sutskever, I., Vinyals, O. and Kaiser, L. 2015. Multi-task sequence to sequence learning. <i>arXiv preprint arXiv:1511.06114</i>. (2015).</span><a class="details" href="https://wanted2.github.io/bibliography/luong2015multi/">Details</a></li>
<li><span id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. 2017. Attention is all you need. <i>Advances in neural information processing systems</i> (2017), 5998–6008.</span><a class="details" href="https://wanted2.github.io/bibliography/vaswani2017attention/">Details</a></li>
<li><span id="bojar2014findings">Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H. and others 2014. Findings of the 2014 workshop on statistical machine translation. <i>Proceedings of the ninth workshop on statistical machine translation</i> (2014), 12–58.</span><a class="details" href="https://wanted2.github.io/bibliography/bojar2014findings/">Details</a></li>
<li><span id="bojar2017findings">Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., Huck, M., Koehn, P., Liu, Q., Logacheva, V. and others 2017. Findings of the 2017 conference on machine translation (wmt17). <i>Proceedings of the Second Conference on Machine Translation</i> (2017), 169–214.</span><a class="details" href="https://wanted2.github.io/bibliography/bojar2017findings/">Details</a></li>
<li><span id="paul2004introduc">Over, P. and Yen, J. 2014. An Introduction to DUC-2004. https://duc.nist.gov/pubs/2004slides/duc2004.intro.pdf.</span><a class="details" href="https://wanted2.github.io/bibliography/paul2004introduc/">Details</a></li>
<li><span id="rajpurkar2018know">Rajpurkar, P., Jia, R. and Liang, P. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. <i>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</i> (2018), 784–789.</span><a class="details" href="https://wanted2.github.io/bibliography/rajpurkar2018know/">Details</a></li>
<li><span id="lin2004rouge">Lin, C.-Y. 2004. Rouge: A package for automatic evaluation of summaries. <i>Text summarization branches out</i> (2004), 74–81.</span><a class="details" href="https://wanted2.github.io/bibliography/lin2004rouge/">Details</a></li>
<li><span id="bottou2007tradeoffs">Bottou, L. and Bousquet, O. 2007. The tradeoffs of large scale learning. <i>Advances in neural information processing systems</i>. 20, (2007).</span><a class="details" href="https://wanted2.github.io/bibliography/bottou2007tradeoffs/">Details</a></li>
<li><span id="rumelhart1986learning">Rumelhart, D.E., Hinton, G.E. and Williams, R.J. 1986. Learning representations by back-propagating errors. <i>nature</i>. 323, 6088 (1986), 533–536.</span><a class="details" href="https://wanted2.github.io/bibliography/rumelhart1986learning/">Details</a></li>
<li><span id="goodfellow2016deep">Goodfellow, I., Bengio, Y. and Courville, A. 2016. <i>Deep learning</i>. MIT press.</span><a class="details" href="https://wanted2.github.io/bibliography/goodfellow2016deep/">Details</a></li>
<li><span id="hochreiter1997long">Hochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. <i>Neural computation</i>. 9, 8 (1997), 1735–1780.</span><a class="details" href="https://wanted2.github.io/bibliography/hochreiter1997long/">Details</a></li>
<li><span id="gers2000learning">Gers, F.A., Schmidhuber, J. and Cummins, F. 2000. Learning to forget: Continual prediction with LSTM. <i>Neural computation</i>. 12, 10 (2000), 2451–2471.</span><a class="details" href="https://wanted2.github.io/bibliography/gers2000learning/">Details</a></li>
<li><span id="cho2014properties">Cho, K., Merrienboer, B. van, Bahdanau, D. and Bengio, Y. 2014. On the properties of neural machine translation: Encoder-decoder approaches. <i>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8), 2014</i> (2014).</span><a class="details" href="https://wanted2.github.io/bibliography/cho2014properties/">Details</a></li>
<li><span id="koehn2009statistical">Koehn, P. 2009. Neural Machine Translation. <i>Statistical Machine Translation</i>. Cambridge University Press.</span><a class="details" href="https://wanted2.github.io/bibliography/koehn2009statistical/">Details</a></li>
<li><span id="vaswani2018tensor2tensor">Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A.N., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner, N., Parmar, N. and others 2018. Tensor2tensor for neural machine translation. <i>arXiv preprint arXiv:1803.07416</i>. (2018).</span><a class="details" href="https://wanted2.github.io/bibliography/vaswani2018tensor2tensor/">Details</a></li>
<li><span id="wolf2020transformers">Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi, A., Cistac, P., Funtowicz, M., Davison, J., Shleifer, S. and others 2020. Transformers: State-of-the-art natural language processing. <i>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</i> (2020), 38–45.</span><a class="details" href="https://wanted2.github.io/bibliography/wolf2020transformers/">Details</a></li>
<li><span id="devlin2019bert">Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i> (2019), 4171–4186.</span><a class="details" href="https://wanted2.github.io/bibliography/devlin2019bert/">Details</a></li>
<li><span id="lan2019albert">Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P. and Soricut, R. 2019. Albert: A lite bert for self-supervised learning of language representations. <i>arXiv preprint arXiv:1909.11942</i>. (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/lan2019albert/">Details</a></li>
<li><span id="liu2019roberta">Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. <i>arXiv preprint arXiv:1907.11692</i>. (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/liu2019roberta/">Details</a></li>
<li><span id="lewis2020bart">Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V. and Zettlemoyer, L. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i> (2020), 7871–7880.</span><a class="details" href="https://wanted2.github.io/bibliography/lewis2020bart/">Details</a></li>
<li><span id="yang2019xlnet">Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R. and Le, Q.V. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. <i>Advances in neural information processing systems</i>. 32, (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/yang2019xlnet/">Details</a></li>
<li><span id="conneau2019cross">Conneau, A. and Lample, G. 2019. Cross-lingual language model pretraining. <i>Advances in Neural Information Processing Systems</i>. 32, (2019), 7059–7069.</span><a class="details" href="https://wanted2.github.io/bibliography/conneau2019cross/">Details</a></li>
<li><span id="clark2020electra">Clark, K., Luong, M.-T., Le, Q.V. and Manning, C.D. 2020. Electra: Pre-training text encoders as discriminators rather than generators. <i>arXiv preprint arXiv:2003.10555</i>. (2020).</span><a class="details" href="https://wanted2.github.io/bibliography/clark2020electra/">Details</a></li>
<li><span id="hinton2006fast">Hinton, G.E., Osindero, S. and Teh, Y.-W. 2006. A fast learning algorithm for deep belief nets. <i>Neural computation</i>. 18, 7 (2006), 1527–1554.</span><a class="details" href="https://wanted2.github.io/bibliography/hinton2006fast/">Details</a></li>
<li><span id="peters2018deep">Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K. and Zettlemoyer, L. 2018. Deep Contextualized Word Representations. <i>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</i> (New Orleans, Louisiana, Jun. 2018), 2227–2237.</span><a class="details" href="https://wanted2.github.io/bibliography/peters2018deep/">Details</a></li>
<li><span id="radford2018improving">Radford, A., Narasimhan, K., Salimans, T. and Sutskever, I. 2018. <i>Improving language understanding by generative pre-training</i>. OpenAI.</span><a class="details" href="https://wanted2.github.io/bibliography/radford2018improving/">Details</a></li>
<li><span id="metatext2022summary">+64 Summarization Datasets - NLP Database.</span><a class="details" href="https://wanted2.github.io/bibliography/metatext2022summary/">Details</a></li>
<li><span id="koupaee2018wikihow">Koupaee, M. and Wang, W.Y. 2018. Wikihow: A large scale text summarization dataset. <i>arXiv preprint arXiv:1810.09305</i>. (2018).</span><a class="details" href="https://wanted2.github.io/bibliography/koupaee2018wikihow/">Details</a></li>
<li><span id="nallapati2016abstractive">Nallapati, R., Zhou, B., Santos, C. dos, Gu̇lçehre Çağlar and Xiang, B. 2016. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. <i>Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</i> (2016), 280–290.</span><a class="details" href="https://wanted2.github.io/bibliography/nallapati2016abstractive/">Details</a></li>
<li><span id="xiong2019tweetqa">Xiong, W., Wu, J., Wang, H., Kulkarni, V., Yu, M., Chang, S., Guo, X. and Wang, W.Y. 2019. TWEETQA: A Social Media Focused Question Answering Dataset. <i>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</i> (2019), 5020–5031.</span><a class="details" href="https://wanted2.github.io/bibliography/xiong2019tweetqa/">Details</a></li>
<li><span id="VCRdataset">VCR: Visual Commonsense Reasoning.</span><a class="details" href="https://wanted2.github.io/bibliography/VCRdataset/">Details</a></li>
<li><span id="zellers2019recognition">Zellers, R., Bisk, Y., Farhadi, A. and Choi, Y. 2019. From recognition to cognition: Visual commonsense reasoning. <i>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</i> (2019), 6720–6731.</span><a class="details" href="https://wanted2.github.io/bibliography/zellers2019recognition/">Details</a></li>
<li><span id="vinyals2015order">Vinyals, O., Bengio, S. and Kudlur, M. 2015. Order matters: Sequence to sequence for sets. <i>arXiv preprint arXiv:1511.06391</i>. (2015).</span><a class="details" href="https://wanted2.github.io/bibliography/vinyals2015order/">Details</a></li>
<li><span id="rw2019timm">Wightman, R. 2019. PyTorch Image Models. <i>GitHub repository</i>. GitHub.</span><a class="details" href="https://wanted2.github.io/bibliography/rw2019timm/">Details</a></li>
<li><span id="wu2019detectron2">Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y. and Girshick, R. 2019. Detectron2.</span><a class="details" href="https://wanted2.github.io/bibliography/wu2019detectron2/">Details</a></li>
<li><span id="mmdetection">Chen, K. et al. 2019. MMDetection: Open MMLab Detection Toolbox and Benchmark. <i>arXiv preprint arXiv:1906.07155</i>. (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/mmdetection/">Details</a></li>
<li><span id="Yakubovskiy:2019">Yakubovskiy, P. 2020. Segmentation Models Pytorch. <i>GitHub repository</i>. GitHub.</span><a class="details" href="https://wanted2.github.io/bibliography/Yakubovskiy_2019/">Details</a></li>
<li><span id="krishna2017visual">Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D.A. and others 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. <i>International journal of computer vision</i>. 123, 1 (2017), 32–73.</span><a class="details" href="https://wanted2.github.io/bibliography/krishna2017visual/">Details</a></li>
<li><span id="goyal2017accurate">Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y. and He, K. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. <i>arXiv preprint arXiv:1706.02677</i>. (2017).</span><a class="details" href="https://wanted2.github.io/bibliography/goyal2017accurate/">Details</a></li>
<li><span id="carreira2017quo">Carreira, J. and Zisserman, A. 2017. Quo vadis, action recognition? a new model and the kinetics dataset. <i>proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i> (2017), 6299–6308.</span><a class="details" href="https://wanted2.github.io/bibliography/carreira2017quo/">Details</a></li></ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="/aws-account/">Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn</a>, hôm nay đọc tiếp cái <code class="language-plaintext highlighter-rouge">sequence-to-sequence</code> (<code class="language-plaintext highlighter-rouge">Seq2Seq</code>) cũng thấy có cả 30k-40k cũng có. Thế nên là cái <code class="language-plaintext highlighter-rouge">Seq2Seq</code> này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái <code class="language-plaintext highlighter-rouge">Seq2Seq</code> này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm). Chứ đọc làm sao mà hết được? Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp. Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. Có bài như bài gốc Transformer (Attention is all you need, <a class="citation" href="#vaswani2017attention">[13]</a>) mới ra đời từ 2017 mà đã hơn 35k trích dẫn! <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2022-02-01">01 Feb 2022</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Encoder-Decoder">#Encoder-Decoder</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Long-Short-Term-Memory">#Long-Short Term Memory</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Machine-learning">#Machine learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Neural-Machine-Translation">#Neural Machine Translation</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Recurrent-Neural-Networks">#Recurrent Neural Networks</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Seq2Seq">#Seq2Seq</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Sequence-to-Sequence">#Sequence-to-Sequence</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="https://wanted2.github.io//ml-ids/"> &laquo; Machine Learning for Network Intrusion Detection: From Local to Production</a>
            
            
            <a class="next d-block col-md-6 text-lg-right" href="https://wanted2.github.io//speech/">Speech and Sequence-to-sequence &raquo; </a>
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'caineng'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi" style="max-height: 48px;" /> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://caineng.us20.list-manage.com/subscribe/post?u=76342d3d74a6807aac5aec0d7&id=b5645e19be" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語 (7)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Project-Management">Project Management (2)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Software-Engineering">Software Engineering (3)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence (5)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Site-Reliable-Engineering">Site Reliable Engineering (3)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2022 AiFi 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="https://wanted2.github.io/assets/js/mediumish.js"></script>


<script src="https://wanted2.github.io/assets/js/lazyload.js"></script>


<script src="https://wanted2.github.io/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//caineng.disqus.com/count.js"></script>


</body>
</html>
