<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="https://wanted2.github.io/assets/images/favicon.ico">

<title>Seq2Seq và kiến trúc Encoder-Decoder | AiFi</title>

 
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-2CDTCF0EP6" crossorigin="anonymous"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-2CDTCF0EP6');
        </script>
    


<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Seq2Seq và kiến trúc Encoder-Decoder | AiFi</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Seq2Seq và kiến trúc Encoder-Decoder" />
<meta name="author" content="tuan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn, hôm nay đọc tiếp cái sequence-to-sequence (Seq2Seq) cũng thấy có cả 30k-40k cũng có. Thế nên là cái Seq2Seq này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái Seq2Seq này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm). Chứ đọc làm sao mà hết được? Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp. Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. Có bài như bài gốc Transformer (Attention is all you need, [35]) mới ra đời từ 2017 mà đã hơn 35k trích dẫn! Seq2Seq [35, 35, 35] là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, [35]), Question-Answering (QA, [35]), Visual Question Answering (VQA, [35]), Text Summarization (TS, [35, 35]) và Video-To-Text (VTT, [35]). Bài toán Image Captioning thì cũng có thể ứng dụng seq2seq nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào seq2seq như trong bài Semantic Attention (SA) [35] hay Densecap [35]. Nên nhìn chung là seq2seq là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng [35]." />
<meta property="og:description" content="Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn, hôm nay đọc tiếp cái sequence-to-sequence (Seq2Seq) cũng thấy có cả 30k-40k cũng có. Thế nên là cái Seq2Seq này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái Seq2Seq này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm). Chứ đọc làm sao mà hết được? Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp. Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. Có bài như bài gốc Transformer (Attention is all you need, [35]) mới ra đời từ 2017 mà đã hơn 35k trích dẫn! Seq2Seq [35, 35, 35] là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, [35]), Question-Answering (QA, [35]), Visual Question Answering (VQA, [35]), Text Summarization (TS, [35, 35]) và Video-To-Text (VTT, [35]). Bài toán Image Captioning thì cũng có thể ứng dụng seq2seq nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào seq2seq như trong bài Semantic Attention (SA) [35] hay Densecap [35]. Nên nhìn chung là seq2seq là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng [35]." />
<meta property="og:site_name" content="AiFi" />
<meta property="og:image" content="/https://wanted2.github.io/assets/images/san01.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-01T00:00:00+09:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="/https://wanted2.github.io/assets/images/san01.jpg" />
<meta property="twitter:title" content="Seq2Seq và kiến trúc Encoder-Decoder" />
<script type="application/ld+json">
{"description":"Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn, hôm nay đọc tiếp cái sequence-to-sequence (Seq2Seq) cũng thấy có cả 30k-40k cũng có. Thế nên là cái Seq2Seq này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái Seq2Seq này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm). Chứ đọc làm sao mà hết được? Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp. Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. Có bài như bài gốc Transformer (Attention is all you need, [35]) mới ra đời từ 2017 mà đã hơn 35k trích dẫn! Seq2Seq [35, 35, 35] là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, [35]), Question-Answering (QA, [35]), Visual Question Answering (VQA, [35]), Text Summarization (TS, [35, 35]) và Video-To-Text (VTT, [35]). Bài toán Image Captioning thì cũng có thể ứng dụng seq2seq nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào seq2seq như trong bài Semantic Attention (SA) [35] hay Densecap [35]. Nên nhìn chung là seq2seq là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng [35].","image":"/https://wanted2.github.io/assets/images/san01.jpg","headline":"Seq2Seq và kiến trúc Encoder-Decoder","dateModified":"2022-02-01T00:00:00+09:00","datePublished":"2022-02-01T00:00:00+09:00","url":"/https://wanted2.github.io/seq2seq/","mainEntityOfPage":{"@type":"WebPage","@id":"/https://wanted2.github.io/seq2seq/"},"author":{"@type":"Person","name":"tuan"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/https://wanted2.github.io/assets/images/favicon.ico"},"name":"tuan"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="https://wanted2.github.io/assets/css/screen.css" rel="stylesheet">

<link href="https://wanted2.github.io/assets/css/main.css" rel="stylesheet">

<script src="https://wanted2.github.io/assets/js/jquery.min.js"></script>
<script src="https://kit.fontawesome.com/d0b91d895e.js" crossorigin="anonymous"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
    }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" crossorigin="anonymous"></script>
<script src="https://d3js.org/d3.v4.js" crossorigin="anonymous"></script>
<!-- <script src="https://bl.ocks.org/mbostock/raw/4061502/0a200ddf998aa75dfdb1ff32e16b680a15e5cb01/box.js" crossorigin="anonymous"></script> -->
</head>


<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="https://wanted2.github.io/">
    <img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="https://wanted2.github.io/">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/about">About</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/projects">Projects</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-wordpress/"><i class="fab fa-wordpress-simple"></i> WP Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-ghost/"><i class="fab fa-snapchat-ghost"></i> Ghost Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/wowthemesnet/mediumish-theme-jekyll"><i class="fab fa-github"></i> Fork on Github</a>
                </li> -->

                <!-- <script src="https://wanted2.github.io/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="https://wanted2.github.io/assets/js/lunrsearchengine.js"></script> -->

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">AiFi</h1>
    <p class="lead">
        An AI Engineer's blog
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Seq2Seq và kiến trúc Encoder-Decoder&url=https://wanted2.github.io/seq2seq/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=https://wanted2.github.io/seq2seq/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=https://wanted2.github.io/seq2seq/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="mailto:?subject=Seq2Seq và kiến trúc Encoder-Decoder&body=https://wanted2.github.io/seq2seq/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fas fa-envelope"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-2 col-lg-2 text-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="https://wanted2.github.io/assets/images/favicon.png" alt="AiFi">
                        
                    </div>
                    <div class="col-xs-12 col-md-10 col-lg-10 text-right">
                        <a target="_blank" class="link-dark" href="">AiFi</a>
                        <!-- <a target="_blank" href="" class="btn follow">Follow</a> -->
                        <!-- LikeBtn.com BEGIN -->
                        <span class="likebtn-wrapper" 
                            data-site_id="61cfccd36fd08b2d68c1929e"
                            data-theme="custom" 
                            data-icon_l_url="/assets/images/OK_EM.png" 
                            data-icon_l_url_v="/assets/images/OK_EM_clicked.png" 
                            data-identifier="/seq2seq/" 
                            data-show_like_label="false" 
                            data-like_enabled="false" 
                            data-dislike_enabled="false" 
                            data-icon_dislike_show="false" 
                            data-voting_cancelable="false" 
                            data-counter_show="true"
                            data-counter_frmt="comma"></span>
                        <script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
                        <!-- LikeBtn.com END -->
                        <span class="author-description"></span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Seq2Seq và kiến trúc Encoder-Decoder</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid lazyimg" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=" data-src="https://wanted2.github.io/assets/images/san01.jpg" alt="Seq2Seq và kiến trúc Encoder-Decoder">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                    
                    <div class="toc mt-4 mb-4 lead">
                        <h3 class="font-weight-bold">Summary</h3>
                        <ul>
  <li><a href="#giới-thiệu-model-seq2seq">Giới thiệu model Seq2Seq</a>
    <ul>
      <li><a href="#recurrent-neural-nets-long-short-term-memory-và-gated-recurrent-units">Recurrent Neural Nets, Long-Short Term Memory và Gated Recurrent Units</a></li>
      <li><a href="#kiến-trúc-seq2seq-s2s">Kiến trúc Seq2Seq (S2S)</a></li>
      <li><a href="#transformers-và-bert">Transformers và BERT</a>
        <ul>
          <li><a href="#transformer-1">Transformer <a class="citation" href="#vaswani2017attention">[1]</a></a></li>
          <li><a href="#bert">BERT</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#ứng-dụng">Ứng dụng</a>
    <ul>
      <li><a href="#cách-tiếp-cận">Cách tiếp cận</a>
        <ul>
          <li><a href="#các-nguồn-tài-nguyên">Các nguồn tài nguyên</a></li>
          <li><a href="#đôi-lời-về-google-colab">Đôi lời về Google Colab</a>
            <ul>
              <li><a href="#chú-ý-về-train-song-song">Chú ý về train song song</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#neural-machine-translation-nmt">Neural Machine Translation (NMT)</a></li>
      <li><a href="#text-summarization">Text Summarization</a></li>
      <li><a href="#qa-và-vqa">QA và VQA</a></li>
      <li><a href="#video-to-text-và-image-captioning">Video-to-Text và Image Captioning</a></li>
    </ul>
  </li>
  <li><a href="#kết-luận">Kết luận</a></li>
  <li><a href="#tài-liệu-tham-khảo">Tài liệu tham khảo</a></li>
</ul>
                    </div>
                
                <!-- End Toc -->
                <blockquote>
  <p><a href="/aws-account/">Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn</a>, hôm nay đọc tiếp cái <code class="language-plaintext highlighter-rouge">sequence-to-sequence</code> (<code class="language-plaintext highlighter-rouge">Seq2Seq</code>) cũng thấy có cả 30k-40k cũng có.
Thế nên là cái <code class="language-plaintext highlighter-rouge">Seq2Seq</code> này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái <code class="language-plaintext highlighter-rouge">Seq2Seq</code> này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm).
Chứ đọc làm sao mà hết được?
Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp.
Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. 
Có bài như bài gốc Transformer (Attention is all you need, <a class="citation" href="#vaswani2017attention">[1]</a>) mới ra đời từ 2017 mà đã hơn 35k trích dẫn!</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">Seq2Seq</code> <a class="citation" href="#kalchbrenner2013recurrent">[2, 3, 4]</a> là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, <a class="citation" href="#sutskever2014sequence">[3]</a>), Question-Answering (QA, <a class="citation" href="#rajpurkar2016squad">[5]</a>), Visual Question Answering (VQA, <a class="citation" href="#antol2015vqa">[6]</a>), Text Summarization (TS, <a class="citation" href="#rush2015neural">[7, 8]</a>) và Video-To-Text (VTT, <a class="citation" href="#venugopalan2015sequence">[9]</a>).
Bài toán Image Captioning thì cũng có thể ứng dụng <code class="language-plaintext highlighter-rouge">seq2seq</code> nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào <code class="language-plaintext highlighter-rouge">seq2seq</code> như trong bài <em>Semantic Attention (SA) <a class="citation" href="#you2016image">[10]</a></em> hay <em>Densecap <a class="citation" href="#johnson2016densecap">[11]</a></em>.
Nên nhìn chung là <code class="language-plaintext highlighter-rouge">seq2seq</code> là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng <a class="citation" href="#luong2015multi">[12]</a>.
<!--more--></p>

<h1 id="giới-thiệu-model-seq2seq">Giới thiệu model Seq2Seq</h1>

<p>Nỗi lòng người làm thày mà hướng dẫn sinh viên thì tùy level mà kỳ vọng thì nó cũng khác nhau, và với những level cao như postdoc là luôn có sự mong muốn nhất định.
Đó là phải vượt qua <code class="language-plaintext highlighter-rouge">thử thách (challenges)</code> của thày.
Thì cái challenge đây có hai nghĩa là cuộc thi và thử thách, thì nói thực là cuộc thi không cần đâu, chỉ cần vượt qua thử thách tối thiểu của thày trong cái kỹ năng làm nghiên cứu thôi là <code class="language-plaintext highlighter-rouge">ok em</code> rồi.</p>

<p>Trong thực tế kinh nghiệm thì tôi thấy mấy cái thử thách cũng không phức tạp lắm đâu, đặc biệt là trong lĩnh vực NLP+Vision này thì tôi thấy cũng chỉ có vài bài như Image Captioning, Video-to-Text hay VQA.
Code thì nhan nhản trong cộng đồng nghiên cứu (đặc thù của ngạch nghiên cứu là kế thừa, vì người ta công bố mà mình không dùng thì phí), rồi tài liệu thì mảng NLP với Vision các ông cũng publish trên ArXiV với mấy hội nghị open, chứ nào có giấu giếm gì nhau?
Thế mà không hiểu vì sao vẫn không vượt qua được cho thày?
Xem lại rốt cuộc là tôi nghĩ thì có lẽ nên thêm câu hỏi sau vào bộ dữ liệu QA:</p>

<blockquote>
  <p>Q: Không làm muốn có ăn thì ăn gì bây giờ?
A: ??? (câu hỏi tự luận nha)
Context: Hãy xem video của Prof. Huan Rose</p>
</blockquote>

<p>Thì nhìn chung là <code class="language-plaintext highlighter-rouge">seq2seq</code> là một model thử thách như vậy.
Với những bài đặc biệt chỉ của NLP như NMT, Text Summarization hay QA thì <code class="language-plaintext highlighter-rouge">seq2seq</code> đã mở ra hẳn cả một mảng riêng mà về sau còn có thêm mấy cái pretraining encoder/decoder như BERT, AlBERT, RoBERT, BART, … mà chủ yếu là representation learning.</p>

<blockquote>
  <p>Mô hình <code class="language-plaintext highlighter-rouge">seq2seq</code> đơn giản chỉ gồm một chuỗi input $\mathbb{x}_1, \mathbb{x}_2, \ldots, \mathbb{x}_m$. 
Chuỗi này đương nhiên là biểu diễn vector (embedding) sau khi đã preprocessing qua tokenizer và thay thế từ không có trong vocabulảy bởi <code class="language-plaintext highlighter-rouge">UNK</code>.
Sau đó là <strong>encoder</strong> với các trạng thái $\mathbb{h}_1, \mathbb{h}_2, \ldots, \mathbb{h}_m$ cũng như biểu diễn đầu ra của encoder là $\mathbb{z}_1, \mathbb{z}_2, \ldots, \mathbb{z}_m$.
Encoder chính là một mạng trí tuệ nhân tạo dạng Long-Short Term Memory (LSTM) mà chúng ta sẽ nói sau.
Bây giờ thì chúng ta cần aggregate chuỗi biểu diễn đầu ra của encoder thành biến vector \(\mathbb{c}_t=\sum_{i=1}^m\alpha_i^t\mathbb{z}_i\).
Ở mỗi bước \(t\) thì <strong>decoder</strong> LSTM sẽ nhận input là output của bước \(t-1\) là \(\mathbb{g}_{t-1}\) và biến context \(\mathbb{c}_t\) để đưa ra output \(\mathbb{g}_t\).</p>

\[\mathbb{g}_t=\mbox{LSTM}\left(\mathbb{g}_{t-1},\mathbb{c}_t\right)\]

  <p>ứng với mỗi \(\mathbb{g}_t\) sẽ được giải mã thành một ký tự trong ngôn ngữ đầu ra.
Quá trình này kết thúc khi ký tự <code class="language-plaintext highlighter-rouge">EOS</code> được giải mã ra.
Chuỗi đầu vào \(\mathbb{x}_i\) và chuỗi đầu ra của decoder \(\mathbb{g}_j\) có thể có độ dài khác nhau.</p>
</blockquote>

<p>Trong bài toán NMT, thì chuỗi đầu vào có thể là tiếng Anh và chuỗi đầu ra là tiếng Nhật.
Nhưng ngược lại thì sẽ cần tokenizer bằng tiếng Nhật.
Trong bài toán Text Summarization (TS), thì cả đầu ra và đầu vào đều sẽ cùng ngôn ngữ, nhưng đầu ra sẽ là một chuỗi ngắn gọn xúc tích hơn.
Cái gọi là <code class="language-plaintext highlighter-rouge">ngắn gọn, xúc tích hơn</code> sẽ được định nghĩa và học thông qua dữ liệu.
Thì NMT có bộ WMT2014 <a class="citation" href="#bojar2014findings">[13]</a>, WMT2017 <a class="citation" href="#bojar2017findings">[14]</a>, còn TS thì có bộ DUC <a class="citation" href="#paul2004introduc">[15]</a>.
Còn bài toán QA thì chuỗi đầu vào là một câu hỏi còn đầu ra là 1 câu trả lời.
Cái hay của QA là đôi khi có thêm chuỗi context (hay gọi là gợi ý), ví dụ như hỏi <code class="language-plaintext highlighter-rouge">Bạn sống ở đâu?</code> thì có thêm context là <code class="language-plaintext highlighter-rouge">Tôi sống ở Nhật</code> thì máy sẽ trả lời luôn là <code class="language-plaintext highlighter-rouge">Nhật</code>.
QA thì có bộ SQuAD v1 <a class="citation" href="#rajpurkar2016squad">[5]</a> và v2 <a class="citation" href="#rajpurkar2018know">[16]</a> với hơn trăm ngàn bộ câu hỏi (nghe như luyện thì TOEIC).
Metrics đánh giá thì có cái BLUE-4 score, CIDEr, ROGUE là có thể dùng để đánh giá chuỗi đầu ra có phù hợp không.
Chúng ta sẽ đi sâu thêm vào từng chi tiết sau.</p>

<p>Các task trong NLP thì là như vậy, dữ liệu, code và metrics hầu như có sẵn.
Thì cũng là kết nối với Vision là khoảng CVPR tầm 2015-2016 gì đó có mấy cái Workshops nói về làm sao để có thể tích hợp nhiều modal (multi-modal) để đưa ra những giải pháp tốt hơn.
Mọc ra trước mắt lúc đó thì có 3 tasks nằm trong định hướng: Image Captioning, Video-to-Text và VQA.
Nói chung nghe lúc đó có vẻ mới, nhưng chỉ có task là mới thôi chứ còn những cái để thực thi những task ấy các sếp promote các task ấy lên họ đã chuẩn bị hết rồi.
Technique thì có sẵn <code class="language-plaintext highlighter-rouge">seq2seq</code>, CNN, RNN, LSTM, Faster R-CNN để extract attributes dưới dạng object detections, …
Metrics thì vì output vẫn là chuỗi text nên lại xài lại BLUE-4, CIDEr rồi ROGUE thôi.
Nên coi như nền tảng rất sẵn rồi, chỉ nhảy vào làm thí nghiệm và viết paper rồi … ăn!</p>

<p><em>Vậy tại sao vẫn cứ không đến nơi đến chốn được?</em></p>

<ol>
  <li>Tôi nghĩ vấn đề đầu tiên lúc làm Image Captioning là thiếu <code class="language-plaintext highlighter-rouge">alignment</code>.</li>
  <li>Lúc làm cái Video-to-Text thì cái visual semantic embedding (VSE) là không có. Mà nói thẳng ra thì cái ấy chính mình <code class="language-plaintext highlighter-rouge">chủ động</code> nghĩ ra mà làm chứ?</li>
  <li>CÒn cái VQA thì lúc ấy nói thật là hai cái captioning với VTT nó đã <code class="language-plaintext highlighter-rouge">bết bát</code> sẵn rồi thì sẽ rất khó vì bản thân VQA tuy chỉ thay cái context là text bởi hình ảnh, nhưng chất lượng detector, rồi alignment mà hai bước trên chưa làm tốt thì sang đến VQA coi như … vỡ trận.</li>
</ol>

<p>Tuy nhiên, nếu ở vào vai trò anh Postdoc mà làm cái mảng này tôi vẫn sẽ đề xuất luồng làm việc <code class="language-plaintext highlighter-rouge">Image Captioning --&gt; Video-to-Text --&gt; VQA</code>.
Bởi luồng làm việc này nó theo chiều hướng tích lùy dần know-how để làm việc ngày càng tốt hơn.
Thứ hai, là vì nó có một vài điểm trigger giữa chừng nên nếu làm postdoc các bạn có thể submit bài báo tại các thời điểm ấy như là làm Image Captioning xong thì 1 bài, …
<strong>Nhưng rốt cuộc cái quan trọng nhất vẫn là phải có làm có ăn.</strong>
Còn không muốn làm thì hỏi giáo sư Huấn để biết phải làm gì.</p>

<h2 id="recurrent-neural-nets-long-short-term-memory-và-gated-recurrent-units">Recurrent Neural Nets, Long-Short Term Memory và Gated Recurrent Units</h2>

<p>Thôi nói chung là cái trường hợp postdoc ở trên là một ví dụ điển hình thôi, mà trường hợp ấy tôi nghĩ đã được giáo sư Huấn chỉ bảo tận tình rồi nên không cần lo lắng nữa.
Chúng ta quay lại với chủ để chính của hôm nay là giới thiệu về <code class="language-plaintext highlighter-rouge">seq2seq</code>.
Để implement được <code class="language-plaintext highlighter-rouge">seq2seq</code> thì chúng ta cần 1 mô hình nhận chuỗi và output đầu ra cũng là một chuỗi khác, và quan trọng hơn là có thể huấn luyện bằng thuật toán Gradient Descent, cụ thể hơn là <em>Stochastic Gradient Descent (SGD, <a class="citation" href="#bottou2007tradeoffs">[17]</a>)</em>.
Thì cái học củ SGD là mình chỉ random sample một phần của dữ liệu học để tính được gradient và update weight của model.
Chúng ta sẽ có <em>learning rate</em> mà nhỏ thì chậm, còn cao quá thì mô hình sẽ khó hội tụ.</p>

<p>Lời giải thì đơn giản nhất là có mô hình <em>Recurrent Neural Nets (RNN, <a class="citation" href="#rumelhart1986learning">[18]</a>, <a class="citation" href="#goodfellow2016deep">[19]</a>)</em> mà mô hình và công thức forward pass như bên dưới:</p>

<p><img src="/assets/images/rnn.svg" alt="rnn" /></p>

<p>Công thức rất rõ ràng nên có thể sử dụng <code class="language-plaintext highlighter-rouge">Numpy</code> để implement forward pass khá đơn giản với vài dòng code chơi thôi.
Cái khó khăn là khi đã tính xong kết quả các biến \(u_t, o_t, h_t, x_t\) và hàm loss \(\mathcal{L}\) thì <strong>làm sao update được các weight \(\mathbf{V}, \mathbf{W}, \mathbf{b}, \mathbf{c}\)</strong>?</p>

<p>Thì có hệ thống các công thức phía dưới:</p>

<p><img src="/assets/images/bptt.svg" alt="bptt" /></p>

<p>Tuy hệ thống công thức này phức tạp hơn, nhưng vẫn có thể implement bằng <code class="language-plaintext highlighter-rouge">Numpy</code> và thực hiện train với SGD (tầm khoảng vài chục dòng code nữa).
Nhìn lại thì công thức và cả phương thức implement RNN là cũng rất <strong>rõ ràng rồi</strong>, vì vậy, nếu không thể implement được thì … thày cũng chịu không thể giải thích tại sao được?</p>

<p>Nhược điểm của RNN là vấn đề ghi nhớ <em>long-term dependencies</em>: ví dụ chúng ta có 1 gradient \(\mathbf{g}\) và giả sử là RNN không có non-linear activations nào thì chúng ta có thể giả sử thêm là cứ sau mỗi time step thì Jacobian matrix là \(\mathbf{J}\), vậy sau \(n\) bước thì gradient của chúng ta sẽ biến thành \(\mathbf{J}^n\mathbf{g}\) và nếu nó có một giá trị riêng \(\lambda\) mà giá trị khác \(\pm 1\), thì sẽ dẫn đến vấn đề là gradient triệt tiêu hoặc tiến ra vô cùng sau hữu hạn bước tính.
Để khắc phục nhược điểm này thì một giải pháp được đưa ra là đưa <em>self-loop</em> vào cùng các <em>non-linear gates</em> như forget gate, output/input gates để control giá trị.
Một kiến trúc đã làm được việc đó là <em>Long-Short Term Memory (LSTM, <a class="citation" href="#hochreiter1997long">[20, 21]</a>)</em>.
Mỗi cổng đều chứa <em>sigmoid</em> activation để đưa giá trị về khoảng \((0,1)\) dẫn đến các giá trị output ở các cổng có thể bị shut-off.</p>

<p><img src="/assets/images/lstm.svg" alt="lstm" /></p>

<p>Việc thêm gates và non-linear activation (<code class="language-plaintext highlighter-rouge">sigmoid</code>) để control giá trị là hợp lý với LSTM.
Tuy nhiên, nếu nhìn vào cái đám công thức thì trong LSTM khi update và forget của \(\mathbf{h}_{t+1}\) là đồng thời thực hiện song song việc forget và quyết định có update hay không.
<strong>Gated Recurrent Unit (GRU, <a class="citation" href="#cho2014properties">[22]</a>)</strong> thực hiện chia ra, tức là thay vì 1 cổng là làm luôn cả hai chức năng, thì tạo 2 cổng <code class="language-plaintext highlighter-rouge">update</code> và <code class="language-plaintext highlighter-rouge">reset</code> cho từng mục đích ấy.</p>

<blockquote>
  <p>Nhìn chung, công thức toán của các kiến trúc RNN/LSTM/GRU đều không phức tạp.
Nếu có thời gian, các bạn có thể tự thực hiện implement các kiến t
rúc trên bằng <code class="language-plaintext highlighter-rouge">numpy</code>.
Forward pass thì chắc là đơn giản hơn, tuy nhiên backward pass với SGD thì sẽ đòi hỏi một chút công sức (vài chục dòng code nữa).</p>
</blockquote>

<p>Nói một chút về hàm loss \(\mathcal{L}\) khi train NMT chẳng hạn, vì chúng ta biết chuỗi <strong>đúng</strong> \(\mathbf{y}=(y_1,y_2,\ldots,y_n)\) nên ở mỗi step \(0\leq t\leq n-1\), ta sẽ xem xét vector output \(\mathbf{u}_t\) (là softmax) và lấy xác suất của vị trí thứ \(y_t\), và tất nhiên lấy log nghịch đảo như mọi khi:</p>

\[\mathcal{L}=\sum_t-\log u_{y_t}\]

<p>Khi testing, chúng ta sẽ sử dụng <strong>beam search</strong> để tìm ra các chuỗi phù hợp (sẽ giải thích thêm ở phần sau).</p>

<h2 id="kiến-trúc-seq2seq-s2s">Kiến trúc Seq2Seq (S2S)</h2>

<p><img src="/assets/images/san01.png" alt="seq2seq" /></p>

<p>Training <code class="language-plaintext highlighter-rouge">seq2seq</code> đòi hỏi cần có GPU phù hợp và tận dụng xử lý song song.
1 đặc điểm không thể tránh khỏi của bộ dữ liệu là độ dài các chuỗi không đồng bộ, nên có thể tạo ra giải pháp là <strong>fill thêm ký tự trống vào để cho tất cả các chuỗi cùng độ dài</strong>.
Việc này sẽ đòi hỏi thêm khá nhiều memory, vì vậy, một giải pháp khá phù hợp là chia ra thành các <code class="language-plaintext highlighter-rouge">maxi-batches</code> rồi trong từng <code class="language-plaintext highlighter-rouge">maxi-batch</code> thì lại chia tiếp thành các <code class="language-plaintext highlighter-rouge">mini-batch</code>.
Trong mỗi <code class="language-plaintext highlighter-rouge">mini-batch</code> như hình dưới thì mới thực hiện fill ký tự trống.</p>

<p><img src="/assets/images/rnn-mini-batches.png" alt="rnn-mini-batches" /></p>

<p>Training <code class="language-plaintext highlighter-rouge">seq2seq</code> thường mất 5-15 epochs (1 lượt qua toàn bộ corpus).
Bạn cũng nên thiết lập một tiêu chí dừng lại khi validation error rate không cải tiến thêm.
Training lâu thêm không những không tạo thêm cải tiến mà có thể dẫn tới overfitting.</p>

<p><strong>Testing <code class="language-plaintext highlighter-rouge">seq2seq</code></strong>. Giả sử chúng ta có một corpus 50,000 từ.
Vậy trong vector \(\mathbf{u}_t\in\mathbb{R}^{50,000}\), ta sẽ chọn phần tử có xác suất cao nhất (dựa trên embedding logits) để output ra ở bước thứ \(t\).
Giải pháp này tức là <strong>chỉ chọn cái tốt nhất ở mỗi bước</strong>.
Vấn đề ở đây là nếu chỉ chọn cái tốt nhất độc lập ở từng bước thì khi đến một thời điểm \(t'\), câu dịch trở lên không đúng thì không thể quay lại sửa từ đầu được.
Do đó, cách tốt hơn là dùng <strong>Beam Search</strong>:</p>

<ul>
  <li>Ở bước \(t=0\) chúng ta chọn một beam gồm \(n\) top words \(p(w_0^1), p(w_0^2), \ldots, p(w_0^n)\).</li>
  <li>Ở thời điểm tiếp theo \(t=1\), ta lại chọn tiếp \(n\) top words \(p(w_1^1), p(w_1^2), \ldots, p(w_1^n)\) và làm tiếp:
    <ul>
      <li>nhân xác xuất của cụm 2 từ liên tiếp \(p(w_0^{i_0})p(w_1^{i_1}), 1\leq i_0, i_1\leq n\).</li>
      <li>Chọn top \(n\) cặp từ \((i_0,i_1)\) giữ lại.</li>
    </ul>
  </li>
  <li>Ở thời điểm \(t=2\), ta lại chọn tiếp top \(n\) words \(p(w_2^1), p(w_2^2), \ldots, p(w_2^n)\), và lại nhân xác suất để tìm ra top \(n\) triplets \((i_0, i_1, i_2)\) để giữ lại trong beam.</li>
  <li>Cứ tiếp tục như vậy đến khi gặp <code class="language-plaintext highlighter-rouge">EOS</code> thì bỏ câu dịch đó ra khỏi beam và giảm size của beam đi 1.</li>
  <li>Khi kích thước của beam giảm xuống còn 0 thì dừng beam search.</li>
</ul>

<p>Một đoạn code demo của <code class="language-plaintext highlighter-rouge">beam search</code>:</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/6397ad4648f7c891ccd41513bb8206e5.js"> </script>

<p>Ngoài ra, nếu có thời gian các bạn có thể tìm hiểu thêm các thủ thuật để tăng hiệu suất khi inference như fusion (ensembling), reranking, hoặc khi train như tăng kích thước vocabularies, training data, back translation, round-trip training, guided alignment training, .v.v… (<a class="citation" href="#koehn2009statistical">[23]</a>).</p>

<blockquote>
  <p>Tất nhiên, cũng cần chú ý là implement những cái này ở local cũng chỉ để học hỏi thôi nhé.
<strong>Chứ còn triển khai vận hành thực sự thì nó có những solutions có sẵn chạy ầm ầm trên AWS/Azure/GCP rồi.</strong>
Mà có khi nó còn thành dịch vụ đem bán khắp nơi rồi ấy chứ (có cả tiếng Việt luôn nhé).</p>
</blockquote>

<h2 id="transformers-và-bert">Transformers và BERT</h2>

<h3 id="transformer-1">Transformer <a class="citation" href="#vaswani2017attention">[1]</a></h3>

<p><img src="/assets/images/transformer.png" style="float: right; margin: 10px; width: 25%;" />
Deep Networks nhưng không có RNNs hay CNNs gì cả!
Transformer cũng có thể coi là một dạng <code class="language-plaintext highlighter-rouge">seq2seq</code> nhưng không chỉ gồm các tầng FC, embedding, …
Điểm khác biệt là context (hay là attention) đã được chuyển thành tầng đề xuất <code class="language-plaintext highlighter-rouge">self-attention</code>.
Nhìn chung, Transformer hay cả các kiến trúc BERT về sau cũng kế thừa tương đối nhiều tính chất của <code class="language-plaintext highlighter-rouge">seq2seq</code>.
Bản thân Transformer cũng là <code class="language-plaintext highlighter-rouge">seq2seq</code> nhưng bỏ đi RNN/CNN.
<img src="/assets/images/mha.png" style="float: left; margin: 10px; width: 15%;" />
Trong kiến trúc thì ngoài residual connection chỉ có điểm đáng chú ý là <code class="language-plaintext highlighter-rouge">multi-head attention</code>.
Như hình vẽ bên trái, có 3 vector mới được đưa ra là \(K, V, Q\), tương ứng cho <code class="language-plaintext highlighter-rouge">keys, values, queries</code>.
Thì công thức attention (không có mask) sẽ như sau:</p>

\[\mbox{Attention}\left(Q,K,V\right)=\mbox{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V\]

<p>trong đó, \(d_k\) là số chiều của vector query \(Q\) và key \(K\).
Trong trường hợp dùng mask (ở decoder chả hạn) thì chủ yếu là để kết hợp với position embedding để hạn chế ảnh hưởng của các token trước thời điểm $t$ chả hạn.</p>

<p>Nói chung lý thuyết của Transformer cũng không có gì quá phức tạp.
Bản thân Transformer cũng đã được implement chi tiết trong thư viện <code class="language-plaintext highlighter-rouge">tensor2tensor</code> <a class="citation" href="#vaswani2018tensor2tensor">[24]</a> nên như tôi đã nói, cái mảng này thực ra tài liệu tài nguyên thì vô cùng dồi dào.
Ngoài ra một thư viện nữa là HuggingFace <a class="citation" href="#wolf2020transformers">[25]</a> với model zoo dồi dào cũng rất đáng xem.
<strong>Thế là lại nói lại câu chuyện postdoc ở trên: <em>không hiểu tại sao lại không làm được?</em></strong></p>

<blockquote>
  <p>Thì tôi nghĩ là ngoài 3 yếu tố đã nói ở mục đầu, thì quá tập trung vào competition! 
Từ <code class="language-plaintext highlighter-rouge">challenge</code> mà người thày nói cũng có nghĩa là cuộc thi (competition) nhưng cũng có nghĩa là thử thách.
Mà vấn đề là cái nghĩa sau nó sẽ quan trọng hơn.
Bởi mảng nghiên cứu là giao thoa giữa Vision và NLP, nên là nếu chỉ tập trung vào 1 competition nhất định của vision mà không cập nhật bên NLP thì có vẻ là tầng nghĩa sau (thử thách) là sẽ bị lose track.
Đấy thế nên là cũng nhắc lại lời người thày: “<strong>Thực ra nhiệm vụ của người làm thày là giảm bớt số lượng những con bò trên thế giới xuống</strong>”.
Tôi nghĩ đúng là để biến một con bò thành người cũng khá vất vả, mà thất bại mãi mới thành công một phát cũng là chuyện thường!</p>
</blockquote>

<blockquote>
  <p>Chốt là cuộc thi, competition là không quan trọng đâu.
Cái quan trọng với postdoc là rèn luyện được một cái tác phong nghiên cứu tốt để có thể hướng tới lâu dài.
<strong>Chứ mất công giành được vị trí trong 1 competition trước mắt, nhưng lại mất vị trí tenure cả đời thì luận về TRÍ là thua rồi!</strong>
Đã xác định gắn bó lâu dài thì không cần thi cử gì cho mất thời gian ra, lo mà tìm ra cống hiến để đời và gắn bó lâu dài thì hơn.</p>
</blockquote>

<p>Mà nhìn lại tôi nghĩ nguyên nhân sâu xa dẫn tới sự vụ hậu quả postdoc kể trên là <strong>thiếu kỹ năng quản lý dự án (Project Management)</strong>.
Triệu chứng bệnh rõ ràng nhất ở đây là</p>
<ul>
  <li>Việc không quản lý được các tri thức về thử thách, dẫn đến bị đánh lạc hướng, sa đà vào các competition. Đây chính là triệu chứng bệnh <strong>thiếu kỹ năng quản lý scope dự án.</strong></li>
  <li>Việc không control được những lời khuyên từ bên NLP chứng tỏ có dấu hiệu <strong>yếu về quản lý stakeholder</strong>.</li>
  <li>Việc ưu tiên competition trước việc nhận ra thách thức thực sự của mình chính là <strong>yếu về quản lý action plan, time và schedule</strong>.</li>
</ul>

<p>Nhìn chung việc thiếu kỹ năng quản lý dự án sẽ không bao giờ đưa người postdoc gia nhập nhóm (dưới) 5% thành công được.
<strong>Mà ví dụ khoảng làm postdoc được 2-3 năm mà người thày nhìn vào thấy đủ thứ bệnh, chỗ nào cũng yếu thế này thì rất là khó để không cho … bật bãi!</strong>
Ở Mỹ chả hạn, làm nghiên cứu muốn tồn tại lâu dài phải có kỹ năng quản lý dự án nghiên cứu. Thế mà chạy postdoc 2 năm mà không ngộ ra chân lý, không thể hiện tiềm năng làm quản lý lãnh đạo, thì không bật bãi chắc chỉ có ở thiên đường thôi!</p>

<h3 id="bert">BERT</h3>
<p><strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers (<strong>BERT</strong>, <a class="citation" href="#devlin2019bert">[26]</a>) lại là 1 kiến trúc mới gần đây (từ 2018?).
Pretraining kiến trúc trên 1 task unsupervised (kiểu tự học), rồi dùng lại kiến trúc đó trong 1 task supervised khác là tư tưởng chính của BERT.
Các phiên bản cải tiến của BERT có thể kể đến AlBERT <a class="citation" href="#lan2019albert">[27]</a>, RoBERTa <a class="citation" href="#liu2019roberta">[28]</a>, BART <a class="citation" href="#lewis2020bart">[29]</a>, XlNet <a class="citation" href="#yang2019xlnet">[30]</a>, XLM <a class="citation" href="#conneau2019cross">[31]</a> hay gần đây là ELECTRA <a class="citation" href="#clark2020electra">[32]</a> đều kế thừa tinh thần này.
Trong quá khứ thì việc sử dụng pretraining để tạo ra một xuất phát điểm tốt hơn cho model là ý tưởng đã được khai thác <a class="citation" href="#hinton2006fast">[33]</a>.
Ví dụ như Hinton thì đã pretrain deep belief nets và gọi là <em>greedy layer-wise unsupervised pretraining</em>.
Thủ thuật này là cần thiết khi train các model lớn:</p>

<blockquote>
  <p>to train the first layer in isolation, then extract all features from the first layer only once, then train the second layer in isolation given those features, and so on. <a class="citation" href="#goodfellow2016deep">[19]</a>.</p>
</blockquote>

<p>Ý tưởng của kiểu pretraining này là ta có một model rất lớn \(A\), việc train \(A\) từ đầu (random weights) sẽ rất khó.
Do vậy, chúng ta sẽ tìm một task unsupervised \(u\) (vì vậy không cần labels), để train \(A\) trước.
Sau đó, khi vào train cho task chính \(T\) (có labels) thì weights của \(A\) đã được khởi tạo bởi việc học \(u\) sẽ khiến training \(A\) cho \(T\) trở nên nhanh chóng hơn.
Trên thực tế có khá nhiều task bên computer vision đã sử dụng ý tưởng này và đưa ra khá nhiều kết quả thuyết phục.</p>

<p>Với bên NLP, thì BERT lựa chọn task pretraining (gọi là <em>pretext</em>) là masked language modeling (MLM).
Ví dụ như trong câu</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>Original: I have a pen &lt;EOS&gt;
Masked: I &lt;MASK&gt; a pen &lt;EOS&gt;
</pre></td></tr></tbody></table></code></pre></div></div>
<p>thì từ câu <code class="language-plaintext highlighter-rouge">Masked</code>, nhiệm vụ của pretext là phải <em>phục hồi</em> lại các từ bị <code class="language-plaintext highlighter-rouge">&lt;MASK&gt;</code>.
Nguyên tắc pretext này nhìn chung giống với <em>denoising auto-encoders</em> <a class="citation" href="#goodfellow2016deep">[19]</a>.
Vì việc tạo bộ dữ liệu masking là dễ dàng và có thể tự làm được (viết script để ngẫu nhiên thay 1 token bởi <code class="language-plaintext highlighter-rouge">&lt;MASK&gt;</code>) nên task này có thể xếp vào hạng mục <strong>unsupervised learning</strong> hoặc <strong>self-supervised learning (SSL)</strong>.
Lợi thế là người train có thể tạo ra bộ dữ liệu lớn tự động để máy học pretext mà không cần gắn nhãn!
Một pretext khác là <em>đoán câu tiếp theo (Next Sentence Prediction, NSP)</em>.
Ví dụ:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>I told the principal that I would like to revolutionize this university if he give me something to do.
Next Sentence: He sent me to the Psychology department.
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Trong kết quả của BERT thì có vẻ pretext NSP giúp cải thiện độ chính xác của task QA.
Tất nhiên, ngoài dùng pretext thì còn những kiểu pretraining khác như sử dụng feature đã pretrain (ELMo, <a class="citation" href="#peters2018deep">[34]</a>) hoặc fine-tune toàn bộ pretrain parameters (OpenAI GPT, <a class="citation" href="#radford2018improving">[35]</a>).
Gần đây có model BART <a class="citation" href="#lewis2020bart">[29]</a> tích hợp cả BERT và GPT để tạo ra một cơ chế pretext có thể mô phỏng hàm noise bất kỳ.</p>

<p>Hầu hết những phương pháp kể trên đều có thể tìm thấy tại thư viện model HuggingFace <a class="citation" href="#wolf2020transformers">[25]</a>.</p>

<h1 id="ứng-dụng">Ứng dụng</h1>

<h2 id="cách-tiếp-cận">Cách tiếp cận</h2>
<h3 id="các-nguồn-tài-nguyên">Các nguồn tài nguyên</h3>
<p>Như đã nói ở trên, mảng này thì code kiếc, tài liệu, tài nguyên thì vô cùng <strong>sẵn</strong> có và dồi dào.
Thế nên thôi mình cứ <strong>ăn “sẵn”</strong> đi cho nó nhanh chứ nấu làm gì mất thời gian.</p>

<blockquote>
  <p>Nó cũng kiểu như đồ ăn Tết ấy mà: 
Hì hục nấu mất cả buổi mà ăn thì chắc được mấy miếng là chán.</p>
</blockquote>

<p>Thế nên tôi mới bảo rồi, những cái này mà mở khóa học truyền bá tri thức phổ cập thì <strong>liệu có khách không? ai học cho mà dạy?</strong></p>
<blockquote>
  <p>Tôi nghĩ chả ai học đâu, giờ người ta ăn sẵn hết.
Có dạy “nấu” thì cũng phải cái gì mà nó kiểu sẵn sẵn mà nấu nhanh ăn luôn.</p>
</blockquote>

<p>Thì tại sao nói mảng này đồ ăn sẵn nhiều thì dưới đây là một số nguồn mà các bạn có thể tận dụng:</p>

<ul>
  <li><strong>Code</strong> thì HuggingFace, tensorflow, Github</li>
  <li><strong>Model Zoo</strong> thì HuggingFace thôi.
    <ul>
      <li><a href="https://huggingface.co/">https://huggingface.co/</a></li>
      <li>Cứ tải về mà hì hục “nấu”.</li>
      <li>Nấu xong chắc cũng chỉ chạm đũa vài miếng là ngấy thôi nhưng nếu thích nấu thì nấu thôi.</li>
    </ul>
  </li>
  <li><strong>Tài liệu</strong> tutorials thì cứ search YouTube là ra hết mà.
    <ul>
      <li><a href="https://www.youtube.com/c/HuggingFace">Kênh hướng dẫn của HuggingFace</a></li>
      <li><a href="https://www.youtube.com/watch?v=G5lmya6eKtc">The Future of Natural Language Processing</a></li>
    </ul>
  </li>
</ul>

<iframe width="100%" height="315" src="https://www.youtube.com/embed/G5lmya6eKtc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<ul>
  <li>Nguồn <strong>tài nguyên tính toán</strong>:
    <ul>
      <li>Đầu tiên cũng chả cần mua GPU/TPU riêng đâu.</li>
      <li>Để làm demo bạn cứ xài tạm Colab ấy.
        <ul>
          <li><a href="https://colab.research.google.com/">https://colab.research.google.com/</a></li>
          <li><a href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL">Một playlist hướng dẫn dùng Colab với Tensorflow</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="đôi-lời-về-google-colab">Đôi lời về Google Colab</h3>

<blockquote>
  <p><a href="https://research.google.com/colaboratory/faq.html">FAQ</a> <strong>What is Colabotory?</strong></p>
</blockquote>

<blockquote>
  <p>Colaboratory, or “Colab” for short, is a product from Google Research. Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning, data analysis and education. More technically, Colab is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources including GPUs.</p>
</blockquote>

<p>Google Colab nhìn chung là 1 dịch vụ Notebook cung cấp sẵn môi trường để chạy các tác vụ liên quan tới machine learning.
Với Colab, bạn có thể định nghĩa form các biến chương trình như hình bên dưới, rồi tạo lập mạng trí tuệ nhân tạo, thiết lập môi trường GPU/TPU để training hoặc inference.</p>

<p><img src="/assets/images/nmt-colab-01.png" alt="colab form" /></p>

<p>Sau khi đã làm quen với môi trường Google Colab thì bạn đã all-set to go!</p>

<h4 id="chú-ý-về-train-song-song">Chú ý về train song song</h4>
<p>Một chú ý nhỏ mang tính hô trợ trong quản lý time thôi là nếu bạn chọn Colab Free plan thì GPU/TPU tốc độ khá thấp.
Ngoài ra, bộ dữ liệu hạng vừa như WMT 2014 thì cũng có 4,468,840 cặp câu dịch (bộ training set).
Mà Colab Free thì tốc độ training sẽ rơi vào tầm 18-20 FPS với model AlBERT, như vậy để train hết 1 epoch với WMT sẽ mất tầm 62-70 (h), tức là khoảng 3 ngày/epoch (liên tục).
Mà để train model NMT thì sẽ mất 5-15 epochs nên sẽ phải mất tầm <strong>15-45 ngày train liên tục</strong>.
Tuy nhiên, Colab Free có giới hạn là 1 session chỉ được 12h liên tục, nên bạn sẽ phải break ra tầm 30-90 sessions.
Ngoài ra đóng browser là mất luôn đấy chứ nó lại không chạy ở background đâu.</p>

<p>Bài viết này chỉ sử dụng có sẵn với viết script để demo inference hoặc show training vài step nó như thế nào thôi, nên chúng tôi cũng chỉ cần Colab Free là đủ.
Tuy nhiên, nếu mà các bạn định làm nghiêm túc kiểu hì hục nấu cả buổi thì tôi nghĩ là nếu dùng GPU thì dưới 8 GPUs là hì hục lâu phết đấy!
Có mấy giải pháp:</p>

<ul>
  <li><strong>Nâng cấp nên Colab Pro (10USD/tháng) và Pro Plus (50USD/tháng).</strong> Tuy nhiên, dù lên Pro Plus thì 1 session cũng chỉ được 24h liên tục. Được cái là cho phép chạy background nên sẽ dễ thở hơn. Có điều là 24h thì lại phải save ra Google Drive, rồi khởi động lại training <strong>bằng tay</strong>. GPU/TPU sẽ nhanh hơn tầm vài lần nên có thể sẽ chỉ mất vài tuần để train thôi. <strong>Hì hục</strong> nó là thế đấy!</li>
  <li><strong>Tự chế ra 1 hệ thống multi-GPU</strong>: kiếm kinh phí mua tầm 24 cái GPUs P100 thì có khi 1 epoch mất 1h thôi, thì train trong ngày sẽ xong. Giá cả thì <strong>Tesla P100 SXM2 16GB</strong> tầm 10,000 USD/cái, 24 cái thì tầm 240,000 USD thôi! Muốn làm big data mà chỉ code thôi là không ăn thua đâu! Bởi vận hành triển khai mỗi tháng hóa đơn đã vài chục ngàn đô, tiền upfront mua GPU cũng đã 240,000 USD thì code hầu như là phần ít giá trị nhất! Code nói chung … rẻ mạt!</li>
  <li><strong>Thuê EC2</strong>: thì có vài lựa chọn là <strong>on-demand</strong> và <strong>reserved</strong>.
    <ul>
      <li>reserved thì 1 tháng 30 ngày 720 h là mình phải trả hết 720 h. Thì rẻ nhất là <code class="language-plaintext highlighter-rouge">g5.xlarge</code> cũng mỗi giờ 0.63 đô. Như vậy là mỗi tháng \(0.63 \times 720 = 453.6\) USD/tháng. Tuy nhiên, <code class="language-plaintext highlighter-rouge">g5.xlarge</code> chỉ có 1 GPU và tính năng thì xấp xỉ Colab Free, nên giả sử train 15 epochs mất 45 ngày liên tục, thì sẽ tốn là \(0.63 \times 45 \times 24 = 680.4\) USD cho lượt train này.
        <ul>
          <li>Nếu thuê GPU tốt hẳn đi là <code class="language-plaintext highlighter-rouge">g5.48xlarge</code> coi như 8 GPUs thì nhanh hơn tầm 8 lần thì giá reserved sẽ là \(10.26 \times 45 \times 24 /8 = 1385.1\) USD cho lượt train. Tuy nhiên, vì reserved nên mình thuê theo tháng thì ngoài lượt train này, hàng tháng mình tổng phải trả là \(10.26 \times 720 = 7387.2\) USD/tháng. Tức là train được tầm 6 lượt WMT 2014.</li>
        </ul>
      </li>
      <li>Thuê <strong>on-demand</strong> thì mình không trả hàng tháng, dùng phát nào xong trả phát ấy thôi. Thì ví dụ, <code class="language-plaintext highlighter-rouge">g5.48xlarge</code> thì sẽ mất \(16.29\times 45 \times 24 /8 = 2199.5\) USD cho mỗi lượt train này kéo dài tầm gần 1 tuần.</li>
      <li>Thuê <a href="https://aws.amazon.com/ec2/spot/pricing/"><strong>Spot</strong></a> thì có thể thuê loại <code class="language-plaintext highlighter-rouge">g5g.16xlarge</code> thì có 2 Tensor core và mất 1.1112 USD/h. Nhìn chung Spot giá sẽ mềm hơn <strong>on-demand</strong> và cùng hạng với reserved. Tuy nhiên, vì là spot nên lúc nào mà tự dưng bị interupt là phải có kế hoạch ứng phó trước:
        <blockquote>
          <p>Spot Instances are a cost-effective choice if you can be flexible about when your applications run and <strong>if your applications can be interrupted</strong></p>
        </blockquote>
      </li>
    </ul>
  </li>
</ul>

<p><em>Như các bạn đã thấy ở trên: cùng mức GPU <code class="language-plaintext highlighter-rouge">g5.xlarge</code> thì Colab Free còn thuê EC2 mất 453.6 USD. Vậy sự khác biệt là gì? Điểm khác chính yếu là thuê EC2 thì bạn không phải lo cái <strong>session 12 h</strong> (sau 12h phải khởi động lại bằng tay và save dữ liệu vào Google Drive).</em>
<strong>Tức là chỉ mỗi cái limit 12h ấy thôi đã trị giá 453.6 USD rồi.</strong></p>

<p>Một điểm đáng lưu ý thứ 2 là giới R&amp;D cạnh tranh khá khốc liệt, nên trên thứ vô thưởng vô phạt kiểu này thì kéo dài 45 ngày và free cũng được.
Nhưng cái gì mà có giá trị kinh tế một chút là cạnh tranh ác liệt: <strong>train là phải xong trong ngày, chứ đợi cả tháng sau có đứa nó publish mất thì sao?</strong>
Thế nên tôi nghĩ nếu thực sự phải cạnh tranh thì chắc lại 240,000 USD hoặc <code class="language-plaintext highlighter-rouge">g5.48xlarge</code> thôi.
Và nếu chọn con đường cạnh tranh này thì các tập đoàn lớn, có sẵn hàng chục cái GPU server trong tay muốn huy động lúc nào cũng <code class="language-plaintext highlighter-rouge">ok em</code> thì chắc chắn là thắng thế.
Nên cạnh tranh vào cái con đường kiểu đọ thông số GPU này có lẽ là chỉ dành cho tập đoàn lớn thôi.</p>

<p>Thứ 3 nữa là tự chế 240,000 USD thì mới là DIY và <code class="language-plaintext highlighter-rouge">self-supervised</code> chứ còn thuê với Colab thì đâu còn là <code class="language-plaintext highlighter-rouge">self</code> nữa nên rốt cuộc là chắc cũng quay lại 240,000 USD nếu thực sự là <code class="language-plaintext highlighter-rouge">self</code>.</p>

<h2 id="neural-machine-translation-nmt">Neural Machine Translation (NMT)</h2>

<p>NMT là 1 bài toán NLP khá điển hình. Lần này chúng ta sẽ chạy thử 1 vài mô hình trên HuggingFace để dịch tiếng Đức sang tiếng Anh.
Đầu tiên, là với mớ lý thuyết phía trên, chúng ta sẽ xem thử cách thức fine-tune model AlBERT cho task NMT.
Tôi dùng bộ dữ liệu WMT 2014 để show:</p>
<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/fab8ab5644bd1a122dd3ad8880d283f9.js"> </script>

<p>Nhìn chung, cứ setup đúng các thông số, tokenizer, hàm loss, optimizer, .v.v… thì hầu như quá trình fine-tune mô hình kiểu BERT diễn ra rất trôi chảy.
Vấn đề tài nguyên để kết thúc quá trình train thì như thảo luận ở trên, các bạn có thể tự lựa chọn giải pháp phù hợp túi tiền.</p>

<p>Nói chung nấu cả buổi nhưng ăn thì nhanh thôi. 
Để demo chức năng dịch Đức-Anh thì tôi xài luôn model có sẵn<a href="https://huggingface.co/google/bert2bert_L-24_wmt_de_en"><code class="language-plaintext highlighter-rouge">google/bert2bert_L-24_wmt_de_en</code></a>.</p>

<noscript><pre>400: Invalid request</pre></noscript>
<script src="https://gist.github.com/88d0506002341d151a8e5ad745f3364f.js"> </script>

<p>Tôi cũng sử dụng <code class="language-plaintext highlighter-rouge">sacrebleu</code> để show được điểm số BLEU của vài câu dịch.</p>

<h2 id="text-summarization">Text Summarization</h2>

<h2 id="qa-và-vqa">QA và VQA</h2>

<h2 id="video-to-text-và-image-captioning">Video-to-Text và Image Captioning</h2>

<h1 id="kết-luận">Kết luận</h1>

<h1 id="tài-liệu-tham-khảo">Tài liệu tham khảo</h1>

<ol class="bibliography"><li><span id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. 2017. Attention is all you need. <i>Advances in neural information processing systems</i> (2017), 5998–6008.</span><a class="details" href="https://wanted2.github.io/bibliography/vaswani2017attention/">Details</a></li>
<li><span id="kalchbrenner2013recurrent">Kalchbrenner, N. and Blunsom, P. 2013. Recurrent continuous translation models. <i>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA. Association for Computational Linguistics</i> (2013).</span><a class="details" href="https://wanted2.github.io/bibliography/kalchbrenner2013recurrent/">Details</a></li>
<li><span id="sutskever2014sequence">Sutskever, I., Vinyals, O. and Le, Q.V.V. 2014. Sequence to sequence learning with neural networks. <i>Advances in Neural Information Processing Systems</i> (2014), 3104–3112.</span><a class="details" href="https://wanted2.github.io/bibliography/sutskever2014sequence/">Details</a></li>
<li><span id="cho2014learning">Cho, K., Merrienboer, B. van, Gulcehre, C., Bougares, F., Schwenk, H. and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. <i>arXiv preprint arXiv:1406.1078</i>. (2014).</span><a class="details" href="https://wanted2.github.io/bibliography/cho2014learning/">Details</a></li>
<li><span id="rajpurkar2016squad">Rajpurkar, P., Zhang, J., Lopyrev, K. and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. <i>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</i> (2016), 2383–2392.</span><a class="details" href="https://wanted2.github.io/bibliography/rajpurkar2016squad/">Details</a></li>
<li><span id="antol2015vqa">Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L. and Parikh, D. 2015. Vqa: Visual question answering. <i>Proceedings of the IEEE international conference on computer vision</i> (2015), 2425–2433.</span><a class="details" href="https://wanted2.github.io/bibliography/antol2015vqa/">Details</a></li>
<li><span id="rush2015neural">Rush, A.M., Chopra, S. and Weston, J. 2015. A Neural Attention Model for Abstractive Sentence Summarization. <i>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</i> (2015), 379–389.</span><a class="details" href="https://wanted2.github.io/bibliography/rush2015neural/">Details</a></li>
<li><span id="ranzato2015sequence">Ranzato, M.A., Chopra, S., Auli, M. and Zaremba, W. 2015. Sequence level training with recurrent neural networks. <i>arXiv preprint arXiv:1511.06732</i>. (2015).</span><a class="details" href="https://wanted2.github.io/bibliography/ranzato2015sequence/">Details</a></li>
<li><span id="venugopalan2015sequence">Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T. and Saenko, K. 2015. Sequence to sequence-video to text. <i>Proceedings of the IEEE international conference on computer vision</i> (2015), 4534–4542.</span><a class="details" href="https://wanted2.github.io/bibliography/venugopalan2015sequence/">Details</a></li>
<li><span id="you2016image">You, Q., Jin, H., Wang, Z., Fang, C. and Luo, J. 2016. Image captioning with semantic attention. <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (2016), 4651–4659.</span><a class="details" href="https://wanted2.github.io/bibliography/you2016image/">Details</a></li>
<li><span id="johnson2016densecap">Johnson, J., Karpathy, A. and Fei-Fei, L. 2016. Densecap: Fully convolutional localization networks for dense captioning. <i>Proceedings of the IEEE conference on computer vision and pattern recognition</i> (2016), 4565–4574.</span><a class="details" href="https://wanted2.github.io/bibliography/johnson2016densecap/">Details</a></li>
<li><span id="luong2015multi">Luong, M.-T., Le, Q.V., Sutskever, I., Vinyals, O. and Kaiser, L. 2015. Multi-task sequence to sequence learning. <i>arXiv preprint arXiv:1511.06114</i>. (2015).</span><a class="details" href="https://wanted2.github.io/bibliography/luong2015multi/">Details</a></li>
<li><span id="bojar2014findings">Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H. and others 2014. Findings of the 2014 workshop on statistical machine translation. <i>Proceedings of the ninth workshop on statistical machine translation</i> (2014), 12–58.</span><a class="details" href="https://wanted2.github.io/bibliography/bojar2014findings/">Details</a></li>
<li><span id="bojar2017findings">Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., Huck, M., Koehn, P., Liu, Q., Logacheva, V. and others 2017. Findings of the 2017 conference on machine translation (wmt17). <i>Proceedings of the Second Conference on Machine Translation</i> (2017), 169–214.</span><a class="details" href="https://wanted2.github.io/bibliography/bojar2017findings/">Details</a></li>
<li><span id="paul2004introduc">Over, P. and Yen, J. 2014. An Introduction to DUC-2004. https://duc.nist.gov/pubs/2004slides/duc2004.intro.pdf.</span><a class="details" href="https://wanted2.github.io/bibliography/paul2004introduc/">Details</a></li>
<li><span id="rajpurkar2018know">Rajpurkar, P., Jia, R. and Liang, P. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. <i>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</i> (2018), 784–789.</span><a class="details" href="https://wanted2.github.io/bibliography/rajpurkar2018know/">Details</a></li>
<li><span id="bottou2007tradeoffs">Bottou, L. and Bousquet, O. 2007. The tradeoffs of large scale learning. <i>Advances in neural information processing systems</i>. 20, (2007).</span><a class="details" href="https://wanted2.github.io/bibliography/bottou2007tradeoffs/">Details</a></li>
<li><span id="rumelhart1986learning">Rumelhart, D.E., Hinton, G.E. and Williams, R.J. 1986. Learning representations by back-propagating errors. <i>nature</i>. 323, 6088 (1986), 533–536.</span><a class="details" href="https://wanted2.github.io/bibliography/rumelhart1986learning/">Details</a></li>
<li><span id="goodfellow2016deep">Goodfellow, I., Bengio, Y. and Courville, A. 2016. <i>Deep learning</i>. MIT press.</span><a class="details" href="https://wanted2.github.io/bibliography/goodfellow2016deep/">Details</a></li>
<li><span id="hochreiter1997long">Hochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. <i>Neural computation</i>. 9, 8 (1997), 1735–1780.</span><a class="details" href="https://wanted2.github.io/bibliography/hochreiter1997long/">Details</a></li>
<li><span id="gers2000learning">Gers, F.A., Schmidhuber, J. and Cummins, F. 2000. Learning to forget: Continual prediction with LSTM. <i>Neural computation</i>. 12, 10 (2000), 2451–2471.</span><a class="details" href="https://wanted2.github.io/bibliography/gers2000learning/">Details</a></li>
<li><span id="cho2014properties">Cho, K., Merrienboer, B. van, Bahdanau, D. and Bengio, Y. 2014. On the properties of neural machine translation: Encoder-decoder approaches. <i>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8), 2014</i> (2014).</span><a class="details" href="https://wanted2.github.io/bibliography/cho2014properties/">Details</a></li>
<li><span id="koehn2009statistical">Koehn, P. 2009. Neural Machine Translation. <i>Statistical Machine Translation</i>. Cambridge University Press.</span><a class="details" href="https://wanted2.github.io/bibliography/koehn2009statistical/">Details</a></li>
<li><span id="vaswani2018tensor2tensor">Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A.N., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner, N., Parmar, N. and others 2018. Tensor2tensor for neural machine translation. <i>arXiv preprint arXiv:1803.07416</i>. (2018).</span><a class="details" href="https://wanted2.github.io/bibliography/vaswani2018tensor2tensor/">Details</a></li>
<li><span id="wolf2020transformers">Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi, A., Cistac, P., Funtowicz, M., Davison, J., Shleifer, S. and others 2020. Transformers: State-of-the-art natural language processing. <i>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</i> (2020), 38–45.</span><a class="details" href="https://wanted2.github.io/bibliography/wolf2020transformers/">Details</a></li>
<li><span id="devlin2019bert">Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i> (2019), 4171–4186.</span><a class="details" href="https://wanted2.github.io/bibliography/devlin2019bert/">Details</a></li>
<li><span id="lan2019albert">Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P. and Soricut, R. 2019. Albert: A lite bert for self-supervised learning of language representations. <i>arXiv preprint arXiv:1909.11942</i>. (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/lan2019albert/">Details</a></li>
<li><span id="liu2019roberta">Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. <i>arXiv preprint arXiv:1907.11692</i>. (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/liu2019roberta/">Details</a></li>
<li><span id="lewis2020bart">Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V. and Zettlemoyer, L. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. <i>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</i> (2020), 7871–7880.</span><a class="details" href="https://wanted2.github.io/bibliography/lewis2020bart/">Details</a></li>
<li><span id="yang2019xlnet">Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R. and Le, Q.V. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. <i>Advances in neural information processing systems</i>. 32, (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/yang2019xlnet/">Details</a></li>
<li><span id="conneau2019cross">Conneau, A. and Lample, G. 2019. Cross-lingual language model pretraining. <i>Advances in Neural Information Processing Systems</i>. 32, (2019), 7059–7069.</span><a class="details" href="https://wanted2.github.io/bibliography/conneau2019cross/">Details</a></li>
<li><span id="clark2020electra">Clark, K., Luong, M.-T., Le, Q.V. and Manning, C.D. 2020. Electra: Pre-training text encoders as discriminators rather than generators. <i>arXiv preprint arXiv:2003.10555</i>. (2020).</span><a class="details" href="https://wanted2.github.io/bibliography/clark2020electra/">Details</a></li>
<li><span id="hinton2006fast">Hinton, G.E., Osindero, S. and Teh, Y.-W. 2006. A fast learning algorithm for deep belief nets. <i>Neural computation</i>. 18, 7 (2006), 1527–1554.</span><a class="details" href="https://wanted2.github.io/bibliography/hinton2006fast/">Details</a></li>
<li><span id="peters2018deep">Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K. and Zettlemoyer, L. 2018. Deep Contextualized Word Representations. <i>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</i> (New Orleans, Louisiana, Jun. 2018), 2227–2237.</span><a class="details" href="https://wanted2.github.io/bibliography/peters2018deep/">Details</a></li>
<li><span id="radford2018improving">Radford, A., Narasimhan, K., Salimans, T. and Sutskever, I. 2018. <i>Improving language understanding by generative pre-training</i>. OpenAI.</span><a class="details" href="https://wanted2.github.io/bibliography/radford2018improving/">Details</a></li></ol>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2022-02-01">01 Feb 2022</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Encoder-Decoder">#Encoder-Decoder</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Long-Short-Term-Memory">#Long-Short Term Memory</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Machine-learning">#Machine learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Neural-Machine-Translation">#Neural Machine Translation</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Recurrent-Neural-Networks">#Recurrent Neural Networks</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Seq2Seq">#Seq2Seq</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Sequence-to-Sequence">#Sequence-to-Sequence</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="https://wanted2.github.io//ml-ids/"> &laquo; Machine Learning for Network Intrusion Detection: From Local to Production</a>
            
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'caineng'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi" style="max-height: 48px;" /> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://caineng.us20.list-manage.com/subscribe/post?u=76342d3d74a6807aac5aec0d7&id=b5645e19be" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Site-Reliable-Engineering">Site Reliable Engineering (16)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Software-Engineering">Software Engineering (37)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Computer-Vision">Computer Vision (5)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence (18)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語 (36)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Project-Management">Project Management (34)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2022 AiFi 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="https://wanted2.github.io/assets/js/mediumish.js"></script>


<script src="https://wanted2.github.io/assets/js/lazyload.js"></script>


<script src="https://wanted2.github.io/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//caineng.disqus.com/count.js"></script>


</body>
</html>
