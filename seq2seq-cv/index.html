<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="https://wanted2.github.io/assets/images/favicon.ico">

<title>Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices | AiFi</title>

 
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-2CDTCF0EP6" crossorigin="anonymous"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-2CDTCF0EP6');
        </script>
    


<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices | AiFi</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices" />
<meta name="author" content="tuan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự án cần seq2seq. Vision Transformers ViT Về nguyên lý chung của seq2seq thì chúng ta có hai bài viết trước nói khá nhiều rồi12 nên các bạn tham khảo nhé. Nhà cũng đang có cái “eo hẹp” là chỉ được tối đa 7 citations và 9 phút đọc thôi nên các bạn cần thì đọc lại hai bài viết12 để xem thêm kiến thức về attention, Transformer, … Vision Transformer (ViT, [7]) đưa khái niệm seq2seq vào vision. Nếu các bạn đã quen seq2seq thì hiểu ngay là ta cần chuyển hình ảnh thành 1 chuỗi: Source: Google Để làm được việc này, các tác giả đề xuất: Image patches: Hình ảnh được chia ra thành patches và đánh số thự tự để input vào transformer. Position embedding: thứ tự chỉ đơn giản là chuỗi 1D. MLP với GELU activation: MLP sử dụng GELU để kích hoạt. Model không chứa CNN, có tối thiểu 12 tầng, với kích cỡ hidden size từ 768, ngoài ra có 12-16 heads. Large-scale pre-training and fine-tuning: Một điểm đáng chú ý khác là pre-training dạng supervised trên một dataset 300 triệu ảnh tạo ra 1 model rất mạnh. Các tác giả báo cáo cải tiến trên khá nhiều bộ dữ liệu lớn, và ngoài ra cả con số TPUv3-core-days = số lượng TPUv3 cores x sô lượng ngày train. Cái con số TPUv3-core-days thì cứ mỗi thí nghiệm là vài ngàn tới vài chục ngàn, mà mỗi core thì cứ 10$/ngày thì các bạn cứ nhẩm tính xem budget của hội con nhà giàu này đầu tư vào nó lớn cỡ nào đấy. DETR DETR [7, 7] tiếp tục ứng dụng transformer vào Object Detection và Segmentation. Source: [7] Backbone: DETR dùng CNN quen thuộc như resnet-50 hoặc 101. Encoder: DETR dùng \(1\times 1\) convolution để dimention reduction các feature maps rồi input vào. Position embedding là cố định vì transformer là không phụ thuộc vào permutation. Decoder: thay vì decode từng object query, thì DETR decode song song cùng lúc tất cả các queries. Sau khi decode thì dùng FFN để predict vị trí và class. Để tính hàm loss thì dùng thuật toán Hungarian để matching. Về mặt giá cả thì DETR tốn 300 epochs để hội tụ trên COCO, nên về sau gần đây có khá nhiều nghiên cứu để giảm giá thành hộ tụ (chỉ cần 50 epochs thôi chả hạn). YOLOX So với phiên bản cũ [7] thì YOLOX [7] ứng dụng khá nhiều kỹ thuật mới như decoupled heads, strong augmentation (Moáic và Mixup), anchor-free, pulti-positives, và SimOTA. Backbone thì ngoài Darknet ra cũng dùng thêm những backbone nhỏ hơn như Tiny. Dưới đây là kết quả inference của model YOLOX-Tiny: Xây dựng tài nguyên GPU Giá thành khi train model State-of-the-art (SOTA) Nhìn chung là nếu chỉ hình ảnh với bộ dữ liệu nhỏ nhỏ như COCO [7] tầm trăm ngàn ảnh thì có bảng giá dưới đây: chúng ta lấy ví dụ từ báo cáo của 1 state-of-the-art thì họ dùng 8 cái V100, train tầm 6 ngày liên tục (\(6\times 24\) giờ) thì tổng tiền cho một lượt trên tầm ngàn Mỹ kim cho 6 ngày, 1 tháng cứ tầm 5 ngàn Mỹ kim. Mà các bạn cũng nhớ giá này là giá Spot tức là có thể bị interrupt giữa chừng nên mới rẻ thế. Chứ nếu bạn mà chọn on-demand thì có mà gấp 10 lần. Nhưng ở trên mới chỉ là giá bộ COCO có hơn 100k ảnh nhé. Bộ Open Images [7] với 1.7 triệu ảnh thì còn máu nữa. Search trên Kaggle mà có đồng chí chịu khó bỏ tiền ra ngồi train và báo cáo kết quả cho anh biết (xin cám ơn đồng chí): Kaggle Open Images 2019 challenge 6th place solution. Thì kết quả là đồng chí ấy báo cáo: train 8 models trên V100 (chắc lại EC2 P3 thôi thì mình cứ dùng p3.xlarge để làm phân tích giá nhé) rồi ensemble. mỗi model train mất 18-36 ngày (tùy model). Thì đồng chí này train 8 GPUs khác nhau. sau khi train xong các model thì mất thêm 1 ngày nữa để inference và 1 ngày nữa để ensemble (dùng NMS). Vậy tổng thể đã tiêu tốn \(36\times 8+1\times 8+1=297\) ngày train, tức là \(297\times 24=7128\) giờ train. p3.xlarge thì giá mềm nhất là Spot cũng tầm $0.918/h. Tức là để train được accuracy tầm 60% đã mất \(7128\times 0.918\) tức là tầm 6543 Mỹ kim và hơn tháng ngồi monitor màn hình train. Xây dựng hệ thống 8~16 GPU Nhìn chung thì theo dòng lịch sử có 3 loại NVIDIA GPU dành cho cloud khá thông dụng như sau (tôi không nói tới hai dòng GTX và RTX nhé): NVIDIA V100 hay Volta: nói đến dòng này chúng ta có những sự lựa chọn chủ yếu liên quan tới V100 Tensor Core mà đại diện cho thuê là p3.16xlarge và p3dn.24xlarge. Với băng thông mạng của phiên bản P3.16xlarge cao hơn tới 4 lần, phiên bản P3dn.24xlarge của Amazon EC2 là sự bổ sung mới nhất cho dòng phiên bản P3, được tối ưu hóa cho machine learning phân tán và các ứng dụng HPC. Các phiên bản này cung cấp thông lượng kết nối mạng lên tới 100 Gbps, 96 vCPU Intel® Xeon® Có thể mở rộng (Skylake) tùy chỉnh, 8 GPU NVIDIA® V100 Tensor Core với 32 GB bộ nhớ mỗi GPU và 1,8 TB ổ lưu trữ SSD cục bộ chuẩn NVMe. Các phiên bản P3dn.24xlarge cũng hỗ trợ Elastic Fabric Adapter (EFA). Giao diện này tăng tốc các ứng dụng machine learning phân tán sử dụng Thư viện giao tiếp chung NVIDIA (NCCL). EFA có thể mở rộng quy mô lên đến hàng nghìn GPU, cải thiện đáng kể thông lượng và khả năng mở rộng của các mô hình huấn luyện deep learning, từ đó cho kết quả nhanh hơn. Source: Amazon Web Service NVIDIA T4 hay Turing: với AWS EC2 thì bạn có thể thuê g4dn.metal. NVIDIA A100 hay Ampere: Với AWS EC2 thì có thể thuê p4d.24xlarge, với Azure HPC thì có thể thuê Standard_ND96amsr_A100_v4. GCP thì có a2-highgpu-8g hoặc bản 16 GPU là a2-highgpu-16g. Thì về mặt spec Ampere là khỏe nhất nếu nói về TFLOPS. Dưới đây là bảng giá thành của NVIDIA 8x A100 Tensor Core. Trong bảng này có 2 cột mà các bạn nên để ý là giá thành thuê theo giờ (Hourly cost) và tỷ lệ GFLOPS/USD (đáng giá thế nào). Giả định chung là hệ thống được xây dựng tối thiểu 4x GPU và được dùng ít nhất 24 tháng, mỗi tháng dùng 22 ngày (T7/CN nghỉ ngơi). Nói chung tự build thì các bạn có thể tham khảo cấu hình của DGX-13, DGX-24, và DGX-A1005 để mua các bộ phận về tự ráp thì sẽ tiết kiệm công lắp ráp, nhưng nhìn chung tôi nghĩ cũng phải 50 ngàn Mỹ Kim. Các cloud solutions Trong trường hợp bạn có bài toán train dữ liệu mà mất hàng tháng trời train với GTX/RTX thì bạn sẽ nghĩ phải thuê GPUs trên data center (8x-16x GPU). Thì ngoài AWS/Azure/GCP là khá cùng rank nên bảng giá không chênh lệch nhau mấy, bạn có thể tham khảo thêm các trang cho thuê GPU bên ngoài để tìm được chỗ thuê hợp lý hơn. Như kết quả tìm kiếm của AIFI thì hiện tại có trang vast.ai cung cấp khá nhiều sự lựa chọn cho thuê ở mức giá thấp hơn 5 USD/hour. Còn lời giải nào khác? Nhìn chung tự build thì có hai khả năng: Mua đồ sẵn như DGX345 thì các bạn cứ chuẩn bị 100k Mỹ kim trở lên. Mua bộ phận về tự ráp thì các bác tham khảo cấu hình của DGX rồi độ lại tùy theo nhu cầu. Tuy nhiên, chắc chỉ giảm được tiền công, và tối ưu một chút kiểu DGX dùng nhiều RAM thì mình giảm RAM xuống. Nói chung chắc cũng phải 50K Mỹ Kim. Về cá nhân, tôi thiên về thuê! Nếu tự build thì mua mấy cái RTX/GTX dòng Ti là ổn rồi. Tuy nhiên nếu bài toán lớn thì bạn bắt buộc phải dùng data center GPU thì lúc ấy phải có TIỀN! Vấn đề của đi thuê là tiền tính theo giờ nên bạn cần phải ước lượng được số giờ sử dụng. Nếu tầm trên 200h/tháng, tôi nghĩ nên thuê theo năm hoặc 3 năm. Spot price thì cũng tàm tạm thôi, vì mất công chờ với nó ngắt điện (interupt) mình cũng phải chịu ấy, nên là rẻ nhưng lại mất thời gian chờ và bị ngắt. Mà vấn đề với Spot là nó không có luôn ấy (phải chờ đến khi cái server ấy nó open mình mới được dùng). Kết luận Hơi buồn nhưng đầu tiên vẫn là ... tiền đâu Như vậy chúng ta đã điểm qua một số SOTAs của seq2seq cho Vision và nhìn chung các seq2seq vẫn đang nắm vị trí số 1. Tuy vậy, vấn đề lớn khi “đua đòi” vào mảng này thì vẫn là tài nguyên thôi. Nếu chuẩn bị được budget và plan nghiên cứu nghiêm chỉnh (mà đầu tiên là tiền đâu) thì về mặt nghiệp vụ PM tôi nghĩ không nên triển khai làm gì mất time anh em. Ít nhất là cần vốn 200k Mỹ Kim thì cũng phải có tầm 100k trong túi hãy nghĩ! Tài liệu tham khảo Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. (2020).Details Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. European conference on computer vision (2020), 213–229.Details Gao, P., Zheng, M., Wang, X., Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. Proceedings of the IEEE/CVF International Conference on Computer Vision (2021), 3621–3630.Details Redmon, J. and Farhadi, A. 2018. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767. (2018).Details Ge, Z., Liu, S., Wang, F., Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430. (2021).Details Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L. 2014. Microsoft coco: Common objects in context. European conference on computer vision (2014), 740–755.Details Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T. and Ferrari, V. 2020. The open images dataset v4. International Journal of Computer Vision. 128, 7 (2020), 1956–1981.Details https://wanted2.github.io/seq2seq/ &#8617; &#8617;2 &#8617;3 https://wanted2.github.io/speech/ &#8617; &#8617;2 &#8617;3 https://www.nvidia.com/en-us/data-center/dgx-1/ &#8617; &#8617;2 https://www.nvidia.com/en-us/data-center/dgx-2/ &#8617; &#8617;2 https://www.nvidia.com/en-us/data-center/dgx-a100/ &#8617; &#8617;2" />
<meta property="og:description" content="Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự án cần seq2seq. Vision Transformers ViT Về nguyên lý chung của seq2seq thì chúng ta có hai bài viết trước nói khá nhiều rồi12 nên các bạn tham khảo nhé. Nhà cũng đang có cái “eo hẹp” là chỉ được tối đa 7 citations và 9 phút đọc thôi nên các bạn cần thì đọc lại hai bài viết12 để xem thêm kiến thức về attention, Transformer, … Vision Transformer (ViT, [7]) đưa khái niệm seq2seq vào vision. Nếu các bạn đã quen seq2seq thì hiểu ngay là ta cần chuyển hình ảnh thành 1 chuỗi: Source: Google Để làm được việc này, các tác giả đề xuất: Image patches: Hình ảnh được chia ra thành patches và đánh số thự tự để input vào transformer. Position embedding: thứ tự chỉ đơn giản là chuỗi 1D. MLP với GELU activation: MLP sử dụng GELU để kích hoạt. Model không chứa CNN, có tối thiểu 12 tầng, với kích cỡ hidden size từ 768, ngoài ra có 12-16 heads. Large-scale pre-training and fine-tuning: Một điểm đáng chú ý khác là pre-training dạng supervised trên một dataset 300 triệu ảnh tạo ra 1 model rất mạnh. Các tác giả báo cáo cải tiến trên khá nhiều bộ dữ liệu lớn, và ngoài ra cả con số TPUv3-core-days = số lượng TPUv3 cores x sô lượng ngày train. Cái con số TPUv3-core-days thì cứ mỗi thí nghiệm là vài ngàn tới vài chục ngàn, mà mỗi core thì cứ 10$/ngày thì các bạn cứ nhẩm tính xem budget của hội con nhà giàu này đầu tư vào nó lớn cỡ nào đấy. DETR DETR [7, 7] tiếp tục ứng dụng transformer vào Object Detection và Segmentation. Source: [7] Backbone: DETR dùng CNN quen thuộc như resnet-50 hoặc 101. Encoder: DETR dùng \(1\times 1\) convolution để dimention reduction các feature maps rồi input vào. Position embedding là cố định vì transformer là không phụ thuộc vào permutation. Decoder: thay vì decode từng object query, thì DETR decode song song cùng lúc tất cả các queries. Sau khi decode thì dùng FFN để predict vị trí và class. Để tính hàm loss thì dùng thuật toán Hungarian để matching. Về mặt giá cả thì DETR tốn 300 epochs để hội tụ trên COCO, nên về sau gần đây có khá nhiều nghiên cứu để giảm giá thành hộ tụ (chỉ cần 50 epochs thôi chả hạn). YOLOX So với phiên bản cũ [7] thì YOLOX [7] ứng dụng khá nhiều kỹ thuật mới như decoupled heads, strong augmentation (Moáic và Mixup), anchor-free, pulti-positives, và SimOTA. Backbone thì ngoài Darknet ra cũng dùng thêm những backbone nhỏ hơn như Tiny. Dưới đây là kết quả inference của model YOLOX-Tiny: Xây dựng tài nguyên GPU Giá thành khi train model State-of-the-art (SOTA) Nhìn chung là nếu chỉ hình ảnh với bộ dữ liệu nhỏ nhỏ như COCO [7] tầm trăm ngàn ảnh thì có bảng giá dưới đây: chúng ta lấy ví dụ từ báo cáo của 1 state-of-the-art thì họ dùng 8 cái V100, train tầm 6 ngày liên tục (\(6\times 24\) giờ) thì tổng tiền cho một lượt trên tầm ngàn Mỹ kim cho 6 ngày, 1 tháng cứ tầm 5 ngàn Mỹ kim. Mà các bạn cũng nhớ giá này là giá Spot tức là có thể bị interrupt giữa chừng nên mới rẻ thế. Chứ nếu bạn mà chọn on-demand thì có mà gấp 10 lần. Nhưng ở trên mới chỉ là giá bộ COCO có hơn 100k ảnh nhé. Bộ Open Images [7] với 1.7 triệu ảnh thì còn máu nữa. Search trên Kaggle mà có đồng chí chịu khó bỏ tiền ra ngồi train và báo cáo kết quả cho anh biết (xin cám ơn đồng chí): Kaggle Open Images 2019 challenge 6th place solution. Thì kết quả là đồng chí ấy báo cáo: train 8 models trên V100 (chắc lại EC2 P3 thôi thì mình cứ dùng p3.xlarge để làm phân tích giá nhé) rồi ensemble. mỗi model train mất 18-36 ngày (tùy model). Thì đồng chí này train 8 GPUs khác nhau. sau khi train xong các model thì mất thêm 1 ngày nữa để inference và 1 ngày nữa để ensemble (dùng NMS). Vậy tổng thể đã tiêu tốn \(36\times 8+1\times 8+1=297\) ngày train, tức là \(297\times 24=7128\) giờ train. p3.xlarge thì giá mềm nhất là Spot cũng tầm $0.918/h. Tức là để train được accuracy tầm 60% đã mất \(7128\times 0.918\) tức là tầm 6543 Mỹ kim và hơn tháng ngồi monitor màn hình train. Xây dựng hệ thống 8~16 GPU Nhìn chung thì theo dòng lịch sử có 3 loại NVIDIA GPU dành cho cloud khá thông dụng như sau (tôi không nói tới hai dòng GTX và RTX nhé): NVIDIA V100 hay Volta: nói đến dòng này chúng ta có những sự lựa chọn chủ yếu liên quan tới V100 Tensor Core mà đại diện cho thuê là p3.16xlarge và p3dn.24xlarge. Với băng thông mạng của phiên bản P3.16xlarge cao hơn tới 4 lần, phiên bản P3dn.24xlarge của Amazon EC2 là sự bổ sung mới nhất cho dòng phiên bản P3, được tối ưu hóa cho machine learning phân tán và các ứng dụng HPC. Các phiên bản này cung cấp thông lượng kết nối mạng lên tới 100 Gbps, 96 vCPU Intel® Xeon® Có thể mở rộng (Skylake) tùy chỉnh, 8 GPU NVIDIA® V100 Tensor Core với 32 GB bộ nhớ mỗi GPU và 1,8 TB ổ lưu trữ SSD cục bộ chuẩn NVMe. Các phiên bản P3dn.24xlarge cũng hỗ trợ Elastic Fabric Adapter (EFA). Giao diện này tăng tốc các ứng dụng machine learning phân tán sử dụng Thư viện giao tiếp chung NVIDIA (NCCL). EFA có thể mở rộng quy mô lên đến hàng nghìn GPU, cải thiện đáng kể thông lượng và khả năng mở rộng của các mô hình huấn luyện deep learning, từ đó cho kết quả nhanh hơn. Source: Amazon Web Service NVIDIA T4 hay Turing: với AWS EC2 thì bạn có thể thuê g4dn.metal. NVIDIA A100 hay Ampere: Với AWS EC2 thì có thể thuê p4d.24xlarge, với Azure HPC thì có thể thuê Standard_ND96amsr_A100_v4. GCP thì có a2-highgpu-8g hoặc bản 16 GPU là a2-highgpu-16g. Thì về mặt spec Ampere là khỏe nhất nếu nói về TFLOPS. Dưới đây là bảng giá thành của NVIDIA 8x A100 Tensor Core. Trong bảng này có 2 cột mà các bạn nên để ý là giá thành thuê theo giờ (Hourly cost) và tỷ lệ GFLOPS/USD (đáng giá thế nào). Giả định chung là hệ thống được xây dựng tối thiểu 4x GPU và được dùng ít nhất 24 tháng, mỗi tháng dùng 22 ngày (T7/CN nghỉ ngơi). Nói chung tự build thì các bạn có thể tham khảo cấu hình của DGX-13, DGX-24, và DGX-A1005 để mua các bộ phận về tự ráp thì sẽ tiết kiệm công lắp ráp, nhưng nhìn chung tôi nghĩ cũng phải 50 ngàn Mỹ Kim. Các cloud solutions Trong trường hợp bạn có bài toán train dữ liệu mà mất hàng tháng trời train với GTX/RTX thì bạn sẽ nghĩ phải thuê GPUs trên data center (8x-16x GPU). Thì ngoài AWS/Azure/GCP là khá cùng rank nên bảng giá không chênh lệch nhau mấy, bạn có thể tham khảo thêm các trang cho thuê GPU bên ngoài để tìm được chỗ thuê hợp lý hơn. Như kết quả tìm kiếm của AIFI thì hiện tại có trang vast.ai cung cấp khá nhiều sự lựa chọn cho thuê ở mức giá thấp hơn 5 USD/hour. Còn lời giải nào khác? Nhìn chung tự build thì có hai khả năng: Mua đồ sẵn như DGX345 thì các bạn cứ chuẩn bị 100k Mỹ kim trở lên. Mua bộ phận về tự ráp thì các bác tham khảo cấu hình của DGX rồi độ lại tùy theo nhu cầu. Tuy nhiên, chắc chỉ giảm được tiền công, và tối ưu một chút kiểu DGX dùng nhiều RAM thì mình giảm RAM xuống. Nói chung chắc cũng phải 50K Mỹ Kim. Về cá nhân, tôi thiên về thuê! Nếu tự build thì mua mấy cái RTX/GTX dòng Ti là ổn rồi. Tuy nhiên nếu bài toán lớn thì bạn bắt buộc phải dùng data center GPU thì lúc ấy phải có TIỀN! Vấn đề của đi thuê là tiền tính theo giờ nên bạn cần phải ước lượng được số giờ sử dụng. Nếu tầm trên 200h/tháng, tôi nghĩ nên thuê theo năm hoặc 3 năm. Spot price thì cũng tàm tạm thôi, vì mất công chờ với nó ngắt điện (interupt) mình cũng phải chịu ấy, nên là rẻ nhưng lại mất thời gian chờ và bị ngắt. Mà vấn đề với Spot là nó không có luôn ấy (phải chờ đến khi cái server ấy nó open mình mới được dùng). Kết luận Hơi buồn nhưng đầu tiên vẫn là ... tiền đâu Như vậy chúng ta đã điểm qua một số SOTAs của seq2seq cho Vision và nhìn chung các seq2seq vẫn đang nắm vị trí số 1. Tuy vậy, vấn đề lớn khi “đua đòi” vào mảng này thì vẫn là tài nguyên thôi. Nếu chuẩn bị được budget và plan nghiên cứu nghiêm chỉnh (mà đầu tiên là tiền đâu) thì về mặt nghiệp vụ PM tôi nghĩ không nên triển khai làm gì mất time anh em. Ít nhất là cần vốn 200k Mỹ Kim thì cũng phải có tầm 100k trong túi hãy nghĩ! Tài liệu tham khảo Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. (2020).Details Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. European conference on computer vision (2020), 213–229.Details Gao, P., Zheng, M., Wang, X., Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. Proceedings of the IEEE/CVF International Conference on Computer Vision (2021), 3621–3630.Details Redmon, J. and Farhadi, A. 2018. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767. (2018).Details Ge, Z., Liu, S., Wang, F., Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430. (2021).Details Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L. 2014. Microsoft coco: Common objects in context. European conference on computer vision (2014), 740–755.Details Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T. and Ferrari, V. 2020. The open images dataset v4. International Journal of Computer Vision. 128, 7 (2020), 1956–1981.Details https://wanted2.github.io/seq2seq/ &#8617; &#8617;2 &#8617;3 https://wanted2.github.io/speech/ &#8617; &#8617;2 &#8617;3 https://www.nvidia.com/en-us/data-center/dgx-1/ &#8617; &#8617;2 https://www.nvidia.com/en-us/data-center/dgx-2/ &#8617; &#8617;2 https://www.nvidia.com/en-us/data-center/dgx-a100/ &#8617; &#8617;2" />
<meta property="og:site_name" content="AiFi" />
<meta property="og:image" content="/https://wanted2.github.io/assets/images/yolox.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-12T00:00:00+09:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="/https://wanted2.github.io/assets/images/yolox.gif" />
<meta property="twitter:title" content="Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"tuan"},"dateModified":"2022-02-12T00:00:00+09:00","datePublished":"2022-02-12T00:00:00+09:00","description":"Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự án cần seq2seq. Vision Transformers ViT Về nguyên lý chung của seq2seq thì chúng ta có hai bài viết trước nói khá nhiều rồi12 nên các bạn tham khảo nhé. Nhà cũng đang có cái “eo hẹp” là chỉ được tối đa 7 citations và 9 phút đọc thôi nên các bạn cần thì đọc lại hai bài viết12 để xem thêm kiến thức về attention, Transformer, … Vision Transformer (ViT, [7]) đưa khái niệm seq2seq vào vision. Nếu các bạn đã quen seq2seq thì hiểu ngay là ta cần chuyển hình ảnh thành 1 chuỗi: Source: Google Để làm được việc này, các tác giả đề xuất: Image patches: Hình ảnh được chia ra thành patches và đánh số thự tự để input vào transformer. Position embedding: thứ tự chỉ đơn giản là chuỗi 1D. MLP với GELU activation: MLP sử dụng GELU để kích hoạt. Model không chứa CNN, có tối thiểu 12 tầng, với kích cỡ hidden size từ 768, ngoài ra có 12-16 heads. Large-scale pre-training and fine-tuning: Một điểm đáng chú ý khác là pre-training dạng supervised trên một dataset 300 triệu ảnh tạo ra 1 model rất mạnh. Các tác giả báo cáo cải tiến trên khá nhiều bộ dữ liệu lớn, và ngoài ra cả con số TPUv3-core-days = số lượng TPUv3 cores x sô lượng ngày train. Cái con số TPUv3-core-days thì cứ mỗi thí nghiệm là vài ngàn tới vài chục ngàn, mà mỗi core thì cứ 10$/ngày thì các bạn cứ nhẩm tính xem budget của hội con nhà giàu này đầu tư vào nó lớn cỡ nào đấy. DETR DETR [7, 7] tiếp tục ứng dụng transformer vào Object Detection và Segmentation. Source: [7] Backbone: DETR dùng CNN quen thuộc như resnet-50 hoặc 101. Encoder: DETR dùng \\(1\\times 1\\) convolution để dimention reduction các feature maps rồi input vào. Position embedding là cố định vì transformer là không phụ thuộc vào permutation. Decoder: thay vì decode từng object query, thì DETR decode song song cùng lúc tất cả các queries. Sau khi decode thì dùng FFN để predict vị trí và class. Để tính hàm loss thì dùng thuật toán Hungarian để matching. Về mặt giá cả thì DETR tốn 300 epochs để hội tụ trên COCO, nên về sau gần đây có khá nhiều nghiên cứu để giảm giá thành hộ tụ (chỉ cần 50 epochs thôi chả hạn). YOLOX So với phiên bản cũ [7] thì YOLOX [7] ứng dụng khá nhiều kỹ thuật mới như decoupled heads, strong augmentation (Moáic và Mixup), anchor-free, pulti-positives, và SimOTA. Backbone thì ngoài Darknet ra cũng dùng thêm những backbone nhỏ hơn như Tiny. Dưới đây là kết quả inference của model YOLOX-Tiny: Xây dựng tài nguyên GPU Giá thành khi train model State-of-the-art (SOTA) Nhìn chung là nếu chỉ hình ảnh với bộ dữ liệu nhỏ nhỏ như COCO [7] tầm trăm ngàn ảnh thì có bảng giá dưới đây: chúng ta lấy ví dụ từ báo cáo của 1 state-of-the-art thì họ dùng 8 cái V100, train tầm 6 ngày liên tục (\\(6\\times 24\\) giờ) thì tổng tiền cho một lượt trên tầm ngàn Mỹ kim cho 6 ngày, 1 tháng cứ tầm 5 ngàn Mỹ kim. Mà các bạn cũng nhớ giá này là giá Spot tức là có thể bị interrupt giữa chừng nên mới rẻ thế. Chứ nếu bạn mà chọn on-demand thì có mà gấp 10 lần. Nhưng ở trên mới chỉ là giá bộ COCO có hơn 100k ảnh nhé. Bộ Open Images [7] với 1.7 triệu ảnh thì còn máu nữa. Search trên Kaggle mà có đồng chí chịu khó bỏ tiền ra ngồi train và báo cáo kết quả cho anh biết (xin cám ơn đồng chí): Kaggle Open Images 2019 challenge 6th place solution. Thì kết quả là đồng chí ấy báo cáo: train 8 models trên V100 (chắc lại EC2 P3 thôi thì mình cứ dùng p3.xlarge để làm phân tích giá nhé) rồi ensemble. mỗi model train mất 18-36 ngày (tùy model). Thì đồng chí này train 8 GPUs khác nhau. sau khi train xong các model thì mất thêm 1 ngày nữa để inference và 1 ngày nữa để ensemble (dùng NMS). Vậy tổng thể đã tiêu tốn \\(36\\times 8+1\\times 8+1=297\\) ngày train, tức là \\(297\\times 24=7128\\) giờ train. p3.xlarge thì giá mềm nhất là Spot cũng tầm $0.918/h. Tức là để train được accuracy tầm 60% đã mất \\(7128\\times 0.918\\) tức là tầm 6543 Mỹ kim và hơn tháng ngồi monitor màn hình train. Xây dựng hệ thống 8~16 GPU Nhìn chung thì theo dòng lịch sử có 3 loại NVIDIA GPU dành cho cloud khá thông dụng như sau (tôi không nói tới hai dòng GTX và RTX nhé): NVIDIA V100 hay Volta: nói đến dòng này chúng ta có những sự lựa chọn chủ yếu liên quan tới V100 Tensor Core mà đại diện cho thuê là p3.16xlarge và p3dn.24xlarge. Với băng thông mạng của phiên bản P3.16xlarge cao hơn tới 4 lần, phiên bản P3dn.24xlarge của Amazon EC2 là sự bổ sung mới nhất cho dòng phiên bản P3, được tối ưu hóa cho machine learning phân tán và các ứng dụng HPC. Các phiên bản này cung cấp thông lượng kết nối mạng lên tới 100 Gbps, 96 vCPU Intel® Xeon® Có thể mở rộng (Skylake) tùy chỉnh, 8 GPU NVIDIA® V100 Tensor Core với 32 GB bộ nhớ mỗi GPU và 1,8 TB ổ lưu trữ SSD cục bộ chuẩn NVMe. Các phiên bản P3dn.24xlarge cũng hỗ trợ Elastic Fabric Adapter (EFA). Giao diện này tăng tốc các ứng dụng machine learning phân tán sử dụng Thư viện giao tiếp chung NVIDIA (NCCL). EFA có thể mở rộng quy mô lên đến hàng nghìn GPU, cải thiện đáng kể thông lượng và khả năng mở rộng của các mô hình huấn luyện deep learning, từ đó cho kết quả nhanh hơn. Source: Amazon Web Service NVIDIA T4 hay Turing: với AWS EC2 thì bạn có thể thuê g4dn.metal. NVIDIA A100 hay Ampere: Với AWS EC2 thì có thể thuê p4d.24xlarge, với Azure HPC thì có thể thuê Standard_ND96amsr_A100_v4. GCP thì có a2-highgpu-8g hoặc bản 16 GPU là a2-highgpu-16g. Thì về mặt spec Ampere là khỏe nhất nếu nói về TFLOPS. Dưới đây là bảng giá thành của NVIDIA 8x A100 Tensor Core. Trong bảng này có 2 cột mà các bạn nên để ý là giá thành thuê theo giờ (Hourly cost) và tỷ lệ GFLOPS/USD (đáng giá thế nào). Giả định chung là hệ thống được xây dựng tối thiểu 4x GPU và được dùng ít nhất 24 tháng, mỗi tháng dùng 22 ngày (T7/CN nghỉ ngơi). Nói chung tự build thì các bạn có thể tham khảo cấu hình của DGX-13, DGX-24, và DGX-A1005 để mua các bộ phận về tự ráp thì sẽ tiết kiệm công lắp ráp, nhưng nhìn chung tôi nghĩ cũng phải 50 ngàn Mỹ Kim. Các cloud solutions Trong trường hợp bạn có bài toán train dữ liệu mà mất hàng tháng trời train với GTX/RTX thì bạn sẽ nghĩ phải thuê GPUs trên data center (8x-16x GPU). Thì ngoài AWS/Azure/GCP là khá cùng rank nên bảng giá không chênh lệch nhau mấy, bạn có thể tham khảo thêm các trang cho thuê GPU bên ngoài để tìm được chỗ thuê hợp lý hơn. Như kết quả tìm kiếm của AIFI thì hiện tại có trang vast.ai cung cấp khá nhiều sự lựa chọn cho thuê ở mức giá thấp hơn 5 USD/hour. Còn lời giải nào khác? Nhìn chung tự build thì có hai khả năng: Mua đồ sẵn như DGX345 thì các bạn cứ chuẩn bị 100k Mỹ kim trở lên. Mua bộ phận về tự ráp thì các bác tham khảo cấu hình của DGX rồi độ lại tùy theo nhu cầu. Tuy nhiên, chắc chỉ giảm được tiền công, và tối ưu một chút kiểu DGX dùng nhiều RAM thì mình giảm RAM xuống. Nói chung chắc cũng phải 50K Mỹ Kim. Về cá nhân, tôi thiên về thuê! Nếu tự build thì mua mấy cái RTX/GTX dòng Ti là ổn rồi. Tuy nhiên nếu bài toán lớn thì bạn bắt buộc phải dùng data center GPU thì lúc ấy phải có TIỀN! Vấn đề của đi thuê là tiền tính theo giờ nên bạn cần phải ước lượng được số giờ sử dụng. Nếu tầm trên 200h/tháng, tôi nghĩ nên thuê theo năm hoặc 3 năm. Spot price thì cũng tàm tạm thôi, vì mất công chờ với nó ngắt điện (interupt) mình cũng phải chịu ấy, nên là rẻ nhưng lại mất thời gian chờ và bị ngắt. Mà vấn đề với Spot là nó không có luôn ấy (phải chờ đến khi cái server ấy nó open mình mới được dùng). Kết luận Hơi buồn nhưng đầu tiên vẫn là ... tiền đâu Như vậy chúng ta đã điểm qua một số SOTAs của seq2seq cho Vision và nhìn chung các seq2seq vẫn đang nắm vị trí số 1. Tuy vậy, vấn đề lớn khi “đua đòi” vào mảng này thì vẫn là tài nguyên thôi. Nếu chuẩn bị được budget và plan nghiên cứu nghiêm chỉnh (mà đầu tiên là tiền đâu) thì về mặt nghiệp vụ PM tôi nghĩ không nên triển khai làm gì mất time anh em. Ít nhất là cần vốn 200k Mỹ Kim thì cũng phải có tầm 100k trong túi hãy nghĩ! Tài liệu tham khảo Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. (2020).Details Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. European conference on computer vision (2020), 213–229.Details Gao, P., Zheng, M., Wang, X., Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. Proceedings of the IEEE/CVF International Conference on Computer Vision (2021), 3621–3630.Details Redmon, J. and Farhadi, A. 2018. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767. (2018).Details Ge, Z., Liu, S., Wang, F., Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430. (2021).Details Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L. 2014. Microsoft coco: Common objects in context. European conference on computer vision (2014), 740–755.Details Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T. and Ferrari, V. 2020. The open images dataset v4. International Journal of Computer Vision. 128, 7 (2020), 1956–1981.Details https://wanted2.github.io/seq2seq/ &#8617; &#8617;2 &#8617;3 https://wanted2.github.io/speech/ &#8617; &#8617;2 &#8617;3 https://www.nvidia.com/en-us/data-center/dgx-1/ &#8617; &#8617;2 https://www.nvidia.com/en-us/data-center/dgx-2/ &#8617; &#8617;2 https://www.nvidia.com/en-us/data-center/dgx-a100/ &#8617; &#8617;2","headline":"Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices","image":"/https://wanted2.github.io/assets/images/yolox.gif","mainEntityOfPage":{"@type":"WebPage","@id":"/https://wanted2.github.io/seq2seq-cv/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/https://wanted2.github.io/assets/images/favicon.ico"},"name":"tuan"},"url":"/https://wanted2.github.io/seq2seq-cv/"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="https://wanted2.github.io/assets/css/screen.css" rel="stylesheet">

<link href="https://wanted2.github.io/assets/css/main.css" rel="stylesheet">

<script src="https://wanted2.github.io/assets/js/jquery.min.js"></script>
<script src="https://kit.fontawesome.com/d0b91d895e.js" crossorigin="anonymous"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
    }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" crossorigin="anonymous"></script>
<script src="https://d3js.org/d3.v4.js" crossorigin="anonymous"></script>
<!-- <script src="https://bl.ocks.org/mbostock/raw/4061502/0a200ddf998aa75dfdb1ff32e16b680a15e5cb01/box.js" crossorigin="anonymous"></script> -->
</head>


<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="https://wanted2.github.io/">
    <img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="https://wanted2.github.io/">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/about">About</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/projects">Projects</a>
                </li>

                <li class="nav-item">
                    <a class="nav-link" href="https://wanted2.github.io/service">Services</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-wordpress/"><i class="fab fa-wordpress-simple"></i> WP Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-ghost/"><i class="fab fa-snapchat-ghost"></i> Ghost Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/wowthemesnet/mediumish-theme-jekyll"><i class="fab fa-github"></i> Fork on Github</a>
                </li> -->

                <!-- <script src="https://wanted2.github.io/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="https://wanted2.github.io/assets/js/lunrsearchengine.js"></script> -->

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">AiFi</h1>
    <p class="lead">
        An AI Engineer's blog (This is a staging site, so the content may be imprecise, refer to official AiFi)
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices&url=https://wanted2.github.io/seq2seq-cv/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=https://wanted2.github.io/seq2seq-cv/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=https://wanted2.github.io/seq2seq-cv/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="mailto:?subject=Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices&body=https://wanted2.github.io/seq2seq-cv/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fas fa-envelope"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
    
    <div class="sep">

    </div>
    <ul>
        <li class="small">
        2121
     words</li>
        <li class="small">11 minutes</li>
    </ul>
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-2 col-lg-2 text-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="https://wanted2.github.io/assets/images/favicon.png" alt="AiFi">
                        
                    </div>
                    <div class="col-xs-12 col-md-10 col-lg-10 text-right">
                        <a target="_blank" class="link-dark" href="">AiFi</a>
                        <!-- <a target="_blank" href="" class="btn follow">Follow</a> -->
                        <!-- LikeBtn.com BEGIN -->
                        <span class="likebtn-wrapper" 
                            data-site_id="61cfccd36fd08b2d68c1929e"
                            data-theme="custom" 
                            data-icon_l_url="/assets/images/OK_EM.png" 
                            data-icon_l_url_v="/assets/images/OK_EM_clicked.png" 
                            data-identifier="/seq2seq-cv/" 
                            data-show_like_label="false" 
                            data-like_enabled="false" 
                            data-dislike_enabled="false" 
                            data-icon_dislike_show="false" 
                            data-voting_cancelable="false" 
                            data-counter_show="true"
                            data-counter_frmt="comma"></span>
                        <script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
                        <!-- LikeBtn.com END -->
                        <span class="author-description"></span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid lazyimg" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=" data-src="https://wanted2.github.io/assets/images/yolox.gif" alt="Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                    
                    <div class="toc mt-4 mb-4 lead">
                        <h3 class="font-weight-bold">Summary</h3>
                        <ul>
  <li><a href="#vision-transformers">Vision Transformers</a>
    <ul>
      <li><a href="#vit">ViT</a></li>
      <li><a href="#detr">DETR</a></li>
      <li><a href="#yolox">YOLOX</a></li>
    </ul>
  </li>
  <li><a href="#xây-dựng-tài-nguyên-gpu">Xây dựng tài nguyên GPU</a>
    <ul>
      <li><a href="#giá-thành-khi-train-model-state-of-the-art-sota">Giá thành khi train model State-of-the-art (SOTA)</a></li>
      <li><a href="#xây-dựng-hệ-thống-816-gpu">Xây dựng hệ thống 8~16 GPU</a></li>
      <li><a href="#các-cloud-solutions">Các cloud solutions</a></li>
      <li><a href="#còn-lời-giải-nào-khác">Còn lời giải nào khác?</a></li>
    </ul>
  </li>
  <li><a href="#kết-luận">Kết luận</a></li>
  <li><a href="#tài-liệu-tham-khảo">Tài liệu tham khảo</a></li>
</ul>
                    </div>
                
                <!-- End Toc -->
                <p>Trong các bài viết trước, chúng ta đã xem xét kha khá về <code class="language-plaintext highlighter-rouge">seq2seq</code> cho NLP/Vision-Language<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup> và âm thanh<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">2</a></sup>.
Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của <code class="language-plaintext highlighter-rouge">seq2seq</code> trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự án cần <code class="language-plaintext highlighter-rouge">seq2seq</code>.</p>

<h1 id="vision-transformers">Vision Transformers</h1>

<h2 id="vit">ViT</h2>

<p>Về nguyên lý chung của <code class="language-plaintext highlighter-rouge">seq2seq</code> thì chúng ta có hai bài viết trước nói khá nhiều rồi<sup id="fnref:4:1" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup><sup id="fnref:5:1" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">2</a></sup> nên các bạn tham khảo nhé.
Nhà cũng đang có cái “eo hẹp” là chỉ được tối đa 7 citations và 9 phút đọc thôi nên các bạn cần thì đọc lại hai bài viết<sup id="fnref:4:2" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">1</a></sup><sup id="fnref:5:2" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">2</a></sup> để xem thêm kiến thức về attention, Transformer, …
Vision Transformer (ViT, <a class="citation" href="#dosovitskiy2020image">[1]</a>) đưa khái niệm <code class="language-plaintext highlighter-rouge">seq2seq</code> vào vision.
Nếu các bạn đã quen <code class="language-plaintext highlighter-rouge">seq2seq</code> thì hiểu ngay là ta cần chuyển hình ảnh thành 1 chuỗi:</p>

<p><img src="https://media.giphy.com/media/ATsWtUsuuFRfq8OhZ7/source.gif" alt="" />
<em>Source: Google</em></p>

<p>Để làm được việc này, các tác giả đề xuất:</p>

<ul>
  <li><strong>Image patches</strong>: Hình ảnh được chia ra thành patches và đánh số thự tự để input vào transformer.</li>
  <li><strong>Position embedding</strong>: thứ tự chỉ đơn giản là chuỗi 1D.</li>
  <li><strong>MLP với GELU activation</strong>: MLP sử dụng GELU để kích hoạt. Model không chứa CNN, có tối thiểu 12 tầng, với kích cỡ hidden size từ 768, ngoài ra có 12-16 heads.</li>
  <li><strong>Large-scale pre-training and fine-tuning</strong>: Một điểm đáng chú ý khác là pre-training dạng supervised trên một dataset 300 triệu ảnh tạo ra 1 model rất mạnh.</li>
</ul>

<p>Các tác giả báo cáo cải tiến trên khá nhiều bộ dữ liệu lớn, và ngoài ra cả con số <code class="language-plaintext highlighter-rouge">TPUv3-core-days = số lượng TPUv3 cores x sô lượng ngày train</code>.
Cái con số <code class="language-plaintext highlighter-rouge">TPUv3-core-days</code> thì cứ mỗi thí nghiệm là vài ngàn tới vài chục ngàn, mà mỗi core thì cứ 10$/ngày thì các bạn cứ nhẩm tính xem budget của hội <code class="language-plaintext highlighter-rouge">con nhà giàu</code> này đầu tư vào nó lớn cỡ nào đấy.</p>

<h2 id="detr">DETR</h2>

<p>DETR <a class="citation" href="#carion2020end">[2, 3]</a> tiếp tục ứng dụng transformer vào Object Detection và Segmentation.</p>

<p><img src="/assets/images/detr.png" alt="detr" />
<em>Source: <a class="citation" href="#carion2020end">[2]</a></em></p>

<ul>
  <li><strong>Backbone</strong>: DETR dùng CNN quen thuộc như resnet-50 hoặc 101.</li>
  <li><strong>Encoder</strong>: DETR dùng \(1\times 1\) convolution để dimention reduction các feature maps rồi input vào. Position embedding là cố định vì transformer là không phụ thuộc vào permutation.</li>
  <li><strong>Decoder</strong>: thay vì decode từng object query, thì DETR decode song song cùng lúc tất cả các queries.</li>
  <li>Sau khi decode thì dùng FFN để predict vị trí và class. Để tính hàm loss thì dùng thuật toán Hungarian để matching.</li>
</ul>

<p>Về mặt giá cả thì DETR tốn 300 epochs để hội tụ trên COCO, nên về sau gần đây có khá nhiều nghiên cứu để giảm giá thành hộ tụ (chỉ cần 50 epochs thôi chả hạn).</p>

<h2 id="yolox">YOLOX</h2>

<p>So với phiên bản cũ <a class="citation" href="#redmon2018yolov3">[4]</a> thì YOLOX <a class="citation" href="#ge2021yolox">[5]</a> ứng dụng khá nhiều kỹ thuật mới như decoupled heads, strong augmentation (Moáic và Mixup), anchor-free, pulti-positives, và SimOTA.
Backbone thì ngoài Darknet ra cũng dùng thêm những backbone nhỏ hơn như Tiny.</p>

<p>Dưới đây là kết quả inference của model YOLOX-Tiny:</p>
<iframe width="100%" height="480" src="https://www.youtube.com/embed/_5inpa6ruUY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<h1 id="xây-dựng-tài-nguyên-gpu">Xây dựng tài nguyên GPU</h1>

<h2 id="giá-thành-khi-train-model-state-of-the-art-sota">Giá thành khi train model State-of-the-art (SOTA)</h2>

<p>Nhìn chung là nếu chỉ hình ảnh với bộ dữ liệu nhỏ nhỏ như COCO <a class="citation" href="#lin2014microsoft">[6]</a> tầm trăm ngàn ảnh thì có bảng giá dưới đây: chúng ta lấy ví dụ từ báo cáo của 1 state-of-the-art thì họ dùng 8 cái V100, train tầm 6 ngày liên tục (\(6\times 24\) giờ) thì tổng tiền cho một lượt trên tầm ngàn Mỹ kim cho 6 ngày, 1 tháng cứ tầm 5 ngàn Mỹ kim.
Mà các bạn cũng nhớ giá này là giá <a href="https://aws.amazon.com/ec2/spot/pricing/"><strong>Spot</strong></a> tức là có thể bị interrupt giữa chừng nên mới rẻ thế.
Chứ nếu bạn mà chọn <a href="https://aws.amazon.com/ec2/pricing/on-demand/"><strong>on-demand</strong></a> thì có mà gấp 10 lần.</p>

<p><img src="/assets/images/detr-cost-analysis.jpg" alt="pricing" /></p>

<p>Nhưng ở trên mới chỉ là giá bộ COCO có hơn 100k ảnh nhé.
Bộ Open Images <a class="citation" href="#kuznetsova2020open">[7]</a> với 1.7 triệu ảnh thì còn máu nữa.
Search trên Kaggle mà có đồng chí chịu khó bỏ tiền ra ngồi train và báo cáo kết quả cho anh biết (xin cám ơn đồng chí): <a href="https://www.kaggle.com/c/open-images-2019-object-detection/discussion/110953">Kaggle Open Images 2019 challenge 6th place solution</a>.
Thì kết quả là đồng chí ấy báo cáo:</p>
<ul>
  <li>train 8 models trên V100 (chắc lại EC2 P3 thôi thì mình cứ dùng <code class="language-plaintext highlighter-rouge">p3.xlarge</code> để làm phân tích giá nhé) rồi ensemble.</li>
  <li>mỗi model train mất 18-36 ngày (tùy model). Thì đồng chí này train 8 GPUs khác nhau.</li>
  <li>sau khi train xong các model thì mất thêm 1 ngày nữa để inference và 1 ngày nữa để ensemble (dùng NMS).</li>
  <li>Vậy tổng thể đã tiêu tốn \(36\times 8+1\times 8+1=297\) ngày train, tức là \(297\times 24=7128\) giờ train.</li>
</ul>

<p><img src="/assets/images/open-images-price.jpg" alt="pricing" /></p>

<p><code class="language-plaintext highlighter-rouge">p3.xlarge</code> thì giá mềm nhất là <a href="https://aws.amazon.com/ec2/spot/pricing/"><strong>Spot</strong></a> cũng tầm $0.918/h.</p>

<p>Tức là để train được accuracy tầm 60% đã mất \(7128\times 0.918\) tức là tầm 6543 Mỹ kim và hơn tháng ngồi monitor màn hình train.</p>

<h2 id="xây-dựng-hệ-thống-816-gpu">Xây dựng hệ thống 8~16 GPU</h2>

<p>Nhìn chung thì theo dòng lịch sử có 3 loại NVIDIA GPU dành cho cloud khá thông dụng như sau (tôi không nói tới hai dòng GTX và RTX nhé):</p>

<ul>
  <li>NVIDIA V100 hay <strong>Volta</strong>: nói đến dòng này chúng ta có những sự lựa chọn chủ yếu liên quan tới V100 Tensor Core mà đại diện cho thuê là <code class="language-plaintext highlighter-rouge">p3.16xlarge</code> và <code class="language-plaintext highlighter-rouge">p3dn.24xlarge</code>.</li>
</ul>

<blockquote>
  <p>Với băng thông mạng của phiên bản P3.16xlarge cao hơn tới 4 lần, phiên bản P3dn.24xlarge của Amazon EC2 là sự bổ sung mới nhất cho dòng phiên bản P3, được tối ưu hóa cho machine learning phân tán và các ứng dụng HPC. Các phiên bản này cung cấp thông lượng kết nối mạng lên tới 100 Gbps, 96 vCPU Intel® Xeon® Có thể mở rộng (Skylake) tùy chỉnh, 8 GPU NVIDIA® V100 Tensor Core với 32 GB bộ nhớ mỗi GPU và 1,8 TB ổ lưu trữ SSD cục bộ chuẩn NVMe. Các phiên bản P3dn.24xlarge cũng hỗ trợ Elastic Fabric Adapter (EFA). Giao diện này tăng tốc các ứng dụng machine learning phân tán sử dụng Thư viện giao tiếp chung NVIDIA (NCCL). EFA có thể mở rộng quy mô lên đến hàng nghìn GPU, cải thiện đáng kể thông lượng và khả năng mở rộng của các mô hình huấn luyện deep learning, từ đó cho kết quả nhanh hơn.
Source: Amazon Web Service</p>
</blockquote>

<ul>
  <li>NVIDIA T4 hay <strong>Turing</strong>: với AWS EC2 thì bạn có thể thuê <code class="language-plaintext highlighter-rouge">g4dn.metal</code>.</li>
  <li>NVIDIA A100 hay <strong>Ampere</strong>: Với AWS EC2 thì có thể thuê <code class="language-plaintext highlighter-rouge">p4d.24xlarge</code>, với Azure HPC thì có thể thuê <code class="language-plaintext highlighter-rouge">Standard_ND96amsr_A100_v4</code>. GCP thì có <code class="language-plaintext highlighter-rouge">a2-highgpu-8g</code> hoặc bản 16 GPU là <code class="language-plaintext highlighter-rouge">a2-highgpu-16g</code>.</li>
</ul>

<p>Thì về mặt spec Ampere là khỏe nhất nếu nói về TFLOPS.
Dưới đây là bảng giá thành của NVIDIA 8x A100 Tensor Core.
Trong bảng này có 2 cột mà các bạn nên để ý là giá thành thuê theo giờ (<strong>Hourly cost</strong>) và tỷ lệ GFLOPS/USD (đáng giá thế nào).
Giả định chung là hệ thống được xây dựng tối thiểu 4x GPU và được dùng ít nhất 24 tháng, mỗi tháng dùng 22 ngày (T7/CN nghỉ ngơi).</p>

<p><img src="/assets/images/ampere.png" alt="ampere" /></p>

<p>Nói chung tự build thì các bạn có thể tham khảo cấu hình của DGX-1<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">3</a></sup>, DGX-2<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">4</a></sup>, và DGX-A100<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">5</a></sup> để mua các bộ phận về tự ráp thì sẽ tiết kiệm công lắp ráp, nhưng nhìn chung tôi nghĩ cũng phải 50 ngàn Mỹ Kim.</p>

<h2 id="các-cloud-solutions">Các cloud solutions</h2>
<p>Trong trường hợp bạn có bài toán train dữ liệu mà mất hàng tháng trời train với GTX/RTX thì bạn sẽ nghĩ phải thuê GPUs trên data center (8x-16x GPU).
Thì ngoài AWS/Azure/GCP là khá cùng rank nên bảng giá không chênh lệch nhau mấy, bạn có thể tham khảo thêm các trang cho thuê GPU bên ngoài để tìm được chỗ thuê hợp lý hơn.
Như kết quả tìm kiếm của AIFI thì hiện tại có trang <a href="https://vast.ai">vast.ai</a> cung cấp khá nhiều sự lựa chọn cho thuê ở mức giá thấp hơn 5 USD/hour.</p>

<h2 id="còn-lời-giải-nào-khác">Còn lời giải nào khác?</h2>

<p>Nhìn chung tự build thì có hai khả năng:</p>
<ul>
  <li><strong>Mua đồ sẵn</strong> như DGX<sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">3</a></sup><sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">4</a></sup><sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">5</a></sup> thì các bạn cứ chuẩn bị 100k Mỹ kim trở lên.</li>
  <li><strong>Mua bộ phận về tự ráp</strong> thì các bác tham khảo cấu hình của DGX rồi độ lại tùy theo nhu cầu. Tuy nhiên, chắc chỉ giảm được tiền công, và tối ưu một chút kiểu DGX dùng nhiều RAM thì mình giảm RAM xuống. Nói chung chắc cũng phải 50K Mỹ Kim.</li>
</ul>

<p>Về cá nhân, tôi thiên về thuê!
Nếu tự build thì mua mấy cái RTX/GTX dòng Ti là ổn rồi.
Tuy nhiên nếu bài toán lớn thì bạn bắt buộc phải dùng data center GPU thì lúc ấy phải có <strong>TIỀN</strong>!</p>

<blockquote>
  <p><strong>Vấn đề của đi thuê là tiền tính theo giờ nên bạn cần phải ước lượng được số giờ sử dụng.</strong> Nếu tầm trên 200h/tháng, tôi nghĩ nên thuê theo năm hoặc 3 năm.
Spot price thì cũng tàm tạm thôi, vì mất công chờ với nó ngắt điện (interupt) mình cũng phải chịu ấy, nên là rẻ nhưng lại mất thời gian chờ và bị ngắt.
Mà vấn đề với Spot là <strong>nó không có luôn ấy (phải chờ đến khi cái server ấy nó open mình mới được dùng)</strong>.</p>
</blockquote>

<h1 id="kết-luận">Kết luận</h1>

<blockquote>
  <p>Hơi buồn nhưng <code class="language-plaintext highlighter-rouge">đầu tiên vẫn là ... tiền đâu</code></p>
</blockquote>

<p>Như vậy chúng ta đã điểm qua một số SOTAs của <code class="language-plaintext highlighter-rouge">seq2seq</code> cho Vision và nhìn chung các <code class="language-plaintext highlighter-rouge">seq2seq</code> vẫn đang nắm vị trí số 1.
Tuy vậy, vấn đề lớn khi “đua đòi” vào mảng này thì vẫn là tài nguyên thôi.
Nếu chuẩn bị được budget và plan nghiên cứu nghiêm chỉnh (mà đầu tiên là tiền đâu) thì về mặt nghiệp vụ PM tôi nghĩ không nên triển khai làm gì mất time anh em.
Ít nhất là cần vốn 200k Mỹ Kim thì cũng phải có tầm 100k trong túi hãy nghĩ!</p>

<h1 id="tài-liệu-tham-khảo">Tài liệu tham khảo</h1>

<ol class="bibliography"><li><span id="dosovitskiy2020image">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. <i>arXiv preprint arXiv:2010.11929</i>. (2020).</span><a class="details" href="https://wanted2.github.io/bibliography/dosovitskiy2020image/">Details</a></li>
<li><span id="carion2020end">Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. <i>European conference on computer vision</i> (2020), 213–229.</span><a class="details" href="https://wanted2.github.io/bibliography/carion2020end/">Details</a></li>
<li><span id="gao2021fast">Gao, P., Zheng, M., Wang, X., Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i> (2021), 3621–3630.</span><a class="details" href="https://wanted2.github.io/bibliography/gao2021fast/">Details</a></li>
<li><span id="redmon2018yolov3">Redmon, J. and Farhadi, A. 2018. Yolov3: An incremental improvement. <i>arXiv preprint arXiv:1804.02767</i>. (2018).</span><a class="details" href="https://wanted2.github.io/bibliography/redmon2018yolov3/">Details</a></li>
<li><span id="ge2021yolox">Ge, Z., Liu, S., Wang, F., Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. <i>arXiv preprint arXiv:2107.08430</i>. (2021).</span><a class="details" href="https://wanted2.github.io/bibliography/ge2021yolox/">Details</a></li>
<li><span id="lin2014microsoft">Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L. 2014. Microsoft coco: Common objects in context. <i>European conference on computer vision</i> (2014), 740–755.</span><a class="details" href="https://wanted2.github.io/bibliography/lin2014microsoft/">Details</a></li>
<li><span id="kuznetsova2020open">Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T. and Ferrari, V. 2020. The open images dataset v4. <i>International Journal of Computer Vision</i>. 128, 7 (2020), 1956–1981.</span><a class="details" href="https://wanted2.github.io/bibliography/kuznetsova2020open/">Details</a></li></ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:4" role="doc-endnote">
      <p><a href="https://wanted2.github.io/seq2seq/">https://wanted2.github.io/seq2seq/</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:4:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p><a href="https://wanted2.github.io/speech/">https://wanted2.github.io/speech/</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:5:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://www.nvidia.com/en-us/data-center/dgx-1/">https://www.nvidia.com/en-us/data-center/dgx-1/</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://www.nvidia.com/en-us/data-center/dgx-2/">https://www.nvidia.com/en-us/data-center/dgx-2/</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://www.nvidia.com/en-us/data-center/dgx-a100/">https://www.nvidia.com/en-us/data-center/dgx-a100/</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2022-02-12">12 Feb 2022</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Computer-Vision">Computer Vision</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#BERT">#BERT</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Image-Segmentation">#Image Segmentation</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Object-Detection">#Object Detection</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Object-Recognition">#Object Recognition</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Self-supervised-learning">#Self-supervised learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Sequence-to-sequence">#Sequence-to-sequence</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Temporal-Segmentation">#Temporal Segmentation</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Transformer">#Transformer</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Vision-Transformer">#Vision Transformer</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#seq2seq">#seq2seq</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="https://wanted2.github.io//ml-ids/"> &laquo; Machine Learning for Network Intrusion Detection: From Local to Production</a>
            
            
            <a class="next d-block col-md-6 text-lg-right" href="https://wanted2.github.io//edge-cloud-tpu/">Edge-Cloud architectures & TPU resources &raquo; </a>
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'caineng'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi" style="max-height: 48px;" /> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://caineng.us20.list-manage.com/subscribe/post?u=76342d3d74a6807aac5aec0d7&id=b5645e19be" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Site-Reliable-Engineering">Site Reliable Engineering (13)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Software-Engineering">Software Engineering (37)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Computer-Vision">Computer Vision (6)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence (17)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語 (34)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Project-Management">Project Management (33)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2022 AiFi 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="https://wanted2.github.io/assets/js/mediumish.js"></script>


<script src="https://wanted2.github.io/assets/js/lazyload.js"></script>


<script src="https://wanted2.github.io/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//caineng.disqus.com/count.js"></script>


</body>
</html>
