<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AiFi</title>
    <description>An AI Engineer&apos;s blog</description>
    <link>https://wanted2.github.io/</link>
    <atom:link href="https://wanted2.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 22 Feb 2022 21:53:08 +0900</pubDate>
    <lastBuildDate>Tue, 22 Feb 2022 21:53:08 +0900</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      <item>
        <title>Speech and Sequence-to-sequence</title>
        <description>&lt;p&gt;Như trong &lt;a href=&quot;/seq2seq/&quot;&gt;bài viết trước&lt;/a&gt; thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; đều đang làm chủ.
Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; và các hậu duệ (thuộc dòng dõi Transformer &lt;a class=&quot;citation&quot; href=&quot;#vaswani2017attention&quot;&gt;[1]&lt;/a&gt; và BERT &lt;a class=&quot;citation&quot; href=&quot;#devlin2019bert&quot;&gt;[2, 3]&lt;/a&gt;) cũng đang nắm thế chủ động.
Khởi đầu bài viết, tôi định viết lại về i-vector &lt;a class=&quot;citation&quot; href=&quot;#kenny2007joint&quot;&gt;[4, 5]&lt;/a&gt; và x-vector &lt;a class=&quot;citation&quot; href=&quot;#snyder2018x&quot;&gt;[6]&lt;/a&gt; là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer &lt;a class=&quot;citation&quot; href=&quot;#gulati2020conformer&quot;&gt;[7, 8]&lt;/a&gt; &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.
Với bài toán SI thì cách làm cổ điển sẽ là dùng &lt;strong&gt;Gaussian Mixture Models (GMM, &lt;a class=&quot;citation&quot; href=&quot;#reynolds1995robust&quot;&gt;[9, 10, 4, 5]&lt;/a&gt;)&lt;/strong&gt;, còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì &lt;strong&gt;Deep Neural Networks (DNN)&lt;/strong&gt; với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, &lt;a class=&quot;citation&quot; href=&quot;#snyder2018x&quot;&gt;[6]&lt;/a&gt;) lẫn ASR (DNN-HMM, &lt;a class=&quot;citation&quot; href=&quot;#hinton2012deep&quot;&gt;[11]&lt;/a&gt;).
Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy.
Gần đây thì đến lượt những tiến bộ bắt nguồn từ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; DNN-RNN, rồi đến những model end-to-end, transformers.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;giới-thiệu-chung-về-asrsi-và-các-bài-toán-liên-quan&quot;&gt;Giới thiệu chung về ASR/SI và các bài toán liên quan&lt;/h1&gt;

&lt;h2 id=&quot;automatic-speech-recognition&quot;&gt;Automatic Speech Recognition&lt;/h2&gt;
&lt;p&gt;Automatic Speech Recognition (ASR, &lt;a class=&quot;citation&quot; href=&quot;#yu2016automatic&quot;&gt;[12]&lt;/a&gt;) là nhiệm vụ chuyển một chuỗi âm thanh giọng nói (waveform) sang một chuỗi văn bản được chứa trong âm thanh đó.
Cách làm truyền thống nhất là sử dụng &lt;strong&gt;Hidden Markov Model (HMM)&lt;/strong&gt; để mô hình thông tin chuỗi, và dùng GMM để fit lượng thông tin của các states trong HMM &lt;a class=&quot;citation&quot; href=&quot;#young2008hmms&quot;&gt;[13]&lt;/a&gt; vào 1 phân bố có sẵn và đưa ra nhận dạng cho âm thanh trong time window tương ứng.
Tuy nhiên, GMM có nhược điểm là khó có thể mô hình được dữ liệu phi tuyến tính (non-linear manifold) dẫn đến nếu lượng dữ liệu ít có thể xảy ra tình trạng overfit hoặc model HMM không mô tả hết manifold dữ liệu (ếch ngồi đáy giếng).
Do đó, thông qua kết quả thực nghiệm &lt;a class=&quot;citation&quot; href=&quot;#hinton2012deep&quot;&gt;[11]&lt;/a&gt;, thì nếu thay GMM bởi DNN để xuất ra xác suất của từng âm một từ states của HMM thì kết quả outperform GMM-HMM trong nhiều bộ dữ liệu lớn.
Và quan trọng là càng train nhiều dữ liệu thì model DNN-HMM càng stable.&lt;/p&gt;

&lt;p&gt;Nhìn chung DNN-HMM thì cũng là một kiểu model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; tức là chuyển chuỗi (audio) thành chuỗi (text hoặc phoneme &lt;a class=&quot;citation&quot; href=&quot;#graves2013speech&quot;&gt;[14, 15]&lt;/a&gt;).
Vì vậy khá đơn giản để ứng dụng model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; vào ASR.
Chuỗi âm thanh (waveform) được chuyển thành chuỗi đặc trưng MFCC và rồi input vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.
Và việc này cũng giúp chúng ta loại bỏ luôn HMM để thực hiện training &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;end-to-end&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Khi decode output của end-to-end (E2E) network, với giả thiết chúng ta có input là chuỗi \(X=\left\{\mathbf{x}_t\right\}_{t=1}^T\subset\mathbb{R}^d\) và output của E2E network là vector xác suất \(\mathbf{p}_t=\left[p_{t,0},p_{t,2},\ldots,p_{t,N}\right]^\top\in [0,1]^N\) với \(p_{t,i}\) là xác suất của từ thứ \(i\) trong vocabulary ở bước thứ \(t\).
Và hiển nhiên \(\sum_{i=1}^Np_{t,i}=1\).
Ở đây ta có vocabulary \(V=\left\{\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_N\right\}\) gồm \(N\) từ.
Ta sẽ phải tính thêm những quãng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ngừng&lt;/code&gt; trong output, nên ta thêm từ rỗng vào vocabulary \(\overline{V}=V\cup\phi\).
&lt;strong&gt;Vậy xác suất của một chuỗi phoneme như \(W=\left[\mathbf{w}_{i_1},\mathbf{w}_{i_2},\ldots,\mathbf{w}_{i_M}\right], M\leq T, 1\leq i_j\leq N\) là bao nhiêu?&lt;/strong&gt;
Chúng ta cần hiểu là từ chuỗi \(P=\left[\mathbf{p}_1,\ldots,\mathbf{p}_T\right]\) để chuyển sang chuỗi \(W\) thì đã thêm từ rỗng \(\phi\) xen kẽ vào \(W\) để có độ dài \(T\).
Ta định nghĩa một ánh xạ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;many-to-one&lt;/code&gt; \(f: V^T\rightarrow V^{\leq T}\) là một phép toán xóa bỏ tất cả các từ rỗng khỏi từ có độ dài \(T\).
Vậy phép toán ngược &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;one-to-many&lt;/code&gt; \(f^{-1}: V^{\leq T}\rightarrow 2^{V^T}\).
Và \(f^{-1}(W)\) là tập các từ có độ dài \(T\) mà loại bỏ hết từ rỗng đi thì còn lại \(W\).
Xác xuất của 1 từ \(\pi = \left[\mathbf{w}_{l_1},\mathbf{w}_{l_2},\ldots,\mathbf{w}_{l_T}\right]\in f^{-1}(W) \subset V^T\) là&lt;/p&gt;

\[p\left(\pi \mid \mathbf{\theta};X\right) = \sum_{t=1}^T p\left(\mathbf{w}_{l_t}\mid \mathbf{\theta};\mathbf{x}_t\right)=\sum_{t=1}^Tp_{t,l_t}\]

&lt;p&gt;trong đó \(\mathbf{\theta}\) là parameters của model E2E.
Vậy xác xuất để model output ra cụm từ \(W\) là&lt;/p&gt;

\[p\left(W\mid \mathbf{\theta};X\right)=\sum_{\pi\in f^{-1}(W)}p\left(\pi\mid \mathbf{\theta};X\right)\]

&lt;p&gt;&lt;strong&gt;Vấn đề bây giờ rút gọn lại thành tìm ứng với model \(\mathbf{\theta}\) và chuỗi đầu vào \(X\) thì cần phải decode ra cụm output \(W\) có độ dài không quá \(T\) tức là tìm ra \(W\) thỏa mãn:&lt;/strong&gt;&lt;/p&gt;

\[W=\mbox{argmax}_{W\in V^{\leq T}}p\left(W\mid \mathbf{\theta};X\right)=\mbox{argmax}_{W\in V^{\leq T}}\sum_{\pi\in f^{-1}(W)}p\left(\pi\mid \mathbf{\theta};X\right)\]

&lt;p&gt;Bài toán đặt ra là số lượng khả năng cần phải tính toán là vô cùng nhiều, do đó cách làm hiệu quả rơi vào 1 trong hai cách:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dùng &lt;strong&gt;dynamic programming&lt;/strong&gt; để tìm ra &lt;strong&gt;alignment&lt;/strong&gt; dựa theo công thức ở trên. Phương pháp này thi thoảng gọi là &lt;strong&gt;Connectionist Temporal Segmentation (CTC)&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Dùng &lt;strong&gt;transducer&lt;/strong&gt;: tạo ra một network để học &lt;strong&gt;alignment&lt;/strong&gt; thông qua backpropagation và dựa vào phoneme ở step trước.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transducer thì nhanh hơn rất nhiều so với dynamic programming, và vì xác suất của từng chuỗi \(p\left(\pi\mid \mathbf{\theta};X\right)\) được mô hình bằng neural network nên tính toán có tính thích ứng cao hơn với dữ liệu.&lt;/p&gt;

&lt;p&gt;Nhìn chung, task ASR khá thuận lợi cho việc ứng dụng mô hình &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; như Transformer hay BERT.
Dữ liệu thì cũng dồi dào nhất là tiếng Anh với hàng ngàn giờ đọc cuả đủ loại speech như audio books &lt;a class=&quot;citation&quot; href=&quot;#panayotov2015librispeech&quot;&gt;[16]&lt;/a&gt;, báo chí Wall Street Journal &lt;a class=&quot;citation&quot; href=&quot;#paul1992design&quot;&gt;[17]&lt;/a&gt;, phát âm &lt;a class=&quot;citation&quot; href=&quot;#garofolo1993darpa&quot;&gt;[18]&lt;/a&gt;, .v.v…
Thì những bộ dữ liệu trên thuộc nhóm task &lt;strong&gt;conversational speech recognition&lt;/strong&gt;.
Những task này đòi hỏi phải transcribe ngay khi âm thanh phát ra.
Ngoài ra cũng có nhóm task &lt;strong&gt;command speech recognition&lt;/strong&gt; tức là kiểu như người dùng nói vào loa Echo và câu lệnh được ghi nhận.
Đó thường là các câu lệnh ngắn.
Với tiếng Anh, có bộ dữ liệu Speech Commands &lt;a class=&quot;citation&quot; href=&quot;#warden2018speech&quot;&gt;[19]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Code kiếc, tool thiếc cũng nói chung sẵn có dồi dào: ESPNet &lt;a class=&quot;citation&quot; href=&quot;#watanabe2018espnet&quot;&gt;[20]&lt;/a&gt;, Microsoft ASR &lt;a class=&quot;citation&quot; href=&quot;#deng2013recent&quot;&gt;[21]&lt;/a&gt;, DeepSpeech v1 &amp;amp; v2 &lt;a class=&quot;citation&quot; href=&quot;#hannun2014deepspeech&quot;&gt;[22, 23]&lt;/a&gt;, PaddlePaddle &lt;a class=&quot;citation&quot; href=&quot;#ao2021end&quot;&gt;[24, 25]&lt;/a&gt; hay &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#ott2019fairseq&quot;&gt;[26]&lt;/a&gt;.
Mà những công việc chuẩn bị dữ liệu &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;recipes&lt;/code&gt; các cái là có sẵn trong các framework trên rồi như là với ESPNet bạn có thể tham khảo tại:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/espnet/espnet/tree/master/egs2&quot;&gt;https://github.com/espnet/espnet/tree/master/egs2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nên nói chung là cũng chả cần code mấy đâu, chủ yếu là code vài dòng thể hiện ý tưởng.
Nếu bạn thích &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ăn sẵn&lt;/code&gt; hơn nữa thì đấy lại có cái Amazon Lex v2&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sound-classification-và-speaker-identification&quot;&gt;Sound classification và Speaker Identification&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Sound classification.&lt;/strong&gt; Ứng với một âm thanh bất kỳ, tag âm thanh đó bởi một &lt;a class=&quot;citation&quot; href=&quot;#gemmeke2017audioset&quot;&gt;[27]&lt;/a&gt; hoặc nhiều labels &lt;a class=&quot;citation&quot; href=&quot;#fonseca2019audio&quot;&gt;[28]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://storage.googleapis.com/kaggle-media/competitions/freesound/task2_freesound_audio_tagging.png&quot; alt=&quot;FAT2019&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Một vấn đề xuất phát từ community này là vấn đề &lt;em&gt;label noise&lt;/em&gt;.
Ví dụ như Freesound Audio Taggings 2019 (FAT 2019, &lt;a class=&quot;citation&quot; href=&quot;#fonseca2019audio&quot;&gt;[28]&lt;/a&gt;) cung cấp 1 bộ dữ liệu noise mở ra nhiều hướng đi mới như unsupervised/semi-supervised và learning from noisy data.
Nhìn chung ngoài xử lý dữ liệu mel spectrogram để lấy ra features thì hầu như phần còn lại như 1 bài toán classification bình thường.
Kiến trúc thì có thể dùng recurrent neural network và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; để học.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Speaker Identification.&lt;/strong&gt; Nhìn chung thì xử lý lấy features mel spectrogram xong thì có thể sử dụng để nhận dạng người dùng, giống như nhận dạng khuôn mặt hay vân tay.
Những cách làm truyền thống của bài toán SI thì chủ yếu dùng GMM &lt;a class=&quot;citation&quot; href=&quot;#reynolds1995robust&quot;&gt;[9, 10, 4, 5]&lt;/a&gt;.
Một model background sẽ được extract từ toàn bộ tập train (không phân biệt cá thể, identity), sau đó thì ứng với mỗi identity thì mình lại xây dựng &lt;em&gt;supervector&lt;/em&gt;.
Nói chung tôi cũng không thấy có gì mới trong hướng đi này.
Gần đây thì có thêm cái DNN embeddings &lt;a class=&quot;citation&quot; href=&quot;#snyder2018x&quot;&gt;[6]&lt;/a&gt; thì vẫn chỉ là thay bởi DNN và chứng tỏ DNN tốt hơn GMM supervectors.
Ngoài ra thì có những hướng đi như contrastive metric learning, deep metric learning, .v.v…
Nói chung trừ cái chỗ extract MFCC ra thì tôi thấy cũng không khác mấy bài toán identification trong các dữ liệu hình ảnh (face và fingerprint).
Mảng này chắc có cái bài thú vị là &lt;strong&gt;Speaker Diarization&lt;/strong&gt; thì trong 1 đoạn hội thoại của nhiều người, thì segment ra từng đoạn xem là ai nói.
Nhìn chung cách làm truyền thống vẫn là GMM và supervector, nhưng gần đây chúng tôi có thêm mấy cái temporal segment networks.
Nói chung so với bên hình ảnh thì ngoài cái phần features âm thanh ra, phần còn lại (machine learning và pattern recognition) thì cũng không quá khác biệt (dùng chung công nghệ).&lt;/p&gt;

&lt;h2 id=&quot;voice-conversion-text-to-speech-và-speech-synthesis&quot;&gt;Voice Conversion, Text-To-Speech và Speech Synthesis&lt;/h2&gt;
&lt;p&gt;Cái mảng Voice Transformation (VT, &lt;a class=&quot;citation&quot; href=&quot;#stylianou2008voice&quot;&gt;[29]&lt;/a&gt;) thì phần cổ điển ở trong SGK &lt;a class=&quot;citation&quot; href=&quot;#benesty2008springer&quot;&gt;[30, 31]&lt;/a&gt; rồi, nên mình chỉ tập hợp xem mảng này gần đây họ làm cái gì thôi nhé.
Đứng về phía ứng dụng bảo mật mà nói thì nói chung mấy cái speech synthesis với voice conversion này là xếp vào hạng mục &lt;strong&gt;spoofing attacks&lt;/strong&gt; vào hệ thống thông tin.
Tức là mình tìm cách bắt chước giọng nói với tạo ra giọng nói giống thật thì mình là kẻ tấn công rồi!&lt;/p&gt;

&lt;p&gt;Nhìn chung cũng có ứng dụng khác như lồng tiếng nhân vật hoạt hình, game, … nhưng nói chung mấy cái tạo dữ liệu giả mạo kiểu synthesis với voice clone này thì có vẻ là nhiều về mảng bảo vệ hệ thống thông tin.
Thì về hướng này có thể kể đến các chuỗi challenges như ASVspoof &lt;a class=&quot;citation&quot; href=&quot;#wu2015asvspoof&quot;&gt;[32, 33, 34, 35, 36]&lt;/a&gt; hoặc Voice Conversion Challenge &lt;a class=&quot;citation&quot; href=&quot;#toda2016voice&quot;&gt;[37, 38, 39]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/spoof-voice.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Bức tranh lớn về mảng anti-spoofing&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nhìn vào bức tranh lớn trên thì trong chuỗi challenge ASVspoof từ năm 2015, mọi người đã tập trung giải quyết các vấn đề như các đòn tấn công &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech synthesis&lt;/code&gt; hay &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;voice conversion&lt;/code&gt;, đến 2017 thì chúng ta làm thêm đòn tấn công &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replay&lt;/code&gt;, năm 2019 thì cũng không có đòn nào mới, nhưng mình mở rộng bộ dữ liệu của các đòn tấn công lên.
Bản thân tôi thấy cái mảng này có vẻ cũng thích tấn công hơn là phòng thủ, nên là có vẻ lại bài toán phân biệt thật giả nhãn hiệu Goodfellow.
Thế là lại hai bên tấn công phòng thủ đuổi bắt lẫn nhau, bên tấn công càng tinh vi thì bên phòng thủ cũng lại nâng cấp vũ trang.
Kết cục là đuổi bắt không có hồi kết mà ông Goodfellow ông ấy viết mấy cái mô hình neuralnetwork với mấy cái chuyên gia làm hàng giả.
Thì bây giờ cái này là bên tấn công thì cần phải xem đòn tấn công phải tạo ra sản phẩm “nhái” giống hệt, càng giống, càng tự nhiên càng tốt.
Rồi nảy sinh cái cycle consitency loss để check xem nhái giống chưa?
Tuy nhiên vấn đề speech thì có một cái là người phải nghe giống thì mới OK, nên lại phải có &lt;strong&gt;subjective evaluation&lt;/strong&gt; tức là bỏ tiền ra thuê mấy đối tượng thí nghiệm nghe và đánh giá chất lượng âm thanh.
Thì họ lại nghĩ cái metric là &lt;strong&gt;Mean Opinion Score (MOS)&lt;/strong&gt; tức là lấy ý kiến những người đánh giá, rồi cộng điểm lấy trung bình, xem đoạn âm thanh đã được nhái ổn thỏa chưa.&lt;/p&gt;

&lt;p&gt;Đấy nên tôi thấy mấy cái ứng dụng TTS với synthesis, voice conversion nhìn thì hay hay, xem mấy cái demo cũng này kia phết, nhưng chắc chủ yếu là quay lại chỗ detect &lt;strong&gt;hàng nhái&lt;/strong&gt; là chủ yếu.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/spoof-det.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Trên đây là một hệ thống phát hiện hàng nhái âm thanh, mà tức là classifier mang tính phòng thủ.
Thì bây giờ có vẻ có nhiều hệ thống tấn công rồi, chắc sắp tới cũng phải nâng cấp hệ thống phòng thủ cho nó tương xứng.&lt;/p&gt;

&lt;p&gt;Nếu theo dõi kết quả của VCC2020 &lt;a class=&quot;citation&quot; href=&quot;#Yi2020vcc&quot;&gt;[39]&lt;/a&gt; thì có vẻ là những team sử dụng mô hình encoder-decoder (tức là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;) đạt kết quả tốt hơn các team dùng GAN.
Ngoài đánh giá bằng MOS thì để đánh giá chất lượng giọng “nhái” cũng sử dụng similarity scores.
Với task VCC sẽ có 2 kiểu bài toán là parallel (target audio cũng nói cùng 1 câu với source audio, chỉ khác giọng) hoặc non-parallel (target audio và source khác nhau cả giọng lẫn nội dung).
ASVspoof thì cũng là challenge sản xuất hàng nhái nhưng có nhiều attacks hơn mỗi voice cloning.
Nhìn chung, method dẫn đầu cũng là 1 hệ thống end-to-end TTS nốt.&lt;/p&gt;

&lt;h1 id=&quot;công-nghệ-chính&quot;&gt;Công nghệ chính&lt;/h1&gt;

&lt;p&gt;Để extract được feature MFCC thì các bạn có thể dùng thư viện &lt;a href=&quot;https://librosa.org/doc/main/generated/librosa.feature.mfcc.html#librosa.feature.mfcc&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;librosa&lt;/code&gt;&lt;/a&gt; hoặc &lt;a href=&quot;https://projets-lium.univ-lemans.fr/sidekit/api/featuresextractor.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sidekit&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# Dùng librosa
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;librosa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mfcc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;sr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;44100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;n_mfcc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;lifter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;hop_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;441&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;win_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1102&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;n_fft&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;n_mels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;26&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;ortho&apos;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Trong ví dụ trên tôi extract MFCC từ audio với độ rộng window là 25ms và bước rộng là 10ms (ví dụ, bước thứ nhất sẽ là 25ms + 10ms = 35ms).
Audio được sample ở 44kHz.
Xử lý extract MFCC thì quá quen thuộc rồi, các bạn cần chia audio thành các frames theo từng bước 10ms, và độ rộng 25ms nói trên.
Sau đó trong từng frame, các bạn tính biểu diễn Fourier rời rạc cũng như spectrogram của window.
Vì tai người khó phân biệt được thay đổi ở tần số thấp nên các bạn cần lọc tín trong không gian tần số theo filter bank để có được feature tốt.
Sau đó các bạn tính hệ số MFCC, và chỉ cần giữ lại 13-20 hệ số đầu (thường chỉ dùng 13 hệ số đâù là đủ).
Như vậy mỗi frame window sẽ được biểu diễn bởi 1 vector 13-20 chiều, và audio có 489 frames thì sẽ được biểu diễn bởi ma trận \(489\times d\). 
Sau khi sử dụng filter bank, bạn cũng cần dùng Discrete Cosine Transform để lấy được feature robust.&lt;/p&gt;

&lt;h2 id=&quot;i-vector-và-x-vector-cho-bài-toán-speaker-identification&quot;&gt;i-vector và x-vector cho bài toán Speaker Identification&lt;/h2&gt;
&lt;p&gt;Thư viện &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sidekit&lt;/code&gt; cung cấp implementations tối ưu (song song hóa đa nhiệm) cho cả &lt;a href=&quot;https://projets-lium.univ-lemans.fr/sidekit/api/factoranalyser.html&quot;&gt;i-vector&lt;/a&gt; và &lt;a href=&quot;https://projets-lium.univ-lemans.fr/sidekit/api/nnet/xvector.html&quot;&gt;x-vector&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Mô hình i-vector như các bạn đã biết:&lt;/p&gt;

\[\begin{eqnarray}
\underbrace{\mathbf{S}}_{\mbox{super-vector}}&amp;amp;=&amp;amp;\underbrace{\mathbf{u}}_{\mbox{speaker-independent UBM vector}}\\\newline
&amp;amp;+&amp;amp;\underbrace{\mathbf{T}}_{\mbox{total variability matrix}}\underbrace{\mathbf{m}}_{\mbox{i-vector}}\sim \mathcal{N}\left(\mathbf{0};\mathbf{T}\mathbf{T}^\top\right)
\end{eqnarray}\]

&lt;p&gt;i-vector được giả định là \(\mathbf{m}\sim\mathcal{N}\left(\mathbf{0};\mathbf{I}\right)\) tức là tuân theo phân bố chuẩn. Như vậy, để có thể extract i-vector, các bạn cần train 2 models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Universal Background Model (GMM)&lt;/li&gt;
  &lt;li&gt;Total Variability (TV)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cả 2 đều train bằng thuật toán EM cả nên rất là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OK EM&lt;/code&gt;.
Ví dụ, các bạn có thể train model UBM với 32 components (thường nên chọn từ 32 components trở lên):&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.mixture&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GMM&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gmm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GMM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Thì ta cứ lấy hết đặc trưng MFCC của tất cả audio files trong bộ train là có ma trận \(X\) thôi.
Training Total Variability đòi hỏi phải tính toán zero-order và first-order statistics như trong &lt;a href=&quot;https://projets-lium.univ-lemans.fr/sidekit/tutorial/tv_estimation.html&quot;&gt;tutorial&lt;/a&gt; của sidekit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/i-vector-timeline.jpg&quot; alt=&quot;speaker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Implementation của &lt;a href=&quot;https://projets-lium.univ-lemans.fr/sidekit/_modules/nnet/xvector.html&quot;&gt;x-vector&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;seq2seq-và-các-transformers-cho-bài-toán-asr&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; và các Transformers cho bài toán ASR&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Listen, Attend and Spell (LAS, &lt;a class=&quot;citation&quot; href=&quot;#chan2016listen&quot;&gt;[40]&lt;/a&gt;).&lt;/strong&gt; LAS là 1 kiến trúc kiểu encoder-decoder (listener-speller).
Phần listener được biểu hiện bằng 1 stacked Bidirectional LSTM (BiLSTM).
Tuy nhiên, có cải tiến là cho nhỏ size của hidden layers ở mỗi tầng cao hơn.
Thì nói chung nó là 1 cái BiLSTM (tôi nghĩ cũng không cần giảm size khi lên tầng cao hơn đâu, cứ để nguyên size ấy cũng được! Cứ tầm 256k/năm là ok rồi.)
Thì công thức chung của BiLSTM với \(\mathbf{h}^j_i\) là biểu diễn &lt;em&gt;ẩn&lt;/em&gt; của tầng \(j\) cho step thứ \(i\).&lt;/p&gt;

\[\mathbf{h}^j_i=\mbox{BiLSTM}\left(\mathbf{h}^j_{i-1},\mathbf{h}^{j-1}_i\right)\]

&lt;p&gt;Còn như LAS thì giảm size khi “thăng chức” lên tầng cao thì sẽ là:&lt;/p&gt;

\[\mathbf{h}^j_i=\mbox{pyramid-BiLSTM}\left(\mathbf{h}^j_{i-1},[\mathbf{h}^{j-1}_{2i}; \mathbf{h}^{j-1}_{2i+1}]\right)\]

&lt;p&gt;Tức là concat (nối) biểu diễn ở tầng dưới nhưng là 2 biểu diễn liên tiếp \(\mathbf{h}^{j-1}_{2i}; \mathbf{h}^{j-1}_{2i+1}\) nhờ vậy mà độ dài của chuỗi output giảm đi một nửa sau mỗi tầng.
Trong bài báo họ có 3 tầng cao, nên độ dài chuỗi biểu diễn ẩn output cuối của listener sẽ giảm đi chỉ còn bằng 1/8.&lt;/p&gt;

&lt;p&gt;Speller thì họ xài một cái transducer bằng encoder LSTM: attention dùng trạng thái ẩn của encoder và output của listener để tính, và output ra xác suất.&lt;/p&gt;

&lt;p&gt;Các tác giả lý luận tại sao thăng chức thì lại giảm size (tức là tại sao phải dùng cái pyramid?) thì là vì input có thể có hàng ngàn frames, mà nội dung có khi chỉ là câu văn có vài chục âm (phone).
Thực ra nếu không giảm size có thể dùng CTC hoặc transducer kiểu như của Graves.
&lt;em&gt;Learning&lt;/em&gt; thì lại dùng hàm loss là log probability, còn decode thì lại dùng beam search.
Một điểm chú ý với bên âm thanh là nên có &lt;em&gt;LM rescoring&lt;/em&gt;.
Thì bình thường là xác suất điều kiện của chuỗi output \(\mathbf{y}\) khi có chuỗi input \(\mathbf{x}\) là \(p\left(\mathbf{y}\mid\mathbf{x}\right)\).
Nhưng giờ có thêm language model được train trên corpus của NLP thì mình có thêm xác xuất \(p_{LM}\left(\mathbf{y}\mid\mathbf{x}\right)\).
Vì vậy mình có thể combine lại thành score mới khi làm beam search:&lt;/p&gt;

\[s\left(\mathbf{y}\mid\mathbf{x}\right)=\frac{\log p\left(\mathbf{y}\mid\mathbf{x}\right)}{\left|\mathbf{y}\right|}+\lambda \log p_{LM}\left(\mathbf{y}\mid\mathbf{x}\right)\]

&lt;p&gt;Lý do chia cho độ dài chuỗi output \(\mathbf{y}\) là vì model có khuynh hướng bias vào âm ngắn.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SpecAugment &lt;a class=&quot;citation&quot; href=&quot;#park2019specaugment&quot;&gt;[41]&lt;/a&gt;.&lt;/strong&gt; SpechAugment cũng dùng LAS nhưng họ thực hiện pretraining theo kiểu pretext là phục hồi đoạn dữ liệu bị xóa trong mel spectrogram: time warping, time masking và frequency masking.
&lt;strong&gt;TERA &lt;a class=&quot;citation&quot; href=&quot;#liu2021tera&quot;&gt;[42]&lt;/a&gt;&lt;/strong&gt; cũng thực hiện masking để pretraining.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tera.png&quot; alt=&quot;TERA&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả thì biểu diễn pretrained như kiểu của TERA đạt hiệu suất tốt hơn MFCC, Filter banks, … (các biểu diễn hand-crafted) trên khá nhiều task như phoneme classification, ASR trên các bộ dữ liệu LibriSpeech, TIMIT.
&lt;strong&gt;HuBERT &lt;a class=&quot;citation&quot; href=&quot;#hsu2021hubert&quot;&gt;[3]&lt;/a&gt;&lt;/strong&gt; cũng là một biểu diễn kiểu self-supervised khác cũng dùng kiểu pretext là masking và đạt hiệu suất cao.
&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wav2vec&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#schneider2019wav2vec&quot;&gt;[43, 44]&lt;/a&gt;&lt;/strong&gt; cũng là pretraining để learn biểu diễn kiểu self-supervised, nhưng task pretext không phải masking/reconstruction mà là predict next words.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Conformer &lt;a class=&quot;citation&quot; href=&quot;#gulati2020conformer&quot;&gt;[7, 8]&lt;/a&gt;.&lt;/strong&gt; thì kết hợp cả SpecAugment và wav2vec 2.0 cho pretraining, cộng thêm cải tạo cái Transformer block thành Conformer block (mang convolution trở lại!).
Mà nói chung là train tốn kém (xem mục bên dưới nha!).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/conformer.png&quot; alt=&quot;TERA&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;lưu-ý-khi-train-model-asrsi&quot;&gt;Lưu ý khi train model ASR/SI&lt;/h1&gt;

&lt;h2 id=&quot;giá-thành-khi-train-model-state-of-the-art-sota&quot;&gt;Giá thành khi train model State-of-the-art (SOTA)&lt;/h2&gt;

&lt;p&gt;Quay lại với mảng ASR, nhìn đi nhìn lại cũng lại hậu duệ của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nên chắc mẩm là giá sẽ không mềm rồi.
Như hình dưới tôi đưa ra ví dụ dựa trên mô tả của một nhóm làm về ASR trong Google Brain &lt;a class=&quot;citation&quot; href=&quot;#zhang2020pushing&quot;&gt;[8]&lt;/a&gt;.
Giá cả của TPU thì v3 mà trên 32 cores thì chỉ mới hỗ trợ tài Hà Lan nên các bạn chú ý nhé.
Kết luận chung là để mà train vài ngày trên bộ Librispeech thôi chả hạn là cũng tốn vài chục ngàn Mỹ kim rồi.
Tóm lại là cuộc chơi của người giàu các bạn ạ!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/asr-price-deep-models.jpg&quot; alt=&quot;pricing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nhìn vào bảng giá trên chắc các bạn cũng thấy là Conformer có sử dụng pretraining của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wav2vec 2.0&lt;/code&gt; nên giá pretraining cũng kha khá.
Trend của mảng này cũng như NLP (mà chắc là cả Computer Vision luôn) là vẫn có pretraining thì độ chính xác cao hơn.
Tuy nhiên, với mức giá này thì rõ ràng là độ chính xác cao không hề rẻ!&lt;/p&gt;

&lt;h2 id=&quot;dữ-liệu&quot;&gt;Dữ liệu&lt;/h2&gt;

&lt;p&gt;Các cụ nói “thiên thời, địa lợi, nhân hòa” là các yếu tố làm nên thành công.
Thì tức là thời điểm, vật tư và con người.
Về vật tư thì ngoài máy móc giá thành hơi “chát” như mình đã nói mục trước thì còn vấn đề dữ liệu.
Nếu các bạn muốn build ứng dụng cho một mảng người dùng nào đó, buộc phải thu thập dữ liệu âm thanh người dùng thì chắc chắn phải chú ý các vấn đề luật pháp, thông tin cá nhân (luật bảo hộ thông tin cá nhân riêng tư, …).
Ấy vừa là thiên thời mà cũng là nhân hòa.&lt;/p&gt;

&lt;h2 id=&quot;đánh-giá-chủ-quan&quot;&gt;Đánh giá chủ quan&lt;/h2&gt;
&lt;p&gt;Mà nhân hòa còn một vấn đề nữa là đánh giá chủ quan (subjective evaluation) tức là tính điểm MOS (Mean Opinion Score).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mean Opinion Score (MOS) tức là dựa trên đánh giá của hội đồng.
Làm sao thuyết phục được hội đồng lại thành ra vấn đề nhân hòa.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;
&lt;p&gt;Nói chung, chúng ta với tầm nhìn quản lý dự án đã loop qua hầu hết các chi tiết trong cái mảng này.
Tôi nghĩ về mặt cũng không có gì khó khăn lắm.
Vấn đề là những cái opinion, dữ liệu cá nhân người dùng và giá cả.
Nên tôi nghĩ cái mảng này cũng lại kiểu ngạn ngữ Trung Quốc: muốn thành công cần hội đủ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thiên thời, địa lợi, nhân hòa&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;vaswani2017attention&quot;&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. 2017. Attention is all you need. &lt;i&gt;Advances in neural information processing systems&lt;/i&gt; (2017), 5998–6008.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/vaswani2017attention/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;devlin2019bert&quot;&gt;Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. &lt;i&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/i&gt; (2019), 4171–4186.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/devlin2019bert/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hsu2021hubert&quot;&gt;Hsu, W.-N., Bolte, B., Tsai, Y.-H.H., Lakhotia, K., Salakhutdinov, R. and Mohamed, A. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. &lt;i&gt;IEEE/ACM Transactions on Audio, Speech, and Language Processing&lt;/i&gt;. 29, (2021), 3451–3460.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hsu2021hubert/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kenny2007joint&quot;&gt;Kenny, P., Boulianne, G., Ouellet, P. and Dumouchel, P. 2007. Joint factor analysis versus eigenchannels in speaker recognition. &lt;i&gt;IEEE Transactions on Audio, Speech, and Language Processing&lt;/i&gt;. 15, 4 (2007), 1435–1447.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kenny2007joint/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dehak2011frontend&quot;&gt;Dehak, N., Kenny, P.J., Dehak, R., Dumouchel, P. and Ouellet, P. 2011. Front-End Factor Analysis for Speaker Verification. &lt;i&gt;IEEE Transactions on Audio, Speech, and Language Processing&lt;/i&gt;. 19, 4 (2011), 788–798. DOI:https://doi.org/10.1109/TASL.2010.2064307.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dehak2011frontend/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;snyder2018x&quot;&gt;Snyder, D., Garcia-Romero, D., Sell, G., Povey, D. and Khudanpur, S. 2018. X-vectors: Robust dnn embeddings for speaker recognition. &lt;i&gt;2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)&lt;/i&gt; (2018), 5329–5333.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/snyder2018x/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gulati2020conformer&quot;&gt;Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y. and others 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. &lt;i&gt;Proc. Interspeech 2020&lt;/i&gt; (2020), 5036–5040.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/gulati2020conformer/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zhang2020pushing&quot;&gt;Zhang, Y., Qin, J., Park, D.S., Han, W., Chiu, C.-C., Pang, R., Le, Q.V. and Wu, Y. 2020. Pushing the limits of semi-supervised learning for automatic speech recognition. &lt;i&gt;arXiv preprint arXiv:2010.10504&lt;/i&gt;. (2020).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/zhang2020pushing/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;reynolds1995robust&quot;&gt;Reynolds, D.A. and Rose, R.C. 1995. Robust text-independent speaker identification using Gaussian mixture speaker models. &lt;i&gt;IEEE transactions on speech and audio processing&lt;/i&gt;. 3, 1 (1995), 72–83.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/reynolds1995robust/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;reynolds2000speaker&quot;&gt;Reynolds, D.A., Quatieri, T.F. and Dunn, R.B. 2000. Speaker verification using adapted Gaussian mixture models. &lt;i&gt;Digital signal processing&lt;/i&gt;. 10, 1 (2000), 19–41.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/reynolds2000speaker/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hinton2012deep&quot;&gt;Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-rahman, Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N. and others 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. &lt;i&gt;IEEE Signal processing magazine&lt;/i&gt;. 29, 6 (2012), 82–97.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hinton2012deep/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yu2016automatic&quot;&gt;Yu, D. and Deng, L. 2016. &lt;i&gt;Automatic speech recognition&lt;/i&gt;. Springer.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/yu2016automatic/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;young2008hmms&quot;&gt;Young, S. 2008. HMMs and related speech recognition technologies. &lt;i&gt;Springer Handbook of Speech Processing&lt;/i&gt;. Springer. 539–558.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/young2008hmms/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;graves2013speech&quot;&gt;Graves, A., Mohamed, A.-rahman and Hinton, G. 2013. Speech recognition with deep recurrent neural networks. &lt;i&gt;2013 IEEE international conference on acoustics, speech and signal processing&lt;/i&gt; (2013), 6645–6649.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/graves2013speech/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;graves2013hybrid&quot;&gt;Graves, A., Jaitly, N. and Mohamed, A.-rahman 2013. Hybrid speech recognition with deep bidirectional LSTM. &lt;i&gt;2013 IEEE workshop on automatic speech recognition and understanding&lt;/i&gt; (2013), 273–278.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/graves2013hybrid/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;panayotov2015librispeech&quot;&gt;Panayotov, V., Chen, G., Povey, D. and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. &lt;i&gt;2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)&lt;/i&gt; (2015), 5206–5210.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/panayotov2015librispeech/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;paul1992design&quot;&gt;Paul, D.B. and Baker, J. 1992. The design for the Wall Street Journal-based CSR corpus. &lt;i&gt;Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992&lt;/i&gt; (1992).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/paul1992design/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;garofolo1993darpa&quot;&gt;Garofolo, J.S., Lamel, L.F., Fisher, W.M., Fiscus, J.G. and Pallett, D.S. 1993. DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1. &lt;i&gt;NASA STI/Recon technical report n&lt;/i&gt;. 93, (1993), 27403.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/garofolo1993darpa/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;warden2018speech&quot;&gt;Warden, P. 2018. Speech commands: A dataset for limited-vocabulary speech recognition. &lt;i&gt;arXiv preprint arXiv:1804.03209&lt;/i&gt;. (2018).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/warden2018speech/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;watanabe2018espnet&quot;&gt;Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., Soplin, N.-E.Y., Heymann, J., Wiesner, M., Chen, N. and others 2018. ESPnet: End-to-End Speech Processing Toolkit. &lt;i&gt;Proc. Interspeech 2018&lt;/i&gt;. (2018), 2207–2211.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/watanabe2018espnet/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;deng2013recent&quot;&gt;Deng, L., Li, J., Huang, J.-T., Yao, K., Yu, D., Seide, F., Seltzer, M., Zweig, G., He, X., Williams, J. and others 2013. Recent advances in deep learning for speech research at Microsoft. &lt;i&gt;2013 IEEE International Conference on Acoustics, Speech and Signal Processing&lt;/i&gt; (2013), 8604–8608.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/deng2013recent/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hannun2014deepspeech&quot;&gt;Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., Prenger, R., Satheesh, S., Sengupta, S., Coates, A. and others 2014. DeepSpeech: Scaling up end-to-end speech recognition. &lt;i&gt;arXiv preprint arXiv:1412.5567&lt;/i&gt;. (2014).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hannun2014deepspeech/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;amodei2016deep&quot;&gt;Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen, G. and others 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. &lt;i&gt;International conference on machine learning&lt;/i&gt; (2016), 173–182.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/amodei2016deep/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ao2021end&quot;&gt;Ao, Y., Wu, Z., Yu, D., Gong, W., Kui, Z., Zhang, M., Ye, Z., Shen, L., Ma, Y., Wu, T. and others 2021. End-to-end Adaptive Distributed Training on PaddlePaddle. &lt;i&gt;arXiv preprint arXiv:2112.02752&lt;/i&gt;. (2021).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ao2021end/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;paddlepaddle_github&quot;&gt;Guides-Document-PaddlePaddle Deep Learning Platform.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/paddlepaddle_github/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ott2019fairseq&quot;&gt;Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D. and Auli, M. 2019. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. &lt;i&gt;Proceedings of NAACL-HLT 2019: Demonstrations&lt;/i&gt; (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ott2019fairseq/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gemmeke2017audioset&quot;&gt;Gemmeke, J.F., Ellis, D.P.W., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M. and Ritter, M. 2017. Audio Set: An ontology and human-labeled dataset for audio events. &lt;i&gt;2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)&lt;/i&gt; (2017), 776–780.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/gemmeke2017audioset/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;fonseca2019audio&quot;&gt;Fonseca, E., Plakal, M., Font, F., Ellis, D.P.W. and Serra, X. 2019. AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION. &lt;i&gt;Acoustic Scenes and Events 2019 Workshop (DCASE2019)&lt;/i&gt; (2019), 69.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/fonseca2019audio/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;stylianou2008voice&quot;&gt;Stylianou, Y. 2008. Voice transformation. &lt;i&gt;Springer handbook of speech processing&lt;/i&gt;. Springer. 489–504.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/stylianou2008voice/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;benesty2008springer&quot;&gt;Benesty, J., Sondhi, M.M., Huang, Y. and others 2008. &lt;i&gt;Springer handbook of speech processing&lt;/i&gt;. Springer.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/benesty2008springer/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rabiner2007introduction&quot;&gt;Rabiner, L.R. and Schafer, R.W. 2007. &lt;i&gt;Introduction to digital speech processing&lt;/i&gt;. Now Publishers Inc.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rabiner2007introduction/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wu2015asvspoof&quot;&gt;Wu, Z., Kinnunen, T., Evans, N., Yamagishi, J., Hanilçi, C., Sahidullah, M. and Sizov, A. 2015. ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge. &lt;i&gt;Sixteenth annual conference of the international speech communication association&lt;/i&gt; (2015).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/wu2015asvspoof/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kinnunen2017asvspoof&quot;&gt;Kinnunen, T., Sahidullah, M., Delgado, H., Todisco, M., Evans, N., Yamagishi, J. and Lee, K.A. 2017. The ASVspoof 2017 challenge: Assessing the limits of replay spoofing attack detection. (2017).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kinnunen2017asvspoof/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;delgado2018asvspoof&quot;&gt;Delgado, H., Todisco, M., Sahidullah, M., Evans, N., Kinnunen, T., Lee, K.A. and Yamagishi, J. 2018. ASVspoof 2017 Version 2.0: meta-data analysis and baseline enhancements. &lt;i&gt;Odyssey 2018-The Speaker and Language Recognition Workshop&lt;/i&gt; (2018).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/delgado2018asvspoof/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wang2020asvspoof&quot;&gt;Wang, X., Yamagishi, J., Todisco, M., Delgado, H., Nautsch, A., Evans, N., Sahidullah, M., Vestman, V., Kinnunen, T., Lee, K.A. and others 2020. ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech. &lt;i&gt;Computer Speech &amp;amp; Language&lt;/i&gt;. 64, (2020), 101114.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/wang2020asvspoof/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kamble2020advances&quot;&gt;Kamble, M.R., Sailor, H.B., Patil, H.A. and Li, H. 2020. Advances in anti-spoofing: from the perspective of ASVspoof challenges. &lt;i&gt;APSIPA Transactions on Signal and Information Processing&lt;/i&gt;. 9, (2020).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kamble2020advances/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;toda2016voice&quot;&gt;Toda, T., Chen, L.-H., Saito, D., Villavicencio, F., Wester, M., Wu, Z. and Yamagishi, J. 2016. The Voice Conversion Challenge 2016. &lt;i&gt;Interspeech&lt;/i&gt; (2016), 1632–1636.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/toda2016voice/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lorenzo2018voice&quot;&gt;Lorenzo-Trueba, J., Yamagishi, J., Toda, T., Saito, D., Villavicencio, F., Kinnunen, T. and Ling, Z. 2018. The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods. &lt;i&gt;Proc. Odyssey 2018 The Speaker and Language Recognition Workshop&lt;/i&gt; (2018), 195–202.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lorenzo2018voice/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yi2020vcc&quot;&gt;Yi, Z., Huang, W.-C., Tian, X., Yamagishi, J., Das, R., Kinnunen, T., Ling, Z.-H. and Toda, T. 2020. Voice Conversion Challenge 2020 –- Intra-lingual semi-parallel and cross-lingual voice conversion –-. (Oct. 2020), 80–98.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/Yi2020vcc/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;chan2016listen&quot;&gt;Chan, W., Jaitly, N., Le, Q. and Vinyals, O. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. &lt;i&gt;2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)&lt;/i&gt; (2016), 4960–4964.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/chan2016listen/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;park2019specaugment&quot;&gt;Park, D.S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E.D. and Le, Q.V. 2019. SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. &lt;i&gt;Proc. Interspeech 2019&lt;/i&gt; (2019), 2613–2617.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/park2019specaugment/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;liu2021tera&quot;&gt;Liu, A.T., Li, S.-W. and Lee, H.-yi 2021. Tera: Self-supervised learning of transformer encoder representation for speech. &lt;i&gt;IEEE/ACM Transactions on Audio, Speech, and Language Processing&lt;/i&gt;. 29, (2021), 2351–2366.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/liu2021tera/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;schneider2019wav2vec&quot;&gt;Schneider, S., Baevski, A., Collobert, R. and Auli, M. 2019. wav2vec: Unsupervised Pre-Training for Speech Recognition. &lt;i&gt;Proc. Interspeech 2019&lt;/i&gt; (2019), 3465–3469.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/schneider2019wav2vec/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;baevski2020wav2vec&quot;&gt;Baevski, A., Zhou, Y., Mohamed, A. and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; (2020), 12449–12460.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/baevski2020wav2vec/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;đồ ăn sẵn&lt;/code&gt; thì các bạn có thể tham khảo &lt;a href=&quot;https://docs.aws.amazon.com/lexv2/latest/dg/what-is.html&quot;&gt;Amazon Lex&lt;/a&gt;. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 06 Feb 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/speech/</link>
        <guid isPermaLink="true">https://wanted2.github.io/speech/</guid>
        
        <category>音声認識</category>
        
        <category>音声合成</category>
        
        <category>音声変換</category>
        
        <category>話者同定</category>
        
        <category>Sequence-to-sequence</category>
        
        <category>Transformer</category>
        
        <category>BERT</category>
        
        <category>speech recognition</category>
        
        <category>speech synthesis</category>
        
        <category>voice conversion</category>
        
        <category>speaker verification</category>
        
        <category>speaker identification</category>
        
        <category>speaker diarization</category>
        
        <category>metric learning</category>
        
        <category>deep metric learning</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>Seq2Seq và kiến trúc Encoder-Decoder</title>
        <description>&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Seq2Seq&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#kalchbrenner2013recurrent&quot;&gt;[1, 2, 3]&lt;/a&gt; là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, &lt;a class=&quot;citation&quot; href=&quot;#sutskever2014sequence&quot;&gt;[2]&lt;/a&gt;), Question-Answering (QA, &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2016squad&quot;&gt;[4]&lt;/a&gt;), Visual Question Answering (VQA, &lt;a class=&quot;citation&quot; href=&quot;#antol2015vqa&quot;&gt;[5, 6]&lt;/a&gt;), Text Summarization (TS, &lt;a class=&quot;citation&quot; href=&quot;#rush2015neural&quot;&gt;[7, 8]&lt;/a&gt;) và Video-To-Text (VTT, &lt;a class=&quot;citation&quot; href=&quot;#venugopalan2015sequence&quot;&gt;[9]&lt;/a&gt;).
Bài toán Image Captioning thì cũng có thể ứng dụng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; như trong bài &lt;em&gt;Semantic Attention (SA) &lt;a class=&quot;citation&quot; href=&quot;#you2016image&quot;&gt;[10]&lt;/a&gt;&lt;/em&gt; hay &lt;em&gt;Densecap &lt;a class=&quot;citation&quot; href=&quot;#johnson2016densecap&quot;&gt;[11]&lt;/a&gt;&lt;/em&gt;.
Nên nhìn chung là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng &lt;a class=&quot;citation&quot; href=&quot;#luong2015multi&quot;&gt;[12]&lt;/a&gt;.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;giới-thiệu-model-seq2seq&quot;&gt;Giới thiệu model Seq2Seq&lt;/h1&gt;

&lt;p&gt;Nỗi lòng người làm thày mà hướng dẫn sinh viên thì tùy level mà kỳ vọng thì nó cũng khác nhau, và với những level cao như postdoc là luôn có sự mong muốn nhất định.
Đó là phải vượt qua &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thử thách (challenges)&lt;/code&gt; của thày.
Thì cái challenge đây có hai nghĩa là cuộc thi và thử thách, thì nói thực là cuộc thi không cần đâu, chỉ cần vượt qua thử thách tối thiểu của thày trong cái kỹ năng làm nghiên cứu thôi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ok em&lt;/code&gt; rồi.&lt;/p&gt;

&lt;p&gt;Trong thực tế kinh nghiệm thì tôi thấy mấy cái thử thách cũng không phức tạp lắm đâu, đặc biệt là trong lĩnh vực NLP+Vision này thì tôi thấy cũng chỉ có vài bài như Image Captioning, Video-to-Text hay VQA.
Code thì nhan nhản trong cộng đồng nghiên cứu (đặc thù của ngạch nghiên cứu là kế thừa, vì người ta công bố mà mình không dùng thì phí), rồi tài liệu thì mảng NLP với Vision các ông cũng publish trên ArXiV với mấy hội nghị open, chứ nào có giấu giếm gì nhau?
Thế mà không hiểu vì sao vẫn không vượt qua được cho thày?
Xem lại rốt cuộc là tôi nghĩ thì có lẽ nên thêm câu hỏi sau vào bộ dữ liệu QA:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q: Không làm muốn có ăn thì ăn gì bây giờ?
A: ??? (câu hỏi tự luận nha)
Context: Hãy xem video của Prof. Huan Rose&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thì nhìn chung là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; là một model thử thách như vậy.
Với những bài đặc biệt chỉ của NLP như NMT, Text Summarization hay QA thì &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; đã mở ra hẳn cả một mảng riêng mà về sau còn có thêm mấy cái pretraining encoder/decoder như BERT, AlBERT, RoBERT, BART, … mà chủ yếu là representation learning.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mô hình &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; đơn giản chỉ gồm một chuỗi input $\mathbb{x}_1, \mathbb{x}_2, \ldots, \mathbb{x}_m$. 
Chuỗi này đương nhiên là biểu diễn vector (embedding) sau khi đã preprocessing qua tokenizer và thay thế từ không có trong vocabulảy bởi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UNK&lt;/code&gt;.
Sau đó là &lt;strong&gt;encoder&lt;/strong&gt; với các trạng thái $\mathbb{h}_1, \mathbb{h}_2, \ldots, \mathbb{h}_m$ cũng như biểu diễn đầu ra của encoder là $\mathbb{z}_1, \mathbb{z}_2, \ldots, \mathbb{z}_m$.
Encoder chính là một mạng trí tuệ nhân tạo dạng Long-Short Term Memory (LSTM) mà chúng ta sẽ nói sau.
Bây giờ thì chúng ta cần aggregate chuỗi biểu diễn đầu ra của encoder thành biến vector \(\mathbb{c}_t=\sum_{i=1}^m\alpha_i^t\mathbb{z}_i\).
Ở mỗi bước \(t\) thì &lt;strong&gt;decoder&lt;/strong&gt; LSTM sẽ nhận input là output của bước \(t-1\) là \(\mathbb{g}_{t-1}\) và biến context \(\mathbb{c}_t\) để đưa ra output \(\mathbb{g}_t\).&lt;/p&gt;

\[\mathbb{g}_t=\mbox{LSTM}\left(\mathbb{g}_{t-1},\mathbb{c}_t\right)\]

  &lt;p&gt;ứng với mỗi \(\mathbb{g}_t\) sẽ được giải mã thành một ký tự trong ngôn ngữ đầu ra.
Quá trình này kết thúc khi ký tự &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EOS&lt;/code&gt; được giải mã ra.
Chuỗi đầu vào \(\mathbb{x}_i\) và chuỗi đầu ra của decoder \(\mathbb{g}_j\) có thể có độ dài khác nhau.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Trong bài toán NMT, thì chuỗi đầu vào có thể là tiếng Anh và chuỗi đầu ra là tiếng Nhật.
Nhưng ngược lại thì sẽ cần tokenizer bằng tiếng Nhật.
Trong bài toán Text Summarization (TS), thì cả đầu ra và đầu vào đều sẽ cùng ngôn ngữ, nhưng đầu ra sẽ là một chuỗi ngắn gọn xúc tích hơn.
Cái gọi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ngắn gọn, xúc tích hơn&lt;/code&gt; sẽ được định nghĩa và học thông qua dữ liệu.
Thì NMT có bộ WMT2014 &lt;a class=&quot;citation&quot; href=&quot;#bojar2014findings&quot;&gt;[14]&lt;/a&gt;, WMT2017 &lt;a class=&quot;citation&quot; href=&quot;#bojar2017findings&quot;&gt;[15]&lt;/a&gt;, còn TS thì có bộ DUC &lt;a class=&quot;citation&quot; href=&quot;#paul2004introduc&quot;&gt;[16]&lt;/a&gt;.
Còn bài toán QA thì chuỗi đầu vào là một câu hỏi còn đầu ra là 1 câu trả lời.
Cái hay của QA là đôi khi có thêm chuỗi context (hay gọi là gợi ý), ví dụ như hỏi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bạn sống ở đâu?&lt;/code&gt; thì có thêm context là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tôi sống ở Nhật&lt;/code&gt; thì máy sẽ trả lời luôn là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Nhật&lt;/code&gt;.
QA thì có bộ SQuAD v1 &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2016squad&quot;&gt;[4]&lt;/a&gt; và v2 &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2018know&quot;&gt;[17]&lt;/a&gt; với hơn trăm ngàn bộ câu hỏi (nghe như luyện thì TOEIC).
Metrics đánh giá thì có cái BLUE-4 score, CIDEr, ROGUE &lt;a class=&quot;citation&quot; href=&quot;#lin2004rouge&quot;&gt;[18]&lt;/a&gt; là có thể dùng để đánh giá chuỗi đầu ra có phù hợp không.
Chúng ta sẽ đi sâu thêm vào từng chi tiết sau.&lt;/p&gt;

&lt;p&gt;Các task trong NLP thì là như vậy, dữ liệu, code và metrics hầu như có sẵn.
Thì cũng là kết nối với Vision là khoảng CVPR tầm 2015-2016 gì đó có mấy cái Workshops nói về làm sao để có thể tích hợp nhiều modal (multi-modal) để đưa ra những giải pháp tốt hơn.
Mọc ra trước mắt lúc đó thì có 3 tasks nằm trong định hướng: Image Captioning, Video-to-Text và VQA.
Nói chung nghe lúc đó có vẻ mới, nhưng chỉ có task là mới thôi chứ còn những cái để thực thi những task ấy các sếp promote các task ấy lên họ đã chuẩn bị hết rồi.
Technique thì có sẵn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;, CNN, RNN, LSTM, Faster R-CNN để extract attributes dưới dạng object detections, …
Metrics thì vì output vẫn là chuỗi text nên lại xài lại BLUE-4, CIDEr rồi ROGUE thôi.
Nên coi như nền tảng rất sẵn rồi, chỉ nhảy vào làm thí nghiệm và viết paper rồi … ăn!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Vậy tại sao vẫn cứ không đến nơi đến chốn được?&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tôi nghĩ vấn đề đầu tiên lúc làm Image Captioning là thiếu &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alignment&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Lúc làm cái Video-to-Text thì cái visual semantic embedding (VSE) là không có. Mà nói thẳng ra thì cái ấy chính mình &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chủ động&lt;/code&gt; nghĩ ra mà làm chứ?&lt;/li&gt;
  &lt;li&gt;CÒn cái VQA thì lúc ấy nói thật là hai cái captioning với VTT nó đã &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bết bát&lt;/code&gt; sẵn rồi thì sẽ rất khó vì bản thân VQA tuy chỉ thay cái context là text bởi hình ảnh, nhưng chất lượng detector, rồi alignment mà hai bước trên chưa làm tốt thì sang đến VQA coi như … vỡ trận.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tuy nhiên, nếu ở vào vai trò anh Postdoc mà làm cái mảng này tôi vẫn sẽ đề xuất luồng làm việc &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Image Captioning --&amp;gt; Video-to-Text --&amp;gt; VQA&lt;/code&gt;.
Bởi luồng làm việc này nó theo chiều hướng tích lùy dần know-how để làm việc ngày càng tốt hơn.
Thứ hai, là vì nó có một vài điểm trigger giữa chừng nên nếu làm postdoc các bạn có thể submit bài báo tại các thời điểm ấy như là làm Image Captioning xong thì 1 bài, …
&lt;strong&gt;Nhưng rốt cuộc cái quan trọng nhất vẫn là phải có làm có ăn.&lt;/strong&gt;
Còn không muốn làm thì hỏi giáo sư Huấn để biết phải làm gì.&lt;/p&gt;

&lt;h2 id=&quot;recurrent-neural-nets-long-short-term-memory-và-gated-recurrent-units&quot;&gt;Recurrent Neural Nets, Long-Short Term Memory và Gated Recurrent Units&lt;/h2&gt;

&lt;p&gt;Thôi nói chung là cái trường hợp postdoc ở trên là một ví dụ điển hình thôi, mà trường hợp ấy tôi nghĩ đã được giáo sư Huấn chỉ bảo tận tình rồi nên không cần lo lắng nữa.
Chúng ta quay lại với chủ để chính của hôm nay là giới thiệu về &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.
Để implement được &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; thì chúng ta cần 1 mô hình nhận chuỗi và output đầu ra cũng là một chuỗi khác, và quan trọng hơn là có thể huấn luyện bằng thuật toán Gradient Descent, cụ thể hơn là &lt;em&gt;Stochastic Gradient Descent (SGD, &lt;a class=&quot;citation&quot; href=&quot;#bottou2007tradeoffs&quot;&gt;[19]&lt;/a&gt;)&lt;/em&gt;.
Thì cái học củ SGD là mình chỉ random sample một phần của dữ liệu học để tính được gradient và update weight của model.
Chúng ta sẽ có &lt;em&gt;learning rate&lt;/em&gt; mà nhỏ thì chậm, còn cao quá thì mô hình sẽ khó hội tụ.&lt;/p&gt;

&lt;p&gt;Lời giải thì đơn giản nhất là có mô hình &lt;em&gt;Recurrent Neural Nets (RNN, &lt;a class=&quot;citation&quot; href=&quot;#rumelhart1986learning&quot;&gt;[20]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#goodfellow2016deep&quot;&gt;[21]&lt;/a&gt;)&lt;/em&gt; mà mô hình và công thức forward pass như bên dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn.svg&quot; alt=&quot;rnn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Công thức rất rõ ràng nên có thể sử dụng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt; để implement forward pass khá đơn giản với vài dòng code chơi thôi.
Cái khó khăn là khi đã tính xong kết quả các biến \(u_t, o_t, h_t, x_t\) và hàm loss \(\mathcal{L}\) thì &lt;strong&gt;làm sao update được các weight \(\mathbf{V}, \mathbf{W}, \mathbf{b}, \mathbf{c}\)&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;Thì có hệ thống các công thức phía dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bptt.svg&quot; alt=&quot;bptt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tuy hệ thống công thức này phức tạp hơn, nhưng vẫn có thể implement bằng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt; và thực hiện train với SGD (tầm khoảng vài chục dòng code nữa).
Nhìn lại thì công thức và cả phương thức implement RNN là cũng rất &lt;strong&gt;rõ ràng rồi&lt;/strong&gt;, vì vậy, nếu không thể implement được thì … thày cũng chịu không thể giải thích tại sao được?&lt;/p&gt;

&lt;p&gt;Nhược điểm của RNN là vấn đề ghi nhớ &lt;em&gt;long-term dependencies&lt;/em&gt;: ví dụ chúng ta có 1 gradient \(\mathbf{g}\) và giả sử là RNN không có non-linear activations nào thì chúng ta có thể giả sử thêm là cứ sau mỗi time step thì Jacobian matrix là \(\mathbf{J}\), vậy sau \(n\) bước thì gradient của chúng ta sẽ biến thành \(\mathbf{J}^n\mathbf{g}\) và nếu nó có một giá trị riêng \(\lambda\) mà giá trị khác \(\pm 1\), thì sẽ dẫn đến vấn đề là gradient triệt tiêu hoặc tiến ra vô cùng sau hữu hạn bước tính.
Để khắc phục nhược điểm này thì một giải pháp được đưa ra là đưa &lt;em&gt;self-loop&lt;/em&gt; vào cùng các &lt;em&gt;non-linear gates&lt;/em&gt; như forget gate, output/input gates để control giá trị.
Một kiến trúc đã làm được việc đó là &lt;em&gt;Long-Short Term Memory (LSTM, &lt;a class=&quot;citation&quot; href=&quot;#hochreiter1997long&quot;&gt;[22, 23]&lt;/a&gt;)&lt;/em&gt;.
Mỗi cổng đều chứa &lt;em&gt;sigmoid&lt;/em&gt; activation để đưa giá trị về khoảng \((0,1)\) dẫn đến các giá trị output ở các cổng có thể bị shut-off.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lstm.svg&quot; alt=&quot;lstm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Việc thêm gates và non-linear activation (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sigmoid&lt;/code&gt;) để control giá trị là hợp lý với LSTM.
Tuy nhiên, nếu nhìn vào cái đám công thức thì trong LSTM khi update và forget của \(\mathbf{h}_{t+1}\) là đồng thời thực hiện song song việc forget và quyết định có update hay không.
&lt;strong&gt;Gated Recurrent Unit (GRU, &lt;a class=&quot;citation&quot; href=&quot;#cho2014properties&quot;&gt;[24]&lt;/a&gt;)&lt;/strong&gt; thực hiện chia ra, tức là thay vì 1 cổng là làm luôn cả hai chức năng, thì tạo 2 cổng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update&lt;/code&gt; và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reset&lt;/code&gt; cho từng mục đích ấy.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nhìn chung, công thức toán của các kiến trúc RNN/LSTM/GRU đều không phức tạp.
Nếu có thời gian, các bạn có thể tự thực hiện implement các kiến t
rúc trên bằng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy&lt;/code&gt;.
Forward pass thì chắc là đơn giản hơn, tuy nhiên backward pass với SGD thì sẽ đòi hỏi một chút công sức (vài chục dòng code nữa).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nói một chút về hàm loss \(\mathcal{L}\) khi train NMT chẳng hạn, vì chúng ta biết chuỗi &lt;strong&gt;đúng&lt;/strong&gt; \(\mathbf{y}=(y_1,y_2,\ldots,y_n)\) nên ở mỗi step \(0\leq t\leq n-1\), ta sẽ xem xét vector output \(\mathbf{u}_t\) (là softmax) và lấy xác suất của vị trí thứ \(y_t\), và tất nhiên lấy log nghịch đảo như mọi khi:&lt;/p&gt;

\[\mathcal{L}=\sum_t-\log u_{y_t}\]

&lt;p&gt;Khi testing, chúng ta sẽ sử dụng &lt;strong&gt;beam search&lt;/strong&gt; để tìm ra các chuỗi phù hợp (sẽ giải thích thêm ở phần sau).&lt;/p&gt;

&lt;h2 id=&quot;kiến-trúc-seq2seq-s2s&quot;&gt;Kiến trúc Seq2Seq (S2S)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/san01.png&quot; alt=&quot;seq2seq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; đòi hỏi cần có GPU phù hợp và tận dụng xử lý song song.
1 đặc điểm không thể tránh khỏi của bộ dữ liệu là độ dài các chuỗi không đồng bộ, nên có thể tạo ra giải pháp là &lt;strong&gt;fill thêm ký tự trống vào để cho tất cả các chuỗi cùng độ dài&lt;/strong&gt;.
Việc này sẽ đòi hỏi thêm khá nhiều memory, vì vậy, một giải pháp khá phù hợp là chia ra thành các &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxi-batches&lt;/code&gt; rồi trong từng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxi-batch&lt;/code&gt; thì lại chia tiếp thành các &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mini-batch&lt;/code&gt;.
Trong mỗi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mini-batch&lt;/code&gt; như hình dưới thì mới thực hiện fill ký tự trống.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-mini-batches.png&quot; alt=&quot;rnn-mini-batches&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; thường mất 5-15 epochs (1 lượt qua toàn bộ corpus).
Bạn cũng nên thiết lập một tiêu chí dừng lại khi validation error rate không cải tiến thêm.
Training lâu thêm không những không tạo thêm cải tiến mà có thể dẫn tới overfitting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Testing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;&lt;/strong&gt;. Giả sử chúng ta có một corpus 50,000 từ.
Vậy trong vector \(\mathbf{u}_t\in\mathbb{R}^{50,000}\), ta sẽ chọn phần tử có xác suất cao nhất (dựa trên embedding logits) để output ra ở bước thứ \(t\).
Giải pháp này tức là &lt;strong&gt;chỉ chọn cái tốt nhất ở mỗi bước&lt;/strong&gt;.
Vấn đề ở đây là nếu chỉ chọn cái tốt nhất độc lập ở từng bước thì khi đến một thời điểm \(t&apos;\), câu dịch trở lên không đúng thì không thể quay lại sửa từ đầu được.
Do đó, cách tốt hơn là dùng &lt;strong&gt;Beam Search&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ở bước \(t=0\) chúng ta chọn một beam gồm \(n\) top words \(p(w_0^1), p(w_0^2), \ldots, p(w_0^n)\).&lt;/li&gt;
  &lt;li&gt;Ở thời điểm tiếp theo \(t=1\), ta lại chọn tiếp \(n\) top words \(p(w_1^1), p(w_1^2), \ldots, p(w_1^n)\) và làm tiếp:
    &lt;ul&gt;
      &lt;li&gt;nhân xác xuất của cụm 2 từ liên tiếp \(p(w_0^{i_0})p(w_1^{i_1}), 1\leq i_0, i_1\leq n\).&lt;/li&gt;
      &lt;li&gt;Chọn top \(n\) cặp từ \((i_0,i_1)\) giữ lại.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ở thời điểm \(t=2\), ta lại chọn tiếp top \(n\) words \(p(w_2^1), p(w_2^2), \ldots, p(w_2^n)\), và lại nhân xác suất để tìm ra top \(n\) triplets \((i_0, i_1, i_2)\) để giữ lại trong beam.&lt;/li&gt;
  &lt;li&gt;Cứ tiếp tục như vậy đến khi gặp &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EOS&lt;/code&gt; thì bỏ câu dịch đó ra khỏi beam và giảm size của beam đi 1.&lt;/li&gt;
  &lt;li&gt;Khi kích thước của beam giảm xuống còn 0 thì dừng beam search.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Một đoạn code demo của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;beam search&lt;/code&gt;:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/6397ad4648f7c891ccd41513bb8206e5.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Ngoài ra, nếu có thời gian các bạn có thể tìm hiểu thêm các thủ thuật để tăng hiệu suất khi inference như fusion (ensembling), reranking, hoặc khi train như tăng kích thước vocabularies, training data, back translation, round-trip training, guided alignment training, .v.v… (&lt;a class=&quot;citation&quot; href=&quot;#koehn2009statistical&quot;&gt;[25]&lt;/a&gt;).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tất nhiên, cũng cần chú ý là implement những cái này ở local cũng chỉ để học hỏi thôi nhé.
&lt;strong&gt;Chứ còn triển khai vận hành thực sự thì nó có những solutions có sẵn chạy ầm ầm trên AWS/Azure/GCP rồi.&lt;/strong&gt;
Mà có khi nó còn thành dịch vụ đem bán khắp nơi rồi ấy chứ (có cả tiếng Việt luôn nhé).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;transformers-và-bert&quot;&gt;Transformers và BERT&lt;/h2&gt;

&lt;h3 id=&quot;transformer-13&quot;&gt;Transformer &lt;a class=&quot;citation&quot; href=&quot;#vaswani2017attention&quot;&gt;[13]&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/transformer.png&quot; style=&quot;float: right; margin: 10px; width: 25%;&quot; /&gt;
Deep Networks nhưng không có RNNs hay CNNs gì cả!
Transformer cũng có thể coi là một dạng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nhưng không chỉ gồm các tầng FC, embedding, …
Điểm khác biệt là context (hay là attention) đã được chuyển thành tầng đề xuất &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self-attention&lt;/code&gt;.
Nhìn chung, Transformer hay cả các kiến trúc BERT về sau cũng kế thừa tương đối nhiều tính chất của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.
Bản thân Transformer cũng là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nhưng bỏ đi RNN/CNN.
&lt;img src=&quot;/assets/images/mha.png&quot; style=&quot;float: left; margin: 10px; width: 15%;&quot; /&gt;
Trong kiến trúc thì ngoài residual connection chỉ có điểm đáng chú ý là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multi-head attention&lt;/code&gt;.
Như hình vẽ bên trái, có 3 vector mới được đưa ra là \(K, V, Q\), tương ứng cho &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keys, values, queries&lt;/code&gt;.
Thì công thức attention (không có mask) sẽ như sau:&lt;/p&gt;

\[\mbox{Attention}\left(Q,K,V\right)=\mbox{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V\]

&lt;p&gt;trong đó, \(d_k\) là số chiều của vector query \(Q\) và key \(K\).
Trong trường hợp dùng mask (ở decoder chả hạn) thì chủ yếu là để kết hợp với position embedding để hạn chế ảnh hưởng của các token trước thời điểm $t$ chả hạn.&lt;/p&gt;

&lt;p&gt;Nói chung lý thuyết của Transformer cũng không có gì quá phức tạp.
Bản thân Transformer cũng đã được implement chi tiết trong thư viện &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor2tensor&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#vaswani2018tensor2tensor&quot;&gt;[26]&lt;/a&gt; nên như tôi đã nói, cái mảng này thực ra tài liệu tài nguyên thì vô cùng dồi dào.
Ngoài ra một thư viện nữa là HuggingFace &lt;a class=&quot;citation&quot; href=&quot;#wolf2020transformers&quot;&gt;[27]&lt;/a&gt; với model zoo dồi dào cũng rất đáng xem.
&lt;strong&gt;Thế là lại nói lại câu chuyện postdoc ở trên: &lt;em&gt;không hiểu tại sao lại không làm được?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Thì tôi nghĩ là ngoài 3 yếu tố đã nói ở mục đầu, thì quá tập trung vào competition! 
Từ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;challenge&lt;/code&gt; mà người thày nói cũng có nghĩa là cuộc thi (competition) nhưng cũng có nghĩa là thử thách.
Mà vấn đề là cái nghĩa sau nó sẽ quan trọng hơn.
Bởi mảng nghiên cứu là giao thoa giữa Vision và NLP, nên là nếu chỉ tập trung vào 1 competition nhất định của vision mà không cập nhật bên NLP thì có vẻ là tầng nghĩa sau (thử thách) là sẽ bị lose track.
Đấy thế nên là cũng nhắc lại lời người thày: “&lt;strong&gt;Thực ra nhiệm vụ của người làm thày là giảm bớt số lượng những con bò trên thế giới xuống&lt;/strong&gt;”.
Tôi nghĩ đúng là để biến một con bò thành người cũng khá vất vả, mà thất bại mãi mới thành công một phát cũng là chuyện thường!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Chốt là cuộc thi, competition là không quan trọng đâu.
Cái quan trọng với postdoc là rèn luyện được một cái tác phong nghiên cứu tốt để có thể hướng tới lâu dài.
&lt;strong&gt;Chứ mất công giành được vị trí trong 1 competition trước mắt, nhưng lại mất vị trí tenure cả đời thì luận về TRÍ là thua rồi!&lt;/strong&gt;
Đã xác định gắn bó lâu dài thì không cần thi cử gì cho mất thời gian ra, lo mà tìm ra cống hiến để đời và gắn bó lâu dài thì hơn.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Mà nhìn lại tôi nghĩ nguyên nhân sâu xa dẫn tới sự vụ hậu quả postdoc kể trên là &lt;strong&gt;thiếu kỹ năng quản lý dự án (Project Management)&lt;/strong&gt;.
Triệu chứng bệnh rõ ràng nhất ở đây là&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Việc không quản lý được các tri thức về thử thách, dẫn đến bị đánh lạc hướng, sa đà vào các competition. Đây chính là triệu chứng bệnh &lt;strong&gt;thiếu kỹ năng quản lý scope dự án.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Việc không control được những lời khuyên từ bên NLP chứng tỏ có dấu hiệu &lt;strong&gt;yếu về quản lý stakeholder&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Việc ưu tiên competition trước việc nhận ra thách thức thực sự của mình chính là &lt;strong&gt;yếu về quản lý action plan, time và schedule&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nhìn chung việc thiếu kỹ năng quản lý dự án sẽ không bao giờ đưa người postdoc gia nhập nhóm (dưới) 5% thành công được.
&lt;strong&gt;Mà ví dụ khoảng làm postdoc được 2-3 năm mà người thày nhìn vào thấy đủ thứ bệnh, chỗ nào cũng yếu thế này thì rất là khó để không cho … bật bãi!&lt;/strong&gt;
Ở Mỹ chả hạn, làm nghiên cứu muốn tồn tại lâu dài phải có kỹ năng quản lý dự án nghiên cứu. Thế mà chạy postdoc 2 năm mà không ngộ ra chân lý, không thể hiện tiềm năng làm quản lý lãnh đạo, thì không bật bãi chắc chỉ có ở thiên đường thôi!&lt;/p&gt;

&lt;h3 id=&quot;bert&quot;&gt;BERT&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from &lt;strong&gt;T&lt;/strong&gt;ransformers (&lt;strong&gt;BERT&lt;/strong&gt;, &lt;a class=&quot;citation&quot; href=&quot;#devlin2019bert&quot;&gt;[28]&lt;/a&gt;) lại là 1 kiến trúc mới gần đây (từ 2018?).
Pretraining kiến trúc trên 1 task unsupervised (kiểu tự học), rồi dùng lại kiến trúc đó trong 1 task supervised khác là tư tưởng chính của BERT.
Các phiên bản cải tiến của BERT có thể kể đến AlBERT &lt;a class=&quot;citation&quot; href=&quot;#lan2019albert&quot;&gt;[29]&lt;/a&gt;, RoBERTa &lt;a class=&quot;citation&quot; href=&quot;#liu2019roberta&quot;&gt;[30]&lt;/a&gt;, BART &lt;a class=&quot;citation&quot; href=&quot;#lewis2020bart&quot;&gt;[31]&lt;/a&gt;, XlNet &lt;a class=&quot;citation&quot; href=&quot;#yang2019xlnet&quot;&gt;[32]&lt;/a&gt;, XLM &lt;a class=&quot;citation&quot; href=&quot;#conneau2019cross&quot;&gt;[33]&lt;/a&gt; hay gần đây là ELECTRA &lt;a class=&quot;citation&quot; href=&quot;#clark2020electra&quot;&gt;[34]&lt;/a&gt; đều kế thừa tinh thần này.
Trong quá khứ thì việc sử dụng pretraining để tạo ra một xuất phát điểm tốt hơn cho model là ý tưởng đã được khai thác &lt;a class=&quot;citation&quot; href=&quot;#hinton2006fast&quot;&gt;[35]&lt;/a&gt;.
Ví dụ như Hinton thì đã pretrain deep belief nets và gọi là &lt;em&gt;greedy layer-wise unsupervised pretraining&lt;/em&gt;.
Thủ thuật này là cần thiết khi train các model lớn:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;to train the first layer in isolation, then extract all features from the first layer only once, then train the second layer in isolation given those features, and so on. &lt;a class=&quot;citation&quot; href=&quot;#goodfellow2016deep&quot;&gt;[21]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ý tưởng của kiểu pretraining này là ta có một model rất lớn \(A\), việc train \(A\) từ đầu (random weights) sẽ rất khó.
Do vậy, chúng ta sẽ tìm một task unsupervised \(u\) (vì vậy không cần labels), để train \(A\) trước.
Sau đó, khi vào train cho task chính \(T\) (có labels) thì weights của \(A\) đã được khởi tạo bởi việc học \(u\) sẽ khiến training \(A\) cho \(T\) trở nên nhanh chóng hơn.
Trên thực tế có khá nhiều task bên computer vision đã sử dụng ý tưởng này và đưa ra khá nhiều kết quả thuyết phục.&lt;/p&gt;

&lt;p&gt;Với bên NLP, thì BERT lựa chọn task pretraining (gọi là &lt;em&gt;pretext&lt;/em&gt;) là masked language modeling (MLM).
Ví dụ như trong câu&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Original: I have a pen &amp;lt;EOS&amp;gt;
Masked: I &amp;lt;MASK&amp;gt; a pen &amp;lt;EOS&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;thì từ câu &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Masked&lt;/code&gt;, nhiệm vụ của pretext là phải &lt;em&gt;phục hồi&lt;/em&gt; lại các từ bị &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;.
Nguyên tắc pretext này nhìn chung giống với &lt;em&gt;denoising auto-encoders&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#goodfellow2016deep&quot;&gt;[21]&lt;/a&gt;.
Vì việc tạo bộ dữ liệu masking là dễ dàng và có thể tự làm được (viết script để ngẫu nhiên thay 1 token bởi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;) nên task này có thể xếp vào hạng mục &lt;strong&gt;unsupervised learning&lt;/strong&gt; hoặc &lt;strong&gt;self-supervised learning (SSL)&lt;/strong&gt;.
Lợi thế là người train có thể tạo ra bộ dữ liệu lớn tự động để máy học pretext mà không cần gắn nhãn!
Một pretext khác là &lt;em&gt;đoán câu tiếp theo (Next Sentence Prediction, NSP)&lt;/em&gt;.
Ví dụ:&lt;/p&gt;
&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;I told the principal that I would like to revolutionize this university if he give me something to do.
Next Sentence: He sent me to the Psychology department.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Trong kết quả của BERT thì có vẻ pretext NSP giúp cải thiện độ chính xác của task QA.
Tất nhiên, ngoài dùng pretext thì còn những kiểu pretraining khác như sử dụng feature đã pretrain (ELMo, &lt;a class=&quot;citation&quot; href=&quot;#peters2018deep&quot;&gt;[36]&lt;/a&gt;) hoặc fine-tune toàn bộ pretrain parameters (OpenAI GPT, &lt;a class=&quot;citation&quot; href=&quot;#radford2018improving&quot;&gt;[37]&lt;/a&gt;).
Gần đây có model BART &lt;a class=&quot;citation&quot; href=&quot;#lewis2020bart&quot;&gt;[31]&lt;/a&gt; tích hợp cả BERT và GPT để tạo ra một cơ chế pretext có thể mô phỏng hàm noise bất kỳ.&lt;/p&gt;

&lt;p&gt;Hầu hết những phương pháp kể trên đều có thể tìm thấy tại thư viện model HuggingFace &lt;a class=&quot;citation&quot; href=&quot;#wolf2020transformers&quot;&gt;[27]&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;ứng-dụng&quot;&gt;Ứng dụng&lt;/h1&gt;

&lt;h2 id=&quot;cách-tiếp-cận&quot;&gt;Cách tiếp cận&lt;/h2&gt;
&lt;h3 id=&quot;các-nguồn-tài-nguyên&quot;&gt;Các nguồn tài nguyên&lt;/h3&gt;
&lt;p&gt;Như đã nói ở trên, mảng này thì code kiếc, tài liệu, tài nguyên thì vô cùng &lt;strong&gt;sẵn&lt;/strong&gt; có và dồi dào.
Thế nên thôi mình cứ &lt;strong&gt;ăn “sẵn”&lt;/strong&gt; đi cho nó nhanh chứ nấu làm gì mất thời gian.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nó cũng kiểu như đồ ăn Tết ấy mà: 
Hì hục nấu mất cả buổi mà ăn thì chắc được mấy miếng là chán.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thế nên tôi mới bảo rồi, những cái này mà mở khóa học truyền bá tri thức phổ cập thì &lt;strong&gt;liệu có khách không? ai học cho mà dạy?&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Tôi nghĩ chả ai học đâu, giờ người ta ăn sẵn hết.
Có dạy “nấu” thì cũng phải cái gì mà nó kiểu sẵn sẵn mà nấu nhanh ăn luôn.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thì tại sao nói mảng này đồ ăn sẵn nhiều thì dưới đây là một số nguồn mà các bạn có thể tận dụng:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt; thì HuggingFace, tensorflow, Github&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt; thì HuggingFace thôi.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;https://huggingface.co/&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Cứ tải về mà hì hục “nấu”.&lt;/li&gt;
      &lt;li&gt;Nấu xong chắc cũng chỉ chạm đũa vài miếng là ngấy thôi nhưng nếu thích nấu thì nấu thôi.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tài liệu&lt;/strong&gt; tutorials thì cứ search YouTube là ra hết mà.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/c/HuggingFace&quot;&gt;Kênh hướng dẫn của HuggingFace&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=G5lmya6eKtc&quot;&gt;The Future of Natural Language Processing&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/G5lmya6eKtc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;Nguồn &lt;strong&gt;tài nguyên tính toán&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Đầu tiên cũng chả cần mua GPU/TPU riêng đâu.&lt;/li&gt;
      &lt;li&gt;Để làm demo bạn cứ xài tạm Colab ấy.
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/&quot;&gt;https://colab.research.google.com/&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL&quot;&gt;Một playlist hướng dẫn dùng Colab với Tensorflow&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;đôi-lời-về-google-colab&quot;&gt;Đôi lời về Google Colab&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://research.google.com/colaboratory/faq.html&quot;&gt;FAQ&lt;/a&gt; &lt;strong&gt;What is Colabotory?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Colaboratory, or “Colab” for short, is a product from Google Research. Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning, data analysis and education. More technically, Colab is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources including GPUs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Google Colab nhìn chung là 1 dịch vụ Notebook cung cấp sẵn môi trường để chạy các tác vụ liên quan tới machine learning.
Với Colab, bạn có thể định nghĩa form các biến chương trình như hình bên dưới, rồi tạo lập mạng trí tuệ nhân tạo, thiết lập môi trường GPU/TPU để training hoặc inference.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nmt-colab-01.png&quot; alt=&quot;colab form&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sau khi đã làm quen với môi trường Google Colab thì bạn đã all-set to go!&lt;/p&gt;

&lt;h4 id=&quot;chú-ý-về-train-song-song&quot;&gt;Chú ý về train song song&lt;/h4&gt;
&lt;p&gt;Một chú ý nhỏ mang tính hô trợ trong quản lý time thôi là nếu bạn chọn Colab Free plan thì GPU/TPU tốc độ khá thấp.
Ngoài ra, bộ dữ liệu hạng vừa như WMT 2014 thì cũng có 4,468,840 cặp câu dịch (bộ training set).
Mà Colab Free thì tốc độ training sẽ rơi vào tầm 18-20 FPS với model AlBERT, như vậy để train hết 1 epoch với WMT sẽ mất tầm 62-70 (h), tức là khoảng 3 ngày/epoch (liên tục).
Mà để train model NMT thì sẽ mất 5-15 epochs nên sẽ phải mất tầm &lt;strong&gt;15-45 ngày train liên tục&lt;/strong&gt;.
Tuy nhiên, Colab Free có giới hạn là 1 session chỉ được 12h liên tục, nên bạn sẽ phải break ra tầm 30-90 sessions.
Ngoài ra đóng browser là mất luôn đấy chứ nó lại không chạy ở background đâu.&lt;/p&gt;

&lt;p&gt;Bài viết này chỉ sử dụng có sẵn với viết script để demo inference hoặc show training vài step nó như thế nào thôi, nên chúng tôi cũng chỉ cần Colab Free là đủ.
Tuy nhiên, nếu mà các bạn định làm nghiêm túc kiểu hì hục nấu cả buổi thì tôi nghĩ là nếu dùng GPU thì dưới 8 GPUs là hì hục lâu phết đấy!
Có mấy giải pháp:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Nâng cấp nên Colab Pro (10USD/tháng) và Pro Plus (50USD/tháng).&lt;/strong&gt; Tuy nhiên, dù lên Pro Plus thì 1 session cũng chỉ được 24h liên tục. Được cái là cho phép chạy background nên sẽ dễ thở hơn. Có điều là 24h thì lại phải save ra Google Drive, rồi khởi động lại training &lt;strong&gt;bằng tay&lt;/strong&gt;. GPU/TPU sẽ nhanh hơn tầm vài lần nên có thể sẽ chỉ mất vài tuần để train thôi. &lt;strong&gt;Hì hục&lt;/strong&gt; nó là thế đấy!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tự chế ra 1 hệ thống multi-GPU&lt;/strong&gt;: kiếm kinh phí mua tầm 24 cái GPUs P100 thì có khi 1 epoch mất 1h thôi, thì train trong ngày sẽ xong. Giá cả thì &lt;strong&gt;Tesla P100 SXM2 16GB&lt;/strong&gt; tầm 10,000 USD/cái, 24 cái thì tầm 240,000 USD thôi! Muốn làm big data mà chỉ code thôi là không ăn thua đâu! Bởi vận hành triển khai mỗi tháng hóa đơn đã vài chục ngàn đô, tiền upfront mua GPU cũng đã 240,000 USD thì code hầu như là phần ít giá trị nhất! Code nói chung … rẻ mạt!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Thuê EC2&lt;/strong&gt;: thì có vài lựa chọn là &lt;strong&gt;on-demand&lt;/strong&gt; và &lt;strong&gt;reserved&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;reserved thì 1 tháng 30 ngày 720 h là mình phải trả hết 720 h. Thì rẻ nhất là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.xlarge&lt;/code&gt; cũng mỗi giờ 0.63 đô. Như vậy là mỗi tháng \(0.63 \times 720 = 453.6\) USD/tháng. Tuy nhiên, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.xlarge&lt;/code&gt; chỉ có 1 GPU và tính năng thì xấp xỉ Colab Free, nên giả sử train 15 epochs mất 45 ngày liên tục, thì sẽ tốn là \(0.63 \times 45 \times 24 = 680.4\) USD cho lượt train này.
        &lt;ul&gt;
          &lt;li&gt;Nếu thuê GPU tốt hẳn đi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.48xlarge&lt;/code&gt; coi như 8 GPUs thì nhanh hơn tầm 8 lần thì giá reserved sẽ là \(10.26 \times 45 \times 24 /8 = 1385.1\) USD cho lượt train. Tuy nhiên, vì reserved nên mình thuê theo tháng thì ngoài lượt train này, hàng tháng mình tổng phải trả là \(10.26 \times 720 = 7387.2\) USD/tháng. Tức là train được tầm 6 lượt WMT 2014.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Thuê &lt;strong&gt;on-demand&lt;/strong&gt; thì mình không trả hàng tháng, dùng phát nào xong trả phát ấy thôi. Thì ví dụ, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.48xlarge&lt;/code&gt; thì sẽ mất \(16.29\times 45 \times 24 /8 = 2199.5\) USD cho mỗi lượt train này kéo dài tầm gần 1 tuần.&lt;/li&gt;
      &lt;li&gt;Thuê &lt;a href=&quot;https://aws.amazon.com/ec2/spot/pricing/&quot;&gt;&lt;strong&gt;Spot&lt;/strong&gt;&lt;/a&gt; thì có thể thuê loại &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5g.16xlarge&lt;/code&gt; thì có 2 Tensor core và mất 1.1112 USD/h. Nhìn chung Spot giá sẽ mềm hơn &lt;strong&gt;on-demand&lt;/strong&gt; và cùng hạng với reserved. Tuy nhiên, vì là spot nên lúc nào mà tự dưng bị interupt là phải có kế hoạch ứng phó trước:
        &lt;blockquote&gt;
          &lt;p&gt;Spot Instances are a cost-effective choice if you can be flexible about when your applications run and &lt;strong&gt;if your applications can be interrupted&lt;/strong&gt;&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Như các bạn đã thấy ở trên: cùng mức GPU &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.xlarge&lt;/code&gt; thì Colab Free còn thuê EC2 mất 453.6 USD. Vậy sự khác biệt là gì? Điểm khác chính yếu là thuê EC2 thì bạn không phải lo cái &lt;strong&gt;session 12 h&lt;/strong&gt; (sau 12h phải khởi động lại bằng tay và save dữ liệu vào Google Drive).&lt;/em&gt;
&lt;strong&gt;Tức là chỉ mỗi cái limit 12h ấy thôi đã trị giá 453.6 USD rồi.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Một điểm đáng lưu ý thứ 2 là giới R&amp;amp;D cạnh tranh khá khốc liệt, nên trên thứ vô thưởng vô phạt kiểu này thì kéo dài 45 ngày và free cũng được.
Nhưng cái gì mà có giá trị kinh tế một chút là cạnh tranh ác liệt: &lt;strong&gt;train là phải xong trong ngày, chứ đợi cả tháng sau có đứa nó publish mất thì sao?&lt;/strong&gt;
Thế nên tôi nghĩ nếu thực sự phải cạnh tranh thì chắc lại 240,000 USD hoặc &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.48xlarge&lt;/code&gt; thôi.
Và nếu chọn con đường cạnh tranh này thì các tập đoàn lớn, có sẵn hàng chục cái GPU server trong tay muốn huy động lúc nào cũng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ok em&lt;/code&gt; thì chắc chắn là thắng thế.
Nên cạnh tranh vào cái con đường kiểu đọ thông số GPU này có lẽ là chỉ dành cho tập đoàn lớn thôi.&lt;/p&gt;

&lt;p&gt;Thứ 3 nữa là tự chế 240,000 USD thì mới là DIY và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self-supervised&lt;/code&gt; chứ còn thuê với Colab thì đâu còn là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self&lt;/code&gt; nữa nên rốt cuộc là chắc cũng quay lại 240,000 USD nếu thực sự là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;neural-machine-translation-nmt&quot;&gt;Neural Machine Translation (NMT)&lt;/h2&gt;

&lt;p&gt;NMT là 1 bài toán NLP khá điển hình. Lần này chúng ta sẽ chạy thử 1 vài mô hình trên HuggingFace để dịch tiếng Đức sang tiếng Anh.
Đầu tiên, là với mớ lý thuyết phía trên, chúng ta sẽ xem thử cách thức fine-tune model AlBERT cho task NMT.
Tôi dùng bộ dữ liệu WMT 2014 để show:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/fab8ab5644bd1a122dd3ad8880d283f9.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Nhìn chung, cứ setup đúng các thông số, tokenizer, hàm loss, optimizer, .v.v… thì hầu như quá trình fine-tune mô hình kiểu BERT diễn ra rất trôi chảy.
Vấn đề tài nguyên để kết thúc quá trình train thì như thảo luận ở trên, các bạn có thể tự lựa chọn giải pháp phù hợp túi tiền.&lt;/p&gt;

&lt;p&gt;Nói chung nấu cả buổi nhưng ăn thì nhanh thôi. 
Để demo chức năng dịch Đức-Anh thì tôi xài luôn model có sẵn &lt;a href=&quot;https://huggingface.co/google/bert2bert_L-24_wmt_de_en&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;google/bert2bert_L-24_wmt_de_en&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/88d0506002341d151a8e5ad745f3364f.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Tôi cũng sử dụng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sacrebleu&lt;/code&gt; để show được điểm số BLEU của vài câu dịch.&lt;/p&gt;

&lt;h2 id=&quot;text-summarization&quot;&gt;Text Summarization&lt;/h2&gt;
&lt;p&gt;Bài toán &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Text Summarization&lt;/code&gt; (TS), yêu cầu đưa ra một đoạn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tóm tắt&lt;/code&gt; ngắn gọn của 1 văn bản dài.
Có khá nhiều bộ dữ liệu hỗ trợ việc training một model như vậy &lt;a class=&quot;citation&quot; href=&quot;#metatext2022summary&quot;&gt;[38]&lt;/a&gt;.
Trong bài này, tôi chọn 1 bộ nhỏ vừa là bộ WikiHow &lt;a class=&quot;citation&quot; href=&quot;#koupaee2018wikihow&quot;&gt;[39]&lt;/a&gt; với tầm 240,000 cặp bài báo-tóm tắt.&lt;/p&gt;

&lt;p&gt;Về mặt model, thì có khá nhiều hệ thống state-of-the-art và là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cho abstractive TS như NAM &lt;a class=&quot;citation&quot; href=&quot;#rush2015neural&quot;&gt;[7]&lt;/a&gt;, Encoder-Decoder RNN &lt;a class=&quot;citation&quot; href=&quot;#nallapati2016abstractive&quot;&gt;[40]&lt;/a&gt; hay BART &lt;a class=&quot;citation&quot; href=&quot;#lewis2020bart&quot;&gt;[31]&lt;/a&gt;.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/08fb46dd6f03981287c5d3155bf7d6fa.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Nhìn chung, tuy WikiHow số lượng cặp dữ liệu ít hơn WMT14 (240,000 vs. 4,500,000) nhưng mỗi cặp dữ liệu lại là hẳn 1 bài dài, nên kết cục là cùng 1 GPU Colab Free thì thời gian train vẫn mất 49h/epoch.
Tức là cứ 1 epoch 2 ngày, và train khoảng 15 epochs thì mất tầm 30 ngày (hay 1 tháng, tính cả nghỉ lễ, T7/CN).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ts_training.png&quot; alt=&quot;ts&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;qa-và-vqa&quot;&gt;QA và VQA&lt;/h2&gt;

&lt;p&gt;Nếu bạn đã làm bài thi TOEIC reading and listening thì cũng dễ giải thích bài QA và VQA thôi.
Task QA là 1 bộ phận trong task lớn Reading Comprehension (RC): cho 1 bài báo (có thể kèm hình ảnh), người/máy sẽ đọc rồi trả lời 1 câu hỏi liên quan tới bài viết đó.
&lt;em&gt;Bài báo&lt;/em&gt; làm reference được gọi là &lt;em&gt;context&lt;/em&gt;, một ví dụ điển hình:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Context: I live in Japan.
Question: Where do I live?
Answer: Japan.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://tweetqa.github.io/image/example.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;
Dạng đơn giản nhất là như vậy chỉ cần đọc kỹ và &lt;strong&gt;paraphrasing&lt;/strong&gt; lại cụm từ có sẵn trong context thôi.
Đây là dạng &lt;strong&gt;extractive QA&lt;/strong&gt; tức là chỉ đơn giản skim bài viết và quote lại thôi.
Bộ dữ liệu đại diện cho kiểu extractive QA này là SQuAD v1 &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2016squad&quot;&gt;[4]&lt;/a&gt; và v2 &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2018know&quot;&gt;[17]&lt;/a&gt;.
Tuy nhiên, nếu bạn làm bài thi TOEIC bạn sẽ hiểu là có cả những câu hỏi mà không chỉ paraphrasing mà phải suy luận.
Đây gọi là &lt;strong&gt;abstractive QA&lt;/strong&gt;, điển hình cho dạng này là bộ TweetQA &lt;a class=&quot;citation&quot; href=&quot;#xiong2019tweetqa&quot;&gt;[41]&lt;/a&gt;.
Như ở ví dụ bên tay trái, bạn có thể thấy cần phải để ý chi tiết và suy luận thì mới trả lời được.
Ví dụ bên dưới là 1 model Abstractive QA dựa trên AlBERT:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/f3def206dd7f578e723a90d3336d04da.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Để đánh giá, có thể dùng các metric như ROUGE-L hoặc METEOR.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-and-why.png&quot; style=&quot;float: left; margin: 10px; width: 40%;&quot; /&gt;
&lt;strong&gt;V&lt;/strong&gt;isual &lt;strong&gt;Q&lt;/strong&gt;uestion &lt;strong&gt;A&lt;/strong&gt;nswering (VQA, &lt;a class=&quot;citation&quot; href=&quot;#antol2015vqa&quot;&gt;[5, 6]&lt;/a&gt;) thay context bởi 1 hình ảnh hoặc video.
Tức là giống câu hỏi bài nghe thứ nhất trong thi TOEIC (nhưng TOEIC thì không có xem video).
Thì nói chung cũng đa dạng đấy!
Vì câu hỏi thì cũng tùy vào mức độ hình ảnh mà đánh đố hay không đánh đố.
Thì làm sao để thấy được sự khác biệt với QA của bên NLP thì chắc là các bạn xem hình ảnh bên trái (lấy từ &lt;a class=&quot;citation&quot; href=&quot;#VCRdataset&quot;&gt;[42, 43]&lt;/a&gt;) thì sẽ hiểu ngay.
Mà tôi nghĩ cũng chỉ nhìn vào mỗi cái hình này là cũng hiểu được là &lt;strong&gt;how and why&lt;/strong&gt; như thế nào rồi đấy.
Dưới đây là bức tranh lớn trong mảng VQA (tập hợp qua các bài trên 300 citations).
Tuy chỉ là một mảng nhỏ, nhưng cái scene này nhìn cũng là 1 graph phức tạp phết!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/vqa-author-paper-graphs.png&quot; alt=&quot;vqa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cũng không nên quên là cách tiếp cận của nhân vật chính postdoc trong bài này là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;, mà nói chung là mảng giao giữa với vision và language nên tôi nghĩ là tiếp cận theo cách nào cũng sẽ đi đến cái chỗ nó như vậy thôi.
Vậy chúng ta lại cứ bám sát vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nhé (mà hậu duệ là Transformers, BERT là những cái tôi thấy có vẻ cũng bắt đầu lấn sân sang Vision gần đây rồi).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Điểm khác biệt lớn nhất giữa vision và language chính là &lt;strong&gt;ORDER&lt;/strong&gt;.
Nếu như đơn vị trong language là words được sắp xếp theo order thành phrase và sentence, words được extract từ documents nhờ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokenizer&lt;/code&gt;.
Trong hình ảnh, có một sự mapping với language, và cái hướng này cũng có mấy nhóm cũng rất mạnh đang làm chủ công nghệ này.
Ví dụ, word, phrase &amp;lt;-&amp;gt; visual concepts
còn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokenizer&lt;/code&gt; &amp;lt;-&amp;gt; classifier, object detectors, visual relation detectors, .v.v…
Nói chung, bên vision cũng có đủ những “vũ khí” tương xứng để extract tokens và feed vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; models.
Tuy nhiên, vấn đề chính là order: cùng trong một hình ảnh, có thể extract ra 2 vật thể, nhưng khi xếp vào 1 câu văn thì thứ tự nào cũng có khả năng cả.
Nó khác language là &lt;strong&gt;ORDER&lt;/strong&gt; đã được input sẵn, còn vision thì không.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Vấn đề &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;order&lt;/code&gt; cũng không phải mới mà được xem xét ngay từ 2015 (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; for sets, &lt;a class=&quot;citation&quot; href=&quot;#vinyals2015order&quot;&gt;[44]&lt;/a&gt;).
Tuy nhiên, họ cũng chỉ dừng ở mức chọn 1 thứ tự nhất định để bắt đầu và optimize thứ tự đó trong quá trình training.
Đó cũng là tinh thần của hầu hết các ứng dụng vision-language về sau có dùng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; và attention như image captioning, Video-To-Text (VTT) và VQA.&lt;/p&gt;

&lt;p&gt;Một cách giải quyết khác chính là tìm một &lt;strong&gt;representation&lt;/strong&gt; tốt hơn cho &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt;.
Vì &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt; trong vision không phải &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence&lt;/code&gt; trong NLP nên vấn đề sẽ xảy ra khi dùng representation mới là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; sẽ phải thay đổi để phù hợp với representation mới.
Trong những publication gần đây, một &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt; representation phù hợp là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt;.
Vấn đề là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt; không phải &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence&lt;/code&gt; nên &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cũng thay thế bằng graph neural networks (GNN).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Là người thày, có thể không trực tiếp bắt tay vào làm, nhưng phải định hướng.
Vì vậy không thể định hướng một hướng đi “cụt”, chỉ làm 1 đời postdoc là hết.
Định hướng, đôi khi dù 10 đời postdoc không làm gì thì cũng không thể hết việc được, mới là định hướng tốt.
Thậm chí, nếu postdoc mà chịu khó làm thì còn sinh sôi nảy nở ra làm mãi không hết.
Vision-Language (VL) nhìn chung là 1 định hướng lâu dài như vậy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Làm người trò, thì khi bắt đầu dự án không nên quá thụ động.
Việc representation có thể phải thay đổi giữa chừng (từ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence&lt;/code&gt; sang &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt;) là chuyện nếu không nhìn ra ngay từ đầu,
có thể dẫn đến dự án bị đình chỉ giữa chừng.
Làm postdoc như vậy là yếu về &lt;strong&gt;quản lý rủi ro&lt;/strong&gt;.
Nhìn chung vẫn là nghiệp vụ quản lý dự án (Project Management) vẫn còn chưa mạnh.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nhìn chung, mảng Vision-Language (VL) này tôi thấy còn nhiều dư địa cả về lý thuyết lẫn ứng dụng:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Về lý thuyết, cái representation vẫn còn chỗ làm. Ứng với mỗi representation có thể sẽ phải thu thập dataset và gắn nhãn riêng. Xây dựng model mới hẳn cho &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt; và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Về ứng dụng, VQA hiện tại vẫn chỉ là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;closed form&lt;/code&gt;: xem 1 hình ảnh và extract concepts rồi dùng kỹ thuật QA để trả lời nội dung trong ảnh, tức là chưa phải dạng suy luận nhiều. VQA ở dạng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;open&lt;/code&gt; hơn sẽ thay context không chỉ bởi 1 hình ảnh hay 1 video, mà bởi 1 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multimedia database&lt;/code&gt;, cũng như trong trò chơi tỷ phú, mỗi câu hỏi không chỉ đòi hỏi phải xem 1 context nhất định mà phải &lt;strong&gt;search&lt;/strong&gt; trong 1 database knowledge mới có thể trả lời được.
    &lt;ul&gt;
      &lt;li&gt;Một bước đơn giản hơn chỉ là thay context hình ảnh bởi context video, và câu hỏi là về 1 details nào đó: ví dụ show cho máy một video nấu ăn, và hỏi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tại sao lại cho hành vào phi trước?&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Thay vì giới hạn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;luật chơi&lt;/code&gt; rằng máy chỉ được xem context là 1 bài báo, 1 hình ảnh, 1 video cụ thể, thì nhưu IBM Watson hay Siri, cho phép máy kết nối Internet và có thể query trong cả 1 multimedia database lớn, thì QA sẽ như thế nào?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tuy vậy, với công nghệ hiện tại cũng đủ để làm 1 demo nho nhỏ dạng: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cho máy xem 1 hình ảnh, và đặt 1 vài câu hỏi xung quanh nội dung hình ảnh&lt;/code&gt; và không cho máy kết nối Internet, thì máy cũng có thể trả lời ở mức độ nhất định. Tức là có thể lấy điểm bài nghe số 1 của TOEIC!&lt;/p&gt;

&lt;p&gt;Đánh giá về tiềm năng bài toán VQA, thì tôi thấy cái mảng này nó cũng dồi dào, code kiếc cũng thừa mứa trên Github, kiểu như HuggingFace ý: classifier thì có TIMM &lt;a class=&quot;citation&quot; href=&quot;#rw2019timm&quot;&gt;[45]&lt;/a&gt;, detector thì có Detectron2 &lt;a class=&quot;citation&quot; href=&quot;#wu2019detectron2&quot;&gt;[46]&lt;/a&gt; hay MMDetection &lt;a class=&quot;citation&quot; href=&quot;#mmdetection&quot;&gt;[47]&lt;/a&gt;, segmentations thì có &lt;a class=&quot;citation&quot; href=&quot;#Yakubovskiy:2019&quot;&gt;[48]&lt;/a&gt;, visual relationship detector (VRD) thì cũng tràn lan vì các tác giả cũng công bố hết lên Github.
Thế nên là những cái low-level detector kiểu visual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokenizer&lt;/code&gt; là sẵn có, mà người ta công bố mà mình không dùng thì cũng phí.
Mà vấn đề là của người ta chất lượng nó cao.
Dữ liệu thì cũng công bố sẵn có hết, bộ dữ liệu VQA hay commonsense hay Visual Genome &lt;a class=&quot;citation&quot; href=&quot;#krishna2017visual&quot;&gt;[49]&lt;/a&gt; thì nhìn chung cũng tầm triệu câu hỏi, với vài trăm ngàn hình ảnh.
&lt;strong&gt;Đấy vấn đề với quy mô dữ liệu này thì lại quay về bài toán &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;240,000&lt;/code&gt; USD thoai!&lt;/strong&gt;
Đấy, mấy cái mảng này nó toàn kiểu code kiếc, dữ liệu thì bằng cho không trên Github rồi, nhưng vấn đề là để chạy được thì hàng trăm ngàn đô.
Các bạn thấy có những nhóm nó chịu khó train để ra model công bố thì toàn 8 GPUs mà cũng hì hục train 1 tuần.
Có cái train ImageNet trong 1 giờ &lt;a class=&quot;citation&quot; href=&quot;#goyal2017accurate&quot;&gt;[50]&lt;/a&gt; thì lại là họ dùng những &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;256 GPUs&lt;/code&gt; mà họ có được nguồn tài nguyên ấy là nhóm của … Facebook!
Nên tôi nghĩ khi làm postdoc mà không kiểu có estimate trước thời gian train là lại … giữa chừng đứt gánh vì … không có tiền!
Mà không có tiền thì đấy như ví dụ postdoc của chúng ta là lặng lẽ giải tán rồi!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Chứ không có tiền thì thày cũng không có, trò cũng không có thì nhìn nhau … cười à?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mà vì tài nguyên cái mảng này nó tràn lan trên Github nên có bảo tôi code thì lấy code làm công cũng không được vì cùng cái code trên mạng Internet nó nhan nhản ra, &lt;strong&gt;code nói chung là rẻ mạt!&lt;/strong&gt;
&lt;em&gt;Cái quan trọng là vượt qua cái khó khăn như 240,000 USD và ra được sản phẩm thoai!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;video-to-text-và-image-captioning&quot;&gt;Video-to-Text và Image Captioning&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Training on videos used standard SGD with momentum set to 0.9 in all cases, with synchronous parallelization across &lt;strong&gt;32 GPUs&lt;/strong&gt; for all models except the 3D ConvNets which receive a large number of input frames and hence require more GPUs to form large batches – we used &lt;strong&gt;64 GPUs&lt;/strong&gt; for these. We trained models on on Kinetics for &lt;strong&gt;110k steps&lt;/strong&gt;, with a 10x reduction of learning rate when validation loss saturated. We tuned the learning rate hyperparameter on the validation set of Kinetics. Models were trained for up to &lt;strong&gt;5k steps&lt;/strong&gt; on UCF-101 and HMDB-51 using a similar learning rate adaptation procedure as for Kinetics but using just &lt;strong&gt;16 GPUs&lt;/strong&gt;. (trích dẫn &lt;a class=&quot;citation&quot; href=&quot;#carreira2017quo&quot;&gt;[51]&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Vấn đề gì trong đoạn văn trên?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Thứ nhất, UCF-101 và HMDB-51 là những bộ dữ liệu nhỏ nhưng họ cũng phải dùng tới &lt;strong&gt;16 GPUs&lt;/strong&gt; để train. Thì gọi cho 16 GPUs sẽ nhanh lên tầm 1s/iteration thì train 5k steps mất tầm 1h-2h. Thì với bộ dữ liệu nhỏ như vậy là ổn.&lt;/li&gt;
  &lt;li&gt;Tuy nhiên, với bộ dữ liệu lớn như Kinetics thì họ phải inputs hàng triệu frames, và để nhanh thì bắt buộc dùng tới 32, thậm chí 64 GPUs để train. Gọi cho là dùng &lt;strong&gt;64 GPUs&lt;/strong&gt; thì sẽ train nhanh đi thì 1s/iteration thì 110k steps cũng mất 1-2 ngày.&lt;/li&gt;
  &lt;li&gt;Nhưng nên nhớ họ có 64 GPUs nhé, chứ còn chỉ có 1-2 GPUs thì xác định mất 1-2 ngày x 30 lần tức là train mất &lt;strong&gt;1-2 tháng&lt;/strong&gt; (không ngủ nghỉ gì cả, T7/CN nghỉ lễ cũng train).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bài toán Video-To-Text hay Video Captioning, Video-Text Retrieval đòi hỏi cần có những biểu diễn phù hợp cho cả vision lẫn language.
Về mặt vision thì có mấy cái hệ biểu diễn C3D/I3D như trên.
Nhìn chung có thể dùng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; hoặc BERT để learn được những biểu diễn như vậy và input vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; để xuất ra captions.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Nhưng nhìn chung, độ khả thi của đề xuất này vẫn phụ thuộc con số &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;16-32-64&lt;/code&gt; GPUs ở trên. Thày nghèo với thày giàu là có phân biệt đấy.
Nói chung làm hình ảnh thì dưới 8 GPUs không nên theo.
Nhưng video thì dưới 16 GPUs thì cũng không nên train triếc mất thời gian.
Mà thày nào dưới 16 GPUs thì cũng không nên theo nếu bạn định &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Các bạn cũng cần hiểu, đó là nguồn tài nguyên trong một lab nghiên cứu luôn là mua chung cho cả lab, chứ không phải cho 1 anh postdoc cụ thể nào.
Ví dụ postdoc muốn xin dùng hệ thống 8 GPUs của thày tầm 1 ngày 2 ngày thì còn ổn.
Chứ postdoc mà muốn chiếm hẳn 1 tuần để train model video captioning thì tự dưng nó nảy sinh ra rất nhiều vấn đề khác.
Nên là nếu bạn chọn vào chỗ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nghèo&lt;/code&gt; quá ấy, thì liệu cơm gắp mắm, nhưng tốt nhất đừng có &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train&lt;/code&gt; gì cả.
Bây giờ làm việc bên mảng vision này, đặc biệt là video, thì tự dưng lại phải ngồi build cả 1 hệ thống GPU server &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;16-32-64-128-256&lt;/code&gt; GPUs để cho đệ tử nó dùng.
Nên là trước khi bắt đầu dự án, việc lên kế hoạch mua sắm, build server các cái là phải chuẩn bị từ trước hết.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Chứ không chạy đến giữa chừng lại ngậm ngùi giải tán vì … không có GPUs (mà thực ra là không có tiền) để chạy thì còn ra cái thể thống cống rãnh gì nữa?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Image Captioning thì cũng dùng RNN/LSTM và Attention từ lâu rồi.
Nhìn chung, cái mảng này vẫn là &lt;strong&gt;cuộc chơi của người giàu&lt;/strong&gt;.
Mà độ giàu của ông thày là số lượng GPUs mà lab thày sở hữu.
Bây giờ, postdoc có ý tưởng, postdoc có đề tài, postdoc lên kế hoạch, postdoc tính toán tỷ mỷ số lượng tài nguyên cần thiết.
Thế bây giờ, postdoc là người chọn thày chứ không phải thày chọn postdoc.
Thì quay lại trường hợp postdoc của chúng ta:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Tại sao lại chọn chỗ nghèo như vậy?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Chưa đi ra đến chợ mà đã hết tiền như thế thì làm thế nào cạnh tranh nổi trong giới postdoc?
Mà đã thấy không có tiền thì đổi đề tài cho nó phù hợp, làm cái task khác nó không cần tiền đi.
Tất nhiên là người nghèo lại làm task không có tiền thì lại thành … &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nghèo bền vững&lt;/code&gt; thôi!
Nói chung là ngay từ bước &lt;strong&gt;quản lý tài nguyên, một phần trong nghiệp vụ quản lý dự án&lt;/strong&gt; là đã có vấn đề.&lt;/p&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;h2 id=&quot;về-seq2seq-trong-nlp&quot;&gt;Về &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; trong NLP&lt;/h2&gt;
&lt;p&gt;Nhìn chung về mấy task bên NLP, thì tất nhiên bài viết này cũng chỉ là 1 phần (bao gồm task NMT/TS/QA với mấy cái tự học) còn nhiều task khác như POS Tagging, semantic roles, …
Tuy nhiên, nhìn chung những tiến bộ trong mảng NLP đã được tóm tắt, và có vẻ cũng không khó khăn lắm đâu.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Có làm thì sẽ có ăn thôi, còn không làm mà đòi có ăn thì …&lt;/p&gt;
&lt;/blockquote&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/peTwj6WK3vo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Cũng xoáy kha khá cái trường hợp postdoc, nhưng tôi nghĩ là cứ chuyển hết cho Prof. Huan Rose chỉ bảo là ok thôi.&lt;/p&gt;

&lt;p&gt;Nói chung phần kỹ thuật thì không có nhiều điểm khó, nhưng cái khó nhất chắc vẫn là cái 240,000 USD thôi. 
Mà cái đó thì chắc chắn không phải là cái anh em kỹ thuật có thể giải quyết nổi.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Nên nếu không có tiền thì … giải tán thôi, chứ biết làm thế nào bây giờ?
Cái này có đố cũng chịu!
Như bình thường thì có thể đổi sang làm những cái bài toán mà nó &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train phát xong luôn, dùng luôn&lt;/code&gt;, những bài toán mà nó có thể chạy chỉ cần CPU, thậm chí chỉ mấy cái device nhỏ nhỏ cũng chạy được ấy.
Nên tôi nghĩ cũng không cần xoáy nhiều vào cái đám big data nữa:&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Xin vô mấy cái tập đoàn lớn đùng ấy mà làm, nó có nhiều người dùng nó sẵn sàng chi tiền tấn ra mà mua. Ví dụ, các anh em thử nghĩ xem mấy cái papers nhiều citation của cái mảng này làm thí nghiệm toàn hàng trăm GPU, TPU thì tác giả toàn nhóm của những công ty như thế nào? phải Google, Facebook, … chứ như anh em &lt;strong&gt;chân đất mắt toét&lt;/strong&gt; thì chờ nó ra model rồi dùng lại, chứ mất công hì hục nấu nướng làm gì cho mất thời gian.&lt;/li&gt;
    &lt;li&gt;Chuyển sang làm mấy cái thiết bị nhỏ nhỏ mà làm. Mà chấm dứt suy nghĩ về GPU với mấy cái model to đùng, nấu thì lâu mà ăn thì chóng đi.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;về-chuyện-tài-nguyên-training&quot;&gt;Về chuyện tài nguyên training&lt;/h2&gt;
&lt;p&gt;Các bạn có thể thấy điển tích &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;240,000 USD&lt;/code&gt; xuất hiện khá nhiều trong bài viết.
Nói chung mảng này lúc bắt đầu làm postdoc hay dự án nghiên cứu mà không nhìn trước được những khó khăn điển tích này thì kỹ năng quản lý dự án là kém.
&lt;strong&gt;Chứ chạy đến giữa postdoc lại ngậm ngùi giải tán vì … không có tiền thì còn ra thể thống gì?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Thứ hai là nếu bạn đã xác định đi con đường train triếc mất thời gian này thì điều tra kỹ về lab hoặc nhóm mình vào:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Nếu thày có &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ít hơn 8 GPUs&lt;/code&gt; (mà phải là GPU xịn khỏe nhé chứ GPU đểu thì cũng nhiều mà) thì đừng vào lab ấy làm gì, vì cơ sở hạ tầng của lab không có và lúc làm bạn phải tự lo tự chạy đấy. Trong trường hợp thuê thì thày phải có ngân sách và chịu chi.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thứ ba là tốt nhất là nên chọn mấy cái đề tài &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;không cần train&lt;/code&gt;, hoặc có train thì cũng tí là xong.
Đấy mấy cái Random Forest, mấy cái model nhẹ nhàng train vèo phát xong.
Rồi mình làm mấy cái deploy lên thiết bị nhỏ nhẹ, vừa khỏe vừa có tiền, cả nhà đều vui :))&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cách đây ít lâu, tôi có làm phỏng vấn tuyển dụng, có bạn trẻ “thị uy” với tôi là từng làm việc với hệ thống 24 GPUs.
Tôi cũng thú nhận là chỗ chúng tôi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;không train&lt;/code&gt; gì hết, chỉ vào làm luôn inference nhỏ nhẹ, vừa khỏe vừa vui thôi.
Nên nếu bạn mà định kiểu dành hàng tuần ngồi train thì chúng tôi không đáp ứng được.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thế nên chốt lại là tốt nhất là nên chọn hướng đi ít train thôi.
Còn nếu chọn con đường train nhiều tốn kém thì &lt;strong&gt;PHẢI CÓ TIỀN&lt;/strong&gt; vì đây là cuộc chơi của người giàu!&lt;/p&gt;

&lt;h2 id=&quot;về-các-task-vision-language&quot;&gt;Về các task vision-language&lt;/h2&gt;
&lt;p&gt;Thật thú vị là cùng thời điểm khoảng 2015-2016, bên vision cũng xuất hiện 1 loạt task kế thừa &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; và đến bây giờ vẫn phát triển song song và lấy những công nghệ tiên tiến nhất của bên NLP như Transformers hay BERT về để ứng dụng.
Thì họ cũng làm lên một nhánh vision-language learning (VL learning), với những task thú vị như VQA, VTT, image captioning, video/image-text retrieval.&lt;/p&gt;

&lt;p&gt;Nhìn chung là thú vị, nhưng có một số vấn đề như representation, thì đặc điểm của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt; nên có những biểu diễn mới như &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt; hay &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3D&lt;/code&gt; (C3D/I3D).
Nếu làm một kế hoạch nghiêm chỉnh cũng có thể làm ra một cái postdoc tốt nhưng không hiểu sao ngày ấy lại không đi đến đâu cả.
Mà cuối cùng ấy, tôi nghĩ có nhiều nguyên nhân như phân tích ở trên, nhưng một nguyên nhân tuy nói ra hơi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;phô&lt;/code&gt; nhưng mà &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thật&lt;/code&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Không có GPUs!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Cái này cũng thể hiện sự yếu kém về mặt nghiệp vụ quản lý dự án (liên quan tới quản lý scope, tài nguyên và kế hoạch).&lt;/p&gt;

&lt;p&gt;Nhìn chung cũng là một bài học kinh nghiệm quý báu.
Tôi nghĩ vẫn có nhiều điều đáng lưu ý để thế hệ sau không vấp phải.&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;kalchbrenner2013recurrent&quot;&gt;Kalchbrenner, N. and Blunsom, P. 2013. Recurrent continuous translation models. &lt;i&gt;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA. Association for Computational Linguistics&lt;/i&gt; (2013).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kalchbrenner2013recurrent/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;sutskever2014sequence&quot;&gt;Sutskever, I., Vinyals, O. and Le, Q.V.V. 2014. Sequence to sequence learning with neural networks. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; (2014), 3104–3112.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/sutskever2014sequence/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cho2014learning&quot;&gt;Cho, K., Merrienboer, B. van, Gulcehre, C., Bougares, F., Schwenk, H. and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. &lt;i&gt;arXiv preprint arXiv:1406.1078&lt;/i&gt;. (2014).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/cho2014learning/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rajpurkar2016squad&quot;&gt;Rajpurkar, P., Zhang, J., Lopyrev, K. and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. &lt;i&gt;Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing&lt;/i&gt; (2016), 2383–2392.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rajpurkar2016squad/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;antol2015vqa&quot;&gt;Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L. and Parikh, D. 2015. Vqa: Visual question answering. &lt;i&gt;Proceedings of the IEEE international conference on computer vision&lt;/i&gt; (2015), 2425–2433.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/antol2015vqa/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;goyal2017making&quot;&gt;Goyal, Y., Khot, T., Summers-Stay, D., Batra, D. and Parikh, D. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. &lt;i&gt;Proceedings of the IEEE conference on computer vision and pattern recognition&lt;/i&gt; (2017), 6904–6913.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/goyal2017making/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rush2015neural&quot;&gt;Rush, A.M., Chopra, S. and Weston, J. 2015. A Neural Attention Model for Abstractive Sentence Summarization. &lt;i&gt;Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing&lt;/i&gt; (2015), 379–389.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rush2015neural/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ranzato2015sequence&quot;&gt;Ranzato, M.A., Chopra, S., Auli, M. and Zaremba, W. 2015. Sequence level training with recurrent neural networks. &lt;i&gt;arXiv preprint arXiv:1511.06732&lt;/i&gt;. (2015).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ranzato2015sequence/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;venugopalan2015sequence&quot;&gt;Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T. and Saenko, K. 2015. Sequence to sequence-video to text. &lt;i&gt;Proceedings of the IEEE international conference on computer vision&lt;/i&gt; (2015), 4534–4542.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/venugopalan2015sequence/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;you2016image&quot;&gt;You, Q., Jin, H., Wang, Z., Fang, C. and Luo, J. 2016. Image captioning with semantic attention. &lt;i&gt;Proceedings of the IEEE conference on computer vision and pattern recognition&lt;/i&gt; (2016), 4651–4659.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/you2016image/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;johnson2016densecap&quot;&gt;Johnson, J., Karpathy, A. and Fei-Fei, L. 2016. Densecap: Fully convolutional localization networks for dense captioning. &lt;i&gt;Proceedings of the IEEE conference on computer vision and pattern recognition&lt;/i&gt; (2016), 4565–4574.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/johnson2016densecap/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;luong2015multi&quot;&gt;Luong, M.-T., Le, Q.V., Sutskever, I., Vinyals, O. and Kaiser, L. 2015. Multi-task sequence to sequence learning. &lt;i&gt;arXiv preprint arXiv:1511.06114&lt;/i&gt;. (2015).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/luong2015multi/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;vaswani2017attention&quot;&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. 2017. Attention is all you need. &lt;i&gt;Advances in neural information processing systems&lt;/i&gt; (2017), 5998–6008.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/vaswani2017attention/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bojar2014findings&quot;&gt;Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H. and others 2014. Findings of the 2014 workshop on statistical machine translation. &lt;i&gt;Proceedings of the ninth workshop on statistical machine translation&lt;/i&gt; (2014), 12–58.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/bojar2014findings/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bojar2017findings&quot;&gt;Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., Huck, M., Koehn, P., Liu, Q., Logacheva, V. and others 2017. Findings of the 2017 conference on machine translation (wmt17). &lt;i&gt;Proceedings of the Second Conference on Machine Translation&lt;/i&gt; (2017), 169–214.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/bojar2017findings/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;paul2004introduc&quot;&gt;Over, P. and Yen, J. 2014. An Introduction to DUC-2004. https://duc.nist.gov/pubs/2004slides/duc2004.intro.pdf.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/paul2004introduc/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rajpurkar2018know&quot;&gt;Rajpurkar, P., Jia, R. and Liang, P. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. &lt;i&gt;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/i&gt; (2018), 784–789.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rajpurkar2018know/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lin2004rouge&quot;&gt;Lin, C.-Y. 2004. Rouge: A package for automatic evaluation of summaries. &lt;i&gt;Text summarization branches out&lt;/i&gt; (2004), 74–81.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lin2004rouge/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bottou2007tradeoffs&quot;&gt;Bottou, L. and Bousquet, O. 2007. The tradeoffs of large scale learning. &lt;i&gt;Advances in neural information processing systems&lt;/i&gt;. 20, (2007).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/bottou2007tradeoffs/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rumelhart1986learning&quot;&gt;Rumelhart, D.E., Hinton, G.E. and Williams, R.J. 1986. Learning representations by back-propagating errors. &lt;i&gt;nature&lt;/i&gt;. 323, 6088 (1986), 533–536.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rumelhart1986learning/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;goodfellow2016deep&quot;&gt;Goodfellow, I., Bengio, Y. and Courville, A. 2016. &lt;i&gt;Deep learning&lt;/i&gt;. MIT press.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/goodfellow2016deep/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hochreiter1997long&quot;&gt;Hochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. &lt;i&gt;Neural computation&lt;/i&gt;. 9, 8 (1997), 1735–1780.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hochreiter1997long/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gers2000learning&quot;&gt;Gers, F.A., Schmidhuber, J. and Cummins, F. 2000. Learning to forget: Continual prediction with LSTM. &lt;i&gt;Neural computation&lt;/i&gt;. 12, 10 (2000), 2451–2471.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/gers2000learning/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cho2014properties&quot;&gt;Cho, K., Merrienboer, B. van, Bahdanau, D. and Bengio, Y. 2014. On the properties of neural machine translation: Encoder-decoder approaches. &lt;i&gt;Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8), 2014&lt;/i&gt; (2014).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/cho2014properties/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;koehn2009statistical&quot;&gt;Koehn, P. 2009. Neural Machine Translation. &lt;i&gt;Statistical Machine Translation&lt;/i&gt;. Cambridge University Press.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/koehn2009statistical/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;vaswani2018tensor2tensor&quot;&gt;Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A.N., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner, N., Parmar, N. and others 2018. Tensor2tensor for neural machine translation. &lt;i&gt;arXiv preprint arXiv:1803.07416&lt;/i&gt;. (2018).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/vaswani2018tensor2tensor/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wolf2020transformers&quot;&gt;Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi, A., Cistac, P., Funtowicz, M., Davison, J., Shleifer, S. and others 2020. Transformers: State-of-the-art natural language processing. &lt;i&gt;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&lt;/i&gt; (2020), 38–45.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/wolf2020transformers/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;devlin2019bert&quot;&gt;Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. &lt;i&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/i&gt; (2019), 4171–4186.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/devlin2019bert/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lan2019albert&quot;&gt;Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P. and Soricut, R. 2019. Albert: A lite bert for self-supervised learning of language representations. &lt;i&gt;arXiv preprint arXiv:1909.11942&lt;/i&gt;. (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lan2019albert/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;liu2019roberta&quot;&gt;Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. &lt;i&gt;arXiv preprint arXiv:1907.11692&lt;/i&gt;. (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/liu2019roberta/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lewis2020bart&quot;&gt;Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V. and Zettlemoyer, L. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. &lt;i&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/i&gt; (2020), 7871–7880.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lewis2020bart/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yang2019xlnet&quot;&gt;Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R. and Le, Q.V. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. &lt;i&gt;Advances in neural information processing systems&lt;/i&gt;. 32, (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/yang2019xlnet/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;conneau2019cross&quot;&gt;Conneau, A. and Lample, G. 2019. Cross-lingual language model pretraining. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;. 32, (2019), 7059–7069.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/conneau2019cross/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;clark2020electra&quot;&gt;Clark, K., Luong, M.-T., Le, Q.V. and Manning, C.D. 2020. Electra: Pre-training text encoders as discriminators rather than generators. &lt;i&gt;arXiv preprint arXiv:2003.10555&lt;/i&gt;. (2020).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/clark2020electra/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hinton2006fast&quot;&gt;Hinton, G.E., Osindero, S. and Teh, Y.-W. 2006. A fast learning algorithm for deep belief nets. &lt;i&gt;Neural computation&lt;/i&gt;. 18, 7 (2006), 1527–1554.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hinton2006fast/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;peters2018deep&quot;&gt;Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K. and Zettlemoyer, L. 2018. Deep Contextualized Word Representations. &lt;i&gt;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)&lt;/i&gt; (New Orleans, Louisiana, Jun. 2018), 2227–2237.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/peters2018deep/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;radford2018improving&quot;&gt;Radford, A., Narasimhan, K., Salimans, T. and Sutskever, I. 2018. &lt;i&gt;Improving language understanding by generative pre-training&lt;/i&gt;. OpenAI.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/radford2018improving/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;metatext2022summary&quot;&gt;+64 Summarization Datasets - NLP Database.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/metatext2022summary/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;koupaee2018wikihow&quot;&gt;Koupaee, M. and Wang, W.Y. 2018. Wikihow: A large scale text summarization dataset. &lt;i&gt;arXiv preprint arXiv:1810.09305&lt;/i&gt;. (2018).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/koupaee2018wikihow/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nallapati2016abstractive&quot;&gt;Nallapati, R., Zhou, B., Santos, C. dos, Gu̇lçehre Çağlar and Xiang, B. 2016. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. &lt;i&gt;Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning&lt;/i&gt; (2016), 280–290.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nallapati2016abstractive/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;xiong2019tweetqa&quot;&gt;Xiong, W., Wu, J., Wang, H., Kulkarni, V., Yu, M., Chang, S., Guo, X. and Wang, W.Y. 2019. TWEETQA: A Social Media Focused Question Answering Dataset. &lt;i&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics&lt;/i&gt; (2019), 5020–5031.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/xiong2019tweetqa/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;VCRdataset&quot;&gt;VCR: Visual Commonsense Reasoning.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/VCRdataset/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zellers2019recognition&quot;&gt;Zellers, R., Bisk, Y., Farhadi, A. and Choi, Y. 2019. From recognition to cognition: Visual commonsense reasoning. &lt;i&gt;Proceedings of the IEEE/CVF conference on computer vision and pattern recognition&lt;/i&gt; (2019), 6720–6731.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/zellers2019recognition/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;vinyals2015order&quot;&gt;Vinyals, O., Bengio, S. and Kudlur, M. 2015. Order matters: Sequence to sequence for sets. &lt;i&gt;arXiv preprint arXiv:1511.06391&lt;/i&gt;. (2015).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/vinyals2015order/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rw2019timm&quot;&gt;Wightman, R. 2019. PyTorch Image Models. &lt;i&gt;GitHub repository&lt;/i&gt;. GitHub.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rw2019timm/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wu2019detectron2&quot;&gt;Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y. and Girshick, R. 2019. Detectron2.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/wu2019detectron2/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mmdetection&quot;&gt;Chen, K. et al. 2019. MMDetection: Open MMLab Detection Toolbox and Benchmark. &lt;i&gt;arXiv preprint arXiv:1906.07155&lt;/i&gt;. (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/mmdetection/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yakubovskiy:2019&quot;&gt;Yakubovskiy, P. 2020. Segmentation Models Pytorch. &lt;i&gt;GitHub repository&lt;/i&gt;. GitHub.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/Yakubovskiy_2019/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;krishna2017visual&quot;&gt;Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D.A. and others 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. &lt;i&gt;International journal of computer vision&lt;/i&gt;. 123, 1 (2017), 32–73.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/krishna2017visual/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;goyal2017accurate&quot;&gt;Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y. and He, K. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. &lt;i&gt;arXiv preprint arXiv:1706.02677&lt;/i&gt;. (2017).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/goyal2017accurate/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;carreira2017quo&quot;&gt;Carreira, J. and Zisserman, A. 2017. Quo vadis, action recognition? a new model and the kinetics dataset. &lt;i&gt;proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt; (2017), 6299–6308.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/carreira2017quo/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;/aws-account/&quot;&gt;Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn&lt;/a&gt;, hôm nay đọc tiếp cái &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence-to-sequence&lt;/code&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Seq2Seq&lt;/code&gt;) cũng thấy có cả 30k-40k cũng có. Thế nên là cái &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Seq2Seq&lt;/code&gt; này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Seq2Seq&lt;/code&gt; này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm). Chứ đọc làm sao mà hết được? Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp. Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. Có bài như bài gốc Transformer (Attention is all you need, &lt;a class=&quot;citation&quot; href=&quot;#vaswani2017attention&quot;&gt;[13]&lt;/a&gt;) mới ra đời từ 2017 mà đã hơn 35k trích dẫn! &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 01 Feb 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/seq2seq/</link>
        <guid isPermaLink="true">https://wanted2.github.io/seq2seq/</guid>
        
        <category>Machine learning</category>
        
        <category>Neural Machine Translation</category>
        
        <category>Long-Short Term Memory</category>
        
        <category>Recurrent Neural Networks</category>
        
        <category>Encoder-Decoder</category>
        
        <category>Seq2Seq</category>
        
        <category>Sequence-to-Sequence</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>Machine Learning for Network Intrusion Detection: From Local to Production</title>
        <description>&lt;h1 id=&quot;network-intrusion-detection-system&quot;&gt;Network Intrusion Detection System&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Simon-Enoch/publication/319637372/figure/fig1/AS:543679434510336@1506634686111/Configuration-of-the-enterprise-network.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Network intrusion detection system (NIDS)&lt;/strong&gt; is an independent platform that examines network traffic patterns to identify intrusions for an entire network. It needs to be placed at a choke point where all traffic traverses. A good location for this is in the DMZ.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An IDS is &lt;em&gt;reactive&lt;/em&gt; in nature: it only monitors and sends alerts to a group of specific people like administrators.
The above figure shows a common NIDS architecture, where a DMZ is placed in between &lt;em&gt;external firewall&lt;/em&gt; and &lt;em&gt;internal firewall (to an internal network)&lt;/em&gt;.
Here, in the DMZ, a NIDS can be set up to monitor the traffic of the whole corporation and identify the anomalies.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Vivek-Singh-70/publication/328572055/figure/fig1/AS:686824250945536@1540763070945/Major-Components-of-Snort-IDS-and-Bro-IDS.ppm&quot; style=&quot;float: right; margin: 10px; width: 50%;&quot; /&gt;
A NIDS can have the following architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;streaming engine&lt;/strong&gt; which ingests packet stream into the NIDS&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;package decoder&lt;/strong&gt; which turns packet content into visibility&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;detection engine&lt;/strong&gt; which identify intrusions&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;policy engine&lt;/strong&gt; which decide and suggest what to do with an intrusion&lt;/li&gt;
  &lt;li&gt;Finally, the intrusion details are collected and &lt;strong&gt;logged&lt;/strong&gt;. &lt;strong&gt;Alerts&lt;/strong&gt; will be sent to admins and optionally, scripted &lt;strong&gt;actions&lt;/strong&gt; can be performed in according to policies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two typical examples of NIDS are &lt;a href=&quot;https://www.snort.org/&quot;&gt;Snort IDS&lt;/a&gt; and &lt;a href=&quot;https://bricata.com/blog/zeek-ids-threat-detection/&quot;&gt;Bro IDS&lt;/a&gt;.
A nicer example that can be integrated into network monitoring can be &lt;a href=&quot;https://www.zabbix.com/features#smart_thresholds&quot;&gt;Zabbix’s Problem Detection Engine&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;machine-learning-at-scale-some-solutions-in-aws&quot;&gt;Machine Learning at scale: some solutions in AWS&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The theory about NIDS perhaps is a huge bundle of knowledge!
Corporations have set up their own NIDS (in most cases in DMZ) for years.
We will not talk about such solutions anymore.
The most interesting part is in the cloud platforms like AWS.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;While companies are moving their resources to the cloud, where is the NIDS?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the &lt;a href=&quot;https://d1.awsstatic.com/Marketplace/scenarios/security/SEC_01_TSB_Final.pdf&quot;&gt;&lt;strong&gt;Shared Responsibility Model (SRM)&lt;/strong&gt;&lt;/a&gt;.
The &lt;strong&gt;responsibility&lt;/strong&gt; of protecting cloud resources like computing instances (EC2) and networking is of AWS.
Users have the responsibility to tune the best configurations of firewall, instances, load balancers, and other resources in AWS.
So a platform IDS is already managed at AWS, and the at the users (application developers), the remaining task is to &lt;strong&gt;implement a best Network/HIDS (endpoint protection)&lt;/strong&gt;.
There are several choices for a HIDS in the AWS Marketplace like &lt;a href=&quot;https://www.trendmicro.com/en_us/business/products/hybrid-cloud/security-data-center-virtualization.html&quot;&gt;TrendMicro’s Deep Security&lt;/a&gt;.
For a custom NIDS, users can implement a &lt;a href=&quot;https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-inspection-architecture-with-aws-gateway-load-balancer-and-aws-transit-gateway/&quot;&gt;Transit DMZ Gateway&lt;/a&gt;. 
An examplar implementation can be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/aws-samples/aws-gateway-load-balancer-code-samples/raw/964874069c0a90d0b6758b2612c2de44a43f2a21/aws-cloudformation/centralized_architecture/images/gwlb_centralized_architecture.jpg&quot; alt=&quot;AWS Transit gateway&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenges-in-nids-operations&quot;&gt;Challenges in NIDS operations&lt;/h2&gt;

&lt;p&gt;A standard DMZ with NIDS can be implemented at the cloud or data center.
However, the &lt;strong&gt;hurdle&lt;/strong&gt; only comes when we operate our solution: this is the hard part!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When it comes to operations (運用保守), it often require an enormous amount of manual work!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We need a large number of low-paid workers who will sit in front of the monitoring screen and then manually mark each access as legal or not.
It is the real-world hurdle!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;However, the technical issues start when we want &lt;strong&gt;automation&lt;/strong&gt;: how to reduce false alarms and misses?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Good automated solutions will reduce manual work a lot.
But it is not straightforward!
Rule-based systems can have many misses or false alarms.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A weak rule misses many, but a strong rule alerts too much! (So both strong and weak ones are useless!)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;machine-learning-at-local&quot;&gt;Machine Learning at local&lt;/h1&gt;

&lt;p&gt;You have a dataset and perform some analytics at your local or edge PC.
You don’t use any server or cloud solution at all.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You should ask us why do you need to care about these works while cloud platforms already prepare so many ready-to-use solutions for you?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Agree! Machine learning engineers behind the platforms already do such works (model engineering).
Then when you do these works, that means you are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A student who is learning ML at an educational place (like university);&lt;/li&gt;
  &lt;li&gt;A researcher (or engineer) who perform a project for NGO, government, or a big company (who is building a platform solution); and&lt;/li&gt;
  &lt;li&gt;The worst luck: you’re only an ML enthusiast who is looking into ML when you have free time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not so many people do model engineering on their own laptop (or desktop), so in my experience, they fall into one of these three categories.
For the second category, people in that category is ML engineer/scientist who will make the model for thousands to million application developers over the world (who uses the platforms).
Such category is quite a few, and to be in, you need &lt;strong&gt;qualification&lt;/strong&gt;!
The most common cases are in the first.
The third category is possible but is rare and complex: while the first and second category have their own goals with ML (for studying and for works), the third category has no particular purpose.
They only do it in their free time and for fun (like doing a hobby)!
The first and second ones will have outcomes (successes and non-successes), but the third one is only for fun!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;No one can ban the hobby of a man, and we only do the hobby: we start when we want and stop when we don’t like it anymore!
That’s why I call it (the third category) the worst luck!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Actually, people in the third category already have another job (but that job does not relate to ML or even AI).
They do ML as hobbies but don’t complete anything (because ML is not their business, even more, ML does not give them bread and butter)!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is said, but in practice, if a thing doesn’t give any benefit, it won’t be done properly!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nevertheless, whatever your category is, when you do these things in your local computing environment, ML matters with you in some ways!
We will see what a &lt;strong&gt;local IDS model would look like&lt;/strong&gt;.
And, in a synthetic way!&lt;/p&gt;

&lt;h2 id=&quot;a-kaggle-synthetic-dataset&quot;&gt;A Kaggle synthetic dataset&lt;/h2&gt;

&lt;p&gt;We start with a synthetic dataset from &lt;a href=&quot;https://www.kaggle.com/sampadab17/network-intrusion-detection&quot;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The dataset to be audited was provided that consists of a wide variety of intrusions simulated in a military network environment. It created an environment to acquire raw TCP/IP dump data for a network by simulating a typical US Air Force LAN. The LAN was focused like a real environment and blasted with multiple attacks. A connection is a sequence of TCP packets starting and ending at some time duration between which data flows to and from a source IP address to a target IP address under some well-defined protocol. Also, each connection is labeled as either normal or as an attack with exactly one specific attack type. Each connection record consists of about 100 bytes.&lt;/p&gt;

  &lt;p&gt;For each TCP/IP connection, 41 quantitative and qualitative features are obtained from normal and attack data (3 qualitative and 38 quantitative features). The class variable has two categories:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Normal&lt;/li&gt;
    &lt;li&gt;Anomalous&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;explorative-analysis&quot;&gt;Explorative analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/ids-kaggle-feature-selection.png&quot; alt=&quot;feature selection&quot; /&gt;
&lt;em&gt;We notice that not all features are useful. Some features like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_Host_login&lt;/code&gt; have only a constant value.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our dataset is big enough (25K entries) and contains both categorical and numerical features:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;RangeIndex: 25192 entries, 0 to 25191
Data columns (total 42 columns):
 #   Column                       Non-Null Count  Dtype  
---  ------                       --------------  -----  
 0   duration                     25192 non-null  int64  
 1   protocol_type                25192 non-null  object 
 2   service                      25192 non-null  object 
 3   flag                         25192 non-null  object 
 4   src_bytes                    25192 non-null  int64  
 5   dst_bytes                    25192 non-null  int64  
 6   land                         25192 non-null  int64  
 7   wrong_fragment               25192 non-null  int64  
 8   urgent                       25192 non-null  int64  
 9   hot                          25192 non-null  int64  
 10  num_failed_logins            25192 non-null  int64  
 11  logged_in                    25192 non-null  int64  
 12  num_compromised              25192 non-null  int64  
 13  root_shell                   25192 non-null  int64  
 14  su_attempted                 25192 non-null  int64  
 15  num_root                     25192 non-null  int64  
 16  num_file_creations           25192 non-null  int64  
 17  num_shells                   25192 non-null  int64  
 18  num_access_files             25192 non-null  int64  
 19  num_outbound_cmds            25192 non-null  int64  
 20  is_Host_login                25192 non-null  int64  
 21  is_guest_login               25192 non-null  int64  
 22  count                        25192 non-null  int64  
 23  srv_count                    25192 non-null  int64  
 24  serror_rate                  25192 non-null  float64
 25  srv_serror_rate              25192 non-null  float64
 26  rerror_rate                  25192 non-null  float64
 27  srv_rerror_rate              25192 non-null  float64
 28  same_srv_rate                25192 non-null  float64
 29  diff_srv_rate                25192 non-null  float64
 30  srv_diff_Host_rate           25192 non-null  float64
 31  dst_Host_count               25192 non-null  int64  
 32  dst_Host_srv_count           25192 non-null  int64  
 33  dst_Host_same_srv_rate       25192 non-null  float64
 34  dst_Host_diff_srv_rate       25192 non-null  float64
 35  dst_Host_same_src_port_rate  25192 non-null  float64
 36  dst_Host_srv_diff_Host_rate  25192 non-null  float64
 37  dst_Host_serror_rate         25192 non-null  float64
 38  dst_Host_srv_serror_rate     25192 non-null  float64
 39  dst_Host_rerror_rate         25192 non-null  float64
 40  dst_Host_srv_rerror_rate     25192 non-null  float64
 41  class                        25192 non-null  object 
dtypes: float64(15), int64(23), object(4)
memory usage: 8.1+ MB
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/ids.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;
Anyway, this is a synthetic dataset with some characteristics based on simulation.
However, it is close to real-world examples enough.
Next, we will try some machine learning models for predicting anomalies.&lt;/p&gt;

&lt;p&gt;With a visualization technique, like T-SNE, we have a diagram of data distribution.
A green point is a normal data point, and a red one is an anomaly.
We can see that it would be hard to draw a linear boundary between normal points and anomalies.&lt;/p&gt;

&lt;h3 id=&quot;play-with-some-machine-learners&quot;&gt;Play with some machine learners&lt;/h3&gt;

&lt;p&gt;In this section, we will try two different ML models with two different training/inference strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Generative model with a reconstruction strategy&lt;/strong&gt;: the Auto-Encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# creating the autoencoder model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;sigmoid&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Discriminative model with a classification strategy&lt;/strong&gt;: the quite classical Random Forest!&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;gini&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;training-and-validation&quot;&gt;Training and validation&lt;/h4&gt;

&lt;p&gt;We need to transform the data a little bit.
It is usual to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MinMaxScaler&lt;/code&gt; to perform data transformation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Auto-Encoder is trained in a reconstruction manner, i. e., it learns to reconstruct the input:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But the Random Forest learns to classify data as usual:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;results-of-auto-encoder&quot;&gt;Results of Auto-Encoder&lt;/h4&gt;

&lt;p&gt;Since the prediction of AE relies on how good it can reconstruct the input:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the reconstruction error is over a threshold, then the reconstruction fails, and the input is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anomaly&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Otherwise, the input is normal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we must define a &lt;strong&gt;threshold&lt;/strong&gt; to separate anomalies and normal inputs.
To find such threshold, we can compute from the training dataset:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# reconstructing the train set
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# finding the mean reconstruction error
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since the mean of construction error is 0.0018520295581492838, we can choose the threshold &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thres = 0.0018&lt;/code&gt;.
Validation in validation set:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# classifying the samples based on threshold
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;anomaly&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thres&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;normal&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Acc. = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[%], F1 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f1_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;anomaly&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[[2901   69]
 [ 563 2765]]
Acc. = 89.96506827564306[%], F1 = 90.17718371153248[%]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;~90%&lt;/strong&gt; of F1 score. Hmm, not so bad!&lt;/p&gt;
&lt;h4 id=&quot;results-of-random-forest&quot;&gt;Results of Random Forest&lt;/h4&gt;

&lt;p&gt;Let’s see the results with Random Forest:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[[3321    7]
 [  14 2956]]
Acc. = 99.66656081295649[%], F1 = 99.64604753076016[%]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;99.6%&lt;/strong&gt;, it is quite good!&lt;/p&gt;

&lt;h2 id=&quot;some-methods-for-machine-learning-based-ids&quot;&gt;Some methods for Machine Learning based IDS&lt;/h2&gt;
&lt;p&gt;There are many ways to identify anomalous access (i. e., intrusion) in the network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rule-based methods&lt;/strong&gt; tend to find a “good” heuristic that can be generalized to a global policy for all feature space.
The same policy can be applied to every input.
For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;every connection which lasted for more than 7 minutes are anomalies&lt;/code&gt; is a policy to identify intrusions.
The problem with this approach is that many false negatives (misses) can be raised.
Because the rule is clear, counterfactuals try to make themselves legal (try to connect faster) and overcome the 7-minute rule.
So a fixed rule is not enough.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The attacks become more and more advanced, but the rules cannot be changed so fast, making an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Achilles&apos; heel&lt;/code&gt; in the defense system.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Machine Learning-based methods&lt;/strong&gt; try to define a soft boundary that can be learned and improved over time.
The main advantage of the Machine Learning approach is that since the boundary is soft and there is no clear rule, the intruders cannot know the rule exactly (how many minutes should they make for a successful intrusion?).
Another important aspect is that since the soft rule is not fixed, it can &lt;strong&gt;evolve&lt;/strong&gt; with the intrusions: the more advanced the intrusion is, the more advanced the defender is.
When this sounds &lt;em&gt;ambiguous&lt;/em&gt;, but for the security systems, it becomes exactly a common strategy to overcome incidents.
The Machine Learning methods can be generative or discriminative, but they must be somewhat non-linear and ambiguous enough to hide details of the system to hackers.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;We have reviewed several views of NIDS: at a corporation network view, cloud solutions, and machine learning models.&lt;/p&gt;

&lt;p&gt;On the synthetic examples, we observed that a quite classical model like &lt;strong&gt;Random Forest&lt;/strong&gt; can outperform neural nets.
It is not a new thing: we already &lt;em&gt;empirically&lt;/em&gt; knew that Random Forest is good at this problem, especially when it is in synthetic environments.
Somebody can argue that there is a chance for overfitting with Random Forest: hmm, I don’t think so.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;And even if it overfits, this is quite good, because &lt;strong&gt;that’s what we want&lt;/strong&gt;: &lt;strong&gt;We want our model to overfit to this dataset&lt;/strong&gt;.&lt;/p&gt;

  &lt;p&gt;Anonymous&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Source code for this article can be found at:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/AIFI-INC/ml-ids&quot;&gt;AIFI-INC’s ML-based IDS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 29 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/ml-ids/</link>
        <guid isPermaLink="true">https://wanted2.github.io/ml-ids/</guid>
        
        <category>Machine learning</category>
        
        <category>network intrusion detection</category>
        
        <category>threat intelligence</category>
        
        <category>edge computing</category>
        
        <category>iot</category>
        
        
        <category>Site Reliable Engineering</category>
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>責任者</title>
        <description>&lt;p&gt;ベトナムに帰ってからベトナムの職場文化になってからはもはや2年間になっております．
ベトナム職場でいうと，恥ずかしいけど，楽しい経験もあるし，悲しい経験もありました．
仕事の責任者として働いた経験もあり，悲しい時で，部下に怒られて，そろそろ殴られる経験もありました．
なぜなら，背景から考えると，文化の違いかなと思います．
ベトナム職場では上下関係は社会的に存在するけど，きちんと働かないとね，上下関係なく殴られるそうです．
「殴られる」は厳しい言葉ですが，主にいうと，ベトナム職場で下記の三大原則はあります．&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則1：&lt;strong&gt;Có làm thì mới có ăn&lt;/strong&gt;. You must work to be fed. 職あり食ある．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則2：&lt;strong&gt;Có lỗi thì sửa là được&lt;/strong&gt;. Don’t be panic, just fix bugs, then it’s OK. 問題にダラダラしないで，修正すれば問題なし．（ただ何もしないなら，↑の原則１をみてください）．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則3：&lt;strong&gt;Làm thì làm cho nghiêm chỉnh.&lt;/strong&gt; You should do the work thoroughly. 徹底的にやりましょう．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ベトナム職場の約2年間の経験をまとめて，一度整理したいと思っております．
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;第一原則職と食&quot;&gt;第一原則：「職」と「食」&lt;/h1&gt;

&lt;p&gt;我々は職場で「仕事」を大事にしております．
仕事が順調であれば，食感もよくなるという発想です．
仕事をしなければ，食えるもんがなく，大変ですね．
ですが，仕事（職）があって，就いたけど何もしないことで組織には不満が引き起こされたことが多いようです．&lt;/p&gt;

&lt;p&gt;ですので，組織全体で皆は統一して，職務をしてから食べるということを理解したらよかったです．
もちろん，海外から来る要員だと，現地の文化がわからず，仕事とは何か現地の人の考え方は最初でわからないこともあります．
しかし，時間が経過すると，その意義を理解すれば，原則を守る姿勢を整い，結局「労働」は第一だという考え方に帰着しました．
組織全体といえば，作業人だけではなく，管理職でもきちんと働き姿勢を見せればよいと思います．&lt;/p&gt;

&lt;p&gt;ここで，一番気になることは，この原則にはもう一つの意味の層があります．
仕事をやるといっても，何をやっても認められるのではないです．
「すごいことをやる」のも間違いです．
ある「職」に就く時，自分の役割と権限は決まっております．
ある場合，期待もあります．
しかし，開発の現場では，ほとんどの場合，役割と権限と責任範囲・仕事内容はすでに決まっております．
（※申し訳ないが，それぐらい曖昧に契約してしまえば，ちょっとそれがまずい職になるかもしれません）．
ですので，「自分の役割と権限・責任範囲の中に行動し，仕事を仕上げる」ことで職を行っているねといいます．
まあ，「職あり食ある」という句です．&lt;/p&gt;

&lt;p&gt;※曖昧に定めてしまった職に入った場合，もちろん仕事をしたから食べることがあるけど，ちょっと食べ物は美味しくないかもしれないです．
おいしい食をどうやって作るか山ほど研究がありますので，それらをGoogleして参考にすればと思います．&lt;/p&gt;

&lt;h1 id=&quot;第二原則チームは障害にだらだらしないで乗り越えることは大事&quot;&gt;第二原則：チームは障害にだらだらしないで，乗り越えることは大事！&lt;/h1&gt;
&lt;p&gt;チーム運営の中に，一緒に働くので，楽しい時も，悲しい時も一緒に乗り越えています．
楽しい時はいいんだけど，悲しい時はどんな時かな？
自分の経験では，パニック状態になるときです．
多くの場合，急に障害が起きるときとかですね．&lt;/p&gt;

&lt;p&gt;確かに，パニック状態のハンドルをうまく取り込めているチームはすごいねとおもいます．
パニック状態になった場合，よいチームは取り込むけど，悪いチームは責任追及ゲームを遊びます．
なぜ責任追及ゲームは悪いのかというと，緊急対応なのに，問題対応をせずに，ゲームをしているからです．
よいチームは問題を見極めて，対処法を第一優先し対応を取り込むのです．&lt;/p&gt;

&lt;p&gt;※ちなみに，チームで開発する場合には，運営中で時間がかかってしまう状態があります．
それが，&lt;strong&gt;パニック状態&lt;/strong&gt;と，&lt;strong&gt;会議状態&lt;/strong&gt;です．
なぜなら，この2つだけは，一人の時間を取るだけではなく，チーム皆の時間を取っているからです．
だらだらして，仕事が進まない罠に落ちやすいのです．
チームのパフォーマンスを上げたい場合に，責任者として，回避と対応策を計画しなければなりません．&lt;/p&gt;

&lt;p&gt;ベトナム職場では，なぜこの原則が取り込まれているかというと，チームがどこかにだらだらすると，めっちゃ時間がかかっているから，まずは乗り越えることを第一優先したいからです．
人がミスをすることは人間の根性ですので，それよりももっと悪いことは，&lt;strong&gt;「なにもしないこと」&lt;/strong&gt;です．
直さないことや修正しないことなどはまた，原則1で処分されると思います．
いつもチームを前向きに進行させることができればと思います．&lt;/p&gt;

&lt;h1 id=&quot;第三原則自分の仕事へのこだわりも重視完成したらもう一度見直そう&quot;&gt;第三原則：自分の仕事へのこだわりも重視！完成したら，もう一度見直そう！&lt;/h1&gt;

&lt;p&gt;2年間ベトナム職場で感じたもう一つの原則です．
第一原則と第二原則を相互に働かせているため，「作成」と「修正」を交互に行っています．
しかし，これらを交互に働かせると，永遠に修正のループに入る可能性はまだ残っています．
ですので，途中でもっと修正してもあまり報われないと感じるときとか，早く止めた方がよいと感じるときとか，もう一つのコントローラーが必要でしょうか．
それは第三原則です．
必ず，（再）作成と（再）修正が完成したら，もう一度&lt;strong&gt;客観的&lt;/strong&gt;にレビューしましょう．
狭い視野で詳細をレビューするだけではなく，広い視野で，この作成と修正は長期的によいか悪いか一回考えるべきというステップがあれば，なおよいです．
ここで，責任者として，技術部分だけではなく，スコープ管理とか要望管理とかスケジュール管理とかうまく併せてやる必要があります．&lt;/p&gt;

&lt;h1 id=&quot;結論&quot;&gt;結論&lt;/h1&gt;

&lt;p&gt;これが，おそらく私が2年間で観測したベトナム職場で支配されている三大原則かと思います．
これらは，なぜか職場をコントロールして，各PJを進行させることが多いそうです．&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/pm-sekininsha/</link>
        <guid isPermaLink="true">https://wanted2.github.io/pm-sekininsha/</guid>
        
        <category>責任者</category>
        
        <category>担当者</category>
        
        <category>権限管理</category>
        
        <category>計画管理</category>
        
        <category>進捗管理</category>
        
        <category>スコープ管理</category>
        
        <category>リスク管理</category>
        
        <category>コミュニケーション管理</category>
        
        <category>リソース管理</category>
        
        <category>プロジェクトマネジメント</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>4 điều nên làm để quản lý tài khoản AWS</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Những tri thức cơ bản như làm thế nào quản lý mật khẩu với đặt mật khẩu như thế nào thì có lẽ các em tự chủ động thiết lập nhé.
Trong bài viết này chúng ta sẽ tập trung vào 4 chức năng mang tính nâng cao (advanced) của bảo mật tài khoản AWS.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Thiết lập và quản lý tài khoản AWS&lt;/strong&gt; không chỉ đơn giản là vấn đề bảo mật dữ liệu mà còn sâu xa hơn là quản lý toàn bộ account trong tổ chức một cách &lt;strong&gt;đồng bộ, tập trung và hiệu quả&lt;/strong&gt; nhất.
Việc này rất quan trọng bởi account chính là cửa ngõ vào của mọi luồng dữ liệu, mọi truy cập, do đó nếu việc quản lý account bị lơi là sẽ dẫn đến việc những truy cập bất hợp pháp có thể xâm nhập vào “hệ miễn dịch” của tổ chức và gây hại.
4 chức năng chúng ta nói đến hôm nay bao gồm:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Bảo mật nhiều lớp cho tài khoản root;&lt;/li&gt;
  &lt;li&gt;Quản lý và thiết lập phân quyền cùng AWS Organizations;&lt;/li&gt;
  &lt;li&gt;AWS Single-Sign On (SSO); và&lt;/li&gt;
  &lt;li&gt;Giám sát hành vi tài khoản trên AWS.
&lt;!--more--&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;giới-thiệu&quot;&gt;Giới thiệu&lt;/h1&gt;
&lt;p&gt;Như trong tiếng Nhật thì chữ tài khoản (tiếng Anh, account) được viết là 口座, bắt đầu bằng bộ “khẩu (miệng)”.
Các bạn đều biết cửa sổ đi vào hệ tiêu hóa mà mọi thứ đồ ăn thức uống phải đi qua đều bắt đầu từ miệng.
Cái miệng chính là nơi mọi thức ăn xâm nhập vào cơ thể con người.
Thức ăn có thể hàm chứa dinh dưỡng, nhưng cũng có thể hàm chứa chất độc, bệnh dịch.
Có thể ngon mà cũng có thể dở.
Việc bảo vệ “bộ nhá” là cực kỳ quan trọng với con người nói chung!&lt;/p&gt;

&lt;p&gt;Trong bài viết này song song với việc thao tác trên AWS chúng ta cũng sẽ nói thêm về các nguyên lý đằng sau các thiết lập tài khoản, các dịch vụ bảo mật đa lớp, single sign-on, .v.v…
Với nguyên tắc là mọi thứ qua miệng đều phải rõ nguồn gốc (cũng không cần chi tiết quá, nhưng ở mức có thể track được).&lt;/p&gt;

&lt;h1 id=&quot;bảo-mật-nhiều-lớp&quot;&gt;Bảo mật nhiều lớp&lt;/h1&gt;

&lt;h2 id=&quot;điểm-qua-những-tiến-bộ-của-mảng-bảo-mật-nhiều-lớp&quot;&gt;Điểm qua những tiến bộ của mảng bảo mật nhiều lớp&lt;/h2&gt;

&lt;p&gt;Những tri thức nền tảng cho bảo mật nhiều lớp xuất hiện lần đầu từ bài báo của &lt;strong&gt;Mitchell Trauring &lt;a class=&quot;citation&quot; href=&quot;#trauring1963automatic&quot;&gt;[1]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#jain201650&quot;&gt;[2]&lt;/a&gt;&lt;/strong&gt; năm 1963.
Trong bài viết về cách thức nhận dạng vân tay tự động bằng &lt;strong&gt;Ridge&lt;/strong&gt; patterns, ông viết:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is the purpose of this article to present, together with some evidence of its feasibility, a method by which decentralized automatic identity verification, such as might be desired for credit, banking or security purposes, can be accomplished through automatic comparison of the minutiae in finger-ridge patterns.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Mitchell Trauring, Nature, March 1963&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/multi-factor.png&quot; style=&quot;float: right; margin: 10px; width: 50%&quot; /&gt;
Như hình vẽ bên các bạn có thể thấy, bình thường bảo mật 1 lớp thì chỉ cần có mật khẩu (&lt;em&gt;something you know&lt;/em&gt;).
Tuy nhiên, để tăng độ bảo mật, ví dụ như khi mật khẩu bị đánh cắp thì nên có nhiều phương pháp bảo mật đi kèm như &lt;strong&gt;token &lt;a class=&quot;citation&quot; href=&quot;#holdsworth2008token&quot;&gt;[3]&lt;/a&gt;, vân tay &lt;a class=&quot;citation&quot; href=&quot;#jain201650&quot;&gt;[2]&lt;/a&gt;, khuôn mặt &lt;a class=&quot;citation&quot; href=&quot;#grother2018ongoing&quot;&gt;[4]&lt;/a&gt;, PIN/key &lt;a class=&quot;citation&quot; href=&quot;#boyd2003protocols&quot;&gt;[5]&lt;/a&gt;, mống mắt, …&lt;/strong&gt;.
Cái quan trọng là phải kết hợp thêm những cái gì user có (&lt;em&gt;something you have&lt;/em&gt;) và những gì của user (&lt;em&gt;something you are&lt;/em&gt;).
&lt;img src=&quot;/assets/images/fingerprint-matching.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Trích dẫn: &lt;a class=&quot;citation&quot; href=&quot;#trauring1963automatic&quot;&gt;[1]&lt;/a&gt; mô hình vân tay và cách matching local points.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fusion.jpg&quot; style=&quot;float: left; margin: 10px; width: 50%&quot; /&gt;
Tuy nhiên, một vấn đề lớn của các phương pháp bảo mật đi kèm là độ chính xác.
Ví dụ như với vân tay &lt;a class=&quot;citation&quot; href=&quot;#pankanti2002individuality&quot;&gt;[6]&lt;/a&gt; thì xác suất &lt;em&gt;false matching probability&lt;/em&gt; vẫn có thể đạt $6.10\times 10^{-8}$ với mẫu vân tay có 36 điểm minutiae.
Do đó việc sử dụng đơn thuần 1 đặc trưng anatomical hoặc behavior có thể tạo ra nhiều &lt;em&gt;false non-matching&lt;/em&gt; cũng như &lt;em&gt;false matching&lt;/em&gt;.
Cũng vì đó mà cần phải dùng nhiều đặc trưng, và khi dùng nhiều đặc trưng thì có đặc trưng quan trọng, có đặc trưng không, phải có &lt;strong&gt;weight&lt;/strong&gt; cho từng đặc trưng.
Và đó chính là ý tưởng &lt;strong&gt;feature fusion&lt;/strong&gt; hay là &lt;strong&gt;multimodal&lt;/strong&gt; cho mảng biometrics.
&lt;em&gt;Nói chung, cũng toàn mấy cái ý tưởng của các fellows từ thời 1990s-2000s&lt;/em&gt;, thời các sếp ấy thì cũng đã 20-30 năm về trước, nên mấy cái ý tưởng này tôi nghĩ mới cũng không mới và trong mảng biometrics cũng đã implement gần hết, thậm chí vào sản phẩm thị trường rồi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/biometrics.png&quot; style=&quot;float: right; margin: 10px; width: 50%&quot; /&gt;
Cái mảng &lt;strong&gt;bảo mật nhiều lớp &lt;a class=&quot;citation&quot; href=&quot;#ometov2018multi&quot;&gt;[7]&lt;/a&gt;&lt;/strong&gt; và &lt;strong&gt;biometrics &lt;a class=&quot;citation&quot; href=&quot;#jain201650&quot;&gt;[2]&lt;/a&gt;&lt;/strong&gt; nhìn chung là cũng liên đới rộng phết, từ bảo mật cloud đến tận thiết bị IoT &lt;a class=&quot;citation&quot; href=&quot;#ngu2016iot&quot;&gt;[8]&lt;/a&gt; cũng liên quan.
Nhưng nhìn chung cái mô hình thực thi thì khá giống nhau:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Về mặt &lt;strong&gt;hệ thống&lt;/strong&gt; thì anh em nào làm nhiều về cloud, web app thì sẽ hiểu ngay là trong framework sẽ có một bộ phận middleware có những function kiểu như &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preLogin(), postAuthentication(), ...&lt;/code&gt; là sẽ là nơi implement thêm các lớp bảo mật. Ở trong middleware &lt;a class=&quot;citation&quot; href=&quot;#ngu2016iot&quot;&gt;[8]&lt;/a&gt; thì chúng ta sẽ call thêm các lớp bảo mật khác để đưa ra kết quả.
    &lt;ul&gt;
      &lt;li&gt;Ngoài ra, việc fusion của các đặc trưng có thể thực hiện tại &lt;strong&gt;biometrics sensor&lt;/strong&gt;, tức là &lt;strong&gt;early fusion&lt;/strong&gt;. Hoặc tại module decision, tức là &lt;strong&gt;late fusion&lt;/strong&gt; và chủ yếu là fuse kết quả nhận dạng của từng modal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Về mặt &lt;strong&gt;module&lt;/strong&gt;, thì module bảo mật của từng lớp thì sẽ như hình vẽ bên phải. Sẽ có enrollment (đăng ký) và verification (đăng nhập/nhận dạng).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nói chung mảng bảo mật nhiều lớp cũng có khá nhiều dư địa, tuy nhiên nếu so với những mảng như &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GAN&lt;/code&gt; hay &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deep learning&lt;/code&gt; thì số lượng papers mỗi năm trong academics có vẻ không nhiều.
Mà số lượng papers có nhiều trích dẫn thì cũng ít hơn.
Vì đơn giản như paper về GAN của Goodfellow năm 2014, đến nay là 7 năm mà đã có gần 40k trích dẫn, mà cái mảng đó hầu như top-2% về nhiều trích dẫn thì cũng toàn những paper vài ngàn citations trở nên.
Nói chung lựa chọn trích dẫn bài nào cũng tuỳ mảng, vì mảng biometrics, tôi thấy những papers của các fellow cũng chỉ vài trăm citations trong 10 năm.
Nhưng cái mảng deep learning với GAN thấy năm nào cũng hàng ngàn papers mà những papers top thì toàn hàng ngàn citations.
Nên đôi khi hỏi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;có cần đọc hết các papers trong mảng không?&lt;/code&gt;, thì tôi nghĩ là nếu những mảng ít ít như biometrics thì đọc cũng được, nhưng mấy cái mảng liên quan tới deep với GAN, thì &lt;strong&gt;tôi nghĩ cũng chả cần đọc đâu&lt;/strong&gt;.
Chỉ cần filter ra top 2% mà chủ yếu những bài có tốc độ tăng trưởng mạnh (mỗi năm trên 200 citations) thì đọc thôi, chứ nó nhiều thế này mà toàn na ná nhau, thì papers mà ít citations chắc chắn là không phải trend, đọc không hết được mà đọc cũng chả để làm gì.
Tất nhiên, cũng phải ưu tiên những papers mới xuất bản năm nay thì mình nên đọc.
Tức là ví dụ năm nay là 2022 thì những bài từ 2021 thì mình ưu tiên đọc (nhưng vẫn chỉ nên đọc top tầm 30%), còn từ 2020 về trước thì chỉ chọn bài có độ tăng trưởng trung bình trên 200 citations/năm, và trong đó cũng chỉ nên chọn top.&lt;/p&gt;

&lt;p&gt;Cuối cùng, là về mảng biometrics này thì có một ứng dụng thú vị là &lt;em&gt;face recognition&lt;/em&gt; mà bên computer vision hay deep learning cũng nổi lên (gọi là &lt;em&gt;deep face recognition&lt;/em&gt;).
Mảng này thì quả là cũng không nhiều papers lắm, cũng giống biometrics, nhưng lại có một số papers có trích dẫn hàng ngàn mặc dù xuất bản sau 2015 như FaceNet hay RetinaFace, ArcFace, CosFace.
Tuy nhiên, nếu nói về thực tiễn sử dụng thì tôi nghĩ tốt nhất là nên tham khảo các con số trong benchmark lớn của chính phủ Mý &lt;a class=&quot;citation&quot; href=&quot;#grother2018ongoing&quot;&gt;[4]&lt;/a&gt;.
Lý do là bởi vì đó là các ứng dụng thực tiễn, cũng như bộ dữ liệu lớn tới vài chục triệu người, và quan trọng là mọi team trên thế giới đều có thể tham gia.
Dưới đây là bảng kết quả hạng mục Verification, tức là cho sẵn một pair hai khuôn mặt, đưa ra kết quả xem đó có cùng 1 identity không?
Kết quả là đường cong thể hiện tỷ lệ &lt;em&gt;false non-matching rate (FNMR) vs false matching rate (FMR)&lt;/em&gt;.
Chúng ta hãy để ý đến tỷ lệ &lt;strong&gt;FNMR@FMR=$10^{-5}$&lt;/strong&gt; thì tên tuổi đứng nhất là InsightFace (là nhóm tác giả của RetinaFace, ArcFace, …).
Họ chính là tác giả của framework &lt;a href=&quot;https://github.com/deepinsight/insightface.git&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;insightface&lt;/code&gt;&lt;/a&gt; để nhận dạng khuôn mặt 2D/3D.
Thành tích đứng nhất là $FNMR=0.0006$ khi $FMR=10^{-5}$, tức là 10 vạn người thì chỉ sai 1 người và miss 60 người, tổng là 61 người bị sai hoặc miss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nist-frvt-2022.png&quot; alt=&quot;NIST FRVT&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Trên đây chỉ là kết quả của dataset VISA, ngoài ra NIST còn có những task khác cùng với những dataset khác.
Từ VN thì cũng có team của một số đội như VNPT hay VIN AI/VIN Bigdata cũng tham gia và có được thứ hạng (tức là đã nộp bài).&lt;/p&gt;

&lt;h2 id=&quot;bật-tính-năng-mfa-của-aws&quot;&gt;Bật tính năng MFA của AWS&lt;/h2&gt;

&lt;h3 id=&quot;với-tài-khoản-root&quot;&gt;Với tài khoản root&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aws-mfa.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Với người dùng có &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html&quot;&gt;tài khoản root&lt;/a&gt; thì chắc thi thoảng bạn cũng sẽ login vào để làm một số việc như:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Quản lý IAM users&lt;/li&gt;
  &lt;li&gt;Quản lý bill service&lt;/li&gt;
  &lt;li&gt;Quản lý dịch vụ sử dụng&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vậy tài khoản root là chứa tất cả của bạn (hoặc tổ chức), vậy nên không thể không nâng độ bảo mật lên được.
Và nên bật MFA lên.
Mặc dù chúng ta đã tìm hiểu về MFA khá nhiều bên trên, thì &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html&quot;&gt;AWS hỗ trợ một số biện pháp MFA&lt;/a&gt; như sau thôi:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Virtual MFA devices&lt;/strong&gt; hay MFA thông qua phần mềm. Bạn có thể cài đặt Microsoft Authenticator vào điện thoại cho mục đích này.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;U2F security key&lt;/strong&gt; tức là 1 cái USB key để bạn cắm vào máy và nó sẽ dùng thông tin của USB này để nhận dạng bạn khi bạn đăng nhập.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hardware MFA device&lt;/strong&gt;. Một thiết bị được đăng ký sẽ tạo 1 code 6 chữ số để cho thuật toán time-synchronized one-time password (OTP).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nhìn chung, có thể vân tay hay khuôn mặt thì sẽ cần dùng một kênh khác, còn tài khoản AWS hiện chỉ support những kênh MFA thêm code.
Ngoài ra MFA thông qua SMS cũng không được hỗ trợ.
Sau khi khởi động phương thức MFA phù hợp, từ lần đăng nhập tiếp theo, người dùng sẽ phải nhập thêm code được sinh ra từ phần mềm hoặc thiết bị đã đăng ký.&lt;/p&gt;

&lt;h3 id=&quot;với-tài-khoản-iam&quot;&gt;Với tài khoản IAM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/iam-mfa.png&quot; style=&quot;float: left; margin: 10px; width: 50%&quot; /&gt;
Bạn có thể cấp &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_sign-in.html&quot;&gt;tài khoản IAM&lt;/a&gt; cho người dùng trong tổ chức của bạn (định nghĩa Roles/Permissions tương ứng với nhiệm vụ của nhân viên).
Nếu bạn muốn bảo mật hơn, bạn có thể bắt buộc nhân viên dùng IAM phải dùng MFA.
Bạn truy cập vào trang cấu hình của IAM user đó và yêu cầu assign MFA devices.
Xem thêm &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/console_sign-in-mfa.html&quot;&gt;hướng dẫn kích hoạt MFA cho tài khoản IAM&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Nhìn chung, MFA là một chức năng đơn giản để cấu hình và nên được áp dụng.
Trong những &lt;a href=&quot;https://aws.amazon.com/blogs/security/new-whitepaper-aws-cloud-security-best-practices/&quot;&gt;best practices của AWS&lt;/a&gt; cũng có hướng dẫn nên cấu hình MFA cho việc xóa object khỏi S3 chẳng hạn.
Và tại sao AWS chỉ áp dụng MFA dạng code 6 chữ số, chứ không hỗ trợ biometrics như vân tay hay khuôn mặt/mống mắt, thậm chí SMS cũng không được?
Có lẽ là do vân tay hay mống mặt và đặc biệt khuôn mặt vẫn có tỷ lệ sai nhất định (chúng phù hợp cho nhiệm vụ điều tra của cảnh sát hơn), còn SMS thì có khá nhiều lỗ hổng bảo mật liên quan rồi nên lựa chọn code theo device có lẽ hợp lý hơn.&lt;/p&gt;

&lt;h1 id=&quot;quản-lý-và-thiết-lập-quyền-với-aws-organizations&quot;&gt;Quản lý và thiết lập quyền với AWS Organizations&lt;/h1&gt;

&lt;h2 id=&quot;giới-thiệu-aws-organizations&quot;&gt;Giới thiệu AWS Organizations&lt;/h2&gt;

&lt;p&gt;Trong nghiệp vụ quản lý tổ chức, có một mảng khá quan trọng là quản lý tài sản (asset management).
Thì không đơn giản là lên danh sách tài sản và quản lý chúng, cái đó thì chỉ 1 file Excel là làm được.
Cái quan trọng hơn với trọng trách này là &lt;strong&gt;quản lý và giám sát&lt;/strong&gt; mọi tài sản trong tổ chức bao gồm cả tài sản, tài nguyên máy móc.
Với AWS thì là quản lý tất cả các tài khoản nhân viên, máy ảo, dịch vụ được dùng một cách &lt;strong&gt;tập trung và hiệu quả&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tại sao phải tập trung lại 1 chỗ?&lt;/code&gt; Là bởi vì nếu phân tán sẽ khó kiểm soát và nhà quản lý sẽ rơi vào trạng thái bị động với mọi thay đổi trong tổ chức.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tại sao phải hiệu quả?&lt;/code&gt; Vì khi lên thành tổ chức, tài nguyên sẽ tăng lên rất nhiều, mà quản lý không hiệu quả thì cũng như người mù ngồi trên đống vàng. Người quản lý sẽ cần một công cụ để có thể ra policy và áp dụng policy đó một cách &lt;em&gt;đồng bộ&lt;/em&gt; trong toàn tổ chức.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lời giải hợp lý ở đây chính là sử dụng &lt;strong&gt;&lt;a href=&quot;https://aws.amazon.com/organizations/&quot;&gt;AWS Organizations&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;#&lt;/th&gt;
      &lt;th&gt;Tên&lt;/th&gt;
      &lt;th&gt;Chức năng&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Manage your AWS accounts&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Tài khoản AWS trong tổ chức là nơi cần tổ chức quản lý quyền, bảo mật, tài nguyên và costs. Để dễ dàng tổ chức quản lý việc nâng cấp lên môi trường đa tài khoản (multi-account enviroment) là cần thiết. AWS hỗ trợ việc quản lý tập trung tài khoản qua CLI, SDKs, APIs và thậm chí là Infrastructure as code như &lt;a href=&quot;https://aws.amazon.com/cloudformation/?org_product_fr_cloudformation&quot;&gt;AWS Cloudformation&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Define and manage your organization&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Ngay khi khởi tạo các tài khoản, người quản lý có thể tạo nhòm tài khoản (OUs) phục vụ cho từ ứng dụng hoặc dịch vụ. Người quản lý có thể sử dụng tags để phân loại và track tài nguyên. Ngoài ra người quản lý có thể truyền lại quyền (delegate) cho nhân viên, khi đó nhân viên có thể coi là đại diện cho tổ chức để thực thi quyền.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Secure and monitor your accounts&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Người quản lý có thể quản lý bảo mật &lt;strong&gt;tập trung&lt;/strong&gt; và cho phép đội bảo mật truy cập cho tổ chức. AWS Organizations cung cấp dịch vụ bảo mật như &lt;a href=&quot;https://aws.amazon.com/guardduty/?org_product_fr_guardduty&quot;&gt;AWS GuardDuty&lt;/a&gt; để phát hiện và xử lý sự cố/nguy cơ bảo mật, &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html&quot;&gt;IAM Access Analyzer&lt;/a&gt; để review những truy cập không mong muốn, và dùng &lt;a href=&quot;https://aws.amazon.com/macie/&quot;&gt;Amazon Macie&lt;/a&gt; để quản lsy dữ liệu nhạy cảm (kiểu như review code xem có sót thông tin mật nào không?).&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Control access and permissions&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Set up Amazon Single Sign-On (SSO) để cung cấp cho tài khoản người dùng AWS quyền truy cập tài khoản thông qua active directory và tối ưu quyền dựa trên roles. Người quản lý còn có thể cho phép người dùng hoặc OUs sử dụng SCPs để quản lý quyền truy cập vào tài nguyên, dịch vụ, và regions trong tổ chức.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Share resources across accounts&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Quản lý tài nguyên tập trung với &lt;a href=&quot;https://aws.amazon.com/ram/?org_product_ow_ram&quot;&gt;AWS Resource Allocation Management (RAM)&lt;/a&gt;. Kể cả license cũng có thể được quản lý tập trung với &lt;a href=&quot;https://aws.amazon.com/license-manager/?org_product_ft_licensemanager&quot;&gt;AWS License Manager&lt;/a&gt;, và chia sẻ dịch vụ trong cả tổ chức với AWS Service Catalog.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Audit your environment for compliance&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Người quản lý có thể kích hoạt &lt;a href=&quot;https://aws.amazon.com/cloudtrail/&quot;&gt;AWS CloudTrail&lt;/a&gt; để log mọi hành vi trong môi trường tổ chức, mà nhân viên không thể tắt đi được. Người quản lý có thể cấu hình AWS Backup để backup mọi tài nguyên, và dùng AWS Config để đồng bộ cấu hình tài nguyên dịch vụ trong cả tổ chức bất cháp tài khoản và regions.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Centrally manage billing and costs&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Quản lý costs tập trung với AWS Cost Explorer, và sử dụng AWS Compute Optimizer để tối ưu giá thành.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;usecase-aws-govcloud&quot;&gt;Usecase: AWS GovCloud&lt;/h2&gt;

&lt;p&gt;Trên trang chủ của AWS Organizations có thể tìm thấy khá nhiều usecases có sẵn.
Trong bài này chúng ta xem lại 1 usecase của AWS GovCloud &lt;a class=&quot;citation&quot; href=&quot;#mahakian2020aws&quot;&gt;[9]&lt;/a&gt;.
Usecase này được đưa ra bởi MITRE.
Họ đưa ra 9 lời khuyên trong việc quản lý AWS Organizations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reserved Instances&lt;/strong&gt;: nên thuê dài hạn để nhận discount.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Spot Instances&lt;/strong&gt;: nên thuê những instances dạng spot để có thể bid giá và chạy khi giá thực vượt quá giá bid.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Auto Scaling and Automating Elasticity&lt;/strong&gt;: auto-scaling instances là chuyện nên làm và policy để scale nên dựa trên volume demand.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;S3 Object Lifecycle Management&lt;/strong&gt;: storage type nên được chọn dựa theo object lifecycle và tần suất truy cập.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Instance States and Stopping Instances&lt;/strong&gt;: hãy cấu hình chỉ bill với những cái gì đang chạy. Không nên thuê kiểu phải trả cho thời gian không chạy.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tagging&lt;/strong&gt;: để giám sát giá thành nên dùng labels.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AWS Config&lt;/strong&gt;: đồng bộ cấu hình tài nguyên trên toàn tổ chức và mọi regions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AWS Organizations&lt;/strong&gt;: quản lý quyền và tài khoản trong tổ chức một tập trung và hiệu quả.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;EC2 Right Sizing&lt;/strong&gt;: nếu dùng EC2 thì phải lựa chọn size phù hợp nhu cầu, không nên dùng lớn hơn nhu cầu dẫn tới lãng phí.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nói chung là nếu quản lý tài nguyên một cách tập trung và hiệu quả thì giá thành của tổ chức sẽ rẻ hơn nhiều so với dùng tài khoản cá nhân.&lt;/p&gt;

&lt;h1 id=&quot;aws-single-sing-on-sso&quot;&gt;AWS Single Sing-On (SSO)&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://d1.awsstatic.com/diagrams/SSO-diagram.7b77570150a19ea35cfe4b923e1aee9f52b3dd06.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;giới-thiệu-về-federated-learning-với-aws&quot;&gt;Giới thiệu về Federated learning với AWS&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Federated Authentication&lt;/strong&gt;. Vậy là chúng ta đã cấu hình xong AWS Organizations.
Bây giờ chúng ta đã tạo 1 tài khoản user, và chúng ta muốn user đó sẽ login và sử dụng AWS account với quyền giới hạn.
Tất nhiên là login bằng tài khoản root thì sẽ có mọi quyền rồi, nhưng vấn đề là việc đó cần phải giới hạn để tránh những sự cố không đáng có.&lt;/p&gt;

&lt;p&gt;Việc cấp quyền này chính là hình thức &lt;strong&gt;federated&lt;/strong&gt; mà chúng ta muốn học hỏi hôm nay.
Nhìn chung là nếu cứ tất cả dồn hết vào tài khoản root, thì chuyện quản lý phân quyền sẽ vô cùng mệt mỏi.
Do vậy, cần có &lt;strong&gt;fedareted learning&lt;/strong&gt; để nhượng quyền cũng như chia quyền cho thích hợp.&lt;/p&gt;

&lt;h2 id=&quot;cấu-hình-federated-account-để-người-dùng-có-thể-truy-cập-aws&quot;&gt;Cấu hình Federated account để người dùng có thể truy cập AWS&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aws-sso-assign-user.png&quot; style=&quot;float: left; margin: 10px; width: 50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Về phía người quản lý.&lt;/strong&gt; Một lời giải cho việc này chính là sử dụng AWS Single Sign-On (SSO), mà khởi đầu chỉ cần vào danh sách dịch vụ của AWS Organizations và bật SSO lên.
Sau đó người quản lý vào danh sách user và ấn check tên user account muốn cấp quyền và nhấn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Assign user&lt;/code&gt;.
Người quản lý sẽ được đưa đến màn hình &lt;strong&gt;Assign Users&lt;/strong&gt; như sau.
&lt;img src=&quot;/assets/images/aws0-sso-set-permission.png&quot; style=&quot;float: right; margin: 10px; width: 50%&quot; /&gt;
Tại đây, người quản lý có thể tạo một &lt;strong&gt;permission set&lt;/strong&gt; mới cho user account đã cho, hoặc cấp ngay một permission set có sẵn.
Trong ví dụ này tôi chọn luôn permission set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataScientist&lt;/code&gt; có sẵn để cho user account đã tạo nhé.
Cần chú ý ở đây là bộ quyền &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataScientist&lt;/code&gt; thì chỉ cho phép user truy cập các dịch vụ Analytics như database, DataLake Formation, RedShift, .v.v… chứ những dịch vụ như CLoudFront, Lambda, … là cần thêm quyền.
Tùy vào nhu cầu sử dụng mà người quản lý có thể tạo những bộ quyền có sẵn để tiện cho việc quản lý.
&lt;img src=&quot;/assets/images/aws-sso-assign-user-2.png&quot; style=&quot;float: left; margin: 10px; width: 50%&quot; /&gt;
Sau khi nhấn thêm vài nút bấm &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OK em&lt;/code&gt; nữa thì bộ quyền đã được assign cho user nhá.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aws-sso-aws.png&quot; style=&quot;float: right; margin: 10px; width: 50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Về phía federated user.&lt;/strong&gt; Thì họ cần truy cập vào link SSO do người quản lý cung cấp.
Trong lần truy cập đầu, họ sẽ cần thay đổi mật khẩu mặc định và đăng nhập.
Như bạn thấy ở hình bên, người dùng sẽ truy cập được vào AWS Account với quyền truy cập giới hạn.
Người dùng cũng có thể cấu hình &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MFA devices&lt;/code&gt; để tăng bảo mật theo yêu cầu của người quản lý.
Chú ý là không chỉ AWS Account mà cả các &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SAML service&lt;/code&gt; nếu có thì người dùng có thể truy cập thông qua cổng này.&lt;/p&gt;

&lt;h1 id=&quot;giám-sát-hành-vi-với-aws-cloudtrail&quot;&gt;Giám sát hành vi với AWS CloudTrail&lt;/h1&gt;

&lt;p&gt;Nếu application logs có thể được lưu trữ và quản lý với CloudWatch thì &lt;a href=&quot;https://aws.amazon.com/cloudtrail/&quot;&gt;CloudTrail&lt;/a&gt; có thể quản lý mọi hành vi diễn ra trên tài khoản AWS Accounts của cả cá nhân lẫn organization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://d1.awsstatic.com/product-marketing/CloudTrail/product-page-diagram_AWS-CloudTrail_HIW%402x.d314033178a16dbbd99111038789685e42f23278.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Với CloudTrail, người quản lý của tổ chức có thể làm rất nhiều việc:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Auditting&lt;/strong&gt;: theo dõi, giám sát tự động mọi hoạt động diễn ra với tài khoản AWS Organizations. Người quản lý có thể truy vết lại &lt;a href=&quot;https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html&quot;&gt;log hành vi trên tài khoản tối đa 90 ngày quá khứ&lt;/a&gt;. Nếu muốn mở rộng trên 90 ngày thì có thể liên hệ với AWS để cấu hình thêm. CloudTrail cũng có thể &lt;a href=&quot;https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-ct-api-tutorial.html&quot;&gt;tích hợp vào EventBridge&lt;/a&gt; để tăng cường theo dõi và lên cảnh báo. Log file của CloudTrail được mã hóa và được bảo vệ khỏi mất mát. Dữ liệu log trên nhiều vùng có thể được &lt;a href=&quot;http://docs.aws.amazon.com/awscloudtrail/latest/userguide/aggregatinglogs.html&quot;&gt;aggregate&lt;/a&gt; lại. ClouTrail Insight và CloudTrail Lake là các dịch vụ tăng cường hỗ trợ việc quản lý log hành vi dễ hơn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Phát hiện và xử lý vi phạm bảo mật&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Đối ứng sự cố&lt;/strong&gt;: phát hiện bất thường và tìm hiểu root cause.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Giá cả của CloudTrail và CloudTrail Lake có thể tìm ở &lt;a href=&quot;https://aws.amazon.com/cloudtrail/pricing/&quot;&gt;đây&lt;/a&gt;.
Nói chung là nên ước tính trước số lượng sự kiện hành vi trên account để có kế hoạch chọn gói sử dụng thích hợp.&lt;/p&gt;

&lt;p&gt;Một mẫu log của CloudTrail cho sự kiện &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start-instances&lt;/code&gt; sẽ như sau:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Records&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;userIdentity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;IAMUser&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;principalId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;EX_PRINCIPAL_ID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;arn&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arn:aws:iam::123456789012:user/Alice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;accessKeyId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;EXAMPLE_KEY_ID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;accountId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;123456789012&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;userName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Alice&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventTime&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2014-03-06T21:22:54Z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventSource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;StartInstances&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;awsRegion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;us-east-2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;sourceIPAddress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;205.251.233.176&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;userAgent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2-api-tools 1.6.12.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;requestParameters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;instancesSet&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;items&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;instanceId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;i-ebeaf9e2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]}},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;responseElements&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;instancesSet&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;items&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;instanceId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;i-ebeaf9e2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;currentState&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;code&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pending&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;previousState&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;code&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stopped&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]}}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Trên đây chúng ta cùng nhau điểm qua kha khá các vấn đề về quản lý tài khoản.
Thực ra chỉ 4 điểm có thể là chưa bao quát hết, nhưng cũng hầu như là nắm trọn mảng này.
Một số vấn đề thú vị khác như RAM, AWS Config, … chúng ta sẽ đề cập khi có dịp.&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;trauring1963automatic&quot;&gt;Trauring, M. 1963. Automatic comparison of finger-ridge patterns. &lt;i&gt;Nature&lt;/i&gt;. 197, 4871 (1963), 938–940.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/trauring1963automatic/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jain201650&quot;&gt;Jain, A.K., Nandakumar, K. and Ross, A. 2016. 50 years of biometric research: Accomplishments, challenges, and opportunities. &lt;i&gt;Pattern recognition letters&lt;/i&gt;. 79, (2016), 80–105.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/jain201650/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;holdsworth2008token&quot;&gt;Holdsworth, J. 2008. Token for use in online electronic transactions. Google Patents.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/holdsworth2008token/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;grother2018ongoing&quot;&gt;Grother, P., Ngan, M., Hanaoka, K., Yang, J.C. and Hom, A. 2022. &lt;i&gt;Ongoing face recognition vendor test (FRVT) part 1: Verification&lt;/i&gt;. US Department of Commerce, National Institute of Standards and Technology.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/grother2018ongoing/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;boyd2003protocols&quot;&gt;Boyd, C., Mathuria, A. and Stebila, D. 2003. &lt;i&gt;Protocols for authentication and key establishment&lt;/i&gt;. Springer.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/boyd2003protocols/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pankanti2002individuality&quot;&gt;Pankanti, S., Prabhakar, S. and Jain, A.K. 2002. On the individuality of fingerprints. &lt;i&gt;Pattern Analysis and Machine Intelligence, IEEE Transactions on&lt;/i&gt;. 24, 8 (2002), 1010–1025.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/pankanti2002individuality/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ometov2018multi&quot;&gt;Ometov, A., Bezzateev, S., Mäkitalo, N., Andreev, S., Mikkonen, T. and Koucheryavy, Y. 2018. Multi-factor authentication: A survey. &lt;i&gt;Cryptography&lt;/i&gt;. 2, 1 (2018), 1.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ometov2018multi/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ngu2016iot&quot;&gt;Ngu, A.H., Gutierrez, M., Metsis, V., Nepal, S. and Sheng, Q.Z. 2016. IoT middleware: A survey on issues and enabling technologies. &lt;i&gt;IEEE Internet of Things Journal&lt;/i&gt;. 4, 1 (2016), 1–20.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ngu2016iot/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mahakian2020aws&quot;&gt;Mahakian, J., Holmdahl, S., Bada, Q., Silva, S. and Tretler, Z. 2020. &lt;i&gt;AWS GovCloud Resource and Cost Analysis&lt;/i&gt;. The MITRE Corporation.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/mahakian2020aws/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Sun, 16 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/aws-account/</link>
        <guid isPermaLink="true">https://wanted2.github.io/aws-account/</guid>
        
        <category>Account management</category>
        
        <category>Account security</category>
        
        <category>AWS</category>
        
        <category>AWS account</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Software Engineering</category>
        
        <category>Site Reliable Engineering</category>
        
      </item>
    
      <item>
        <title>Cán cân thu nhập: hàn lâm, khởi nghiệp và công ty</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Cộng đồng mạng xã hội hiện đại gần đây có nổ ra tranh cãi về câu nói &lt;a href=&quot;https://www.youtube.com/watch?v=p9D0KWI9O6o&quot;&gt;Có làm thì mới có ăn, cái loại không làm mà lại muốn có ăn thì chỉ có ăn …&lt;/a&gt; của tác giả &lt;strong&gt;Huấn Hoa Hồng&lt;/strong&gt;.
Những bô lão của cộng đồng hàm lâm như &lt;strong&gt;Giáo sư kiêm lính thủy đánh bạc&lt;/strong&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=V8YTqQkXGx0&quot;&gt;Tiến Bịp&lt;/a&gt; cũng nhanh chóng xuất bản các papers để phản bác Huấn.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Chú ý: video chứa nội dung và ngôn từ nhạy cảm!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nghe thì chả có vẻ liên quan gì nhưng những chân lý do các vị giáo sư “tự phong” của cộng đồng mạng này cũng có sức nặng nhất định: &lt;em&gt;Có làm thì mới có ăn&lt;/em&gt; thì đã là nguyên lý của lao động sản xuất có hệ thống trong xã hội.
Nhưng vấn đề là làm gì và làm thế nào?
Định hướng nghề nghiệp chính là một khâu quan trọng giúp các bạn trẻ phát triển sự nghiệp.
Tất nhiên ngoài hai vị “giáo sư” kể trên thì còn vô số học thuyết và lý thuyết trong cái mảng này với hàng ngàn cuốn sách đủ thể loại của các tác giả trên thế giới để định hướng các bạn trẻ.
Nhưng thôi chúng ta cứ đưa ra góc nhìn thiết thực nhất với mọi người lao động bình thường: &lt;strong&gt;thu nhập&lt;/strong&gt; trước đã nhé!
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;lựa-chọn-sự-nghiệp&quot;&gt;Lựa chọn sự nghiệp&lt;/h1&gt;

&lt;p&gt;Tất nhiên mức lương tối thiểu vùng miền là đã có quy định, mức đóng các khoản như bảo hiểm, thuế thu nhập cá nhân thì cũng đã có quy định công thức, cứ bỏ vào mấy cái Excel là nó tính rẹc phát ra ngay nên chúng ta cũng không quan tâm sâu tới những vấn đề đó mà chỉ chú ý là lương cao thì trách nhiệm thuế sẽ cao theo là thiết kế của khá nhiều hệ thống thuế TNCN của nhiều nước.&lt;/p&gt;

&lt;p&gt;Vậy giờ có mấy con đường?
Xin hãy nhìn vào biểu đồ bên dưới: hầu như chỉ có 2 nhánh&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Làm thuê&lt;/strong&gt;: thì lại phân nhánh làm 4 con đường con
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Làm cho hàn lâm&lt;/em&gt; như làm giảng viên, các vị trí trong trường đại học, nghiên cứu tại viện, …&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Làm cho chính phủ/NGO&lt;/em&gt;. Tuy nhiên hướng này hơi khó vì ít vị trí nên trong bài điều tra này xin phép không bàn về con đường này.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Làm cho công ty lớn&lt;/em&gt;. Đây là hướng chủ đạo của mọi anh em không đi hai con đường trên: kiếm doanh nghiệp kiểu quốc doanh, phúc lợi dồi dào ổn định để mưu cầu sự nghiệp … an bài.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Làm cho công ty nhỏ/ khởi nghiệp&lt;/em&gt;. Đây là con đường mạo hiểm bởi không biết nó sập lúc nào. Chúng ta sẽ đi sâu vào sau, nhưng nhìn chung cty nhỏ hay khởi nghiệp vốn không thể lớn như công ty lớn. Có một dạng thú vị là các công ty khởi nghiệp xuất phát từ 1 nghiên cứu trong academics mà tôi sẽ bàn trong bài viết này.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Khởi nghiệp&lt;/strong&gt;: tức là không làm thuê cho người khác mà tự mở ra mà làm lấy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/careers.png&quot; alt=&quot;careers&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;làm-thuê&quot;&gt;Làm thuê&lt;/h1&gt;

&lt;p&gt;Thực ra mấy cái mảng này thì cũng có nhiều surveys có sẵn rồi và với các bạn trẻ cũng chỉ quan trọng là nhìn thấy mấy con số đáng tin cậy để quyết định hướng đi sự nghiệp nên tôi cứ phang thẳng các bài survey có nhiều citations vào nhé.&lt;/p&gt;

&lt;h2 id=&quot;làm-cho-khởi-nghiệp-và-công-ty-nhỏ-vừa&quot;&gt;Làm cho khởi nghiệp và công ty nhỏ vừa&lt;/h2&gt;

&lt;p&gt;Hướng đi mang tính entrepreneur này có vẻ mạo hiểm với cả chủ lẫn tớ: công ty chả biết lúc nào “sập”?
Một bài survey dưới góc độ employee xem tại sao họ lại chọn hướng đi làm thuê cho khởi nghiệp mà tôi thích là bài của Nystrӧm &lt;a class=&quot;citation&quot; href=&quot;#nystrom2021working&quot;&gt;[1]&lt;/a&gt; ngay vừa 2021 rồi.
Hiểm nguy đi cùng khởi nghiệp sẽ bao gồm:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Great uncertainty&lt;/strong&gt; tức là bấp bênh, lúc có việc, lúc rảnh thôi rồi.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Low wages&lt;/strong&gt; lương thấp tẹt vì bản thân khởi nghiệp cũng đang chờ vốn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Và khả năng công ty không sống sót&lt;/strong&gt;. Có suy thoái kiểu 2008 hay dịch Covid 19 là một loạt chết trắng bụng!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vậy &lt;em&gt;những tên dở người (insane) nào lại đi làm cho khởi nghiệp và các công ty nhỏ?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Đầu tiên chúng ta cũng cần hiểu quy trình tuyển dụng dù công ty nhỏ hay lớn thì cũng sẽ đại loại là trước khi hai bên bắt tay nhau thì phải có một quá trình &lt;strong&gt;matching&lt;/strong&gt;.
Vấn đề là employer cũng không có ngay được thông tin về skill cũng như abilities của employee và ngược lại employee cũng không biết được là cái khởi nghiệp này nó có skill và tiềm năng ra sao.
Cái này trong lĩnh vực này gọi là &lt;strong&gt;information assymmetry&lt;/strong&gt;, tức là sự thiếu thông tin về nhau của hai bên khi tiếp xúc.
Tất nhiên khi hai bên đang thiếu thông tin về nhau như vậy, việc tìm điểm cân bằng equibilirum là cần thiết.&lt;/p&gt;

&lt;p&gt;Theo &lt;a class=&quot;citation&quot; href=&quot;#nystrom2021working&quot;&gt;[1]&lt;/a&gt; thì trong một survey ở labor market Thụy Điển thì sinh viên đại học bên đó đặt các yếu tố sau lên đầu khi tìm việc:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lương khởi điểm cao&lt;/li&gt;
  &lt;li&gt;Cơ hội phát triển bản thân và có training&lt;/li&gt;
  &lt;li&gt;Cơ hội làm việc lâu dài&lt;/li&gt;
  &lt;li&gt;Môi trường làm việc năng động và phát triển tốt.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Một điểm thú vị &lt;a class=&quot;citation&quot; href=&quot;#nystrom2021working&quot;&gt;[1]&lt;/a&gt; là trong những người làm việc R&amp;amp;D (cả chuyển việc lẫn mới tốt nghiệp) thì yếu tố quan trọng nhất với họ không phải lương hay job security mà là &lt;strong&gt;independence và responsibilty (sự độc lập và trách nhiệm)&lt;/strong&gt;.
Tức là lương cao và chuyện không hợp hoặc công ty chết thì nhảy việc thì không quan trọng với người làm R&amp;amp;D, mà quan trọng với họ là được làm việc độc lập (được làm chủ nghiên cứu của mình và toàn quyền quyết định) và trách nhiệm rõ ràng.&lt;/p&gt;

&lt;p&gt;Ngoài việc lương bèo thì những nguy cơ tiềm ẩn như làm việc nhiều giờ, stress cũng dai dẳng với các nhân viên của công ty khởi nghiệp.&lt;/p&gt;

&lt;p&gt;Và Nystrӧm cũng chỉ ra kết quả thú vị: hầu như &lt;strong&gt;khởi nghiệp và cty vừa nhỏ chỉ thuê được các vị trí yếu hơn so với các công ty lớn&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Doner và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#dorner2017wages&quot;&gt;[2]&lt;/a&gt; cũng nghiên cứu về lương khởi nghiệp có thể trả nhưng tập trung vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;academics spin-offs&lt;/code&gt; tức là khởi nghiệp xuất phát từ academics.
Những nhà sáng lập xuất thân từ academics hoặc từ 1 nghiên cứu nào đó trong academics.
Câu hỏi nghiên cứu ở đây là liệu spin-offs có sẵn sàng trả cao cho nhân viên không?
Các tác giả tập trung vào bộ dữ liệu spin-offs ở Đức và thật thú vị sau khi họ làm mấy cái regression test linh tinh và đưa ra kết quả cũng không mới lắm: &lt;strong&gt;spin-offs dù nhận vốn academics (có thể dồi dào hay không biết) nhưng cũng không trả cao đâu!&lt;/strong&gt;
Họ cũng chỉ ra rằng một nhận định cũ rằng spin-offs có tốc độ mở rộng quy mô nhân sự cao hơn các khởi nghiệp khác là sai vì số liệu của họ cũng không chỉ ra significance nào.
&lt;em&gt;Mất công làm một đống regression tests mà kết quả là lương cũng không cao hơn là bao thì quả cũng của đáng tội!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nhìn con số từ Đức và Thụy Điển có vẻ không khả quan lắm, nên tôi chuyển hướng nhìn sang Mỹ xem sao.
Thế là có ngay bài điều tra của Kim &lt;a class=&quot;citation&quot; href=&quot;#kim2018there&quot;&gt;[3]&lt;/a&gt; ngay năm 2018 thôi nên số liệu thực tế và rất mới.
Dữ liệu dựa trên khởi nghiệp xuất phát từ OB của MIT.
Thì thật thú vị là nhóm VC-backed tức là được nhận đầu tư của VC thì có thu nhập khá hơn 10% (non-founder employees) so với mặt bằng chung.
Sau khi thêm hiệu ứng cố định (fixed-effect) dựa trên quan sát là nhiều employees nhận multi-offers thì con số lại không significant lắm, chứng tỏ là việc được VC đầu tư sẽ dẫn tới lương trả cao lên.
Và điều đó cho thấy ở Mỹ thì nên tìm vào khởi nghiệp có VC!&lt;/p&gt;

&lt;p&gt;Một bài điều tra thú vị khác về &lt;strong&gt;chất lượng của job&lt;/strong&gt; từ khởi nghiệp được Block và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#block2018quantity&quot;&gt;[4]&lt;/a&gt; hoàn thành cũng vào năm 2018.
Chủ yếu là vì khởi nghiệp thì tạo ra công ăn việc làm, nhưng công ăn việc làm ấy chất lượng có cao không?
Thì kết quả là &lt;strong&gt;chất lượng thấp&lt;/strong&gt;: phần lớn là lương thấp, bonus bèo hoặc không có, bảo hiểm cũng bấp bênh, công việc cũng không biết công ty chết lúc nào.
Block nói chung cũng không đưa ra được dấu hiệu cho thấy đãi ngộ khấm khá hơn là bao.
Tôi nghĩ mấy cái Block này chắc cũng là tình hình chung của mảng này rồi nên thôi chắc mình cũng không nên mất công vào kẻo lại của đáng tội!&lt;/p&gt;

&lt;p&gt;Thôi điều tra một hồi thấy đến cả mấy cái work của Block &lt;a class=&quot;citation&quot; href=&quot;#block2018quantity&quot;&gt;[4]&lt;/a&gt; mà cũng chỉ được bèo như vậy thì chắc làm thuê cho khởi nghiệp mình đọc đến đây thôi!
Đổi mới sáng tạo thì cũng thú vị lắm nhưng nghèo thì gượm để suy nghĩ đã :))&lt;/p&gt;

&lt;h2 id=&quot;làm-cho-hàn-lâm&quot;&gt;Làm cho hàn lâm&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Làm cho hàn lâm có mong giàu không?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nhìn chung là cũng có lý do tại sao đi dạy đi nghiên cứu lại mong giàu: Nghiên cứu của Currall và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#currall2005pay&quot;&gt;[5]&lt;/a&gt; vào năm 2005 trên tập giáo viên từ gần 7,000 trường học ở Mỹ cho thấy, độ thỏa mãn về lương lậu có tương quan dương với hiệu suất làm việc ở trường và tương quan âm với ý định bỏ việc của giáo viên.
Tức là lương lậu OK thì sẽ không bỏ việc và hiệu suất lại tốt hơn. 
Tất nhiên độ thỏa mãn về lương lậu cũng chỉ là survey kiểu &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lương có đủ sống không em?&lt;/code&gt; chứ chưa nói đến giàu, nhưng qua đây cũng thấy là lương mà không cao là giáo viên cũng bỏ dạy đấy.&lt;/p&gt;

&lt;p&gt;Đấy là ngành dạy dỗ nhé, nhưng còn ngạch nghiên cứu thì sao?
Lại có bài của Stern &lt;a class=&quot;citation&quot; href=&quot;#stern2004scientists&quot;&gt;[6]&lt;/a&gt; năm 2004.
Ô đọc xong mà lại thấy nghèo quá!
Kết quả chỉ ra 1 tương quan âm giữa lương và độ khoa học trong tổ chức: những cơ quan nghiên cứu mà cho phép nhà nghiên cứu xuất bản bài báo và hội nghị thì nhận được 25% discount trong lương.
&lt;strong&gt;Tức là nhà nghiên cứu sẵn sàng nhận lương thấp đi 25% nếu cho phép họ xuất bản bài báo lên tạp chí khoa học uy tín hoặc hội nghị lớn.&lt;/strong&gt;
Thế thì nghèo!
&lt;em&gt;Thế nên xuất bản papers là chấp nhận nghèo đi đấy anh em ạ!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mấy cái nghiên cứu này cũng dựa trên giả thuyết multi-offers rồi lại làm mấy cái fixed-effect regression tests, thế mà xong lại ra lương không cao hơn mà lại thấp đi 25% thế này thì có lẽ cũng không cần điều tra hướng này sâu thêm nữa!
Mà lại còn nhiều thử thách, ví dụ như muốn hợp tác nghiên cứu với industry mà hay gọi là &lt;strong&gt;University-Industry (UI) collaboration&lt;/strong&gt;, thì cái UI này cũng tù lắm: vừa bị control của trường đại học (university administration) lại còn vấn đề sở hữu trí tuệ (IP) của doanh nghiệp.
Nói chung là có nhiều thử thách, mà thế mới đúng vì &lt;strong&gt;có làm thì mới có ăn&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;làm-cho-công-ty-lớn&quot;&gt;Làm cho công ty lớn&lt;/h2&gt;

&lt;p&gt;Đây là hướng đi còn lại trong nhánh &lt;strong&gt;Làm thuê&lt;/strong&gt; và cũng là phổ biến nhất trong nhóm lao động trẻ mới tốt nghiệp ra trường.
Nhìn chung vì nó là đại trà nên lương ở mức mặt bằng chung, phúc lợi thì công ty này công ty kia nó cũng same same.
Anh em trải qua hết rồi cũng chả có thấy điểm ưu trội nào mấy.&lt;/p&gt;

&lt;p&gt;Thôi thì xem thử có bài nào hay hay thì thấy có bài của Andersson và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#andersson2009reaching&quot;&gt;[7]&lt;/a&gt; xuất bản năm 2009 về câu hỏi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;những công ty nào sẽ trả tiền thuê talents?&lt;/code&gt;
Thì thu nhập cũng phải hiểu là có thu nhập khởi điểm và thu nhập khi đã có kinh nghiệm.
Ngoài ra còn stock options nữa cũng là một yếu tốt trong thu nhập khi đi làm thuê.
Hay là nghiên cứu này lại tổ chức trên đúng tập dữ liệu là các công ty software ở Mỹ nên có lẽ rất phù hợp cho ngành IT.
Thì câu hỏi là &lt;strong&gt;những công ty như thế nào sẽ trả lương cao và họ sẽ trả cao cho nhân viên như thế nào?&lt;/strong&gt;.
Nhóm của Andersson chỉ ra rằng những công ty ở khu vực đầu tư mạo hiểm (thắng thì sẽ thắng lớn nhưng thua thì cũng dặt dẹo) thì sẵn sàng trả cao để thu hút nhân tài.
Thế nên là anh em thấy những dự án thử thách mạo hiểm thì cứ nhảy vào chiến thôi, đừng lo “sập”!
Chỉ cần lo chiến đấu thôi!&lt;/p&gt;

&lt;h1 id=&quot;khởi-nghiệp&quot;&gt;Khởi nghiệp&lt;/h1&gt;

&lt;p&gt;Làm thuê thì có vẻ cũng hòm hòm vậy, đi vào chỗ nào lương sẽ như thế nào là đã có hình dung trong đầu nên tôi nghĩ chắc cũng chả có gì mới nữa.
&lt;strong&gt;Xem khởi nghiệp tức là mình mở hẳn công ty của mình xem có khá khẩm hơn không nào?&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;đặc-điểm-của-khởi-nghiệp&quot;&gt;Đặc điểm của khởi nghiệp&lt;/h2&gt;

&lt;p&gt;Đầu tiên là vấn đề ý tưởng thì có bài của Homfeldt và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#homfeldt2019suppliers&quot;&gt;[8]&lt;/a&gt; ngay năm 2019.
Họ trả lời câu hỏi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;liệu ý tưởng của startups có thực sự mới mẻ hơn của các công ty lớn không?&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We find that start-ups’ ideas are characterized by a higher degree of novelty and to some extent higher benefit for end customers but, on the downside, are less likely to be implemented than suppliers’ ideas.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Câu trả lời là ý tưởng thì đúng là có vẻ tân tiến nhưng lại kiểu ít có khả năng thực thi hơn của công ty lớn.
Tức là mới nhưng mà hơi mơ mộng.
Thì đúng là vậy, ý tưởng mới sẽ khó được đón nhận ngay và thường bị xem là mơ mộng, viển vông.
Nhưng qua thời gian nếu startup trụ vững và chứng minh được thì nó lại thành hiện thực và startup sẽ cất cánh.
Rủi ro lớn nhất là startup không thực hiện được thôi, nhưng đó không phải vấn đề mới mà là vấn đề chung của mọi startup hiện nay!&lt;/p&gt;

&lt;p&gt;Khi khởi nghiệp thì nếu xuất phát từ 1 nghiên cứu hàn lâm thì startups sẽ được gọi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spin-off&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#rodriguez2018role&quot;&gt;[9]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#dianez2021drivers&quot;&gt;[10]&lt;/a&gt;.
Khi xây dựng spin-off thì sẽ phải chú ý vấn đề &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hướng&lt;/code&gt; và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chiến lược&lt;/code&gt; (EO) cũng như xây dựng đội ngũ.
&lt;a class=&quot;citation&quot; href=&quot;#dianez2021drivers&quot;&gt;[10]&lt;/a&gt; cũng chỉ ra một số yếu tố hỗ trợ vấn đề &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hướng&lt;/code&gt; là&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sự có mặt của 1 quản lý không phải academics (non-academics manager) trong đội ngũ quản lý&lt;/li&gt;
  &lt;li&gt;Có vốn của VC&lt;/li&gt;
  &lt;li&gt;Hỗ trợ của tổ chức chuyên tài trợ spin-off trong chuyển giao công nghệ
Trên đây có vẻ là yếu tố chiến lược hỗ trợ spin-off phát triển.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rồi làm khởi nghiệp còn phải đau đầu lo vấn đề hợp tác mà thực ra là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mua&lt;/code&gt; affiliation &lt;a class=&quot;citation&quot; href=&quot;#hsu2004entrepreneurs&quot;&gt;[11]&lt;/a&gt;.
Không có affiliation thì chẳng có publish gì cả vì reputation thấp.
Muốn reputation cao thì lại phải dựa vào những affiliation là VC có reputation cao.&lt;/p&gt;

&lt;p&gt;Đấy là nếu gọi vốn VC, còn nếu gọi vốn crowdfunding &lt;a class=&quot;citation&quot; href=&quot;#mollick2014dynamics&quot;&gt;[12]&lt;/a&gt; thì cũng là hướng mới mà nhiều anh em đang làm nhưng nói chung &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gian nan&lt;/code&gt; lắm.&lt;/p&gt;

&lt;h2 id=&quot;thu-nhập-của-nhà-khởi-nghiệp&quot;&gt;Thu nhập của nhà khởi nghiệp&lt;/h2&gt;

&lt;p&gt;Quay lại chủ để chính là &lt;strong&gt;thu nhập&lt;/strong&gt;.
Thì có bài của Åstebro và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#aastebro2013does&quot;&gt;[13]&lt;/a&gt; năm 2013 cũng nghiên cứu trên tập các nhà sáng lập xuất thân từ academics xem &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thu nhập của họ có khá lên sau khi rời academics không?&lt;/code&gt;
Đau cái là nó lại không khá lên, qua thống kê của Åstebro và cộng sự thì thu nhập before/after khởi nghiệp nó lại như nhau: trước khi khởi nghiệp thì thu nhập trung bình là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;397,000 (39.7 man)&lt;/code&gt; còn sau khi khởi nghiệp bình quân là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;450,000 (45 man)&lt;/code&gt;.
Mà phương sai lại cao gấp 3 lần tức là sau khi khởi nghiệp thu nhập lại “bấp bênh” hơn.
Ngoài ra 60% những nhà khởi nghiệp này lại từ bỏ khởi nghiệp trong vòng 2 năm và 66% lại quay trở về academics!&lt;/p&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Nói chung &lt;strong&gt;Có làm thì mới có ăn&lt;/strong&gt; là chân lý của lao động rồi.
Bài điều tra điểm qua hầu hết mọi hướng đi khả thi, trừ hướng chính phủ/NGO chưa muốn động vào.
Nhưng nhìn chung là làm thuê thì chấp nhận là lương sẽ chỉ cao nếu đi đúng đường.
Còn tự thân khởi nghiệp thì nghĩ béo bở nhưng đau đầu và thực tế là cũng chả tăng thêm thu nhập bao nhiêu (từ 39.7 man lên 45 man), mà phần lớn (60%) là thất bại sau 2 năm &lt;a class=&quot;citation&quot; href=&quot;#aastebro2013does&quot;&gt;[13]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thế nên quay lại câu hỏi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nếu làm R&amp;amp;D khởi nghiệp mà không béo bằng đi làm thuê phát triển thì có nên làm nghiên cứu không?&lt;/code&gt; thì tôi nghĩ là tùy mỗi người thôi, nhưng nhìn quan điểm là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thu nhập&lt;/code&gt; thì có lẽ con đường đi nghiên cứu nó không thông minh lắm đâu!&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;nystrom2021working&quot;&gt;Nyström, K. 2021. Working for an entrepreneur: heaven or hell? &lt;i&gt;Small Business Economics&lt;/i&gt;. 56, 2 (2021), 919–931.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nystrom2021working/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dorner2017wages&quot;&gt;Dorner, M., Fryges, H. and Schopen, K. 2017. Wages in high-tech start-ups–Do academic spin-offs pay a wage premium? &lt;i&gt;Research Policy&lt;/i&gt;. 46, 1 (2017), 1–18.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dorner2017wages/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kim2018there&quot;&gt;Kim, J.D. 2018. Is there a startup wage premium? Evidence from MIT graduates. &lt;i&gt;Research Policy&lt;/i&gt;. 47, 3 (2018), 637–649.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kim2018there/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;block2018quantity&quot;&gt;Block, J.H., Fisch, C.O. and Van Praag, M. 2018. Quantity and quality of jobs by entrepreneurial firms. &lt;i&gt;Oxford Review of Economic Policy&lt;/i&gt;. 34, 4 (2018), 565–583.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/block2018quantity/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;currall2005pay&quot;&gt;Currall, S.C., Towler, A.J., Judge, T.A. and Kohn, L. 2005. Pay satisfaction and organizational outcomes. &lt;i&gt;Personnel psychology&lt;/i&gt;. 58, 3 (2005), 613–640.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/currall2005pay/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;stern2004scientists&quot;&gt;Stern, S. 2004. Do scientists pay to be scientists? &lt;i&gt;Management science&lt;/i&gt;. 50, 6 (2004), 835–853.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/stern2004scientists/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;andersson2009reaching&quot;&gt;Andersson, F., Freedman, M., Haltiwanger, J., Lane, J. and Shaw, K. 2009. Reaching for the stars: who pays for talent in innovative industries? &lt;i&gt;The Economic Journal&lt;/i&gt;. 119, 538 (2009), F308–F332.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/andersson2009reaching/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;homfeldt2019suppliers&quot;&gt;Homfeldt, F., Rese, A. and Simon, F. 2019. Suppliers versus start-ups: Where do better innovation ideas come from? &lt;i&gt;Research policy&lt;/i&gt;. 48, 7 (2019), 1738–1757.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/homfeldt2019suppliers/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rodriguez2018role&quot;&gt;Rodrı́guez-Gulı́as Marı́a Jesús, Fernández-López, S., Rodeiro-Pazos, D., Corsi, C. and Prencipe, A. 2018. The role of knowledge spillovers on the university spin-offs innovation. &lt;i&gt;Science and Public Policy&lt;/i&gt;. 45, 6 (2018), 875–883.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rodriguez2018role/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dianez2021drivers&quot;&gt;Diánez-González, J.P., Camelo-Ordaz, C. and Fernández-Alles, M. 2021. Drivers and implications of entrepreneurial orientation for academic spin-offs. &lt;i&gt;International Entrepreneurship and Management Journal&lt;/i&gt;. 17, 2 (2021), 1007–1035.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dianez2021drivers/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hsu2004entrepreneurs&quot;&gt;Hsu, D.H. 2004. What do entrepreneurs pay for venture capital affiliation? &lt;i&gt;The journal of finance&lt;/i&gt;. 59, 4 (2004), 1805–1844.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hsu2004entrepreneurs/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mollick2014dynamics&quot;&gt;Mollick, E. 2014. The dynamics of crowdfunding: An exploratory study. &lt;i&gt;Journal of business venturing&lt;/i&gt;. 29, 1 (2014), 1–16.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/mollick2014dynamics/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;aastebro2013does&quot;&gt;Åstebro, T., Braunerhjelm, P. and Broström, A. 2013. Does academic entrepreneurship pay? &lt;i&gt;Industrial and Corporate Change&lt;/i&gt;. 22, 1 (2013), 281–311.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/aastebro2013does/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Mon, 10 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/pay/</link>
        <guid isPermaLink="true">https://wanted2.github.io/pay/</guid>
        
        <category>Career</category>
        
        <category>academics</category>
        
        <category>industry</category>
        
        <category>government</category>
        
        <category>entrepreneurship</category>
        
        <category>startups</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
      </item>
    
      <item>
        <title>RapidAPI and RapidAPI Hub</title>
        <description>&lt;p&gt;&lt;em&gt;Image Credit: &lt;a href=&quot;https://financefeeds.com/rakuten-launches-api-marketplace/&quot;&gt;FinanceFeeds&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Rakuten launched RapidAPI Marketplace in 2018 as a result of the collaboration between Japan’s Rakuten Inc and San Francisco-based startup RapidAPI.
&lt;a href=&quot;https://api.rakuten.co.jp/en/&quot;&gt;The API marketplace&lt;/a&gt; aims to provide software developers in Japan and Asia unified access to more than 8,000 APIs with localized documentation and resources in Japan’s language and English.
The API marketplace platform will connect API providers and developers.
Developers in Japan and across Asia will be able to find, test, and connect to thousands of APIs for their applications.
The marketplace will also allow API providers to connect with the global developer community through personalized API portals.
 &lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-is-rapidapi&quot;&gt;What is RapidAPI?&lt;/h1&gt;

&lt;p&gt;Let us assume that you have an API that is ready for production.
You need to add authentication like API key, OAuth 2, or something else.
You need to deploy your API to somewhere that is stable and reliable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the &lt;strong&gt;shortest path&lt;/strong&gt; to achieving your goal?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You are an application developer, and you need to manage the records of some data for the app.
For example, you need to maintain the list of public holidays in your app.
You don’t want to hardcode those things in the code.
Note that the public holidays change between countries and sometimes due to the law it will change between years.
It is somewhat troublesome to maintain the records in your database as it will make you allocate some effort and human resources there.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the most convenient way to maintain such data?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In both scenarios, Rakuten RapidAPI Marketplace gives you excellent solutions.
Either maintenance of the data (public holidays) or publishing a new API, you can do all of the lifecycles in one platform.&lt;/p&gt;

&lt;p&gt;For example, when you want to check a day is a holiday or not, you can thus search for a free API like this &lt;a href=&quot;https://english.api.rakuten.net/theapiguy/api/public-holiday&quot;&gt;one&lt;/a&gt; and make a request.
Because all maintenance is up on the providers, this solution costs you nothing: you don’t need to worry about maintaining the records of holidays data (which shouldn’t be your matter in any way) and focus on your own application logic.
Note that the &lt;a href=&quot;https://english.api.rakuten.net/theapiguy/api/public-holiday&quot;&gt;Public Holidays API&lt;/a&gt; has low latency (59ms) and is completely free.
Another solution is to build an endpoint in your own API like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/api/v1/holidays&lt;/code&gt; to validate the holidays, but while such a ready-to-use solution is there, why should you waste time and money to build/manage/maintain on your own?&lt;/p&gt;

&lt;p&gt;RapidAPI helps your API to distribute and monetize.
Adding your API to the RapidAPI Hub gets you instant exposure to our growing user base, a search-engine-optimized profile page for your API, as well as features like user management and billing services.
RapidAPI also serves functional testings, API monitoring dashboards, and many other premiere features like API authentication.&lt;/p&gt;

&lt;h1 id=&quot;rapidapi-for-api-vendors&quot;&gt;RapidAPI for API Vendors&lt;/h1&gt;

&lt;p&gt;The workflow between an app developer’s client to a vendor API can be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rapidapi.svg&quot; alt=&quot;rapidapi&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An &lt;a href=&quot;https://docs.rapidapi.com/docs/keys&quot;&gt;API Key&lt;/a&gt; is generated and appended to the request’s header to RapidAPI servers.&lt;/li&gt;
  &lt;li&gt;RapidAPI authenticate the request (using API Key and optionally a configured authentication method like OAuth 2). Then it modifies the requests header to append &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X-RapidAPI-*&lt;/code&gt; headers.&lt;/li&gt;
  &lt;li&gt;The vendor API (destination API in the diagram) checks the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X-RapidAPI-*&lt;/code&gt; headers and authenticates the modified requests.&lt;/li&gt;
  &lt;li&gt;A response is generated according to the requested information and is then returned to RapidAPI.&lt;/li&gt;
  &lt;li&gt;RapidAPI modifies the response from vendor servers. It appends Rapid API headers (for example, headers about rate limits) or generates a new response.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As you can see, RapidAPI Marketplace acts as a proxy between app servers (client in the diagram) and the vendor API servers.
The vendors &lt;a href=&quot;https://docs.rapidapi.com/docs/add-an-api-basics&quot;&gt;register&lt;/a&gt; their APIs and &lt;a href=&quot;https://docs.rapidapi.com/docs/add-an-api-advanced-settings&quot;&gt;fine-tune&lt;/a&gt; the settings in RapidAPI dashboard.
All API endpoints are relative to a base URL, which is added as a “prefix” to all API endpoints.
This approach avoids the need to define absolute URLs for endpoints every time and increases API portability by changing the base URL.&lt;/p&gt;

&lt;p&gt;API vendors can &lt;a href=&quot;https://docs.rapidapi.com/docs/configuring-api-authentication&quot;&gt;add&lt;/a&gt; basic authentication or OAuth 2 to their APIs.&lt;/p&gt;

&lt;p&gt;RapidAPI supports &lt;a href=&quot;https://docs.rapidapi.com/docs/automating-api-provisioning&quot;&gt;automatic API provisioning using OpenAPI&lt;/a&gt; and &lt;a href=&quot;https://docs.rapidapi.com/docs/transformations&quot;&gt;custom transformations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;RapidAPI has basic plan options so app developers can choose among these options to pay:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;API Type&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Free APIs&lt;/td&gt;
      &lt;td&gt;APIs that do not require a credit card or subscription to consume.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pay Per Use&lt;/td&gt;
      &lt;td&gt;APIs that don’t have a subscription fee associated with them. A credit card is required as you pay for what you use on the API.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Freemium APIs&lt;/td&gt;
      &lt;td&gt;Paid APIs that also include a limited free tier. These require a credit card, even for the free plan.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Paid APIs&lt;/td&gt;
      &lt;td&gt;APIs that require a paid subscription plan and credit card to consume.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;some-notes-on-security&quot;&gt;Some notes on security&lt;/h2&gt;

&lt;p&gt;RapidAPI supports &lt;a href=&quot;https://docs.rapidapi.com/docs/secret-headers-parameters&quot;&gt;secret headers and parameters&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;RapidAPI allows you to add secret headers and/or query string parameters to API requests. The RapidAPI proxy adds these secrets to every request but is &lt;strong&gt;hidden from the API consumers&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that even the consumers who make the requests do not know about these secrets.
This differs from header and query authentication methods where consumers know all secrets in the requests they make to RapidAPI.&lt;/p&gt;

&lt;p&gt;Users should configure RapidAPI &lt;a href=&quot;https://docs.rapidapi.com/docs/security-threat-protection&quot;&gt;security&lt;/a&gt; features like firewalls, threat protection, schema validation, and request size limit (which returns error code 413).&lt;/p&gt;

&lt;p&gt;Vendors can set their API to &lt;a href=&quot;https://docs.rapidapi.com/docs/private-apis-api-logo&quot;&gt;private&lt;/a&gt; where only invited users can access.&lt;/p&gt;

&lt;h2 id=&quot;audit-and-marketing-tools&quot;&gt;Audit and marketing tools&lt;/h2&gt;

&lt;p&gt;RapidAPI provides &lt;a href=&quot;https://docs.rapidapi.com/docs/provider-dashboard&quot;&gt;Provider Dashboard&lt;/a&gt; where vendors can monitor their API usages.
Another nice thing is that as a vendor, you can make your monetization more useful using &lt;a href=&quot;https://docs.rapidapi.com/docs/ive-added-my-api-to-rapidapi-now-what&quot;&gt;Marketing API&lt;/a&gt;.
When you have an API, you should make sure you don’t miss a checklist when publishing your solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://api.rakuten.co.jp/docs/ja-images/ProviderWelcome_1.png&quot; alt=&quot;RapidAPI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This checklist helps you have a better SEO for your API.&lt;/p&gt;

&lt;h2 id=&quot;api-testing&quot;&gt;API Testing&lt;/h2&gt;

&lt;p&gt;Testing is quite tedious!
RapidAPI helps vendors reduce testing costs with their &lt;a href=&quot;https://docs.rapidapi.com/docs/rapidapi-testing-overview&quot;&gt;API testing feature&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.readme.io/726dc84-run-code.png&quot; alt=&quot;RapidAPI testing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you are already familiar with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postman-tool&lt;/code&gt; you are ready to go with RapidAPI &lt;a href=&quot;https://docs.rapidapi.com/docs/create-a-test-advanced&quot;&gt;advanced testing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.readme.io/fabfeb1-Screen_Shot_2020-12-03_at_4.00.53_PM.png&quot; alt=&quot;Advanced testing&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;rapidapi-for-app-developers&quot;&gt;RapidAPI for App Developers&lt;/h1&gt;

&lt;p&gt;As an app developer, you can find that &lt;a href=&quot;https://rapidapi.com/hub&quot;&gt;RapidAPI Hub&lt;/a&gt; now has more than 10,000 APIs.
Even you want to develop an OCR app or a Translation app, you can find your API right away.&lt;/p&gt;

&lt;p&gt;All you need is to register a RapidAPI account, choose your API and then &lt;strong&gt;make a payment&lt;/strong&gt;.
Finally, you can &lt;a href=&quot;https://docs.rapidapi.com/docs/connecting-to-an-api&quot;&gt;connect&lt;/a&gt; to your paid API using the API key.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rapidapi-vin.png&quot; alt=&quot;RapidAPI VIN&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It is worth noting that RapidAPI supports not only REST API but also &lt;a href=&quot;https://docs.rapidapi.com/docs/graphql-apis&quot;&gt;GraphQL&lt;/a&gt;, &lt;a href=&quot;https://docs.rapidapi.com/docs/adding-soap-apis&quot;&gt;SOAP&lt;/a&gt;, and &lt;a href=&quot;https://docs.rapidapi.com/docs/kafka-apis&quot;&gt;Kafka&lt;/a&gt; APIs.
We did not touch &lt;a href=&quot;https://docs.rapidapi.com/docs/what-is-rapidapi-for-teams&quot;&gt;RapidAPI for Teams&lt;/a&gt;, but it might be useful at the organization level.&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/rapidapi/</link>
        <guid isPermaLink="true">https://wanted2.github.io/rapidapi/</guid>
        
        <category>api development</category>
        
        <category>backend</category>
        
        <category>infrastructure</category>
        
        <category>rapidapi</category>
        
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
        <category>Site Reliable Engineering</category>
        
      </item>
    
      <item>
        <title>Chào 2022 và câu chuyện khoảng cách công nghệ giữa các quốc gia, tổ chức</title>
        <description>&lt;p&gt;&lt;a href=&quot;/year-end/&quot;&gt;Năm 2021&lt;/a&gt; đã kết thúc và cũng là lúc để nghĩ đến dự định cho năm 2022.
Bản thân AiFi luôn nhắm đến mục tiêu lấy chứng chỉ Project Manager (PM) trong thời gian ngắn nhất có thể.
Ngoài ra, tất nhiên nâng cao năng lực quản lý cũng là một bước chuẩn bị tốt nhưng quan trọng hơn vẫn là nhanh chóng xây dựng roadmap để sớm launch một cái gì đó thú vị, ví dụ như 1 công ty chẳng hạn.
Có lẽ thời điểm đầu xuân cũng là lúc phù hợp để chốt các mục tiêu cho năm 2022 và lên kế hoạch.
Tất nhiên cũng cần hiểu là &lt;strong&gt;năm 2022 vẫn chỉ là một năm chuẩn bị&lt;/strong&gt; thôi.
Cuối cùng là một chút tản mạn về chủ để &lt;strong&gt;khoảng cách công nghệ&lt;/strong&gt; giữa các quốc gia, firm.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;dự-định-2022&quot;&gt;Dự định 2022&lt;/h1&gt;

&lt;h2 id=&quot;trong-công-việc&quot;&gt;Trong công việc&lt;/h2&gt;

&lt;p&gt;Trong công việc thì cứ tiến hành như bình thường.
Cố gắng duy trì nhịp độ làm việc và sớm chuyển sang Nhật Bản.&lt;/p&gt;

&lt;h2 id=&quot;phát-triển-bản-thân&quot;&gt;Phát triển bản thân&lt;/h2&gt;

&lt;h3 id=&quot;về-mặt-quản-lý&quot;&gt;Về mặt quản lý&lt;/h3&gt;
&lt;p&gt;Bản thân AiFi luôn nhắm đến mục tiêu lấy chứng chỉ Project Manager (PM) trong thời gian ngắn nhất có thể.
Tuy nhiên, đó là một việc không thể nhanh chóng vì PMI &lt;a class=&quot;citation&quot; href=&quot;#PMPCerti18:online&quot;&gt;[1]&lt;/a&gt; đòi hỏi:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;36 tháng (3 năm) kinh nghiệm PM (PM experience, không nhất thiết vị trí PM).&lt;/strong&gt; Tuy nhiên, có vẻ là thường những người dự thi đều có &lt;strong&gt;khoảng 7-10 năm kinh nghiệm PM.&lt;/strong&gt; Tức là các bác có nhiều năm kinh nghiệm mới đi thi. Nên một giá trị phù hợp là làm PM khoảng 3 năm thì nghĩ đến chuẩn bị PMP, rồi khoảng năm thứ 4 hoặc thứ 5 thì đi thi là tốt, còn bình thường chắc tầm năm thứ 7 trở ra đi thi thì chắc ăn hơn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;35 giờ học hướng dẫn (contact hours, hay 35 PDUs)&lt;/strong&gt;. Việc lấy 35 PDUs có thể hoàn thành thông qua 1 khóa khoảng chục triệu của 1 đối tác của PMI ngay tại Hà Nội nên cũng sẽ nhanh chóng thôi và lấy lúc nào cũng được.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Có nhiều bạn sẽ thắc mắc là cứ làm việc thôi chứ cần gì phải lấy chứng chỉ PMP?
Thực ra thì cũng đúng vì nếu đã có kinh nghiệm 7-10 năm làm PM rồi thì chứng chỉ mang tính chất vật biểu trưng việc mình là professional nhiều hơn.
&lt;em&gt;Tuy nhiên, theo thống kê của PMI bên Mỹ thì những PM có chứng chỉ PMP thì thu nhập tăng lên 25% so với các bạn PM không có PMP.&lt;/em&gt;
Do đó, theo tôi nghĩ vẫn nên có chứng chỉ PMP!
Bạn cũng nên biết là PMI cũng khá khắt khe trong tiêu chí dự thi, nếu 3 năm kinh nghiệm mà không phải kinh nghiệm PM thì họ cũng không chấp nhận cho thi.
Vì vậy, lấy được chứng chỉ PMP của PMI tôi nghĩ là một achievement không nhỏ!
Ít nhất là chúng ta cũng đã hiểu được góc nhìn của PM rất khác so với Dev và vì vậy, bắt buộc phải chuẩn bị kinh nghiệm PM thì mới được thi PMP.
Bản thân tôi ở vị trí BrSE là một vị trí mà nghiệp vụ thì nói chung là overlap khá nhiều với PM, tức là có PM experience.&lt;/p&gt;

&lt;p&gt;Năm 2022 chắc sẽ là 1 năm dài chuẩn bị cho PMP.
Mục tiêu là hè 2023 có thể bắt đầu tính đến dự thi PMP.
Năm 2021 cũng đã là 1 năm khá dài và cũng có ích:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Việc học sách PMBok mất khoảng nửa năm cũng có giá trị.&lt;/li&gt;
  &lt;li&gt;Việc thực hành quản lý các dự án nội bộ của công ty cũ cũng có giá trị kinh nghiệm nhất định.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;về-mặt-kinh-doanh-khởi-nghiệp&quot;&gt;Về mặt kinh doanh, khởi nghiệp&lt;/h3&gt;

&lt;p&gt;Làm dev thì nói chung là “tù”, vì đôi khi kiểu “chỉ đâu đánh đấy”, bao nằm nghiêng thì nghiêng mà nằm ngửa thì ngửa.
Tức là không có bức tranh lớn, nghe theo sự sai bảo của quản lý.
Có câu chuyện hài hước là có lần tôi làm quản lý mà có cậu dev làm lead mà cứ kiên quyết đòi làm 1 chức năng mà &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thực ra có lẽ không làm thì tốt hơn&lt;/code&gt;.
Bởi vì là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bỏ thì thương mà vương thì tội&lt;/code&gt;, làm thì mất công mà thu lại chả được mấy đồng bạc.
Nhưng vì cậu là lead nên cũng ậm ừ để cho cậu có thể diện mà nói chuyện với đàn em, chứ còn nói thực là có khi không sửa cứ để thế nó còn tốt hơn.
Khổ nỗi nếu sửa thì em lại phải theo nó đến cùng em ạ!
Những trường hợp này ấy thì theo kinh nghiệm của tôi cứ đẩy hết về owner, bởi &lt;strong&gt;trong hệ thống tự nhiên có 1 cái gì đó nó bất hợp lý thì chắc chắn là nên để cái ông nắm business chịu trách nhiệm&lt;/strong&gt;.
&lt;em&gt;Đấy, nên nói chung làm dev là “tù” lắm, không đi đến đâu cả đâu&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Năm 2022, mặt quản lý thì chắc lại tiếp tục thực hành thực chiến tích lũy tự tin để năm sau nữa đi thi PMP thôi.
Còn làm dev thì không cần.
Vì vậy, chỉ còn một chuẩn bị là &lt;strong&gt;mặt kinh doanh, khởi nghiệp&lt;/strong&gt;.
Có lẽ bước chuẩn bị trong năm 2022 sẽ là tập dượt xây dựng một community theo chủ đề nhất định.
Có lẽ trong năm 2022 sẽ khám phá “lại” một lĩnh vực công nghệ nào đó, theo kiểu đào sâu và tìm mới.&lt;/p&gt;

&lt;h1 id=&quot;khoảng-cách-công-nghệ&quot;&gt;Khoảng cách công nghệ&lt;/h1&gt;

&lt;h2 id=&quot;câu-chuyện-7-năm-trước&quot;&gt;Câu chuyện 7 năm trước&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Knowledge is inherently a public good. (Jaffe, 1986 &lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&gt;[2]&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tri thức về cơ bản là một tài sản chung (trích Jaffe &amp;lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&amp;gt;[2]&amp;lt;/a&amp;gt;)&lt;/code&gt;.
Nó mang hai hàm nghĩa:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Thứ nhất, không thể giấu giếm tri thức làm của riêng (nó khác với &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trí tuệ&lt;/code&gt; là thứ có thể được đăng ký bảo hộ).&lt;/li&gt;
  &lt;li&gt;Thứ hai, tri thức có thể lan tỏa như thông tin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Câu chuyện về khoảng cách công nghệ có nguồn gốc từ 7 năm trước, khi tôi còn tham gia một chương trình lãnh đạo trẻ.
Thật thú vị là trong một bài thuyết trình cuối khóa về thành tích thực tập tại các công ty.
Khi đó một điều kiện bắt buộc tham gia chương trình lãnh đạo trẻ là sẽ phải tham gia thực tập vài tháng tại 1 doanh nghiệp ở nước ngoài (nhưng không được là doanh nghiệp ở quốc gia của chính mình, tránh cảnh tranh thủ về nước).
Có nhiều người xin sang châu Âu, Mỹ để làm intern ngắn hạn.
Trong các bạn lúc đó phát biểu có một sinh viên PhD khi đó lên thuyết trình, nhưng chỉ sau 2 slide giới thiệu cậu ta kết thúc rằng vì đã ký NDA với doanh nghiệp lớn ở Mỹ về lĩnh vực phần mềm (lớn nhất thế giới) nên không thể trình bày tiếp.
Tất nhiên là thanh niên đó đã lĩnh đủ của các giáo sư vì ở trong trường đại học &lt;strong&gt;cái quan trọng nhất chính là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tri thức&lt;/code&gt;&lt;/strong&gt;, mà tri thức thì là public good.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Đến khi nào anh hiểu được rằng những tri thức vĩ đại và to lớn mà người ta hay gọi là trí tuệ ấy, luôn tồn tại dưới vỏ bọc tầm thường và vớ vẩn, thì anh mới hành động như người có tri thức được.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bây giờ tôi thấy cái trường hợp này đúng là cần phải hiểu là chỉ cần làm mấy cái vớ vẩn, cũng không cần phải to tát rồi không dám chia sẻ mất đi ý nghĩa &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tri thức&lt;/code&gt; trong hàn lâm.
Và có lẽ cũng đúng như lời người thày lúc ấy nói, thực ra những công nghệ to tát và vĩ đại có khi thực ra là những thứ vớ vẩn thôi mà phải không?&lt;/p&gt;

&lt;h2 id=&quot;đến-câu-chuyện-rd-spillovers-và-knowledge-diffusion&quot;&gt;Đến câu chuyện R&amp;amp;D spillovers và knowledge diffusion&lt;/h2&gt;

&lt;p&gt;Các nghiên cứu về mối tương quan giữa không gian công nghệ, lợi nhuận và giá trị thị trường của các firm đã có từ khá lâu.
Thật thú vị là câu chuyện kể trên lại khá liên quan bởi công nghệ chốt trong phần nghiên cứu PhD đó lại vốn là một tri thức đã phổ biến và được đào sâu nghiên cứu ở trong hàn lâm.
Do vậy, ở mức độ nào đó dù là đăng ký bản quyền cho một thứ mà giới hàn lâm đã coi là public good thì nó rất là … vớ vẩn.
Tất nhiên chuyện đăng ký bản quyền là thể hiện giá trị R&amp;amp;D trong các doanh nghiệp, và là một phần quan trọng trong cái gọi là tương tác khoa học giữa hàn lâm và giới công nghiệp.&lt;/p&gt;

&lt;p&gt;Trong nhánh nghiên cứu này có thể kể đến công trình của Jaffe &lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&gt;[2]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt; và rất nhiều công trình sau đó của các nhà nghiên cứu khác.
Thì có lẽ có một khái niệm khá quan trọng là &lt;strong&gt;technological space&lt;/strong&gt; tức là &lt;strong&gt;không gian công nghệ&lt;/strong&gt;.
Ở đây, Jaffe đã xây dựng 1 vector 49 chiều (thu gọn từ 328 classes trong hệ thống patent toàn cầu) để biểu diễn vị trí của 1 firm trong không gian công nghệ.
Ví dụ, hệ thống patent toàn cầu có phân nhóm lĩnh vực thành 328 classes, thì tác giả Jaffe đã group lại thành 49 categories bao quát nhất mọi lĩnh vực công nghệ.
Giả sử công ty $A$ có $N$ patent thuộc 49 categories trên (đánh số từ 1) thì $f_i(A),~i=1,2, \ldots $ là số lượng pattern của $A$ trong lĩnh vực $i$, thì&lt;/p&gt;

&lt;p&gt;$\sum_{i=1}^{49} f_i(A)=N$&lt;/p&gt;

&lt;p&gt;và vector biểu diễn của công ty A (&lt;strong&gt;technological vector&lt;/strong&gt;) là&lt;/p&gt;

&lt;p&gt;$F_A=\begin{bmatrix}
f_1 \newline
f_2 \newline
\vdots \newline
f_{49}
\end{bmatrix}$&lt;/p&gt;

&lt;p&gt;với $f_i=\frac{f_i(A)}{N},~i=1, 2, \ldots $.&lt;/p&gt;

&lt;p&gt;Cứ như vậy, các điểm vector đều nằm trên một mặt phẳng (hyperplane) 49 chiều $\sum_{i=1}^{49}x_i=1$.
Khi 2 công ty có cùng một mối quan tâm về 1 công nghệ nào đó thì bằng chứng sẽ là về mặt R&amp;amp;D ở lĩnh vực đó ($i$ chả hạn) chỉ số sẽ cao lên.&lt;/p&gt;

&lt;p&gt;Jaffe đã nghiên cứu về một hiện tượng thú vị trong innovation đó là &lt;strong&gt;R&amp;amp;D spillovers&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&gt;[2]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt;, hay là sự khuếch tán của tri thức R&amp;amp;D, hay là &lt;strong&gt;knowledge diffusion&lt;/strong&gt;.
Nghiên cứu của Jaffe vào năm 1986 chủ yếu dựa trên quan sát của những người đi trước rằng khi có một công ty trong một lĩnh vực nào đó nâng cao đầu tư vào R&amp;amp;D thì đột nhiên những công ty xung quanh cũng bắt đầu phải chạy theo đầu tư vào mảng đó.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Firms whose research is in areas where there is much research by other firms have, on average, more patents per dollar of R&amp;amp;D and a higher return to R&amp;amp;D in terms of accounting profits or market value, though firms with very low own—R&amp;amp;D suffer lower profits and market value if their neighbors do a lot.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tức là nếu trong cùng 1 mảng mà cty đầu tư nhiều hơn vào R&amp;amp;D so với các công ty khác cùng lĩnh vực thì sẽ có giá trị thị thường lợi nhuận cao hơn.
Và ngược lại nếu đầu tư ít hơn các công ty neighbors thì sẽ bị thụt lùi về lợi nhuận lẫn giá trị thị trường.
Điều này cũng giải thích tại sao các tập đoàn công nghệ lớn không tiếc tiền đầu tư để đứng đầu một lĩnh vực nào đó so với các công ty lân cận.&lt;/p&gt;

&lt;p&gt;Ở đây chúng ta cũng cần hiểu khái niệm &lt;strong&gt;neighbor (lân cận)&lt;/strong&gt; là hiểu theo nghĩa địa lý (geographical).
Rất nhiều nghiên cứu theo sau Jaffe về sau đều chỉ ra mối correlation mạnh giữa khoảng cách địa lý và khoảng cách trong không gian công nghệ (đo bằng vector 49 chiều) &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#audretsch1996r&quot;&gt;[4]&lt;/a&gt;.
R&amp;amp;D spillovers không chỉ tồn tại trong các firms cùng lĩnh vực mà còn tồn tại giữa hàn lâm và công nghiệp theo Jaffe, 1989 &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt;.
Thì nghiên cứu hàn lâm lại có vẻ thúc đẩy tiến bộ trong các công ty xung quanh (gần về địa lý) hoặc có hợp tác với trường đại học ấy &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt;.
Khá nhiều nghiên cứu về &lt;strong&gt;spillovers (knowledge diffusion)&lt;/strong&gt; đã trải thành một mạch dài từ sau Jaffe, 1986 &lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&gt;[2]&lt;/a&gt;:
David và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#david2000public&quot;&gt;[5]&lt;/a&gt; thì nghiên cứu về tác động của public R&amp;amp;D trong việc &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kích hoạt&lt;/code&gt; private R&amp;amp;D.
Baptista và Swann &lt;a class=&quot;citation&quot; href=&quot;#baptista1998firms&quot;&gt;[6]&lt;/a&gt; cũng chỉ ra khá nhiều bằng chứng cho thấy “tốt nhất là đi cùng nhau” giữa các firm trong cùng cluster.
Những nghiên cứu về &lt;strong&gt;open innovation&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#salter2001economic&quot;&gt;[7]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#perkmann2007university&quot;&gt;[8]&lt;/a&gt; cũng cho thấy benefits của kinh tế từ R&amp;amp;D trong các trường đại học.&lt;/p&gt;

&lt;p&gt;Trên đây là để hiểu rõ nguồn gốc của đống nghiên cứu đã có. Tại sao lại có cái đống này, và có căn bản.
Gần đây từ những năm 2020 trở ra có cái gì thì cũng chịu khó xem thì thấy vẫn có người làm theo hướng này kha khá:
Sun và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#sun2021energy&quot;&gt;[9]&lt;/a&gt; thì tập trung vào vấn đề tối ưu hiệu suất sử dụng năng lượng như 1 benefit của R&amp;amp;D spillover.
Fini và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#fini2021attention&quot;&gt;[10]&lt;/a&gt; thì cho thấy là nếu nhà nghiên cứu &lt;strong&gt;khởi nghiệp&lt;/strong&gt; và đóng góp cho việc diffuse tri thức thì sẽ tăng hiệu suất nghiên cứu lên.
Ufuk và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#akcigit2021back&quot;&gt;[11]&lt;/a&gt; thì nghiên cứu trên tập dữ liệu của Pháp thì thấy rõ spillovers đang bị lệch về phía nghiên cứu ứng dụng mà ít về phía nghiên cứu cơ bản.
Hsu và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#hsu2021rich&quot;&gt;[12]&lt;/a&gt; thì nghiên cứu về vấn đề Trung Quốc: spillovers tại Trung Quốc.
Martinez và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#martinez2021knowledge&quot;&gt;[13]&lt;/a&gt; thì trả lời vấn đề crowdfunding ảnh hưởng tới spillovers ra sao.&lt;/p&gt;

&lt;p&gt;Nhìn chung là&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Đến khi nào anh hiểu được rằng những tri thức vĩ đại và to lớn mà người ta hay gọi là trí tuệ ấy, luôn tồn tại dưới vỏ bọc tầm thường và vớ vẩn, thì anh mới hành động như người có tri thức được.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Chào 2022! Và coi như bắt đầu một năm mới nhiều dự định cũng như kế hoạch.
Luôn có những quy luật xã hội nhất định chi phối cộng đồng của chúng ta: ví dụ như các công ty trong giới công nghiệp cũng như trường đại học sẽ có những tương tác qua lại, tạo nên một mô hình spillover (diffusion) tồn tại mãi mãi trong chúng ta.
Tương tác qua lại giữa các firm trong cùng một cluster đôi khi có ích: 1 tiến bộ R&amp;amp;D từ 1 cty có thể tạo ra lợi ích cho các công ty khác.
Điều đó có cái tốt mà cũng đôi khi không: khi có một công ty gia nhập vào 1 lĩnh vực, nếu công ty đó cùng stack công nghệ với cty bạn thì nên welcome, còn không thì phải đối xử với nó như rival.
Bởi vì nếu cùng stack công nghệ thì R&amp;amp;D của công ty đó có tiến bộ gì thì mình sẽ được hưởng lợi.
Ngược lại, nếu khác stack công nghệ và lại cùng bán 1 sản phẩm thì rõ ràng tiến bộ của họ không hề có ích gì cho mình mà lại tranh giành thị trường với mình.
Do đó, phần lớn các công ty giống nhau cả về sản phẩm lẫn stack công nghệ thì cuối cùng cứ tập trung lại thành 1 cluster và spillover (diffuse) cùng nhau.&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;PMPCerti18:online&quot;&gt;PMA PMP Certification Requirements | Project Management Certification &amp;amp; PMP Certification Eligibility.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/PMPCerti18_online/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jaffe1986technological&quot;&gt;Jaffe, A.B. 1986. Technological Opportunity and Spillovers of R&amp;amp;D: Evidence from Firms’ Patents, Profits, and Market Value. &lt;i&gt;The American Economic Review&lt;/i&gt;. 76, 5 (1986), 984–1001.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/jaffe1986technological/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jaffe1989real&quot;&gt;Jaffe, A.B. 1989. Real effects of academic research. &lt;i&gt;The American economic review&lt;/i&gt;. (1989), 957–970.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/jaffe1989real/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;audretsch1996r&quot;&gt;Audretsch, D.B. and Feldman, M.P. 1996. R&amp;amp;D spillovers and the geography of innovation and production. &lt;i&gt;The American economic review&lt;/i&gt;. 86, 3 (1996), 630–640.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/audretsch1996r/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;david2000public&quot;&gt;David, P.A., Hall, B.H. and Toole, A.A. 2000. Is public R&amp;amp;D a complement or substitute for private R&amp;amp;D? A review of the econometric evidence. &lt;i&gt;Research policy&lt;/i&gt;. 29, 4-5 (2000), 497–529.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/david2000public/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;baptista1998firms&quot;&gt;Baptista, R. and Swann, P. 1998. Do firms in clusters innovate more? &lt;i&gt;Research policy&lt;/i&gt;. 27, 5 (1998), 525–540.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/baptista1998firms/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;salter2001economic&quot;&gt;Salter, A.J. and Martin, B.R. 2001. The economic benefits of publicly funded basic research: a critical review. &lt;i&gt;Research policy&lt;/i&gt;. 30, 3 (2001), 509–532.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/salter2001economic/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;perkmann2007university&quot;&gt;Perkmann, M. and Walsh, K. 2007. University–industry relationships and open innovation: Towards a research agenda. &lt;i&gt;International journal of management reviews&lt;/i&gt;. 9, 4 (2007), 259–280.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/perkmann2007university/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;sun2021energy&quot;&gt;Sun, H., Edziah, B.K., Kporsu, A.K., Sarkodie, S.A. and Taghizadeh-Hesary, F. 2021. Energy efficiency: The role of technological innovation and knowledge spillover. &lt;i&gt;Technological Forecasting and Social Change&lt;/i&gt;. 167, (2021), 120659.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/sun2021energy/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;fini2021attention&quot;&gt;Fini, R., Perkmann, M. and Michael Ross, J. 2021. Attention to exploration: The effect of academic entrepreneurship on the production of scientific knowledge. &lt;i&gt;Organization Science&lt;/i&gt;. (2021).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/fini2021attention/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;akcigit2021back&quot;&gt;Akcigit, U., Hanley, D. and Serrano-Velarde, N. 2021. Back to basics: Basic research spillovers, innovation policy, and growth. &lt;i&gt;The Review of Economic Studies&lt;/i&gt;. 88, 1 (2021), 1–43.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/akcigit2021back/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hsu2021rich&quot;&gt;Hsu, D.H., Hsu, P.-H. and Zhao, Q. 2021. Rich on paper? Chinese firms’ academic publications, patents, and market value. &lt;i&gt;Research Policy&lt;/i&gt;. 50, 9 (2021), 104319.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hsu2021rich/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;martinez2021knowledge&quot;&gt;Martı́nez-Climent Carla, Mastrangelo, L. and Ribeiro-Soriano, D. 2021. The knowledge spillover effect of crowdfunding. &lt;i&gt;Knowledge Management Research &amp;amp; Practice&lt;/i&gt;. 19, 1 (2021), 106–116.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/martinez2021knowledge/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Sat, 01 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/new-year/</link>
        <guid isPermaLink="true">https://wanted2.github.io/new-year/</guid>
        
        <category>Event</category>
        
        <category>Newyear event</category>
        
        <category>technological distance</category>
        
        <category>innovation</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>Chia tay 2021!</title>
        <description>&lt;p&gt;Đây là bài post thứ 61 của blog AiFi trong năm 2021, cũng là bài viết chia tay 2021, trong tâm thế đón chờ 2022 tươi mới hơn.
Theo quan điểm làm việc scrum, thì coi như đây là thời điểm kết thúc 1 chu kỳ, cũng là lúc làm một số việc để nhìn lại một năm đã qua (bao gồm cả GKPT hay &lt;em&gt;Good, Keep, Problem, Try&lt;/em&gt;).
2021年中61番目の投稿です．
2021年と別れて，2022年を迎える時期の投稿です．
一年間を1スプリントとすると，いろいろなことができたと思いますので，スクラムの行事として，レビューとレトロ会をここで開催したいと思います．
&lt;em&gt;Good, Keep, Problem, Try&lt;/em&gt; も含めてやります．
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;nhìn-lại-năm-2021-của-blog-aifi&quot;&gt;Nhìn lại năm 2021 của blog AiFi&lt;/h1&gt;

&lt;h2 id=&quot;nhìn-từ-thống-kê-người-dùng&quot;&gt;Nhìn từ thống kê người dùng&lt;/h2&gt;

&lt;p&gt;Hiện tại AiFi blog sử dụng Google Analytics để track và lấy thống kê người dùng.
Các sự kiện như view, scroll, referal, … được báo cáo theo phút lên server của Google.&lt;/p&gt;

&lt;p&gt;Đầu tiên là thống kê về người dùng và nguồn giới thiệu.
Trong năm 2021, blog tuy mới ra mắt và còn nhiều khó khăn vất vả nhưng đã thu hút được 552 user mới từ khắp nơi trên thế giới.
&lt;strong&gt;552 người dùng này đã ghi lại 7309 sự kiện.&lt;/strong&gt;
Một con số đáng khích lệ với blog mới 1 năm tuổi đời.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Một điểm đáng chú ý là dù facebook.com là nơi tác giả hay chia sẻ bài viết, nhưng &lt;strong&gt;user lại phần lớn đến từ 2 nguồn: google và direct&lt;/strong&gt;.
Về yếu tố địa lý thì đa phần người dùng đến từ &lt;strong&gt;Việt Nam, Mỹ và Nhật Bản&lt;/strong&gt;.
Các nước khác vẫn chưa đóng tỷ trọng lớn trong cơ cấu người dùng của AiFi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tỷ lệ người dùng của AiFi gia tăng tính từ tháng &lt;strong&gt;7&lt;/strong&gt;.
Trong năm 2021, &lt;strong&gt;số lượng sự kiện &lt;em&gt;user engagement&lt;/em&gt; là 1852, và số &lt;em&gt;page view&lt;/em&gt; là 2622 lượt&lt;/strong&gt;.
Ngoài ra, 3 bài viết đạt số lượng truy cập cao nhất (không tính trang chủ) là:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/mOCR-mlkit-androidx-example/&quot;&gt;mOCR: A real-time application of OCR with Google MLKit and Android CameraX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/adobe-creative-cloud/&quot;&gt;Adobe Creative Cloud: An All-in-One Platform for Creators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/aws-lambda-spacy-mxnet-possible-but-shouldnt/&quot;&gt;Implementing a complex system in AWS Lambda: Should or shouldn’t?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sự “vùng lên” của bài viết &lt;a href=&quot;/adobe-creative-cloud/&quot;&gt;Adobe Creative Cloud: An All-in-One Platform for Creators&lt;/a&gt; thật thú vị vì bài viết được xuất bản trên blog AiFi vào tháng cuối năm nhưng lại đứng thứ nhì.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-03.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Về hệ điều hành, trình duyệt và ngôn ngữ đầu vẫn là &lt;strong&gt;Windows, Chrome và English&lt;/strong&gt;.
Theo sau lần lượt là &lt;strong&gt;MacOS, Safari và tiếng Nhật&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-04.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nhìn-từ-kết-quả-tìm-kiếm&quot;&gt;Nhìn từ kết quả tìm kiếm&lt;/h2&gt;

&lt;p&gt;Kết quả tìm kiếm về “AiFi Caineng” trên google.com và Bing Search trong ngày 31 tháng 12 năm 2021 như sau:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aifi-search-engines-2021.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả tìm kiếm từ khóa “aifi” và thậm chí “aifi caineng” quả là hơi nghèo nàn và dễ bị lẫn vào các từ khóa tìm kiếm khác như “wifi” chẳng hạn.
Đây cũng là 1 thiếu sót do blog mới chỉ 1 năm, và tác giả vẫn đang bận bịu công việc chính cuả tác giả.
Tuy nhiên, từ năm 2022, ở mức độ nhất định việc nâng rank trong các cỗ máy tìm kiếm từ khóa sẽ được &lt;strong&gt;tối ưu hóa&lt;/strong&gt; nhằm đưa tri thức của AiFi đến với đông đảo bạn đọc và nâng cao chất lượng phục vụ.&lt;/p&gt;

&lt;h1 id=&quot;good-keep-problem-try&quot;&gt;Good, Keep, Problem, Try&lt;/h1&gt;

&lt;p&gt;Việc chạy sprint kéo dài 1 năm quả là hơi lạ, tuy nhiên là cũng dễ hiểu vì viết blog chỉ là việc phụ làm trong thời gian rảnh rỗi của tác giả.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Good&lt;/th&gt;
      &lt;th&gt;Keep&lt;/th&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Try&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Đã tạo được và thu hút lượng người dùng nhất định.&lt;/td&gt;
      &lt;td&gt;Duy trì tần suất chia sẻ bài viết.&lt;/td&gt;
      &lt;td&gt;Thứ hạng trên search engine chưa cao.&lt;/td&gt;
      &lt;td&gt;Tối ưu hóa SEO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Tối ưu hóa từ khóa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Tối ưu thẻ HTML, …&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Chưa tạo ra thu nhập từ blog&lt;/td&gt;
      &lt;td&gt;Xem xét đưa vào và tối ưu hóa quảng cáo.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Các nguồn Google và Facebook đã đem đến lượng người dùng nhất định.&lt;/td&gt;
      &lt;td&gt;Tiếp tục duy trì quảng bá trên Google và Facebook.&lt;/td&gt;
      &lt;td&gt;Nguồn Facebook chưa đem lại nhiều người dùng mới.&lt;/td&gt;
      &lt;td&gt;Tối ưu hóa quảng bá blog trên Facebook.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Một số nguồn cấp khác như Twitter và LinkedIn vẫn chưa đem lại nhiều người dùng.&lt;/td&gt;
      &lt;td&gt;Lên chiến lược quảng bá trên các nền tảng này.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;tổng-kết&quot;&gt;Tổng kết&lt;/h1&gt;

&lt;p&gt;Kết thúc Sprint 2021, hướng tới Sprint 2022, blog AiFi xin cám ơn đông đảo bạn đọc, đặc biệt là 552 người dùng đã có, vì sự quan tâm và thịnh tình trong năm qua.
Trong năm 2022, AiFi sẽ tiếp tục cập nhật và mong muốn lan tỏa tri thức cho anh em, với phương châm, troll trước học sau.&lt;/p&gt;
</description>
        <pubDate>Fri, 31 Dec 2021 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/year-end/</link>
        <guid isPermaLink="true">https://wanted2.github.io/year-end/</guid>
        
        <category>Event</category>
        
        <category>Year-end event</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
      </item>
    
  </channel>
</rss>
