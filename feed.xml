<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AiFi</title>
    <description>An AI Engineer&apos;s blog</description>
    <link>https://wanted2.github.io/</link>
    <atom:link href="https://wanted2.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 09 Feb 2022 07:22:35 +0900</pubDate>
    <lastBuildDate>Wed, 09 Feb 2022 07:22:35 +0900</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      <item>
        <title>Seq2Seq và kiến trúc Encoder-Decoder</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;/aws-account/&quot;&gt;Hôm trước ngồi đọc về GAN thấy nhiều bài trên vài ngàn tới 20,000 trích dẫn&lt;/a&gt;, hôm nay đọc tiếp cái &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence-to-sequence&lt;/code&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Seq2Seq&lt;/code&gt;) cũng thấy có cả 30k-40k cũng có.
Thế nên là cái &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Seq2Seq&lt;/code&gt; này cũng phải theo bài cũ: chỉ đọc những cái có trung bình trên 300 citations/năm (GAN thì ngưỡng threshold là 200 citations/năm, nhưng sang cái &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Seq2Seq&lt;/code&gt; này là chỉ cần tìm hiểu những bài có độ tăng trưởng trên 300 trích dẫn/năm).
Chứ đọc làm sao mà hết được?
Ví dụ mấy bài từ năm 2018 mà tính đến nay 2022 là 4 năm mà dưới 1200 trích dẫn là nhìn chung độ tăng trưởng thấp.
Tập hợp lại những papers có độ tăng trưởng mạnh từ tầm 2013 trở lại thì liên quan tới chủ để này tầm hơn trăm tấm, nói chung thượng vàng hạ cám. 
Có bài như bài gốc Transformer (Attention is all you need, &lt;a class=&quot;citation&quot; href=&quot;#vaswani2017attention&quot;&gt;[1]&lt;/a&gt;) mới ra đời từ 2017 mà đã hơn 35k trích dẫn!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Seq2Seq&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#kalchbrenner2013recurrent&quot;&gt;[2, 3, 4]&lt;/a&gt; là một giải pháp kiến trúc được dùng khá nhiều trong các bài toán NLP và vision như Neural Machine Translation (NMT, &lt;a class=&quot;citation&quot; href=&quot;#sutskever2014sequence&quot;&gt;[3]&lt;/a&gt;), Question-Answering (QA, &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2016squad&quot;&gt;[5]&lt;/a&gt;), Visual Question Answering (VQA, &lt;a class=&quot;citation&quot; href=&quot;#antol2015vqa&quot;&gt;[6, 7]&lt;/a&gt;), Text Summarization (TS, &lt;a class=&quot;citation&quot; href=&quot;#rush2015neural&quot;&gt;[8, 9]&lt;/a&gt;) và Video-To-Text (VTT, &lt;a class=&quot;citation&quot; href=&quot;#venugopalan2015sequence&quot;&gt;[10]&lt;/a&gt;).
Bài toán Image Captioning thì cũng có thể ứng dụng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nếu thông minh hơn một tí, sử dụng object detector để detect attributes và coi attribute sequence đó thành input vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; như trong bài &lt;em&gt;Semantic Attention (SA) &lt;a class=&quot;citation&quot; href=&quot;#you2016image&quot;&gt;[11]&lt;/a&gt;&lt;/em&gt; hay &lt;em&gt;Densecap &lt;a class=&quot;citation&quot; href=&quot;#johnson2016densecap&quot;&gt;[12]&lt;/a&gt;&lt;/em&gt;.
Nên nhìn chung là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; là 1 technique mà có thể dùng vào nhiều nhiệm vụ và rất hữu dụng &lt;a class=&quot;citation&quot; href=&quot;#luong2015multi&quot;&gt;[13]&lt;/a&gt;.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;giới-thiệu-model-seq2seq&quot;&gt;Giới thiệu model Seq2Seq&lt;/h1&gt;

&lt;p&gt;Nỗi lòng người làm thày mà hướng dẫn sinh viên thì tùy level mà kỳ vọng thì nó cũng khác nhau, và với những level cao như postdoc là luôn có sự mong muốn nhất định.
Đó là phải vượt qua &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thử thách (challenges)&lt;/code&gt; của thày.
Thì cái challenge đây có hai nghĩa là cuộc thi và thử thách, thì nói thực là cuộc thi không cần đâu, chỉ cần vượt qua thử thách tối thiểu của thày trong cái kỹ năng làm nghiên cứu thôi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ok em&lt;/code&gt; rồi.&lt;/p&gt;

&lt;p&gt;Trong thực tế kinh nghiệm thì tôi thấy mấy cái thử thách cũng không phức tạp lắm đâu, đặc biệt là trong lĩnh vực NLP+Vision này thì tôi thấy cũng chỉ có vài bài như Image Captioning, Video-to-Text hay VQA.
Code thì nhan nhản trong cộng đồng nghiên cứu (đặc thù của ngạch nghiên cứu là kế thừa, vì người ta công bố mà mình không dùng thì phí), rồi tài liệu thì mảng NLP với Vision các ông cũng publish trên ArXiV với mấy hội nghị open, chứ nào có giấu giếm gì nhau?
Thế mà không hiểu vì sao vẫn không vượt qua được cho thày?
Xem lại rốt cuộc là tôi nghĩ thì có lẽ nên thêm câu hỏi sau vào bộ dữ liệu QA:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q: Không làm muốn có ăn thì ăn gì bây giờ?
A: ??? (câu hỏi tự luận nha)
Context: Hãy xem video của Prof. Huan Rose&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thì nhìn chung là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; là một model thử thách như vậy.
Với những bài đặc biệt chỉ của NLP như NMT, Text Summarization hay QA thì &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; đã mở ra hẳn cả một mảng riêng mà về sau còn có thêm mấy cái pretraining encoder/decoder như BERT, AlBERT, RoBERT, BART, … mà chủ yếu là representation learning.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mô hình &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; đơn giản chỉ gồm một chuỗi input $\mathbb{x}_1, \mathbb{x}_2, \ldots, \mathbb{x}_m$. 
Chuỗi này đương nhiên là biểu diễn vector (embedding) sau khi đã preprocessing qua tokenizer và thay thế từ không có trong vocabulảy bởi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UNK&lt;/code&gt;.
Sau đó là &lt;strong&gt;encoder&lt;/strong&gt; với các trạng thái $\mathbb{h}_1, \mathbb{h}_2, \ldots, \mathbb{h}_m$ cũng như biểu diễn đầu ra của encoder là $\mathbb{z}_1, \mathbb{z}_2, \ldots, \mathbb{z}_m$.
Encoder chính là một mạng trí tuệ nhân tạo dạng Long-Short Term Memory (LSTM) mà chúng ta sẽ nói sau.
Bây giờ thì chúng ta cần aggregate chuỗi biểu diễn đầu ra của encoder thành biến vector \(\mathbb{c}_t=\sum_{i=1}^m\alpha_i^t\mathbb{z}_i\).
Ở mỗi bước \(t\) thì &lt;strong&gt;decoder&lt;/strong&gt; LSTM sẽ nhận input là output của bước \(t-1\) là \(\mathbb{g}_{t-1}\) và biến context \(\mathbb{c}_t\) để đưa ra output \(\mathbb{g}_t\).&lt;/p&gt;

\[\mathbb{g}_t=\mbox{LSTM}\left(\mathbb{g}_{t-1},\mathbb{c}_t\right)\]

  &lt;p&gt;ứng với mỗi \(\mathbb{g}_t\) sẽ được giải mã thành một ký tự trong ngôn ngữ đầu ra.
Quá trình này kết thúc khi ký tự &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EOS&lt;/code&gt; được giải mã ra.
Chuỗi đầu vào \(\mathbb{x}_i\) và chuỗi đầu ra của decoder \(\mathbb{g}_j\) có thể có độ dài khác nhau.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Trong bài toán NMT, thì chuỗi đầu vào có thể là tiếng Anh và chuỗi đầu ra là tiếng Nhật.
Nhưng ngược lại thì sẽ cần tokenizer bằng tiếng Nhật.
Trong bài toán Text Summarization (TS), thì cả đầu ra và đầu vào đều sẽ cùng ngôn ngữ, nhưng đầu ra sẽ là một chuỗi ngắn gọn xúc tích hơn.
Cái gọi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ngắn gọn, xúc tích hơn&lt;/code&gt; sẽ được định nghĩa và học thông qua dữ liệu.
Thì NMT có bộ WMT2014 &lt;a class=&quot;citation&quot; href=&quot;#bojar2014findings&quot;&gt;[14]&lt;/a&gt;, WMT2017 &lt;a class=&quot;citation&quot; href=&quot;#bojar2017findings&quot;&gt;[15]&lt;/a&gt;, còn TS thì có bộ DUC &lt;a class=&quot;citation&quot; href=&quot;#paul2004introduc&quot;&gt;[16]&lt;/a&gt;.
Còn bài toán QA thì chuỗi đầu vào là một câu hỏi còn đầu ra là 1 câu trả lời.
Cái hay của QA là đôi khi có thêm chuỗi context (hay gọi là gợi ý), ví dụ như hỏi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Bạn sống ở đâu?&lt;/code&gt; thì có thêm context là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tôi sống ở Nhật&lt;/code&gt; thì máy sẽ trả lời luôn là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Nhật&lt;/code&gt;.
QA thì có bộ SQuAD v1 &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2016squad&quot;&gt;[5]&lt;/a&gt; và v2 &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2018know&quot;&gt;[17]&lt;/a&gt; với hơn trăm ngàn bộ câu hỏi (nghe như luyện thì TOEIC).
Metrics đánh giá thì có cái BLUE-4 score, CIDEr, ROGUE &lt;a class=&quot;citation&quot; href=&quot;#lin2004rouge&quot;&gt;[18]&lt;/a&gt; là có thể dùng để đánh giá chuỗi đầu ra có phù hợp không.
Chúng ta sẽ đi sâu thêm vào từng chi tiết sau.&lt;/p&gt;

&lt;p&gt;Các task trong NLP thì là như vậy, dữ liệu, code và metrics hầu như có sẵn.
Thì cũng là kết nối với Vision là khoảng CVPR tầm 2015-2016 gì đó có mấy cái Workshops nói về làm sao để có thể tích hợp nhiều modal (multi-modal) để đưa ra những giải pháp tốt hơn.
Mọc ra trước mắt lúc đó thì có 3 tasks nằm trong định hướng: Image Captioning, Video-to-Text và VQA.
Nói chung nghe lúc đó có vẻ mới, nhưng chỉ có task là mới thôi chứ còn những cái để thực thi những task ấy các sếp promote các task ấy lên họ đã chuẩn bị hết rồi.
Technique thì có sẵn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;, CNN, RNN, LSTM, Faster R-CNN để extract attributes dưới dạng object detections, …
Metrics thì vì output vẫn là chuỗi text nên lại xài lại BLUE-4, CIDEr rồi ROGUE thôi.
Nên coi như nền tảng rất sẵn rồi, chỉ nhảy vào làm thí nghiệm và viết paper rồi … ăn!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Vậy tại sao vẫn cứ không đến nơi đến chốn được?&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Tôi nghĩ vấn đề đầu tiên lúc làm Image Captioning là thiếu &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alignment&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Lúc làm cái Video-to-Text thì cái visual semantic embedding (VSE) là không có. Mà nói thẳng ra thì cái ấy chính mình &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chủ động&lt;/code&gt; nghĩ ra mà làm chứ?&lt;/li&gt;
  &lt;li&gt;CÒn cái VQA thì lúc ấy nói thật là hai cái captioning với VTT nó đã &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bết bát&lt;/code&gt; sẵn rồi thì sẽ rất khó vì bản thân VQA tuy chỉ thay cái context là text bởi hình ảnh, nhưng chất lượng detector, rồi alignment mà hai bước trên chưa làm tốt thì sang đến VQA coi như … vỡ trận.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tuy nhiên, nếu ở vào vai trò anh Postdoc mà làm cái mảng này tôi vẫn sẽ đề xuất luồng làm việc &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Image Captioning --&amp;gt; Video-to-Text --&amp;gt; VQA&lt;/code&gt;.
Bởi luồng làm việc này nó theo chiều hướng tích lùy dần know-how để làm việc ngày càng tốt hơn.
Thứ hai, là vì nó có một vài điểm trigger giữa chừng nên nếu làm postdoc các bạn có thể submit bài báo tại các thời điểm ấy như là làm Image Captioning xong thì 1 bài, …
&lt;strong&gt;Nhưng rốt cuộc cái quan trọng nhất vẫn là phải có làm có ăn.&lt;/strong&gt;
Còn không muốn làm thì hỏi giáo sư Huấn để biết phải làm gì.&lt;/p&gt;

&lt;h2 id=&quot;recurrent-neural-nets-long-short-term-memory-và-gated-recurrent-units&quot;&gt;Recurrent Neural Nets, Long-Short Term Memory và Gated Recurrent Units&lt;/h2&gt;

&lt;p&gt;Thôi nói chung là cái trường hợp postdoc ở trên là một ví dụ điển hình thôi, mà trường hợp ấy tôi nghĩ đã được giáo sư Huấn chỉ bảo tận tình rồi nên không cần lo lắng nữa.
Chúng ta quay lại với chủ để chính của hôm nay là giới thiệu về &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.
Để implement được &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; thì chúng ta cần 1 mô hình nhận chuỗi và output đầu ra cũng là một chuỗi khác, và quan trọng hơn là có thể huấn luyện bằng thuật toán Gradient Descent, cụ thể hơn là &lt;em&gt;Stochastic Gradient Descent (SGD, &lt;a class=&quot;citation&quot; href=&quot;#bottou2007tradeoffs&quot;&gt;[19]&lt;/a&gt;)&lt;/em&gt;.
Thì cái học củ SGD là mình chỉ random sample một phần của dữ liệu học để tính được gradient và update weight của model.
Chúng ta sẽ có &lt;em&gt;learning rate&lt;/em&gt; mà nhỏ thì chậm, còn cao quá thì mô hình sẽ khó hội tụ.&lt;/p&gt;

&lt;p&gt;Lời giải thì đơn giản nhất là có mô hình &lt;em&gt;Recurrent Neural Nets (RNN, &lt;a class=&quot;citation&quot; href=&quot;#rumelhart1986learning&quot;&gt;[20]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#goodfellow2016deep&quot;&gt;[21]&lt;/a&gt;)&lt;/em&gt; mà mô hình và công thức forward pass như bên dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn.svg&quot; alt=&quot;rnn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Công thức rất rõ ràng nên có thể sử dụng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt; để implement forward pass khá đơn giản với vài dòng code chơi thôi.
Cái khó khăn là khi đã tính xong kết quả các biến \(u_t, o_t, h_t, x_t\) và hàm loss \(\mathcal{L}\) thì &lt;strong&gt;làm sao update được các weight \(\mathbf{V}, \mathbf{W}, \mathbf{b}, \mathbf{c}\)&lt;/strong&gt;?&lt;/p&gt;

&lt;p&gt;Thì có hệ thống các công thức phía dưới:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bptt.svg&quot; alt=&quot;bptt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tuy hệ thống công thức này phức tạp hơn, nhưng vẫn có thể implement bằng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt; và thực hiện train với SGD (tầm khoảng vài chục dòng code nữa).
Nhìn lại thì công thức và cả phương thức implement RNN là cũng rất &lt;strong&gt;rõ ràng rồi&lt;/strong&gt;, vì vậy, nếu không thể implement được thì … thày cũng chịu không thể giải thích tại sao được?&lt;/p&gt;

&lt;p&gt;Nhược điểm của RNN là vấn đề ghi nhớ &lt;em&gt;long-term dependencies&lt;/em&gt;: ví dụ chúng ta có 1 gradient \(\mathbf{g}\) và giả sử là RNN không có non-linear activations nào thì chúng ta có thể giả sử thêm là cứ sau mỗi time step thì Jacobian matrix là \(\mathbf{J}\), vậy sau \(n\) bước thì gradient của chúng ta sẽ biến thành \(\mathbf{J}^n\mathbf{g}\) và nếu nó có một giá trị riêng \(\lambda\) mà giá trị khác \(\pm 1\), thì sẽ dẫn đến vấn đề là gradient triệt tiêu hoặc tiến ra vô cùng sau hữu hạn bước tính.
Để khắc phục nhược điểm này thì một giải pháp được đưa ra là đưa &lt;em&gt;self-loop&lt;/em&gt; vào cùng các &lt;em&gt;non-linear gates&lt;/em&gt; như forget gate, output/input gates để control giá trị.
Một kiến trúc đã làm được việc đó là &lt;em&gt;Long-Short Term Memory (LSTM, &lt;a class=&quot;citation&quot; href=&quot;#hochreiter1997long&quot;&gt;[22, 23]&lt;/a&gt;)&lt;/em&gt;.
Mỗi cổng đều chứa &lt;em&gt;sigmoid&lt;/em&gt; activation để đưa giá trị về khoảng \((0,1)\) dẫn đến các giá trị output ở các cổng có thể bị shut-off.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lstm.svg&quot; alt=&quot;lstm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Việc thêm gates và non-linear activation (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sigmoid&lt;/code&gt;) để control giá trị là hợp lý với LSTM.
Tuy nhiên, nếu nhìn vào cái đám công thức thì trong LSTM khi update và forget của \(\mathbf{h}_{t+1}\) là đồng thời thực hiện song song việc forget và quyết định có update hay không.
&lt;strong&gt;Gated Recurrent Unit (GRU, &lt;a class=&quot;citation&quot; href=&quot;#cho2014properties&quot;&gt;[24]&lt;/a&gt;)&lt;/strong&gt; thực hiện chia ra, tức là thay vì 1 cổng là làm luôn cả hai chức năng, thì tạo 2 cổng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update&lt;/code&gt; và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reset&lt;/code&gt; cho từng mục đích ấy.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nhìn chung, công thức toán của các kiến trúc RNN/LSTM/GRU đều không phức tạp.
Nếu có thời gian, các bạn có thể tự thực hiện implement các kiến t
rúc trên bằng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy&lt;/code&gt;.
Forward pass thì chắc là đơn giản hơn, tuy nhiên backward pass với SGD thì sẽ đòi hỏi một chút công sức (vài chục dòng code nữa).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nói một chút về hàm loss \(\mathcal{L}\) khi train NMT chẳng hạn, vì chúng ta biết chuỗi &lt;strong&gt;đúng&lt;/strong&gt; \(\mathbf{y}=(y_1,y_2,\ldots,y_n)\) nên ở mỗi step \(0\leq t\leq n-1\), ta sẽ xem xét vector output \(\mathbf{u}_t\) (là softmax) và lấy xác suất của vị trí thứ \(y_t\), và tất nhiên lấy log nghịch đảo như mọi khi:&lt;/p&gt;

\[\mathcal{L}=\sum_t-\log u_{y_t}\]

&lt;p&gt;Khi testing, chúng ta sẽ sử dụng &lt;strong&gt;beam search&lt;/strong&gt; để tìm ra các chuỗi phù hợp (sẽ giải thích thêm ở phần sau).&lt;/p&gt;

&lt;h2 id=&quot;kiến-trúc-seq2seq-s2s&quot;&gt;Kiến trúc Seq2Seq (S2S)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/san01.png&quot; alt=&quot;seq2seq&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; đòi hỏi cần có GPU phù hợp và tận dụng xử lý song song.
1 đặc điểm không thể tránh khỏi của bộ dữ liệu là độ dài các chuỗi không đồng bộ, nên có thể tạo ra giải pháp là &lt;strong&gt;fill thêm ký tự trống vào để cho tất cả các chuỗi cùng độ dài&lt;/strong&gt;.
Việc này sẽ đòi hỏi thêm khá nhiều memory, vì vậy, một giải pháp khá phù hợp là chia ra thành các &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxi-batches&lt;/code&gt; rồi trong từng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxi-batch&lt;/code&gt; thì lại chia tiếp thành các &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mini-batch&lt;/code&gt;.
Trong mỗi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mini-batch&lt;/code&gt; như hình dưới thì mới thực hiện fill ký tự trống.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn-mini-batches.png&quot; alt=&quot;rnn-mini-batches&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Training &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; thường mất 5-15 epochs (1 lượt qua toàn bộ corpus).
Bạn cũng nên thiết lập một tiêu chí dừng lại khi validation error rate không cải tiến thêm.
Training lâu thêm không những không tạo thêm cải tiến mà có thể dẫn tới overfitting.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Testing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;&lt;/strong&gt;. Giả sử chúng ta có một corpus 50,000 từ.
Vậy trong vector \(\mathbf{u}_t\in\mathbb{R}^{50,000}\), ta sẽ chọn phần tử có xác suất cao nhất (dựa trên embedding logits) để output ra ở bước thứ \(t\).
Giải pháp này tức là &lt;strong&gt;chỉ chọn cái tốt nhất ở mỗi bước&lt;/strong&gt;.
Vấn đề ở đây là nếu chỉ chọn cái tốt nhất độc lập ở từng bước thì khi đến một thời điểm \(t&apos;\), câu dịch trở lên không đúng thì không thể quay lại sửa từ đầu được.
Do đó, cách tốt hơn là dùng &lt;strong&gt;Beam Search&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ở bước \(t=0\) chúng ta chọn một beam gồm \(n\) top words \(p(w_0^1), p(w_0^2), \ldots, p(w_0^n)\).&lt;/li&gt;
  &lt;li&gt;Ở thời điểm tiếp theo \(t=1\), ta lại chọn tiếp \(n\) top words \(p(w_1^1), p(w_1^2), \ldots, p(w_1^n)\) và làm tiếp:
    &lt;ul&gt;
      &lt;li&gt;nhân xác xuất của cụm 2 từ liên tiếp \(p(w_0^{i_0})p(w_1^{i_1}), 1\leq i_0, i_1\leq n\).&lt;/li&gt;
      &lt;li&gt;Chọn top \(n\) cặp từ \((i_0,i_1)\) giữ lại.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ở thời điểm \(t=2\), ta lại chọn tiếp top \(n\) words \(p(w_2^1), p(w_2^2), \ldots, p(w_2^n)\), và lại nhân xác suất để tìm ra top \(n\) triplets \((i_0, i_1, i_2)\) để giữ lại trong beam.&lt;/li&gt;
  &lt;li&gt;Cứ tiếp tục như vậy đến khi gặp &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EOS&lt;/code&gt; thì bỏ câu dịch đó ra khỏi beam và giảm size của beam đi 1.&lt;/li&gt;
  &lt;li&gt;Khi kích thước của beam giảm xuống còn 0 thì dừng beam search.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Một đoạn code demo của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;beam search&lt;/code&gt;:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/6397ad4648f7c891ccd41513bb8206e5.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Ngoài ra, nếu có thời gian các bạn có thể tìm hiểu thêm các thủ thuật để tăng hiệu suất khi inference như fusion (ensembling), reranking, hoặc khi train như tăng kích thước vocabularies, training data, back translation, round-trip training, guided alignment training, .v.v… (&lt;a class=&quot;citation&quot; href=&quot;#koehn2009statistical&quot;&gt;[25]&lt;/a&gt;).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tất nhiên, cũng cần chú ý là implement những cái này ở local cũng chỉ để học hỏi thôi nhé.
&lt;strong&gt;Chứ còn triển khai vận hành thực sự thì nó có những solutions có sẵn chạy ầm ầm trên AWS/Azure/GCP rồi.&lt;/strong&gt;
Mà có khi nó còn thành dịch vụ đem bán khắp nơi rồi ấy chứ (có cả tiếng Việt luôn nhé).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;transformers-và-bert&quot;&gt;Transformers và BERT&lt;/h2&gt;

&lt;h3 id=&quot;transformer-1&quot;&gt;Transformer &lt;a class=&quot;citation&quot; href=&quot;#vaswani2017attention&quot;&gt;[1]&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/transformer.png&quot; style=&quot;float: right; margin: 10px; width: 25%;&quot; /&gt;
Deep Networks nhưng không có RNNs hay CNNs gì cả!
Transformer cũng có thể coi là một dạng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nhưng không chỉ gồm các tầng FC, embedding, …
Điểm khác biệt là context (hay là attention) đã được chuyển thành tầng đề xuất &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self-attention&lt;/code&gt;.
Nhìn chung, Transformer hay cả các kiến trúc BERT về sau cũng kế thừa tương đối nhiều tính chất của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.
Bản thân Transformer cũng là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nhưng bỏ đi RNN/CNN.
&lt;img src=&quot;/assets/images/mha.png&quot; style=&quot;float: left; margin: 10px; width: 15%;&quot; /&gt;
Trong kiến trúc thì ngoài residual connection chỉ có điểm đáng chú ý là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multi-head attention&lt;/code&gt;.
Như hình vẽ bên trái, có 3 vector mới được đưa ra là \(K, V, Q\), tương ứng cho &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keys, values, queries&lt;/code&gt;.
Thì công thức attention (không có mask) sẽ như sau:&lt;/p&gt;

\[\mbox{Attention}\left(Q,K,V\right)=\mbox{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V\]

&lt;p&gt;trong đó, \(d_k\) là số chiều của vector query \(Q\) và key \(K\).
Trong trường hợp dùng mask (ở decoder chả hạn) thì chủ yếu là để kết hợp với position embedding để hạn chế ảnh hưởng của các token trước thời điểm $t$ chả hạn.&lt;/p&gt;

&lt;p&gt;Nói chung lý thuyết của Transformer cũng không có gì quá phức tạp.
Bản thân Transformer cũng đã được implement chi tiết trong thư viện &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tensor2tensor&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#vaswani2018tensor2tensor&quot;&gt;[26]&lt;/a&gt; nên như tôi đã nói, cái mảng này thực ra tài liệu tài nguyên thì vô cùng dồi dào.
Ngoài ra một thư viện nữa là HuggingFace &lt;a class=&quot;citation&quot; href=&quot;#wolf2020transformers&quot;&gt;[27]&lt;/a&gt; với model zoo dồi dào cũng rất đáng xem.
&lt;strong&gt;Thế là lại nói lại câu chuyện postdoc ở trên: &lt;em&gt;không hiểu tại sao lại không làm được?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Thì tôi nghĩ là ngoài 3 yếu tố đã nói ở mục đầu, thì quá tập trung vào competition! 
Từ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;challenge&lt;/code&gt; mà người thày nói cũng có nghĩa là cuộc thi (competition) nhưng cũng có nghĩa là thử thách.
Mà vấn đề là cái nghĩa sau nó sẽ quan trọng hơn.
Bởi mảng nghiên cứu là giao thoa giữa Vision và NLP, nên là nếu chỉ tập trung vào 1 competition nhất định của vision mà không cập nhật bên NLP thì có vẻ là tầng nghĩa sau (thử thách) là sẽ bị lose track.
Đấy thế nên là cũng nhắc lại lời người thày: “&lt;strong&gt;Thực ra nhiệm vụ của người làm thày là giảm bớt số lượng những con bò trên thế giới xuống&lt;/strong&gt;”.
Tôi nghĩ đúng là để biến một con bò thành người cũng khá vất vả, mà thất bại mãi mới thành công một phát cũng là chuyện thường!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Chốt là cuộc thi, competition là không quan trọng đâu.
Cái quan trọng với postdoc là rèn luyện được một cái tác phong nghiên cứu tốt để có thể hướng tới lâu dài.
&lt;strong&gt;Chứ mất công giành được vị trí trong 1 competition trước mắt, nhưng lại mất vị trí tenure cả đời thì luận về TRÍ là thua rồi!&lt;/strong&gt;
Đã xác định gắn bó lâu dài thì không cần thi cử gì cho mất thời gian ra, lo mà tìm ra cống hiến để đời và gắn bó lâu dài thì hơn.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Mà nhìn lại tôi nghĩ nguyên nhân sâu xa dẫn tới sự vụ hậu quả postdoc kể trên là &lt;strong&gt;thiếu kỹ năng quản lý dự án (Project Management)&lt;/strong&gt;.
Triệu chứng bệnh rõ ràng nhất ở đây là&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Việc không quản lý được các tri thức về thử thách, dẫn đến bị đánh lạc hướng, sa đà vào các competition. Đây chính là triệu chứng bệnh &lt;strong&gt;thiếu kỹ năng quản lý scope dự án.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Việc không control được những lời khuyên từ bên NLP chứng tỏ có dấu hiệu &lt;strong&gt;yếu về quản lý stakeholder&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Việc ưu tiên competition trước việc nhận ra thách thức thực sự của mình chính là &lt;strong&gt;yếu về quản lý action plan, time và schedule&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nhìn chung việc thiếu kỹ năng quản lý dự án sẽ không bao giờ đưa người postdoc gia nhập nhóm (dưới) 5% thành công được.
&lt;strong&gt;Mà ví dụ khoảng làm postdoc được 2-3 năm mà người thày nhìn vào thấy đủ thứ bệnh, chỗ nào cũng yếu thế này thì rất là khó để không cho … bật bãi!&lt;/strong&gt;
Ở Mỹ chả hạn, làm nghiên cứu muốn tồn tại lâu dài phải có kỹ năng quản lý dự án nghiên cứu. Thế mà chạy postdoc 2 năm mà không ngộ ra chân lý, không thể hiện tiềm năng làm quản lý lãnh đạo, thì không bật bãi chắc chỉ có ở thiên đường thôi!&lt;/p&gt;

&lt;h3 id=&quot;bert&quot;&gt;BERT&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;B&lt;/strong&gt;idirectional &lt;strong&gt;E&lt;/strong&gt;ncoder &lt;strong&gt;R&lt;/strong&gt;epresentations from &lt;strong&gt;T&lt;/strong&gt;ransformers (&lt;strong&gt;BERT&lt;/strong&gt;, &lt;a class=&quot;citation&quot; href=&quot;#devlin2019bert&quot;&gt;[28]&lt;/a&gt;) lại là 1 kiến trúc mới gần đây (từ 2018?).
Pretraining kiến trúc trên 1 task unsupervised (kiểu tự học), rồi dùng lại kiến trúc đó trong 1 task supervised khác là tư tưởng chính của BERT.
Các phiên bản cải tiến của BERT có thể kể đến AlBERT &lt;a class=&quot;citation&quot; href=&quot;#lan2019albert&quot;&gt;[29]&lt;/a&gt;, RoBERTa &lt;a class=&quot;citation&quot; href=&quot;#liu2019roberta&quot;&gt;[30]&lt;/a&gt;, BART &lt;a class=&quot;citation&quot; href=&quot;#lewis2020bart&quot;&gt;[31]&lt;/a&gt;, XlNet &lt;a class=&quot;citation&quot; href=&quot;#yang2019xlnet&quot;&gt;[32]&lt;/a&gt;, XLM &lt;a class=&quot;citation&quot; href=&quot;#conneau2019cross&quot;&gt;[33]&lt;/a&gt; hay gần đây là ELECTRA &lt;a class=&quot;citation&quot; href=&quot;#clark2020electra&quot;&gt;[34]&lt;/a&gt; đều kế thừa tinh thần này.
Trong quá khứ thì việc sử dụng pretraining để tạo ra một xuất phát điểm tốt hơn cho model là ý tưởng đã được khai thác &lt;a class=&quot;citation&quot; href=&quot;#hinton2006fast&quot;&gt;[35]&lt;/a&gt;.
Ví dụ như Hinton thì đã pretrain deep belief nets và gọi là &lt;em&gt;greedy layer-wise unsupervised pretraining&lt;/em&gt;.
Thủ thuật này là cần thiết khi train các model lớn:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;to train the first layer in isolation, then extract all features from the first layer only once, then train the second layer in isolation given those features, and so on. &lt;a class=&quot;citation&quot; href=&quot;#goodfellow2016deep&quot;&gt;[21]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ý tưởng của kiểu pretraining này là ta có một model rất lớn \(A\), việc train \(A\) từ đầu (random weights) sẽ rất khó.
Do vậy, chúng ta sẽ tìm một task unsupervised \(u\) (vì vậy không cần labels), để train \(A\) trước.
Sau đó, khi vào train cho task chính \(T\) (có labels) thì weights của \(A\) đã được khởi tạo bởi việc học \(u\) sẽ khiến training \(A\) cho \(T\) trở nên nhanh chóng hơn.
Trên thực tế có khá nhiều task bên computer vision đã sử dụng ý tưởng này và đưa ra khá nhiều kết quả thuyết phục.&lt;/p&gt;

&lt;p&gt;Với bên NLP, thì BERT lựa chọn task pretraining (gọi là &lt;em&gt;pretext&lt;/em&gt;) là masked language modeling (MLM).
Ví dụ như trong câu&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Original: I have a pen &amp;lt;EOS&amp;gt;
Masked: I &amp;lt;MASK&amp;gt; a pen &amp;lt;EOS&amp;gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;thì từ câu &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Masked&lt;/code&gt;, nhiệm vụ của pretext là phải &lt;em&gt;phục hồi&lt;/em&gt; lại các từ bị &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;.
Nguyên tắc pretext này nhìn chung giống với &lt;em&gt;denoising auto-encoders&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#goodfellow2016deep&quot;&gt;[21]&lt;/a&gt;.
Vì việc tạo bộ dữ liệu masking là dễ dàng và có thể tự làm được (viết script để ngẫu nhiên thay 1 token bởi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;) nên task này có thể xếp vào hạng mục &lt;strong&gt;unsupervised learning&lt;/strong&gt; hoặc &lt;strong&gt;self-supervised learning (SSL)&lt;/strong&gt;.
Lợi thế là người train có thể tạo ra bộ dữ liệu lớn tự động để máy học pretext mà không cần gắn nhãn!
Một pretext khác là &lt;em&gt;đoán câu tiếp theo (Next Sentence Prediction, NSP)&lt;/em&gt;.
Ví dụ:&lt;/p&gt;
&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;I told the principal that I would like to revolutionize this university if he give me something to do.
Next Sentence: He sent me to the Psychology department.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Trong kết quả của BERT thì có vẻ pretext NSP giúp cải thiện độ chính xác của task QA.
Tất nhiên, ngoài dùng pretext thì còn những kiểu pretraining khác như sử dụng feature đã pretrain (ELMo, &lt;a class=&quot;citation&quot; href=&quot;#peters2018deep&quot;&gt;[36]&lt;/a&gt;) hoặc fine-tune toàn bộ pretrain parameters (OpenAI GPT, &lt;a class=&quot;citation&quot; href=&quot;#radford2018improving&quot;&gt;[37]&lt;/a&gt;).
Gần đây có model BART &lt;a class=&quot;citation&quot; href=&quot;#lewis2020bart&quot;&gt;[31]&lt;/a&gt; tích hợp cả BERT và GPT để tạo ra một cơ chế pretext có thể mô phỏng hàm noise bất kỳ.&lt;/p&gt;

&lt;p&gt;Hầu hết những phương pháp kể trên đều có thể tìm thấy tại thư viện model HuggingFace &lt;a class=&quot;citation&quot; href=&quot;#wolf2020transformers&quot;&gt;[27]&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;ứng-dụng&quot;&gt;Ứng dụng&lt;/h1&gt;

&lt;h2 id=&quot;cách-tiếp-cận&quot;&gt;Cách tiếp cận&lt;/h2&gt;
&lt;h3 id=&quot;các-nguồn-tài-nguyên&quot;&gt;Các nguồn tài nguyên&lt;/h3&gt;
&lt;p&gt;Như đã nói ở trên, mảng này thì code kiếc, tài liệu, tài nguyên thì vô cùng &lt;strong&gt;sẵn&lt;/strong&gt; có và dồi dào.
Thế nên thôi mình cứ &lt;strong&gt;ăn “sẵn”&lt;/strong&gt; đi cho nó nhanh chứ nấu làm gì mất thời gian.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Nó cũng kiểu như đồ ăn Tết ấy mà: 
Hì hục nấu mất cả buổi mà ăn thì chắc được mấy miếng là chán.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thế nên tôi mới bảo rồi, những cái này mà mở khóa học truyền bá tri thức phổ cập thì &lt;strong&gt;liệu có khách không? ai học cho mà dạy?&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Tôi nghĩ chả ai học đâu, giờ người ta ăn sẵn hết.
Có dạy “nấu” thì cũng phải cái gì mà nó kiểu sẵn sẵn mà nấu nhanh ăn luôn.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thì tại sao nói mảng này đồ ăn sẵn nhiều thì dưới đây là một số nguồn mà các bạn có thể tận dụng:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Code&lt;/strong&gt; thì HuggingFace, tensorflow, Github&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt; thì HuggingFace thôi.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://huggingface.co/&quot;&gt;https://huggingface.co/&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Cứ tải về mà hì hục “nấu”.&lt;/li&gt;
      &lt;li&gt;Nấu xong chắc cũng chỉ chạm đũa vài miếng là ngấy thôi nhưng nếu thích nấu thì nấu thôi.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tài liệu&lt;/strong&gt; tutorials thì cứ search YouTube là ra hết mà.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/c/HuggingFace&quot;&gt;Kênh hướng dẫn của HuggingFace&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=G5lmya6eKtc&quot;&gt;The Future of Natural Language Processing&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/G5lmya6eKtc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;Nguồn &lt;strong&gt;tài nguyên tính toán&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Đầu tiên cũng chả cần mua GPU/TPU riêng đâu.&lt;/li&gt;
      &lt;li&gt;Để làm demo bạn cứ xài tạm Colab ấy.
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/&quot;&gt;https://colab.research.google.com/&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL&quot;&gt;Một playlist hướng dẫn dùng Colab với Tensorflow&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;đôi-lời-về-google-colab&quot;&gt;Đôi lời về Google Colab&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://research.google.com/colaboratory/faq.html&quot;&gt;FAQ&lt;/a&gt; &lt;strong&gt;What is Colabotory?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Colaboratory, or “Colab” for short, is a product from Google Research. Colab allows anybody to write and execute arbitrary python code through the browser, and is especially well suited to machine learning, data analysis and education. More technically, Colab is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources including GPUs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Google Colab nhìn chung là 1 dịch vụ Notebook cung cấp sẵn môi trường để chạy các tác vụ liên quan tới machine learning.
Với Colab, bạn có thể định nghĩa form các biến chương trình như hình bên dưới, rồi tạo lập mạng trí tuệ nhân tạo, thiết lập môi trường GPU/TPU để training hoặc inference.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nmt-colab-01.png&quot; alt=&quot;colab form&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sau khi đã làm quen với môi trường Google Colab thì bạn đã all-set to go!&lt;/p&gt;

&lt;h4 id=&quot;chú-ý-về-train-song-song&quot;&gt;Chú ý về train song song&lt;/h4&gt;
&lt;p&gt;Một chú ý nhỏ mang tính hô trợ trong quản lý time thôi là nếu bạn chọn Colab Free plan thì GPU/TPU tốc độ khá thấp.
Ngoài ra, bộ dữ liệu hạng vừa như WMT 2014 thì cũng có 4,468,840 cặp câu dịch (bộ training set).
Mà Colab Free thì tốc độ training sẽ rơi vào tầm 18-20 FPS với model AlBERT, như vậy để train hết 1 epoch với WMT sẽ mất tầm 62-70 (h), tức là khoảng 3 ngày/epoch (liên tục).
Mà để train model NMT thì sẽ mất 5-15 epochs nên sẽ phải mất tầm &lt;strong&gt;15-45 ngày train liên tục&lt;/strong&gt;.
Tuy nhiên, Colab Free có giới hạn là 1 session chỉ được 12h liên tục, nên bạn sẽ phải break ra tầm 30-90 sessions.
Ngoài ra đóng browser là mất luôn đấy chứ nó lại không chạy ở background đâu.&lt;/p&gt;

&lt;p&gt;Bài viết này chỉ sử dụng có sẵn với viết script để demo inference hoặc show training vài step nó như thế nào thôi, nên chúng tôi cũng chỉ cần Colab Free là đủ.
Tuy nhiên, nếu mà các bạn định làm nghiêm túc kiểu hì hục nấu cả buổi thì tôi nghĩ là nếu dùng GPU thì dưới 8 GPUs là hì hục lâu phết đấy!
Có mấy giải pháp:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Nâng cấp nên Colab Pro (10USD/tháng) và Pro Plus (50USD/tháng).&lt;/strong&gt; Tuy nhiên, dù lên Pro Plus thì 1 session cũng chỉ được 24h liên tục. Được cái là cho phép chạy background nên sẽ dễ thở hơn. Có điều là 24h thì lại phải save ra Google Drive, rồi khởi động lại training &lt;strong&gt;bằng tay&lt;/strong&gt;. GPU/TPU sẽ nhanh hơn tầm vài lần nên có thể sẽ chỉ mất vài tuần để train thôi. &lt;strong&gt;Hì hục&lt;/strong&gt; nó là thế đấy!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tự chế ra 1 hệ thống multi-GPU&lt;/strong&gt;: kiếm kinh phí mua tầm 24 cái GPUs P100 thì có khi 1 epoch mất 1h thôi, thì train trong ngày sẽ xong. Giá cả thì &lt;strong&gt;Tesla P100 SXM2 16GB&lt;/strong&gt; tầm 10,000 USD/cái, 24 cái thì tầm 240,000 USD thôi! Muốn làm big data mà chỉ code thôi là không ăn thua đâu! Bởi vận hành triển khai mỗi tháng hóa đơn đã vài chục ngàn đô, tiền upfront mua GPU cũng đã 240,000 USD thì code hầu như là phần ít giá trị nhất! Code nói chung … rẻ mạt!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Thuê EC2&lt;/strong&gt;: thì có vài lựa chọn là &lt;strong&gt;on-demand&lt;/strong&gt; và &lt;strong&gt;reserved&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;reserved thì 1 tháng 30 ngày 720 h là mình phải trả hết 720 h. Thì rẻ nhất là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.xlarge&lt;/code&gt; cũng mỗi giờ 0.63 đô. Như vậy là mỗi tháng \(0.63 \times 720 = 453.6\) USD/tháng. Tuy nhiên, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.xlarge&lt;/code&gt; chỉ có 1 GPU và tính năng thì xấp xỉ Colab Free, nên giả sử train 15 epochs mất 45 ngày liên tục, thì sẽ tốn là \(0.63 \times 45 \times 24 = 680.4\) USD cho lượt train này.
        &lt;ul&gt;
          &lt;li&gt;Nếu thuê GPU tốt hẳn đi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.48xlarge&lt;/code&gt; coi như 8 GPUs thì nhanh hơn tầm 8 lần thì giá reserved sẽ là \(10.26 \times 45 \times 24 /8 = 1385.1\) USD cho lượt train. Tuy nhiên, vì reserved nên mình thuê theo tháng thì ngoài lượt train này, hàng tháng mình tổng phải trả là \(10.26 \times 720 = 7387.2\) USD/tháng. Tức là train được tầm 6 lượt WMT 2014.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Thuê &lt;strong&gt;on-demand&lt;/strong&gt; thì mình không trả hàng tháng, dùng phát nào xong trả phát ấy thôi. Thì ví dụ, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.48xlarge&lt;/code&gt; thì sẽ mất \(16.29\times 45 \times 24 /8 = 2199.5\) USD cho mỗi lượt train này kéo dài tầm gần 1 tuần.&lt;/li&gt;
      &lt;li&gt;Thuê &lt;a href=&quot;https://aws.amazon.com/ec2/spot/pricing/&quot;&gt;&lt;strong&gt;Spot&lt;/strong&gt;&lt;/a&gt; thì có thể thuê loại &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5g.16xlarge&lt;/code&gt; thì có 2 Tensor core và mất 1.1112 USD/h. Nhìn chung Spot giá sẽ mềm hơn &lt;strong&gt;on-demand&lt;/strong&gt; và cùng hạng với reserved. Tuy nhiên, vì là spot nên lúc nào mà tự dưng bị interupt là phải có kế hoạch ứng phó trước:
        &lt;blockquote&gt;
          &lt;p&gt;Spot Instances are a cost-effective choice if you can be flexible about when your applications run and &lt;strong&gt;if your applications can be interrupted&lt;/strong&gt;&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Như các bạn đã thấy ở trên: cùng mức GPU &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.xlarge&lt;/code&gt; thì Colab Free còn thuê EC2 mất 453.6 USD. Vậy sự khác biệt là gì? Điểm khác chính yếu là thuê EC2 thì bạn không phải lo cái &lt;strong&gt;session 12 h&lt;/strong&gt; (sau 12h phải khởi động lại bằng tay và save dữ liệu vào Google Drive).&lt;/em&gt;
&lt;strong&gt;Tức là chỉ mỗi cái limit 12h ấy thôi đã trị giá 453.6 USD rồi.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Một điểm đáng lưu ý thứ 2 là giới R&amp;amp;D cạnh tranh khá khốc liệt, nên trên thứ vô thưởng vô phạt kiểu này thì kéo dài 45 ngày và free cũng được.
Nhưng cái gì mà có giá trị kinh tế một chút là cạnh tranh ác liệt: &lt;strong&gt;train là phải xong trong ngày, chứ đợi cả tháng sau có đứa nó publish mất thì sao?&lt;/strong&gt;
Thế nên tôi nghĩ nếu thực sự phải cạnh tranh thì chắc lại 240,000 USD hoặc &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.48xlarge&lt;/code&gt; thôi.
Và nếu chọn con đường cạnh tranh này thì các tập đoàn lớn, có sẵn hàng chục cái GPU server trong tay muốn huy động lúc nào cũng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ok em&lt;/code&gt; thì chắc chắn là thắng thế.
Nên cạnh tranh vào cái con đường kiểu đọ thông số GPU này có lẽ là chỉ dành cho tập đoàn lớn thôi.&lt;/p&gt;

&lt;p&gt;Thứ 3 nữa là tự chế 240,000 USD thì mới là DIY và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self-supervised&lt;/code&gt; chứ còn thuê với Colab thì đâu còn là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self&lt;/code&gt; nữa nên rốt cuộc là chắc cũng quay lại 240,000 USD nếu thực sự là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;neural-machine-translation-nmt&quot;&gt;Neural Machine Translation (NMT)&lt;/h2&gt;

&lt;p&gt;NMT là 1 bài toán NLP khá điển hình. Lần này chúng ta sẽ chạy thử 1 vài mô hình trên HuggingFace để dịch tiếng Đức sang tiếng Anh.
Đầu tiên, là với mớ lý thuyết phía trên, chúng ta sẽ xem thử cách thức fine-tune model AlBERT cho task NMT.
Tôi dùng bộ dữ liệu WMT 2014 để show:&lt;/p&gt;
&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/fab8ab5644bd1a122dd3ad8880d283f9.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Nhìn chung, cứ setup đúng các thông số, tokenizer, hàm loss, optimizer, .v.v… thì hầu như quá trình fine-tune mô hình kiểu BERT diễn ra rất trôi chảy.
Vấn đề tài nguyên để kết thúc quá trình train thì như thảo luận ở trên, các bạn có thể tự lựa chọn giải pháp phù hợp túi tiền.&lt;/p&gt;

&lt;p&gt;Nói chung nấu cả buổi nhưng ăn thì nhanh thôi. 
Để demo chức năng dịch Đức-Anh thì tôi xài luôn model có sẵn &lt;a href=&quot;https://huggingface.co/google/bert2bert_L-24_wmt_de_en&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;google/bert2bert_L-24_wmt_de_en&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/88d0506002341d151a8e5ad745f3364f.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Tôi cũng sử dụng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sacrebleu&lt;/code&gt; để show được điểm số BLEU của vài câu dịch.&lt;/p&gt;

&lt;h2 id=&quot;text-summarization&quot;&gt;Text Summarization&lt;/h2&gt;
&lt;p&gt;Bài toán &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Text Summarization&lt;/code&gt; (TS), yêu cầu đưa ra một đoạn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tóm tắt&lt;/code&gt; ngắn gọn của 1 văn bản dài.
Có khá nhiều bộ dữ liệu hỗ trợ việc training một model như vậy &lt;a class=&quot;citation&quot; href=&quot;#metatext2022summary&quot;&gt;[38]&lt;/a&gt;.
Trong bài này, tôi chọn 1 bộ nhỏ vừa là bộ WikiHow &lt;a class=&quot;citation&quot; href=&quot;#koupaee2018wikihow&quot;&gt;[39]&lt;/a&gt; với tầm 240,000 cặp bài báo-tóm tắt.&lt;/p&gt;

&lt;p&gt;Về mặt model, thì có khá nhiều hệ thống state-of-the-art và là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cho abstractive TS như NAM &lt;a class=&quot;citation&quot; href=&quot;#rush2015neural&quot;&gt;[8]&lt;/a&gt;, Encoder-Decoder RNN &lt;a class=&quot;citation&quot; href=&quot;#nallapati2016abstractive&quot;&gt;[40]&lt;/a&gt; hay BART &lt;a class=&quot;citation&quot; href=&quot;#lewis2020bart&quot;&gt;[31]&lt;/a&gt;.&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/08fb46dd6f03981287c5d3155bf7d6fa.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Nhìn chung, tuy WikiHow số lượng cặp dữ liệu ít hơn WMT14 (240,000 vs. 4,500,000) nhưng mỗi cặp dữ liệu lại là hẳn 1 bài dài, nên kết cục là cùng 1 GPU Colab Free thì thời gian train vẫn mất 49h/epoch.
Tức là cứ 1 epoch 2 ngày, và train khoảng 15 epochs thì mất tầm 30 ngày (hay 1 tháng, tính cả nghỉ lễ, T7/CN).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ts_training.png&quot; alt=&quot;ts&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;qa-và-vqa&quot;&gt;QA và VQA&lt;/h2&gt;

&lt;p&gt;Nếu bạn đã làm bài thi TOEIC reading and listening thì cũng dễ giải thích bài QA và VQA thôi.
Task QA là 1 bộ phận trong task lớn Reading Comprehension (RC): cho 1 bài báo (có thể kèm hình ảnh), người/máy sẽ đọc rồi trả lời 1 câu hỏi liên quan tới bài viết đó.
&lt;em&gt;Bài báo&lt;/em&gt; làm reference được gọi là &lt;em&gt;context&lt;/em&gt;, một ví dụ điển hình:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Context: I live in Japan.
Question: Where do I live?
Answer: Japan.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://tweetqa.github.io/image/example.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;
Dạng đơn giản nhất là như vậy chỉ cần đọc kỹ và &lt;strong&gt;paraphrasing&lt;/strong&gt; lại cụm từ có sẵn trong context thôi.
Đây là dạng &lt;strong&gt;extractive QA&lt;/strong&gt; tức là chỉ đơn giản skim bài viết và quote lại thôi.
Bộ dữ liệu đại diện cho kiểu extractive QA này là SQuAD v1 &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2016squad&quot;&gt;[5]&lt;/a&gt; và v2 &lt;a class=&quot;citation&quot; href=&quot;#rajpurkar2018know&quot;&gt;[17]&lt;/a&gt;.
Tuy nhiên, nếu bạn làm bài thi TOEIC bạn sẽ hiểu là có cả những câu hỏi mà không chỉ paraphrasing mà phải suy luận.
Đây gọi là &lt;strong&gt;abstractive QA&lt;/strong&gt;, điển hình cho dạng này là bộ TweetQA &lt;a class=&quot;citation&quot; href=&quot;#xiong2019tweetqa&quot;&gt;[41]&lt;/a&gt;.
Như ở ví dụ bên tay trái, bạn có thể thấy cần phải để ý chi tiết và suy luận thì mới trả lời được.
Ví dụ bên dưới là 1 model Abstractive QA dựa trên AlBERT:&lt;/p&gt;

&lt;noscript&gt;&lt;pre&gt;400: Invalid request&lt;/pre&gt;&lt;/noscript&gt;
&lt;script src=&quot;https://gist.github.com/f3def206dd7f578e723a90d3336d04da.js&quot;&gt; &lt;/script&gt;

&lt;p&gt;Để đánh giá, có thể dùng các metric như ROUGE-L hoặc METEOR.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/how-and-why.png&quot; style=&quot;float: left; margin: 10px; width: 40%;&quot; /&gt;
&lt;strong&gt;V&lt;/strong&gt;isual &lt;strong&gt;Q&lt;/strong&gt;uestion &lt;strong&gt;A&lt;/strong&gt;nswering (VQA, &lt;a class=&quot;citation&quot; href=&quot;#antol2015vqa&quot;&gt;[6, 7]&lt;/a&gt;) thay context bởi 1 hình ảnh hoặc video.
Tức là giống câu hỏi bài nghe thứ nhất trong thi TOEIC (nhưng TOEIC thì không có xem video).
Thì nói chung cũng đa dạng đấy!
Vì câu hỏi thì cũng tùy vào mức độ hình ảnh mà đánh đố hay không đánh đố.
Thì làm sao để thấy được sự khác biệt với QA của bên NLP thì chắc là các bạn xem hình ảnh bên trái (lấy từ &lt;a class=&quot;citation&quot; href=&quot;#VCRdataset&quot;&gt;[42, 43]&lt;/a&gt;) thì sẽ hiểu ngay.
Mà tôi nghĩ cũng chỉ nhìn vào mỗi cái hình này là cũng hiểu được là &lt;strong&gt;how and why&lt;/strong&gt; như thế nào rồi đấy.
Dưới đây là bức tranh lớn trong mảng VQA (tập hợp qua các bài trên 300 citations).
Tuy chỉ là một mảng nhỏ, nhưng cái scene này nhìn cũng là 1 graph phức tạp phết!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/vqa-author-paper-graphs.png&quot; alt=&quot;vqa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cũng không nên quên là cách tiếp cận của nhân vật chính postdoc trong bài này là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;, mà nói chung là mảng giao giữa với vision và language nên tôi nghĩ là tiếp cận theo cách nào cũng sẽ đi đến cái chỗ nó như vậy thôi.
Vậy chúng ta lại cứ bám sát vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; nhé (mà hậu duệ là Transformers, BERT là những cái tôi thấy có vẻ cũng bắt đầu lấn sân sang Vision gần đây rồi).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Điểm khác biệt lớn nhất giữa vision và language chính là &lt;strong&gt;ORDER&lt;/strong&gt;.
Nếu như đơn vị trong language là words được sắp xếp theo order thành phrase và sentence, words được extract từ documents nhờ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokenizer&lt;/code&gt;.
Trong hình ảnh, có một sự mapping với language, và cái hướng này cũng có mấy nhóm cũng rất mạnh đang làm chủ công nghệ này.
Ví dụ, word, phrase &amp;lt;-&amp;gt; visual concepts
còn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokenizer&lt;/code&gt; &amp;lt;-&amp;gt; classifier, object detectors, visual relation detectors, .v.v…
Nói chung, bên vision cũng có đủ những “vũ khí” tương xứng để extract tokens và feed vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; models.
Tuy nhiên, vấn đề chính là order: cùng trong một hình ảnh, có thể extract ra 2 vật thể, nhưng khi xếp vào 1 câu văn thì thứ tự nào cũng có khả năng cả.
Nó khác language là &lt;strong&gt;ORDER&lt;/strong&gt; đã được input sẵn, còn vision thì không.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Vấn đề &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;order&lt;/code&gt; cũng không phải mới mà được xem xét ngay từ 2015 (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; for sets, &lt;a class=&quot;citation&quot; href=&quot;#vinyals2015order&quot;&gt;[44]&lt;/a&gt;).
Tuy nhiên, họ cũng chỉ dừng ở mức chọn 1 thứ tự nhất định để bắt đầu và optimize thứ tự đó trong quá trình training.
Đó cũng là tinh thần của hầu hết các ứng dụng vision-language về sau có dùng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; và attention như image captioning, Video-To-Text (VTT) và VQA.&lt;/p&gt;

&lt;p&gt;Một cách giải quyết khác chính là tìm một &lt;strong&gt;representation&lt;/strong&gt; tốt hơn cho &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt;.
Vì &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt; trong vision không phải &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence&lt;/code&gt; trong NLP nên vấn đề sẽ xảy ra khi dùng representation mới là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; sẽ phải thay đổi để phù hợp với representation mới.
Trong những publication gần đây, một &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set&lt;/code&gt; representation phù hợp là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt;.
Vấn đề là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt; không phải &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence&lt;/code&gt; nên &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cũng thay thế bằng graph neural networks (GNN).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Là người thày, có thể không trực tiếp bắt tay vào làm, nhưng phải định hướng.
Vì vậy không thể định hướng một hướng đi “cụt”, chỉ làm 1 đời postdoc là hết.
Định hướng, đôi khi dù 10 đời postdoc không làm gì thì cũng không thể hết việc được, mới là định hướng tốt.
Thậm chí, nếu postdoc mà chịu khó làm thì còn sinh sôi nảy nở ra làm mãi không hết.
Vision-Language (VL) nhìn chung là 1 định hướng lâu dài như vậy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Làm người trò, thì khi bắt đầu dự án không nên quá thụ động.
Việc representation có thể phải thay đổi giữa chừng (từ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sequence&lt;/code&gt; sang &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt;) là chuyện nếu không nhìn ra ngay từ đầu,
có thể dẫn đến dự án bị đình chỉ giữa chừng.
Làm postdoc như vậy là yếu về &lt;strong&gt;quản lý rủi ro&lt;/strong&gt;.
Nhìn chung vẫn là nghiệp vụ quản lý dự án (Project Management) vẫn còn chưa mạnh.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nhìn chung, mảng Vision-Language (VL) này tôi thấy còn nhiều dư địa cả về lý thuyết lẫn ứng dụng:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Về lý thuyết, cái representation vẫn còn chỗ làm. Ứng với mỗi representation có thể sẽ phải thu thập dataset và gắn nhãn riêng. Xây dựng model mới hẳn cho &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;graph&lt;/code&gt; và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Về ứng dụng, VQA hiện tại vẫn chỉ là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;closed form&lt;/code&gt;: xem 1 hình ảnh và extract concepts rồi dùng kỹ thuật QA để trả lời nội dung trong ảnh, tức là chưa phải dạng suy luận nhiều. VQA ở dạng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;open&lt;/code&gt; hơn sẽ thay context không chỉ bởi 1 hình ảnh hay 1 video, mà bởi 1 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multimedia database&lt;/code&gt;, cũng như trong trò chơi tỷ phú, mỗi câu hỏi không chỉ đòi hỏi phải xem 1 context nhất định mà phải &lt;strong&gt;search&lt;/strong&gt; trong 1 database knowledge mới có thể trả lời được.
    &lt;ul&gt;
      &lt;li&gt;Một bước đơn giản hơn chỉ là thay context hình ảnh bởi context video, và câu hỏi là về 1 details nào đó: ví dụ show cho máy một video nấu ăn, và hỏi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tại sao lại cho hành vào phi trước?&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Thay vì giới hạn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;luật chơi&lt;/code&gt; rằng máy chỉ được xem context là 1 bài báo, 1 hình ảnh, 1 video cụ thể, thì nhưu IBM Watson hay Siri, cho phép máy kết nối Internet và có thể query trong cả 1 multimedia database lớn, thì QA sẽ như thế nào?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tuy vậy, với công nghệ hiện tại cũng đủ để làm 1 demo nho nhỏ dạng: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cho máy xem 1 hình ảnh, và đặt 1 vài câu hỏi xung quanh nội dung hình ảnh&lt;/code&gt; và không cho máy kết nối Internet, thì máy cũng có thể trả lời ở mức độ nhất định. Tức là có thể lấy điểm bài nghe số 1 của TOEIC!&lt;/p&gt;

&lt;p&gt;Đánh giá về tiềm năng bài toán VQA, thì tôi thấy cái mảng này nó cũng dồi dào, code kiếc cũng thừa mứa trên Github, kiểu như HuggingFace ý: classifier thì có TIMM &lt;a class=&quot;citation&quot; href=&quot;#rw2019timm&quot;&gt;[45]&lt;/a&gt;, detector thì có Detectron2 &lt;a class=&quot;citation&quot; href=&quot;#wu2019detectron2&quot;&gt;[46]&lt;/a&gt; hay MMDetection &lt;a class=&quot;citation&quot; href=&quot;#mmdetection&quot;&gt;[47]&lt;/a&gt;, segmentations thì có &lt;a class=&quot;citation&quot; href=&quot;#Yakubovskiy:2019&quot;&gt;[48]&lt;/a&gt;, visual relationship detector (VRD) thì cũng tràn lan vì các tác giả cũng công bố hết lên Github.
Thế nên là những cái low-level detector kiểu visual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tokenizer&lt;/code&gt; là sẵn có, mà người ta công bố mà mình không dùng thì cũng phí.
Mà vấn đề là của người ta chất lượng nó cao.
Dữ liệu thì cũng công bố sẵn có hết, bộ dữ liệu VQA hay commonsense hay Visual Genome &lt;a class=&quot;citation&quot; href=&quot;#krishna2017visual&quot;&gt;[49]&lt;/a&gt; thì nhìn chung cũng tầm triệu câu hỏi, với vài trăm ngàn hình ảnh.
&lt;strong&gt;Đấy vấn đề với quy mô dữ liệu này thì lại quay về bài toán &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;240,000&lt;/code&gt; USD thoai!&lt;/strong&gt;
Đấy, mấy cái mảng này nó toàn kiểu code kiếc, dữ liệu thì bằng cho không trên Github rồi, nhưng vấn đề là để chạy được thì hàng trăm ngàn đô.
Các bạn thấy có những nhóm nó chịu khó train để ra model công bố thì toàn 8 GPUs mà cũng hì hục train 1 tuần.
Có cái train ImageNet trong 1 giờ &lt;a class=&quot;citation&quot; href=&quot;#goyal2017accurate&quot;&gt;[50]&lt;/a&gt; thì lại là họ dùng những &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;256 GPUs&lt;/code&gt; mà họ có được nguồn tài nguyên ấy là nhóm của … Facebook!
Nên tôi nghĩ khi làm postdoc mà không kiểu có estimate trước thời gian train là lại … giữa chừng đứt gánh vì … không có tiền!
Mà không có tiền thì đấy như ví dụ postdoc của chúng ta là lặng lẽ giải tán rồi!&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Chứ không có tiền thì thày cũng không có, trò cũng không có thì nhìn nhau … cười à?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Mà vì tài nguyên cái mảng này nó tràn lan trên Github nên có bảo tôi code thì lấy code làm công cũng không được vì cùng cái code trên mạng Internet nó nhan nhản ra, &lt;strong&gt;code nói chung là rẻ mạt!&lt;/strong&gt;
&lt;em&gt;Cái quan trọng là vượt qua cái khó khăn như 240,000 USD và ra được sản phẩm thoai!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;video-to-text-và-image-captioning&quot;&gt;Video-to-Text và Image Captioning&lt;/h2&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Nhìn chung về mấy task bên NLP, thì tất nhiên bài viết này cũng chỉ là 1 phần (bao gồm task NMT/TS/QA với mấy cái tự học) còn nhiều task khác như POS Tagging, semantic roles, …
Tuy nhiên, nhìn chung những tiến bộ trong mảng NLP đã được tóm tắt, và có vẻ cũng không khó khăn lắm đâu.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Có làm thì sẽ có ăn thôi, còn không làm mà đòi có ăn thì …&lt;/p&gt;
&lt;/blockquote&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/peTwj6WK3vo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Cũng xoáy kha khá cái trường hợp postdoc, nhưng tôi nghĩ là cứ chuyển hết cho Prof. Huan Rose chỉ bảo là ok thôi.&lt;/p&gt;

&lt;p&gt;Nói chung phần kỹ thuật thì không có nhiều điểm khó, nhưng cái khó nhất chắc vẫn là cái 240,000 USD thôi. 
Mà cái đó thì chắc chắn không phải là cái anh em kỹ thuật có thể giải quyết nổi.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Nên nếu không có tiền thì … giải tán thôi, chứ biết làm thế nào bây giờ?
Cái này có đố cũng chịu!
Như bình thường thì có thể đổi sang làm những cái bài toán mà nó &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train phát xong luôn, dùng luôn&lt;/code&gt;, những bài toán mà nó có thể chạy chỉ cần CPU, thậm chí chỉ mấy cái device nhỏ nhỏ cũng chạy được ấy.
Nên tôi nghĩ cũng không cần xoáy nhiều vào cái đám big data nữa:&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Xin vô mấy cái tập đoàn lớn đùng ấy mà làm, nó có nhiều người dùng nó sẵn sàng chi tiền tấn ra mà mua. Ví dụ, các anh em thử nghĩ xem mấy cái papers nhiều citation của cái mảng này làm thí nghiệm toàn hàng trăm GPU, TPU thì tác giả toàn nhóm của những công ty như thế nào? phải Google, Facebook, … chứ như anh em &lt;strong&gt;chân đất mắt toét&lt;/strong&gt; thì chờ nó ra model rồi dùng lại, chứ mất công hì hục nấu nướng làm gì cho mất thời gian.&lt;/li&gt;
    &lt;li&gt;Chuyển sang làm mấy cái thiết bị nhỏ nhỏ mà làm. Mà chấm dứt suy nghĩ về GPU với mấy cái model to đùng, nấu thì lâu mà ăn thì chóng đi.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;vaswani2017attention&quot;&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. 2017. Attention is all you need. &lt;i&gt;Advances in neural information processing systems&lt;/i&gt; (2017), 5998–6008.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/vaswani2017attention/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kalchbrenner2013recurrent&quot;&gt;Kalchbrenner, N. and Blunsom, P. 2013. Recurrent continuous translation models. &lt;i&gt;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle, USA. Association for Computational Linguistics&lt;/i&gt; (2013).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kalchbrenner2013recurrent/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;sutskever2014sequence&quot;&gt;Sutskever, I., Vinyals, O. and Le, Q.V.V. 2014. Sequence to sequence learning with neural networks. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt; (2014), 3104–3112.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/sutskever2014sequence/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cho2014learning&quot;&gt;Cho, K., Merrienboer, B. van, Gulcehre, C., Bougares, F., Schwenk, H. and Bengio, Y. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. &lt;i&gt;arXiv preprint arXiv:1406.1078&lt;/i&gt;. (2014).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/cho2014learning/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rajpurkar2016squad&quot;&gt;Rajpurkar, P., Zhang, J., Lopyrev, K. and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. &lt;i&gt;Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing&lt;/i&gt; (2016), 2383–2392.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rajpurkar2016squad/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;antol2015vqa&quot;&gt;Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L. and Parikh, D. 2015. Vqa: Visual question answering. &lt;i&gt;Proceedings of the IEEE international conference on computer vision&lt;/i&gt; (2015), 2425–2433.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/antol2015vqa/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;goyal2017making&quot;&gt;Goyal, Y., Khot, T., Summers-Stay, D., Batra, D. and Parikh, D. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. &lt;i&gt;Proceedings of the IEEE conference on computer vision and pattern recognition&lt;/i&gt; (2017), 6904–6913.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/goyal2017making/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rush2015neural&quot;&gt;Rush, A.M., Chopra, S. and Weston, J. 2015. A Neural Attention Model for Abstractive Sentence Summarization. &lt;i&gt;Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing&lt;/i&gt; (2015), 379–389.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rush2015neural/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ranzato2015sequence&quot;&gt;Ranzato, M.A., Chopra, S., Auli, M. and Zaremba, W. 2015. Sequence level training with recurrent neural networks. &lt;i&gt;arXiv preprint arXiv:1511.06732&lt;/i&gt;. (2015).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ranzato2015sequence/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;venugopalan2015sequence&quot;&gt;Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T. and Saenko, K. 2015. Sequence to sequence-video to text. &lt;i&gt;Proceedings of the IEEE international conference on computer vision&lt;/i&gt; (2015), 4534–4542.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/venugopalan2015sequence/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;you2016image&quot;&gt;You, Q., Jin, H., Wang, Z., Fang, C. and Luo, J. 2016. Image captioning with semantic attention. &lt;i&gt;Proceedings of the IEEE conference on computer vision and pattern recognition&lt;/i&gt; (2016), 4651–4659.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/you2016image/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;johnson2016densecap&quot;&gt;Johnson, J., Karpathy, A. and Fei-Fei, L. 2016. Densecap: Fully convolutional localization networks for dense captioning. &lt;i&gt;Proceedings of the IEEE conference on computer vision and pattern recognition&lt;/i&gt; (2016), 4565–4574.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/johnson2016densecap/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;luong2015multi&quot;&gt;Luong, M.-T., Le, Q.V., Sutskever, I., Vinyals, O. and Kaiser, L. 2015. Multi-task sequence to sequence learning. &lt;i&gt;arXiv preprint arXiv:1511.06114&lt;/i&gt;. (2015).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/luong2015multi/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bojar2014findings&quot;&gt;Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H. and others 2014. Findings of the 2014 workshop on statistical machine translation. &lt;i&gt;Proceedings of the ninth workshop on statistical machine translation&lt;/i&gt; (2014), 12–58.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/bojar2014findings/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bojar2017findings&quot;&gt;Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., Huck, M., Koehn, P., Liu, Q., Logacheva, V. and others 2017. Findings of the 2017 conference on machine translation (wmt17). &lt;i&gt;Proceedings of the Second Conference on Machine Translation&lt;/i&gt; (2017), 169–214.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/bojar2017findings/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;paul2004introduc&quot;&gt;Over, P. and Yen, J. 2014. An Introduction to DUC-2004. https://duc.nist.gov/pubs/2004slides/duc2004.intro.pdf.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/paul2004introduc/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rajpurkar2018know&quot;&gt;Rajpurkar, P., Jia, R. and Liang, P. 2018. Know What You Don’t Know: Unanswerable Questions for SQuAD. &lt;i&gt;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/i&gt; (2018), 784–789.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rajpurkar2018know/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lin2004rouge&quot;&gt;Lin, C.-Y. 2004. Rouge: A package for automatic evaluation of summaries. &lt;i&gt;Text summarization branches out&lt;/i&gt; (2004), 74–81.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lin2004rouge/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;bottou2007tradeoffs&quot;&gt;Bottou, L. and Bousquet, O. 2007. The tradeoffs of large scale learning. &lt;i&gt;Advances in neural information processing systems&lt;/i&gt;. 20, (2007).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/bottou2007tradeoffs/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rumelhart1986learning&quot;&gt;Rumelhart, D.E., Hinton, G.E. and Williams, R.J. 1986. Learning representations by back-propagating errors. &lt;i&gt;nature&lt;/i&gt;. 323, 6088 (1986), 533–536.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rumelhart1986learning/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;goodfellow2016deep&quot;&gt;Goodfellow, I., Bengio, Y. and Courville, A. 2016. &lt;i&gt;Deep learning&lt;/i&gt;. MIT press.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/goodfellow2016deep/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hochreiter1997long&quot;&gt;Hochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. &lt;i&gt;Neural computation&lt;/i&gt;. 9, 8 (1997), 1735–1780.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hochreiter1997long/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gers2000learning&quot;&gt;Gers, F.A., Schmidhuber, J. and Cummins, F. 2000. Learning to forget: Continual prediction with LSTM. &lt;i&gt;Neural computation&lt;/i&gt;. 12, 10 (2000), 2451–2471.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/gers2000learning/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cho2014properties&quot;&gt;Cho, K., Merrienboer, B. van, Bahdanau, D. and Bengio, Y. 2014. On the properties of neural machine translation: Encoder-decoder approaches. &lt;i&gt;Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8), 2014&lt;/i&gt; (2014).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/cho2014properties/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;koehn2009statistical&quot;&gt;Koehn, P. 2009. Neural Machine Translation. &lt;i&gt;Statistical Machine Translation&lt;/i&gt;. Cambridge University Press.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/koehn2009statistical/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;vaswani2018tensor2tensor&quot;&gt;Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A.N., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner, N., Parmar, N. and others 2018. Tensor2tensor for neural machine translation. &lt;i&gt;arXiv preprint arXiv:1803.07416&lt;/i&gt;. (2018).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/vaswani2018tensor2tensor/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wolf2020transformers&quot;&gt;Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi, A., Cistac, P., Funtowicz, M., Davison, J., Shleifer, S. and others 2020. Transformers: State-of-the-art natural language processing. &lt;i&gt;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&lt;/i&gt; (2020), 38–45.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/wolf2020transformers/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;devlin2019bert&quot;&gt;Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. &lt;i&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/i&gt; (2019), 4171–4186.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/devlin2019bert/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lan2019albert&quot;&gt;Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P. and Soricut, R. 2019. Albert: A lite bert for self-supervised learning of language representations. &lt;i&gt;arXiv preprint arXiv:1909.11942&lt;/i&gt;. (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lan2019albert/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;liu2019roberta&quot;&gt;Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. &lt;i&gt;arXiv preprint arXiv:1907.11692&lt;/i&gt;. (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/liu2019roberta/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lewis2020bart&quot;&gt;Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V. and Zettlemoyer, L. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. &lt;i&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/i&gt; (2020), 7871–7880.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lewis2020bart/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;yang2019xlnet&quot;&gt;Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R. and Le, Q.V. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. &lt;i&gt;Advances in neural information processing systems&lt;/i&gt;. 32, (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/yang2019xlnet/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;conneau2019cross&quot;&gt;Conneau, A. and Lample, G. 2019. Cross-lingual language model pretraining. &lt;i&gt;Advances in Neural Information Processing Systems&lt;/i&gt;. 32, (2019), 7059–7069.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/conneau2019cross/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;clark2020electra&quot;&gt;Clark, K., Luong, M.-T., Le, Q.V. and Manning, C.D. 2020. Electra: Pre-training text encoders as discriminators rather than generators. &lt;i&gt;arXiv preprint arXiv:2003.10555&lt;/i&gt;. (2020).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/clark2020electra/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hinton2006fast&quot;&gt;Hinton, G.E., Osindero, S. and Teh, Y.-W. 2006. A fast learning algorithm for deep belief nets. &lt;i&gt;Neural computation&lt;/i&gt;. 18, 7 (2006), 1527–1554.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hinton2006fast/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;peters2018deep&quot;&gt;Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K. and Zettlemoyer, L. 2018. Deep Contextualized Word Representations. &lt;i&gt;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)&lt;/i&gt; (New Orleans, Louisiana, Jun. 2018), 2227–2237.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/peters2018deep/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;radford2018improving&quot;&gt;Radford, A., Narasimhan, K., Salimans, T. and Sutskever, I. 2018. &lt;i&gt;Improving language understanding by generative pre-training&lt;/i&gt;. OpenAI.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/radford2018improving/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;metatext2022summary&quot;&gt;+64 Summarization Datasets - NLP Database.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/metatext2022summary/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;koupaee2018wikihow&quot;&gt;Koupaee, M. and Wang, W.Y. 2018. Wikihow: A large scale text summarization dataset. &lt;i&gt;arXiv preprint arXiv:1810.09305&lt;/i&gt;. (2018).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/koupaee2018wikihow/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nallapati2016abstractive&quot;&gt;Nallapati, R., Zhou, B., Santos, C. dos, Gu̇lçehre Çağlar and Xiang, B. 2016. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. &lt;i&gt;Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning&lt;/i&gt; (2016), 280–290.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nallapati2016abstractive/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;xiong2019tweetqa&quot;&gt;Xiong, W., Wu, J., Wang, H., Kulkarni, V., Yu, M., Chang, S., Guo, X. and Wang, W.Y. 2019. TWEETQA: A Social Media Focused Question Answering Dataset. &lt;i&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics&lt;/i&gt; (2019), 5020–5031.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/xiong2019tweetqa/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;VCRdataset&quot;&gt;VCR: Visual Commonsense Reasoning.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/VCRdataset/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;zellers2019recognition&quot;&gt;Zellers, R., Bisk, Y., Farhadi, A. and Choi, Y. 2019. From recognition to cognition: Visual commonsense reasoning. &lt;i&gt;Proceedings of the IEEE/CVF conference on computer vision and pattern recognition&lt;/i&gt; (2019), 6720–6731.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/zellers2019recognition/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;vinyals2015order&quot;&gt;Vinyals, O., Bengio, S. and Kudlur, M. 2015. Order matters: Sequence to sequence for sets. &lt;i&gt;arXiv preprint arXiv:1511.06391&lt;/i&gt;. (2015).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/vinyals2015order/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rw2019timm&quot;&gt;Wightman, R. 2019. PyTorch Image Models. &lt;i&gt;GitHub repository&lt;/i&gt;. GitHub.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rw2019timm/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wu2019detectron2&quot;&gt;Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y. and Girshick, R. 2019. Detectron2.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/wu2019detectron2/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mmdetection&quot;&gt;Chen, K. et al. 2019. MMDetection: Open MMLab Detection Toolbox and Benchmark. &lt;i&gt;arXiv preprint arXiv:1906.07155&lt;/i&gt;. (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/mmdetection/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Yakubovskiy:2019&quot;&gt;Yakubovskiy, P. 2020. Segmentation Models Pytorch. &lt;i&gt;GitHub repository&lt;/i&gt;. GitHub.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/Yakubovskiy_2019/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;krishna2017visual&quot;&gt;Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D.A. and others 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. &lt;i&gt;International journal of computer vision&lt;/i&gt;. 123, 1 (2017), 32–73.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/krishna2017visual/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;goyal2017accurate&quot;&gt;Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y. and He, K. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. &lt;i&gt;arXiv preprint arXiv:1706.02677&lt;/i&gt;. (2017).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/goyal2017accurate/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Tue, 01 Feb 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/seq2seq/</link>
        <guid isPermaLink="true">https://wanted2.github.io/seq2seq/</guid>
        
        <category>Machine learning</category>
        
        <category>Neural Machine Translation</category>
        
        <category>Long-Short Term Memory</category>
        
        <category>Recurrent Neural Networks</category>
        
        <category>Encoder-Decoder</category>
        
        <category>Seq2Seq</category>
        
        <category>Sequence-to-Sequence</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>Machine Learning for Network Intrusion Detection: From Local to Production</title>
        <description>&lt;h1 id=&quot;network-intrusion-detection-system&quot;&gt;Network Intrusion Detection System&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Simon-Enoch/publication/319637372/figure/fig1/AS:543679434510336@1506634686111/Configuration-of-the-enterprise-network.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Network intrusion detection system (NIDS)&lt;/strong&gt; is an independent platform that examines network traffic patterns to identify intrusions for an entire network. It needs to be placed at a choke point where all traffic traverses. A good location for this is in the DMZ.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An IDS is &lt;em&gt;reactive&lt;/em&gt; in nature: it only monitors and sends alerts to a group of specific people like administrators.
The above figure shows a common NIDS architecture, where a DMZ is placed in between &lt;em&gt;external firewall&lt;/em&gt; and &lt;em&gt;internal firewall (to an internal network)&lt;/em&gt;.
Here, in the DMZ, a NIDS can be set up to monitor the traffic of the whole corporation and identify the anomalies.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Vivek-Singh-70/publication/328572055/figure/fig1/AS:686824250945536@1540763070945/Major-Components-of-Snort-IDS-and-Bro-IDS.ppm&quot; style=&quot;float: right; margin: 10px; width: 50%;&quot; /&gt;
A NIDS can have the following architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;streaming engine&lt;/strong&gt; which ingests packet stream into the NIDS&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;package decoder&lt;/strong&gt; which turns packet content into visibility&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;detection engine&lt;/strong&gt; which identify intrusions&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;policy engine&lt;/strong&gt; which decide and suggest what to do with an intrusion&lt;/li&gt;
  &lt;li&gt;Finally, the intrusion details are collected and &lt;strong&gt;logged&lt;/strong&gt;. &lt;strong&gt;Alerts&lt;/strong&gt; will be sent to admins and optionally, scripted &lt;strong&gt;actions&lt;/strong&gt; can be performed in according to policies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two typical examples of NIDS are &lt;a href=&quot;https://www.snort.org/&quot;&gt;Snort IDS&lt;/a&gt; and &lt;a href=&quot;https://bricata.com/blog/zeek-ids-threat-detection/&quot;&gt;Bro IDS&lt;/a&gt;.
A nicer example that can be integrated into network monitoring can be &lt;a href=&quot;https://www.zabbix.com/features#smart_thresholds&quot;&gt;Zabbix’s Problem Detection Engine&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;machine-learning-at-scale-some-solutions-in-aws&quot;&gt;Machine Learning at scale: some solutions in AWS&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The theory about NIDS perhaps is a huge bundle of knowledge!
Corporations have set up their own NIDS (in most cases in DMZ) for years.
We will not talk about such solutions anymore.
The most interesting part is in the cloud platforms like AWS.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;While companies are moving their resources to the cloud, where is the NIDS?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the &lt;a href=&quot;https://d1.awsstatic.com/Marketplace/scenarios/security/SEC_01_TSB_Final.pdf&quot;&gt;&lt;strong&gt;Shared Responsibility Model (SRM)&lt;/strong&gt;&lt;/a&gt;.
The &lt;strong&gt;responsibility&lt;/strong&gt; of protecting cloud resources like computing instances (EC2) and networking is of AWS.
Users have the responsibility to tune the best configurations of firewall, instances, load balancers, and other resources in AWS.
So a platform IDS is already managed at AWS, and the at the users (application developers), the remaining task is to &lt;strong&gt;implement a best Network/HIDS (endpoint protection)&lt;/strong&gt;.
There are several choices for a HIDS in the AWS Marketplace like &lt;a href=&quot;https://www.trendmicro.com/en_us/business/products/hybrid-cloud/security-data-center-virtualization.html&quot;&gt;TrendMicro’s Deep Security&lt;/a&gt;.
For a custom NIDS, users can implement a &lt;a href=&quot;https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-inspection-architecture-with-aws-gateway-load-balancer-and-aws-transit-gateway/&quot;&gt;Transit DMZ Gateway&lt;/a&gt;. 
An examplar implementation can be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/aws-samples/aws-gateway-load-balancer-code-samples/raw/964874069c0a90d0b6758b2612c2de44a43f2a21/aws-cloudformation/centralized_architecture/images/gwlb_centralized_architecture.jpg&quot; alt=&quot;AWS Transit gateway&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenges-in-nids-operations&quot;&gt;Challenges in NIDS operations&lt;/h2&gt;

&lt;p&gt;A standard DMZ with NIDS can be implemented at the cloud or data center.
However, the &lt;strong&gt;hurdle&lt;/strong&gt; only comes when we operate our solution: this is the hard part!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When it comes to operations (運用保守), it often require an enormous amount of manual work!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We need a large number of low-paid workers who will sit in front of the monitoring screen and then manually mark each access as legal or not.
It is the real-world hurdle!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;However, the technical issues start when we want &lt;strong&gt;automation&lt;/strong&gt;: how to reduce false alarms and misses?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Good automated solutions will reduce manual work a lot.
But it is not straightforward!
Rule-based systems can have many misses or false alarms.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A weak rule misses many, but a strong rule alerts too much! (So both strong and weak ones are useless!)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;machine-learning-at-local&quot;&gt;Machine Learning at local&lt;/h1&gt;

&lt;p&gt;You have a dataset and perform some analytics at your local or edge PC.
You don’t use any server or cloud solution at all.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You should ask us why do you need to care about these works while cloud platforms already prepare so many ready-to-use solutions for you?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Agree! Machine learning engineers behind the platforms already do such works (model engineering).
Then when you do these works, that means you are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A student who is learning ML at an educational place (like university);&lt;/li&gt;
  &lt;li&gt;A researcher (or engineer) who perform a project for NGO, government, or a big company (who is building a platform solution); and&lt;/li&gt;
  &lt;li&gt;The worst luck: you’re only an ML enthusiast who is looking into ML when you have free time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not so many people do model engineering on their own laptop (or desktop), so in my experience, they fall into one of these three categories.
For the second category, people in that category is ML engineer/scientist who will make the model for thousands to million application developers over the world (who uses the platforms).
Such category is quite a few, and to be in, you need &lt;strong&gt;qualification&lt;/strong&gt;!
The most common cases are in the first.
The third category is possible but is rare and complex: while the first and second category have their own goals with ML (for studying and for works), the third category has no particular purpose.
They only do it in their free time and for fun (like doing a hobby)!
The first and second ones will have outcomes (successes and non-successes), but the third one is only for fun!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;No one can ban the hobby of a man, and we only do the hobby: we start when we want and stop when we don’t like it anymore!
That’s why I call it (the third category) the worst luck!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Actually, people in the third category already have another job (but that job does not relate to ML or even AI).
They do ML as hobbies but don’t complete anything (because ML is not their business, even more, ML does not give them bread and butter)!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is said, but in practice, if a thing doesn’t give any benefit, it won’t be done properly!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nevertheless, whatever your category is, when you do these things in your local computing environment, ML matters with you in some ways!
We will see what a &lt;strong&gt;local IDS model would look like&lt;/strong&gt;.
And, in a synthetic way!&lt;/p&gt;

&lt;h2 id=&quot;a-kaggle-synthetic-dataset&quot;&gt;A Kaggle synthetic dataset&lt;/h2&gt;

&lt;p&gt;We start with a synthetic dataset from &lt;a href=&quot;https://www.kaggle.com/sampadab17/network-intrusion-detection&quot;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The dataset to be audited was provided that consists of a wide variety of intrusions simulated in a military network environment. It created an environment to acquire raw TCP/IP dump data for a network by simulating a typical US Air Force LAN. The LAN was focused like a real environment and blasted with multiple attacks. A connection is a sequence of TCP packets starting and ending at some time duration between which data flows to and from a source IP address to a target IP address under some well-defined protocol. Also, each connection is labeled as either normal or as an attack with exactly one specific attack type. Each connection record consists of about 100 bytes.&lt;/p&gt;

  &lt;p&gt;For each TCP/IP connection, 41 quantitative and qualitative features are obtained from normal and attack data (3 qualitative and 38 quantitative features). The class variable has two categories:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Normal&lt;/li&gt;
    &lt;li&gt;Anomalous&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;explorative-analysis&quot;&gt;Explorative analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/ids-kaggle-feature-selection.png&quot; alt=&quot;feature selection&quot; /&gt;
&lt;em&gt;We notice that not all features are useful. Some features like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_Host_login&lt;/code&gt; have only a constant value.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our dataset is big enough (25K entries) and contains both categorical and numerical features:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;RangeIndex: 25192 entries, 0 to 25191
Data columns (total 42 columns):
 #   Column                       Non-Null Count  Dtype  
---  ------                       --------------  -----  
 0   duration                     25192 non-null  int64  
 1   protocol_type                25192 non-null  object 
 2   service                      25192 non-null  object 
 3   flag                         25192 non-null  object 
 4   src_bytes                    25192 non-null  int64  
 5   dst_bytes                    25192 non-null  int64  
 6   land                         25192 non-null  int64  
 7   wrong_fragment               25192 non-null  int64  
 8   urgent                       25192 non-null  int64  
 9   hot                          25192 non-null  int64  
 10  num_failed_logins            25192 non-null  int64  
 11  logged_in                    25192 non-null  int64  
 12  num_compromised              25192 non-null  int64  
 13  root_shell                   25192 non-null  int64  
 14  su_attempted                 25192 non-null  int64  
 15  num_root                     25192 non-null  int64  
 16  num_file_creations           25192 non-null  int64  
 17  num_shells                   25192 non-null  int64  
 18  num_access_files             25192 non-null  int64  
 19  num_outbound_cmds            25192 non-null  int64  
 20  is_Host_login                25192 non-null  int64  
 21  is_guest_login               25192 non-null  int64  
 22  count                        25192 non-null  int64  
 23  srv_count                    25192 non-null  int64  
 24  serror_rate                  25192 non-null  float64
 25  srv_serror_rate              25192 non-null  float64
 26  rerror_rate                  25192 non-null  float64
 27  srv_rerror_rate              25192 non-null  float64
 28  same_srv_rate                25192 non-null  float64
 29  diff_srv_rate                25192 non-null  float64
 30  srv_diff_Host_rate           25192 non-null  float64
 31  dst_Host_count               25192 non-null  int64  
 32  dst_Host_srv_count           25192 non-null  int64  
 33  dst_Host_same_srv_rate       25192 non-null  float64
 34  dst_Host_diff_srv_rate       25192 non-null  float64
 35  dst_Host_same_src_port_rate  25192 non-null  float64
 36  dst_Host_srv_diff_Host_rate  25192 non-null  float64
 37  dst_Host_serror_rate         25192 non-null  float64
 38  dst_Host_srv_serror_rate     25192 non-null  float64
 39  dst_Host_rerror_rate         25192 non-null  float64
 40  dst_Host_srv_rerror_rate     25192 non-null  float64
 41  class                        25192 non-null  object 
dtypes: float64(15), int64(23), object(4)
memory usage: 8.1+ MB
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/ids.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;
Anyway, this is a synthetic dataset with some characteristics based on simulation.
However, it is close to real-world examples enough.
Next, we will try some machine learning models for predicting anomalies.&lt;/p&gt;

&lt;p&gt;With a visualization technique, like T-SNE, we have a diagram of data distribution.
A green point is a normal data point, and a red one is an anomaly.
We can see that it would be hard to draw a linear boundary between normal points and anomalies.&lt;/p&gt;

&lt;h3 id=&quot;play-with-some-machine-learners&quot;&gt;Play with some machine learners&lt;/h3&gt;

&lt;p&gt;In this section, we will try two different ML models with two different training/inference strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Generative model with a reconstruction strategy&lt;/strong&gt;: the Auto-Encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# creating the autoencoder model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;sigmoid&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Discriminative model with a classification strategy&lt;/strong&gt;: the quite classical Random Forest!&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;gini&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;training-and-validation&quot;&gt;Training and validation&lt;/h4&gt;

&lt;p&gt;We need to transform the data a little bit.
It is usual to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MinMaxScaler&lt;/code&gt; to perform data transformation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Auto-Encoder is trained in a reconstruction manner, i. e., it learns to reconstruct the input:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But the Random Forest learns to classify data as usual:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;results-of-auto-encoder&quot;&gt;Results of Auto-Encoder&lt;/h4&gt;

&lt;p&gt;Since the prediction of AE relies on how good it can reconstruct the input:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the reconstruction error is over a threshold, then the reconstruction fails, and the input is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anomaly&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Otherwise, the input is normal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we must define a &lt;strong&gt;threshold&lt;/strong&gt; to separate anomalies and normal inputs.
To find such threshold, we can compute from the training dataset:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# reconstructing the train set
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# finding the mean reconstruction error
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since the mean of construction error is 0.0018520295581492838, we can choose the threshold &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thres = 0.0018&lt;/code&gt;.
Validation in validation set:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# classifying the samples based on threshold
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;anomaly&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thres&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;normal&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Acc. = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[%], F1 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f1_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;anomaly&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[[2901   69]
 [ 563 2765]]
Acc. = 89.96506827564306[%], F1 = 90.17718371153248[%]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;~90%&lt;/strong&gt; of F1 score. Hmm, not so bad!&lt;/p&gt;
&lt;h4 id=&quot;results-of-random-forest&quot;&gt;Results of Random Forest&lt;/h4&gt;

&lt;p&gt;Let’s see the results with Random Forest:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[[3321    7]
 [  14 2956]]
Acc. = 99.66656081295649[%], F1 = 99.64604753076016[%]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;99.6%&lt;/strong&gt;, it is quite good!&lt;/p&gt;

&lt;h2 id=&quot;some-methods-for-machine-learning-based-ids&quot;&gt;Some methods for Machine Learning based IDS&lt;/h2&gt;
&lt;p&gt;There are many ways to identify anomalous access (i. e., intrusion) in the network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rule-based methods&lt;/strong&gt; tend to find a “good” heuristic that can be generalized to a global policy for all feature space.
The same policy can be applied to every input.
For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;every connection which lasted for more than 7 minutes are anomalies&lt;/code&gt; is a policy to identify intrusions.
The problem with this approach is that many false negatives (misses) can be raised.
Because the rule is clear, counterfactuals try to make themselves legal (try to connect faster) and overcome the 7-minute rule.
So a fixed rule is not enough.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The attacks become more and more advanced, but the rules cannot be changed so fast, making an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Achilles&apos; heel&lt;/code&gt; in the defense system.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Machine Learning-based methods&lt;/strong&gt; try to define a soft boundary that can be learned and improved over time.
The main advantage of the Machine Learning approach is that since the boundary is soft and there is no clear rule, the intruders cannot know the rule exactly (how many minutes should they make for a successful intrusion?).
Another important aspect is that since the soft rule is not fixed, it can &lt;strong&gt;evolve&lt;/strong&gt; with the intrusions: the more advanced the intrusion is, the more advanced the defender is.
When this sounds &lt;em&gt;ambiguous&lt;/em&gt;, but for the security systems, it becomes exactly a common strategy to overcome incidents.
The Machine Learning methods can be generative or discriminative, but they must be somewhat non-linear and ambiguous enough to hide details of the system to hackers.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;We have reviewed several views of NIDS: at a corporation network view, cloud solutions, and machine learning models.&lt;/p&gt;

&lt;p&gt;On the synthetic examples, we observed that a quite classical model like &lt;strong&gt;Random Forest&lt;/strong&gt; can outperform neural nets.
It is not a new thing: we already &lt;em&gt;empirically&lt;/em&gt; knew that Random Forest is good at this problem, especially when it is in synthetic environments.
Somebody can argue that there is a chance for overfitting with Random Forest: hmm, I don’t think so.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;And even if it overfits, this is quite good, because &lt;strong&gt;that’s what we want&lt;/strong&gt;: &lt;strong&gt;We want our model to overfit to this dataset&lt;/strong&gt;.&lt;/p&gt;

  &lt;p&gt;Anonymous&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Source code for this article can be found at:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/AIFI-INC/ml-ids&quot;&gt;AIFI-INC’s ML-based IDS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 29 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/ml-ids/</link>
        <guid isPermaLink="true">https://wanted2.github.io/ml-ids/</guid>
        
        <category>Machine learning</category>
        
        <category>network intrusion detection</category>
        
        <category>threat intelligence</category>
        
        <category>edge computing</category>
        
        <category>iot</category>
        
        
        <category>Site Reliable Engineering</category>
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>責任者</title>
        <description>&lt;p&gt;ベトナムに帰ってからベトナムの職場文化になってからはもはや2年間になっております．
ベトナム職場でいうと，恥ずかしいけど，楽しい経験もあるし，悲しい経験もありました．
仕事の責任者として働いた経験もあり，悲しい時で，部下に怒られて，そろそろ殴られる経験もありました．
なぜなら，背景から考えると，文化の違いかなと思います．
ベトナム職場では上下関係は社会的に存在するけど，きちんと働かないとね，上下関係なく殴られるそうです．
「殴られる」は厳しい言葉ですが，主にいうと，ベトナム職場で下記の三大原則はあります．&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則1：&lt;strong&gt;Có làm thì mới có ăn&lt;/strong&gt;. You must work to be fed. 職あり食ある．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則2：&lt;strong&gt;Có lỗi thì sửa là được&lt;/strong&gt;. Don’t be panic, just fix bugs, then it’s OK. 問題にダラダラしないで，修正すれば問題なし．（ただ何もしないなら，↑の原則１をみてください）．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則3：&lt;strong&gt;Làm thì làm cho nghiêm chỉnh.&lt;/strong&gt; You should do the work thoroughly. 徹底的にやりましょう．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ベトナム職場の約2年間の経験をまとめて，一度整理したいと思っております．
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;第一原則職と食&quot;&gt;第一原則：「職」と「食」&lt;/h1&gt;

&lt;p&gt;我々は職場で「仕事」を大事にしております．
仕事が順調であれば，食感もよくなるという発想です．
仕事をしなければ，食えるもんがなく，大変ですね．
ですが，仕事（職）があって，就いたけど何もしないことで組織には不満が引き起こされたことが多いようです．&lt;/p&gt;

&lt;p&gt;ですので，組織全体で皆は統一して，職務をしてから食べるということを理解したらよかったです．
もちろん，海外から来る要員だと，現地の文化がわからず，仕事とは何か現地の人の考え方は最初でわからないこともあります．
しかし，時間が経過すると，その意義を理解すれば，原則を守る姿勢を整い，結局「労働」は第一だという考え方に帰着しました．
組織全体といえば，作業人だけではなく，管理職でもきちんと働き姿勢を見せればよいと思います．&lt;/p&gt;

&lt;p&gt;ここで，一番気になることは，この原則にはもう一つの意味の層があります．
仕事をやるといっても，何をやっても認められるのではないです．
「すごいことをやる」のも間違いです．
ある「職」に就く時，自分の役割と権限は決まっております．
ある場合，期待もあります．
しかし，開発の現場では，ほとんどの場合，役割と権限と責任範囲・仕事内容はすでに決まっております．
（※申し訳ないが，それぐらい曖昧に契約してしまえば，ちょっとそれがまずい職になるかもしれません）．
ですので，「自分の役割と権限・責任範囲の中に行動し，仕事を仕上げる」ことで職を行っているねといいます．
まあ，「職あり食ある」という句です．&lt;/p&gt;

&lt;p&gt;※曖昧に定めてしまった職に入った場合，もちろん仕事をしたから食べることがあるけど，ちょっと食べ物は美味しくないかもしれないです．
おいしい食をどうやって作るか山ほど研究がありますので，それらをGoogleして参考にすればと思います．&lt;/p&gt;

&lt;h1 id=&quot;第二原則チームは障害にだらだらしないで乗り越えることは大事&quot;&gt;第二原則：チームは障害にだらだらしないで，乗り越えることは大事！&lt;/h1&gt;
&lt;p&gt;チーム運営の中に，一緒に働くので，楽しい時も，悲しい時も一緒に乗り越えています．
楽しい時はいいんだけど，悲しい時はどんな時かな？
自分の経験では，パニック状態になるときです．
多くの場合，急に障害が起きるときとかですね．&lt;/p&gt;

&lt;p&gt;確かに，パニック状態のハンドルをうまく取り込めているチームはすごいねとおもいます．
パニック状態になった場合，よいチームは取り込むけど，悪いチームは責任追及ゲームを遊びます．
なぜ責任追及ゲームは悪いのかというと，緊急対応なのに，問題対応をせずに，ゲームをしているからです．
よいチームは問題を見極めて，対処法を第一優先し対応を取り込むのです．&lt;/p&gt;

&lt;p&gt;※ちなみに，チームで開発する場合には，運営中で時間がかかってしまう状態があります．
それが，&lt;strong&gt;パニック状態&lt;/strong&gt;と，&lt;strong&gt;会議状態&lt;/strong&gt;です．
なぜなら，この2つだけは，一人の時間を取るだけではなく，チーム皆の時間を取っているからです．
だらだらして，仕事が進まない罠に落ちやすいのです．
チームのパフォーマンスを上げたい場合に，責任者として，回避と対応策を計画しなければなりません．&lt;/p&gt;

&lt;p&gt;ベトナム職場では，なぜこの原則が取り込まれているかというと，チームがどこかにだらだらすると，めっちゃ時間がかかっているから，まずは乗り越えることを第一優先したいからです．
人がミスをすることは人間の根性ですので，それよりももっと悪いことは，&lt;strong&gt;「なにもしないこと」&lt;/strong&gt;です．
直さないことや修正しないことなどはまた，原則1で処分されると思います．
いつもチームを前向きに進行させることができればと思います．&lt;/p&gt;

&lt;h1 id=&quot;第三原則自分の仕事へのこだわりも重視完成したらもう一度見直そう&quot;&gt;第三原則：自分の仕事へのこだわりも重視！完成したら，もう一度見直そう！&lt;/h1&gt;

&lt;p&gt;2年間ベトナム職場で感じたもう一つの原則です．
第一原則と第二原則を相互に働かせているため，「作成」と「修正」を交互に行っています．
しかし，これらを交互に働かせると，永遠に修正のループに入る可能性はまだ残っています．
ですので，途中でもっと修正してもあまり報われないと感じるときとか，早く止めた方がよいと感じるときとか，もう一つのコントローラーが必要でしょうか．
それは第三原則です．
必ず，（再）作成と（再）修正が完成したら，もう一度&lt;strong&gt;客観的&lt;/strong&gt;にレビューしましょう．
狭い視野で詳細をレビューするだけではなく，広い視野で，この作成と修正は長期的によいか悪いか一回考えるべきというステップがあれば，なおよいです．
ここで，責任者として，技術部分だけではなく，スコープ管理とか要望管理とかスケジュール管理とかうまく併せてやる必要があります．&lt;/p&gt;

&lt;h1 id=&quot;結論&quot;&gt;結論&lt;/h1&gt;

&lt;p&gt;これが，おそらく私が2年間で観測したベトナム職場で支配されている三大原則かと思います．
これらは，なぜか職場をコントロールして，各PJを進行させることが多いそうです．&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/pm-sekininsha/</link>
        <guid isPermaLink="true">https://wanted2.github.io/pm-sekininsha/</guid>
        
        <category>責任者</category>
        
        <category>担当者</category>
        
        <category>権限管理</category>
        
        <category>計画管理</category>
        
        <category>進捗管理</category>
        
        <category>スコープ管理</category>
        
        <category>リスク管理</category>
        
        <category>コミュニケーション管理</category>
        
        <category>リソース管理</category>
        
        <category>プロジェクトマネジメント</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>4 điều nên làm để quản lý tài khoản AWS</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Những tri thức cơ bản như làm thế nào quản lý mật khẩu với đặt mật khẩu như thế nào thì có lẽ các em tự chủ động thiết lập nhé.
Trong bài viết này chúng ta sẽ tập trung vào 4 chức năng mang tính nâng cao (advanced) của bảo mật tài khoản AWS.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Thiết lập và quản lý tài khoản AWS&lt;/strong&gt; không chỉ đơn giản là vấn đề bảo mật dữ liệu mà còn sâu xa hơn là quản lý toàn bộ account trong tổ chức một cách &lt;strong&gt;đồng bộ, tập trung và hiệu quả&lt;/strong&gt; nhất.
Việc này rất quan trọng bởi account chính là cửa ngõ vào của mọi luồng dữ liệu, mọi truy cập, do đó nếu việc quản lý account bị lơi là sẽ dẫn đến việc những truy cập bất hợp pháp có thể xâm nhập vào “hệ miễn dịch” của tổ chức và gây hại.
4 chức năng chúng ta nói đến hôm nay bao gồm:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Bảo mật nhiều lớp cho tài khoản root;&lt;/li&gt;
  &lt;li&gt;Quản lý và thiết lập phân quyền cùng AWS Organizations;&lt;/li&gt;
  &lt;li&gt;AWS Single-Sign On (SSO); và&lt;/li&gt;
  &lt;li&gt;Giám sát hành vi tài khoản trên AWS.
&lt;!--more--&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;giới-thiệu&quot;&gt;Giới thiệu&lt;/h1&gt;
&lt;p&gt;Như trong tiếng Nhật thì chữ tài khoản (tiếng Anh, account) được viết là 口座, bắt đầu bằng bộ “khẩu (miệng)”.
Các bạn đều biết cửa sổ đi vào hệ tiêu hóa mà mọi thứ đồ ăn thức uống phải đi qua đều bắt đầu từ miệng.
Cái miệng chính là nơi mọi thức ăn xâm nhập vào cơ thể con người.
Thức ăn có thể hàm chứa dinh dưỡng, nhưng cũng có thể hàm chứa chất độc, bệnh dịch.
Có thể ngon mà cũng có thể dở.
Việc bảo vệ “bộ nhá” là cực kỳ quan trọng với con người nói chung!&lt;/p&gt;

&lt;p&gt;Trong bài viết này song song với việc thao tác trên AWS chúng ta cũng sẽ nói thêm về các nguyên lý đằng sau các thiết lập tài khoản, các dịch vụ bảo mật đa lớp, single sign-on, .v.v…
Với nguyên tắc là mọi thứ qua miệng đều phải rõ nguồn gốc (cũng không cần chi tiết quá, nhưng ở mức có thể track được).&lt;/p&gt;

&lt;h1 id=&quot;bảo-mật-nhiều-lớp&quot;&gt;Bảo mật nhiều lớp&lt;/h1&gt;

&lt;h2 id=&quot;điểm-qua-những-tiến-bộ-của-mảng-bảo-mật-nhiều-lớp&quot;&gt;Điểm qua những tiến bộ của mảng bảo mật nhiều lớp&lt;/h2&gt;

&lt;p&gt;Những tri thức nền tảng cho bảo mật nhiều lớp xuất hiện lần đầu từ bài báo của &lt;strong&gt;Mitchell Trauring &lt;a class=&quot;citation&quot; href=&quot;#trauring1963automatic&quot;&gt;[1]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#jain201650&quot;&gt;[2]&lt;/a&gt;&lt;/strong&gt; năm 1963.
Trong bài viết về cách thức nhận dạng vân tay tự động bằng &lt;strong&gt;Ridge&lt;/strong&gt; patterns, ông viết:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is the purpose of this article to present, together with some evidence of its feasibility, a method by which decentralized automatic identity verification, such as might be desired for credit, banking or security purposes, can be accomplished through automatic comparison of the minutiae in finger-ridge patterns.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Mitchell Trauring, Nature, March 1963&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/multi-factor.png&quot; style=&quot;float: right; margin: 10px; width: 50%&quot; /&gt;
Như hình vẽ bên các bạn có thể thấy, bình thường bảo mật 1 lớp thì chỉ cần có mật khẩu (&lt;em&gt;something you know&lt;/em&gt;).
Tuy nhiên, để tăng độ bảo mật, ví dụ như khi mật khẩu bị đánh cắp thì nên có nhiều phương pháp bảo mật đi kèm như &lt;strong&gt;token &lt;a class=&quot;citation&quot; href=&quot;#holdsworth2008token&quot;&gt;[3]&lt;/a&gt;, vân tay &lt;a class=&quot;citation&quot; href=&quot;#jain201650&quot;&gt;[2]&lt;/a&gt;, khuôn mặt &lt;a class=&quot;citation&quot; href=&quot;#grother2018ongoing&quot;&gt;[4]&lt;/a&gt;, PIN/key &lt;a class=&quot;citation&quot; href=&quot;#boyd2003protocols&quot;&gt;[5]&lt;/a&gt;, mống mắt, …&lt;/strong&gt;.
Cái quan trọng là phải kết hợp thêm những cái gì user có (&lt;em&gt;something you have&lt;/em&gt;) và những gì của user (&lt;em&gt;something you are&lt;/em&gt;).
&lt;img src=&quot;/assets/images/fingerprint-matching.png&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Trích dẫn: &lt;a class=&quot;citation&quot; href=&quot;#trauring1963automatic&quot;&gt;[1]&lt;/a&gt; mô hình vân tay và cách matching local points.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fusion.jpg&quot; style=&quot;float: left; margin: 10px; width: 50%&quot; /&gt;
Tuy nhiên, một vấn đề lớn của các phương pháp bảo mật đi kèm là độ chính xác.
Ví dụ như với vân tay &lt;a class=&quot;citation&quot; href=&quot;#pankanti2002individuality&quot;&gt;[6]&lt;/a&gt; thì xác suất &lt;em&gt;false matching probability&lt;/em&gt; vẫn có thể đạt $6.10\times 10^{-8}$ với mẫu vân tay có 36 điểm minutiae.
Do đó việc sử dụng đơn thuần 1 đặc trưng anatomical hoặc behavior có thể tạo ra nhiều &lt;em&gt;false non-matching&lt;/em&gt; cũng như &lt;em&gt;false matching&lt;/em&gt;.
Cũng vì đó mà cần phải dùng nhiều đặc trưng, và khi dùng nhiều đặc trưng thì có đặc trưng quan trọng, có đặc trưng không, phải có &lt;strong&gt;weight&lt;/strong&gt; cho từng đặc trưng.
Và đó chính là ý tưởng &lt;strong&gt;feature fusion&lt;/strong&gt; hay là &lt;strong&gt;multimodal&lt;/strong&gt; cho mảng biometrics.
&lt;em&gt;Nói chung, cũng toàn mấy cái ý tưởng của các fellows từ thời 1990s-2000s&lt;/em&gt;, thời các sếp ấy thì cũng đã 20-30 năm về trước, nên mấy cái ý tưởng này tôi nghĩ mới cũng không mới và trong mảng biometrics cũng đã implement gần hết, thậm chí vào sản phẩm thị trường rồi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/biometrics.png&quot; style=&quot;float: right; margin: 10px; width: 50%&quot; /&gt;
Cái mảng &lt;strong&gt;bảo mật nhiều lớp &lt;a class=&quot;citation&quot; href=&quot;#ometov2018multi&quot;&gt;[7]&lt;/a&gt;&lt;/strong&gt; và &lt;strong&gt;biometrics &lt;a class=&quot;citation&quot; href=&quot;#jain201650&quot;&gt;[2]&lt;/a&gt;&lt;/strong&gt; nhìn chung là cũng liên đới rộng phết, từ bảo mật cloud đến tận thiết bị IoT &lt;a class=&quot;citation&quot; href=&quot;#ngu2016iot&quot;&gt;[8]&lt;/a&gt; cũng liên quan.
Nhưng nhìn chung cái mô hình thực thi thì khá giống nhau:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Về mặt &lt;strong&gt;hệ thống&lt;/strong&gt; thì anh em nào làm nhiều về cloud, web app thì sẽ hiểu ngay là trong framework sẽ có một bộ phận middleware có những function kiểu như &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preLogin(), postAuthentication(), ...&lt;/code&gt; là sẽ là nơi implement thêm các lớp bảo mật. Ở trong middleware &lt;a class=&quot;citation&quot; href=&quot;#ngu2016iot&quot;&gt;[8]&lt;/a&gt; thì chúng ta sẽ call thêm các lớp bảo mật khác để đưa ra kết quả.
    &lt;ul&gt;
      &lt;li&gt;Ngoài ra, việc fusion của các đặc trưng có thể thực hiện tại &lt;strong&gt;biometrics sensor&lt;/strong&gt;, tức là &lt;strong&gt;early fusion&lt;/strong&gt;. Hoặc tại module decision, tức là &lt;strong&gt;late fusion&lt;/strong&gt; và chủ yếu là fuse kết quả nhận dạng của từng modal.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Về mặt &lt;strong&gt;module&lt;/strong&gt;, thì module bảo mật của từng lớp thì sẽ như hình vẽ bên phải. Sẽ có enrollment (đăng ký) và verification (đăng nhập/nhận dạng).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nói chung mảng bảo mật nhiều lớp cũng có khá nhiều dư địa, tuy nhiên nếu so với những mảng như &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GAN&lt;/code&gt; hay &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deep learning&lt;/code&gt; thì số lượng papers mỗi năm trong academics có vẻ không nhiều.
Mà số lượng papers có nhiều trích dẫn thì cũng ít hơn.
Vì đơn giản như paper về GAN của Goodfellow năm 2014, đến nay là 7 năm mà đã có gần 40k trích dẫn, mà cái mảng đó hầu như top-2% về nhiều trích dẫn thì cũng toàn những paper vài ngàn citations trở nên.
Nói chung lựa chọn trích dẫn bài nào cũng tuỳ mảng, vì mảng biometrics, tôi thấy những papers của các fellow cũng chỉ vài trăm citations trong 10 năm.
Nhưng cái mảng deep learning với GAN thấy năm nào cũng hàng ngàn papers mà những papers top thì toàn hàng ngàn citations.
Nên đôi khi hỏi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;có cần đọc hết các papers trong mảng không?&lt;/code&gt;, thì tôi nghĩ là nếu những mảng ít ít như biometrics thì đọc cũng được, nhưng mấy cái mảng liên quan tới deep với GAN, thì &lt;strong&gt;tôi nghĩ cũng chả cần đọc đâu&lt;/strong&gt;.
Chỉ cần filter ra top 2% mà chủ yếu những bài có tốc độ tăng trưởng mạnh (mỗi năm trên 200 citations) thì đọc thôi, chứ nó nhiều thế này mà toàn na ná nhau, thì papers mà ít citations chắc chắn là không phải trend, đọc không hết được mà đọc cũng chả để làm gì.
Tất nhiên, cũng phải ưu tiên những papers mới xuất bản năm nay thì mình nên đọc.
Tức là ví dụ năm nay là 2022 thì những bài từ 2021 thì mình ưu tiên đọc (nhưng vẫn chỉ nên đọc top tầm 30%), còn từ 2020 về trước thì chỉ chọn bài có độ tăng trưởng trung bình trên 200 citations/năm, và trong đó cũng chỉ nên chọn top.&lt;/p&gt;

&lt;p&gt;Cuối cùng, là về mảng biometrics này thì có một ứng dụng thú vị là &lt;em&gt;face recognition&lt;/em&gt; mà bên computer vision hay deep learning cũng nổi lên (gọi là &lt;em&gt;deep face recognition&lt;/em&gt;).
Mảng này thì quả là cũng không nhiều papers lắm, cũng giống biometrics, nhưng lại có một số papers có trích dẫn hàng ngàn mặc dù xuất bản sau 2015 như FaceNet hay RetinaFace, ArcFace, CosFace.
Tuy nhiên, nếu nói về thực tiễn sử dụng thì tôi nghĩ tốt nhất là nên tham khảo các con số trong benchmark lớn của chính phủ Mý &lt;a class=&quot;citation&quot; href=&quot;#grother2018ongoing&quot;&gt;[4]&lt;/a&gt;.
Lý do là bởi vì đó là các ứng dụng thực tiễn, cũng như bộ dữ liệu lớn tới vài chục triệu người, và quan trọng là mọi team trên thế giới đều có thể tham gia.
Dưới đây là bảng kết quả hạng mục Verification, tức là cho sẵn một pair hai khuôn mặt, đưa ra kết quả xem đó có cùng 1 identity không?
Kết quả là đường cong thể hiện tỷ lệ &lt;em&gt;false non-matching rate (FNMR) vs false matching rate (FMR)&lt;/em&gt;.
Chúng ta hãy để ý đến tỷ lệ &lt;strong&gt;FNMR@FMR=$10^{-5}$&lt;/strong&gt; thì tên tuổi đứng nhất là InsightFace (là nhóm tác giả của RetinaFace, ArcFace, …).
Họ chính là tác giả của framework &lt;a href=&quot;https://github.com/deepinsight/insightface.git&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;insightface&lt;/code&gt;&lt;/a&gt; để nhận dạng khuôn mặt 2D/3D.
Thành tích đứng nhất là $FNMR=0.0006$ khi $FMR=10^{-5}$, tức là 10 vạn người thì chỉ sai 1 người và miss 60 người, tổng là 61 người bị sai hoặc miss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nist-frvt-2022.png&quot; alt=&quot;NIST FRVT&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Trên đây chỉ là kết quả của dataset VISA, ngoài ra NIST còn có những task khác cùng với những dataset khác.
Từ VN thì cũng có team của một số đội như VNPT hay VIN AI/VIN Bigdata cũng tham gia và có được thứ hạng (tức là đã nộp bài).&lt;/p&gt;

&lt;h2 id=&quot;bật-tính-năng-mfa-của-aws&quot;&gt;Bật tính năng MFA của AWS&lt;/h2&gt;

&lt;h3 id=&quot;với-tài-khoản-root&quot;&gt;Với tài khoản root&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aws-mfa.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Với người dùng có &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html&quot;&gt;tài khoản root&lt;/a&gt; thì chắc thi thoảng bạn cũng sẽ login vào để làm một số việc như:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Quản lý IAM users&lt;/li&gt;
  &lt;li&gt;Quản lý bill service&lt;/li&gt;
  &lt;li&gt;Quản lý dịch vụ sử dụng&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vậy tài khoản root là chứa tất cả của bạn (hoặc tổ chức), vậy nên không thể không nâng độ bảo mật lên được.
Và nên bật MFA lên.
Mặc dù chúng ta đã tìm hiểu về MFA khá nhiều bên trên, thì &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html&quot;&gt;AWS hỗ trợ một số biện pháp MFA&lt;/a&gt; như sau thôi:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Virtual MFA devices&lt;/strong&gt; hay MFA thông qua phần mềm. Bạn có thể cài đặt Microsoft Authenticator vào điện thoại cho mục đích này.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;U2F security key&lt;/strong&gt; tức là 1 cái USB key để bạn cắm vào máy và nó sẽ dùng thông tin của USB này để nhận dạng bạn khi bạn đăng nhập.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hardware MFA device&lt;/strong&gt;. Một thiết bị được đăng ký sẽ tạo 1 code 6 chữ số để cho thuật toán time-synchronized one-time password (OTP).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nhìn chung, có thể vân tay hay khuôn mặt thì sẽ cần dùng một kênh khác, còn tài khoản AWS hiện chỉ support những kênh MFA thêm code.
Ngoài ra MFA thông qua SMS cũng không được hỗ trợ.
Sau khi khởi động phương thức MFA phù hợp, từ lần đăng nhập tiếp theo, người dùng sẽ phải nhập thêm code được sinh ra từ phần mềm hoặc thiết bị đã đăng ký.&lt;/p&gt;

&lt;h3 id=&quot;với-tài-khoản-iam&quot;&gt;Với tài khoản IAM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/iam-mfa.png&quot; style=&quot;float: left; margin: 10px; width: 50%&quot; /&gt;
Bạn có thể cấp &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_sign-in.html&quot;&gt;tài khoản IAM&lt;/a&gt; cho người dùng trong tổ chức của bạn (định nghĩa Roles/Permissions tương ứng với nhiệm vụ của nhân viên).
Nếu bạn muốn bảo mật hơn, bạn có thể bắt buộc nhân viên dùng IAM phải dùng MFA.
Bạn truy cập vào trang cấu hình của IAM user đó và yêu cầu assign MFA devices.
Xem thêm &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/console_sign-in-mfa.html&quot;&gt;hướng dẫn kích hoạt MFA cho tài khoản IAM&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Nhìn chung, MFA là một chức năng đơn giản để cấu hình và nên được áp dụng.
Trong những &lt;a href=&quot;https://aws.amazon.com/blogs/security/new-whitepaper-aws-cloud-security-best-practices/&quot;&gt;best practices của AWS&lt;/a&gt; cũng có hướng dẫn nên cấu hình MFA cho việc xóa object khỏi S3 chẳng hạn.
Và tại sao AWS chỉ áp dụng MFA dạng code 6 chữ số, chứ không hỗ trợ biometrics như vân tay hay khuôn mặt/mống mắt, thậm chí SMS cũng không được?
Có lẽ là do vân tay hay mống mặt và đặc biệt khuôn mặt vẫn có tỷ lệ sai nhất định (chúng phù hợp cho nhiệm vụ điều tra của cảnh sát hơn), còn SMS thì có khá nhiều lỗ hổng bảo mật liên quan rồi nên lựa chọn code theo device có lẽ hợp lý hơn.&lt;/p&gt;

&lt;h1 id=&quot;quản-lý-và-thiết-lập-quyền-với-aws-organizations&quot;&gt;Quản lý và thiết lập quyền với AWS Organizations&lt;/h1&gt;

&lt;h2 id=&quot;giới-thiệu-aws-organizations&quot;&gt;Giới thiệu AWS Organizations&lt;/h2&gt;

&lt;p&gt;Trong nghiệp vụ quản lý tổ chức, có một mảng khá quan trọng là quản lý tài sản (asset management).
Thì không đơn giản là lên danh sách tài sản và quản lý chúng, cái đó thì chỉ 1 file Excel là làm được.
Cái quan trọng hơn với trọng trách này là &lt;strong&gt;quản lý và giám sát&lt;/strong&gt; mọi tài sản trong tổ chức bao gồm cả tài sản, tài nguyên máy móc.
Với AWS thì là quản lý tất cả các tài khoản nhân viên, máy ảo, dịch vụ được dùng một cách &lt;strong&gt;tập trung và hiệu quả&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tại sao phải tập trung lại 1 chỗ?&lt;/code&gt; Là bởi vì nếu phân tán sẽ khó kiểm soát và nhà quản lý sẽ rơi vào trạng thái bị động với mọi thay đổi trong tổ chức.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tại sao phải hiệu quả?&lt;/code&gt; Vì khi lên thành tổ chức, tài nguyên sẽ tăng lên rất nhiều, mà quản lý không hiệu quả thì cũng như người mù ngồi trên đống vàng. Người quản lý sẽ cần một công cụ để có thể ra policy và áp dụng policy đó một cách &lt;em&gt;đồng bộ&lt;/em&gt; trong toàn tổ chức.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Lời giải hợp lý ở đây chính là sử dụng &lt;strong&gt;&lt;a href=&quot;https://aws.amazon.com/organizations/&quot;&gt;AWS Organizations&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;#&lt;/th&gt;
      &lt;th&gt;Tên&lt;/th&gt;
      &lt;th&gt;Chức năng&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Manage your AWS accounts&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Tài khoản AWS trong tổ chức là nơi cần tổ chức quản lý quyền, bảo mật, tài nguyên và costs. Để dễ dàng tổ chức quản lý việc nâng cấp lên môi trường đa tài khoản (multi-account enviroment) là cần thiết. AWS hỗ trợ việc quản lý tập trung tài khoản qua CLI, SDKs, APIs và thậm chí là Infrastructure as code như &lt;a href=&quot;https://aws.amazon.com/cloudformation/?org_product_fr_cloudformation&quot;&gt;AWS Cloudformation&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Define and manage your organization&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Ngay khi khởi tạo các tài khoản, người quản lý có thể tạo nhòm tài khoản (OUs) phục vụ cho từ ứng dụng hoặc dịch vụ. Người quản lý có thể sử dụng tags để phân loại và track tài nguyên. Ngoài ra người quản lý có thể truyền lại quyền (delegate) cho nhân viên, khi đó nhân viên có thể coi là đại diện cho tổ chức để thực thi quyền.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Secure and monitor your accounts&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Người quản lý có thể quản lý bảo mật &lt;strong&gt;tập trung&lt;/strong&gt; và cho phép đội bảo mật truy cập cho tổ chức. AWS Organizations cung cấp dịch vụ bảo mật như &lt;a href=&quot;https://aws.amazon.com/guardduty/?org_product_fr_guardduty&quot;&gt;AWS GuardDuty&lt;/a&gt; để phát hiện và xử lý sự cố/nguy cơ bảo mật, &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html&quot;&gt;IAM Access Analyzer&lt;/a&gt; để review những truy cập không mong muốn, và dùng &lt;a href=&quot;https://aws.amazon.com/macie/&quot;&gt;Amazon Macie&lt;/a&gt; để quản lsy dữ liệu nhạy cảm (kiểu như review code xem có sót thông tin mật nào không?).&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Control access and permissions&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Set up Amazon Single Sign-On (SSO) để cung cấp cho tài khoản người dùng AWS quyền truy cập tài khoản thông qua active directory và tối ưu quyền dựa trên roles. Người quản lý còn có thể cho phép người dùng hoặc OUs sử dụng SCPs để quản lý quyền truy cập vào tài nguyên, dịch vụ, và regions trong tổ chức.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Share resources across accounts&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Quản lý tài nguyên tập trung với &lt;a href=&quot;https://aws.amazon.com/ram/?org_product_ow_ram&quot;&gt;AWS Resource Allocation Management (RAM)&lt;/a&gt;. Kể cả license cũng có thể được quản lý tập trung với &lt;a href=&quot;https://aws.amazon.com/license-manager/?org_product_ft_licensemanager&quot;&gt;AWS License Manager&lt;/a&gt;, và chia sẻ dịch vụ trong cả tổ chức với AWS Service Catalog.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Audit your environment for compliance&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Người quản lý có thể kích hoạt &lt;a href=&quot;https://aws.amazon.com/cloudtrail/&quot;&gt;AWS CloudTrail&lt;/a&gt; để log mọi hành vi trong môi trường tổ chức, mà nhân viên không thể tắt đi được. Người quản lý có thể cấu hình AWS Backup để backup mọi tài nguyên, và dùng AWS Config để đồng bộ cấu hình tài nguyên dịch vụ trong cả tổ chức bất cháp tài khoản và regions.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Centrally manage billing and costs&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Quản lý costs tập trung với AWS Cost Explorer, và sử dụng AWS Compute Optimizer để tối ưu giá thành.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;usecase-aws-govcloud&quot;&gt;Usecase: AWS GovCloud&lt;/h2&gt;

&lt;p&gt;Trên trang chủ của AWS Organizations có thể tìm thấy khá nhiều usecases có sẵn.
Trong bài này chúng ta xem lại 1 usecase của AWS GovCloud &lt;a class=&quot;citation&quot; href=&quot;#mahakian2020aws&quot;&gt;[9]&lt;/a&gt;.
Usecase này được đưa ra bởi MITRE.
Họ đưa ra 9 lời khuyên trong việc quản lý AWS Organizations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reserved Instances&lt;/strong&gt;: nên thuê dài hạn để nhận discount.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Spot Instances&lt;/strong&gt;: nên thuê những instances dạng spot để có thể bid giá và chạy khi giá thực vượt quá giá bid.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Auto Scaling and Automating Elasticity&lt;/strong&gt;: auto-scaling instances là chuyện nên làm và policy để scale nên dựa trên volume demand.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;S3 Object Lifecycle Management&lt;/strong&gt;: storage type nên được chọn dựa theo object lifecycle và tần suất truy cập.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Instance States and Stopping Instances&lt;/strong&gt;: hãy cấu hình chỉ bill với những cái gì đang chạy. Không nên thuê kiểu phải trả cho thời gian không chạy.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tagging&lt;/strong&gt;: để giám sát giá thành nên dùng labels.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AWS Config&lt;/strong&gt;: đồng bộ cấu hình tài nguyên trên toàn tổ chức và mọi regions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;AWS Organizations&lt;/strong&gt;: quản lý quyền và tài khoản trong tổ chức một tập trung và hiệu quả.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;EC2 Right Sizing&lt;/strong&gt;: nếu dùng EC2 thì phải lựa chọn size phù hợp nhu cầu, không nên dùng lớn hơn nhu cầu dẫn tới lãng phí.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nói chung là nếu quản lý tài nguyên một cách tập trung và hiệu quả thì giá thành của tổ chức sẽ rẻ hơn nhiều so với dùng tài khoản cá nhân.&lt;/p&gt;

&lt;h1 id=&quot;aws-single-sing-on-sso&quot;&gt;AWS Single Sing-On (SSO)&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://d1.awsstatic.com/diagrams/SSO-diagram.7b77570150a19ea35cfe4b923e1aee9f52b3dd06.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;giới-thiệu-về-federated-learning-với-aws&quot;&gt;Giới thiệu về Federated learning với AWS&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Federated Authentication&lt;/strong&gt;. Vậy là chúng ta đã cấu hình xong AWS Organizations.
Bây giờ chúng ta đã tạo 1 tài khoản user, và chúng ta muốn user đó sẽ login và sử dụng AWS account với quyền giới hạn.
Tất nhiên là login bằng tài khoản root thì sẽ có mọi quyền rồi, nhưng vấn đề là việc đó cần phải giới hạn để tránh những sự cố không đáng có.&lt;/p&gt;

&lt;p&gt;Việc cấp quyền này chính là hình thức &lt;strong&gt;federated&lt;/strong&gt; mà chúng ta muốn học hỏi hôm nay.
Nhìn chung là nếu cứ tất cả dồn hết vào tài khoản root, thì chuyện quản lý phân quyền sẽ vô cùng mệt mỏi.
Do vậy, cần có &lt;strong&gt;fedareted learning&lt;/strong&gt; để nhượng quyền cũng như chia quyền cho thích hợp.&lt;/p&gt;

&lt;h2 id=&quot;cấu-hình-federated-account-để-người-dùng-có-thể-truy-cập-aws&quot;&gt;Cấu hình Federated account để người dùng có thể truy cập AWS&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aws-sso-assign-user.png&quot; style=&quot;float: left; margin: 10px; width: 50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Về phía người quản lý.&lt;/strong&gt; Một lời giải cho việc này chính là sử dụng AWS Single Sign-On (SSO), mà khởi đầu chỉ cần vào danh sách dịch vụ của AWS Organizations và bật SSO lên.
Sau đó người quản lý vào danh sách user và ấn check tên user account muốn cấp quyền và nhấn &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Assign user&lt;/code&gt;.
Người quản lý sẽ được đưa đến màn hình &lt;strong&gt;Assign Users&lt;/strong&gt; như sau.
&lt;img src=&quot;/assets/images/aws0-sso-set-permission.png&quot; style=&quot;float: right; margin: 10px; width: 50%&quot; /&gt;
Tại đây, người quản lý có thể tạo một &lt;strong&gt;permission set&lt;/strong&gt; mới cho user account đã cho, hoặc cấp ngay một permission set có sẵn.
Trong ví dụ này tôi chọn luôn permission set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataScientist&lt;/code&gt; có sẵn để cho user account đã tạo nhé.
Cần chú ý ở đây là bộ quyền &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataScientist&lt;/code&gt; thì chỉ cho phép user truy cập các dịch vụ Analytics như database, DataLake Formation, RedShift, .v.v… chứ những dịch vụ như CLoudFront, Lambda, … là cần thêm quyền.
Tùy vào nhu cầu sử dụng mà người quản lý có thể tạo những bộ quyền có sẵn để tiện cho việc quản lý.
&lt;img src=&quot;/assets/images/aws-sso-assign-user-2.png&quot; style=&quot;float: left; margin: 10px; width: 50%&quot; /&gt;
Sau khi nhấn thêm vài nút bấm &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OK em&lt;/code&gt; nữa thì bộ quyền đã được assign cho user nhá.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aws-sso-aws.png&quot; style=&quot;float: right; margin: 10px; width: 50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Về phía federated user.&lt;/strong&gt; Thì họ cần truy cập vào link SSO do người quản lý cung cấp.
Trong lần truy cập đầu, họ sẽ cần thay đổi mật khẩu mặc định và đăng nhập.
Như bạn thấy ở hình bên, người dùng sẽ truy cập được vào AWS Account với quyền truy cập giới hạn.
Người dùng cũng có thể cấu hình &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MFA devices&lt;/code&gt; để tăng bảo mật theo yêu cầu của người quản lý.
Chú ý là không chỉ AWS Account mà cả các &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SAML service&lt;/code&gt; nếu có thì người dùng có thể truy cập thông qua cổng này.&lt;/p&gt;

&lt;h1 id=&quot;giám-sát-hành-vi-với-aws-cloudtrail&quot;&gt;Giám sát hành vi với AWS CloudTrail&lt;/h1&gt;

&lt;p&gt;Nếu application logs có thể được lưu trữ và quản lý với CloudWatch thì &lt;a href=&quot;https://aws.amazon.com/cloudtrail/&quot;&gt;CloudTrail&lt;/a&gt; có thể quản lý mọi hành vi diễn ra trên tài khoản AWS Accounts của cả cá nhân lẫn organization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://d1.awsstatic.com/product-marketing/CloudTrail/product-page-diagram_AWS-CloudTrail_HIW%402x.d314033178a16dbbd99111038789685e42f23278.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Với CloudTrail, người quản lý của tổ chức có thể làm rất nhiều việc:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Auditting&lt;/strong&gt;: theo dõi, giám sát tự động mọi hoạt động diễn ra với tài khoản AWS Organizations. Người quản lý có thể truy vết lại &lt;a href=&quot;https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html&quot;&gt;log hành vi trên tài khoản tối đa 90 ngày quá khứ&lt;/a&gt;. Nếu muốn mở rộng trên 90 ngày thì có thể liên hệ với AWS để cấu hình thêm. CloudTrail cũng có thể &lt;a href=&quot;https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-ct-api-tutorial.html&quot;&gt;tích hợp vào EventBridge&lt;/a&gt; để tăng cường theo dõi và lên cảnh báo. Log file của CloudTrail được mã hóa và được bảo vệ khỏi mất mát. Dữ liệu log trên nhiều vùng có thể được &lt;a href=&quot;http://docs.aws.amazon.com/awscloudtrail/latest/userguide/aggregatinglogs.html&quot;&gt;aggregate&lt;/a&gt; lại. ClouTrail Insight và CloudTrail Lake là các dịch vụ tăng cường hỗ trợ việc quản lý log hành vi dễ hơn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Phát hiện và xử lý vi phạm bảo mật&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Đối ứng sự cố&lt;/strong&gt;: phát hiện bất thường và tìm hiểu root cause.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Giá cả của CloudTrail và CloudTrail Lake có thể tìm ở &lt;a href=&quot;https://aws.amazon.com/cloudtrail/pricing/&quot;&gt;đây&lt;/a&gt;.
Nói chung là nên ước tính trước số lượng sự kiện hành vi trên account để có kế hoạch chọn gói sử dụng thích hợp.&lt;/p&gt;

&lt;p&gt;Một mẫu log của CloudTrail cho sự kiện &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start-instances&lt;/code&gt; sẽ như sau:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Records&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;userIdentity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;IAMUser&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;principalId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;EX_PRINCIPAL_ID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;arn&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arn:aws:iam::123456789012:user/Alice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;accessKeyId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;EXAMPLE_KEY_ID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;accountId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;123456789012&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;userName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Alice&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventTime&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2014-03-06T21:22:54Z&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventSource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2.amazonaws.com&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;eventName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;StartInstances&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;awsRegion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;us-east-2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;sourceIPAddress&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;205.251.233.176&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;userAgent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ec2-api-tools 1.6.12.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;requestParameters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;instancesSet&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;items&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;instanceId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;i-ebeaf9e2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]}},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;responseElements&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;instancesSet&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;items&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;instanceId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;i-ebeaf9e2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;currentState&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;code&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;pending&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;previousState&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;code&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stopped&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]}}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Trên đây chúng ta cùng nhau điểm qua kha khá các vấn đề về quản lý tài khoản.
Thực ra chỉ 4 điểm có thể là chưa bao quát hết, nhưng cũng hầu như là nắm trọn mảng này.
Một số vấn đề thú vị khác như RAM, AWS Config, … chúng ta sẽ đề cập khi có dịp.&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;trauring1963automatic&quot;&gt;Trauring, M. 1963. Automatic comparison of finger-ridge patterns. &lt;i&gt;Nature&lt;/i&gt;. 197, 4871 (1963), 938–940.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/trauring1963automatic/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jain201650&quot;&gt;Jain, A.K., Nandakumar, K. and Ross, A. 2016. 50 years of biometric research: Accomplishments, challenges, and opportunities. &lt;i&gt;Pattern recognition letters&lt;/i&gt;. 79, (2016), 80–105.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/jain201650/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;holdsworth2008token&quot;&gt;Holdsworth, J. 2008. Token for use in online electronic transactions. Google Patents.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/holdsworth2008token/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;grother2018ongoing&quot;&gt;Grother, P., Ngan, M., Hanaoka, K., Yang, J.C. and Hom, A. 2022. &lt;i&gt;Ongoing face recognition vendor test (FRVT) part 1: Verification&lt;/i&gt;. US Department of Commerce, National Institute of Standards and Technology.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/grother2018ongoing/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;boyd2003protocols&quot;&gt;Boyd, C., Mathuria, A. and Stebila, D. 2003. &lt;i&gt;Protocols for authentication and key establishment&lt;/i&gt;. Springer.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/boyd2003protocols/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pankanti2002individuality&quot;&gt;Pankanti, S., Prabhakar, S. and Jain, A.K. 2002. On the individuality of fingerprints. &lt;i&gt;Pattern Analysis and Machine Intelligence, IEEE Transactions on&lt;/i&gt;. 24, 8 (2002), 1010–1025.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/pankanti2002individuality/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ometov2018multi&quot;&gt;Ometov, A., Bezzateev, S., Mäkitalo, N., Andreev, S., Mikkonen, T. and Koucheryavy, Y. 2018. Multi-factor authentication: A survey. &lt;i&gt;Cryptography&lt;/i&gt;. 2, 1 (2018), 1.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ometov2018multi/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ngu2016iot&quot;&gt;Ngu, A.H., Gutierrez, M., Metsis, V., Nepal, S. and Sheng, Q.Z. 2016. IoT middleware: A survey on issues and enabling technologies. &lt;i&gt;IEEE Internet of Things Journal&lt;/i&gt;. 4, 1 (2016), 1–20.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ngu2016iot/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mahakian2020aws&quot;&gt;Mahakian, J., Holmdahl, S., Bada, Q., Silva, S. and Tretler, Z. 2020. &lt;i&gt;AWS GovCloud Resource and Cost Analysis&lt;/i&gt;. The MITRE Corporation.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/mahakian2020aws/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Sun, 16 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/aws-account/</link>
        <guid isPermaLink="true">https://wanted2.github.io/aws-account/</guid>
        
        <category>Account management</category>
        
        <category>Account security</category>
        
        <category>AWS</category>
        
        <category>AWS account</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Software Engineering</category>
        
        <category>Site Reliable Engineering</category>
        
      </item>
    
      <item>
        <title>Cán cân thu nhập: hàn lâm, khởi nghiệp và công ty</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Cộng đồng mạng xã hội hiện đại gần đây có nổ ra tranh cãi về câu nói &lt;a href=&quot;https://www.youtube.com/watch?v=p9D0KWI9O6o&quot;&gt;Có làm thì mới có ăn, cái loại không làm mà lại muốn có ăn thì chỉ có ăn …&lt;/a&gt; của tác giả &lt;strong&gt;Huấn Hoa Hồng&lt;/strong&gt;.
Những bô lão của cộng đồng hàm lâm như &lt;strong&gt;Giáo sư kiêm lính thủy đánh bạc&lt;/strong&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=V8YTqQkXGx0&quot;&gt;Tiến Bịp&lt;/a&gt; cũng nhanh chóng xuất bản các papers để phản bác Huấn.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Chú ý: video chứa nội dung và ngôn từ nhạy cảm!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nghe thì chả có vẻ liên quan gì nhưng những chân lý do các vị giáo sư “tự phong” của cộng đồng mạng này cũng có sức nặng nhất định: &lt;em&gt;Có làm thì mới có ăn&lt;/em&gt; thì đã là nguyên lý của lao động sản xuất có hệ thống trong xã hội.
Nhưng vấn đề là làm gì và làm thế nào?
Định hướng nghề nghiệp chính là một khâu quan trọng giúp các bạn trẻ phát triển sự nghiệp.
Tất nhiên ngoài hai vị “giáo sư” kể trên thì còn vô số học thuyết và lý thuyết trong cái mảng này với hàng ngàn cuốn sách đủ thể loại của các tác giả trên thế giới để định hướng các bạn trẻ.
Nhưng thôi chúng ta cứ đưa ra góc nhìn thiết thực nhất với mọi người lao động bình thường: &lt;strong&gt;thu nhập&lt;/strong&gt; trước đã nhé!
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;lựa-chọn-sự-nghiệp&quot;&gt;Lựa chọn sự nghiệp&lt;/h1&gt;

&lt;p&gt;Tất nhiên mức lương tối thiểu vùng miền là đã có quy định, mức đóng các khoản như bảo hiểm, thuế thu nhập cá nhân thì cũng đã có quy định công thức, cứ bỏ vào mấy cái Excel là nó tính rẹc phát ra ngay nên chúng ta cũng không quan tâm sâu tới những vấn đề đó mà chỉ chú ý là lương cao thì trách nhiệm thuế sẽ cao theo là thiết kế của khá nhiều hệ thống thuế TNCN của nhiều nước.&lt;/p&gt;

&lt;p&gt;Vậy giờ có mấy con đường?
Xin hãy nhìn vào biểu đồ bên dưới: hầu như chỉ có 2 nhánh&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Làm thuê&lt;/strong&gt;: thì lại phân nhánh làm 4 con đường con
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Làm cho hàn lâm&lt;/em&gt; như làm giảng viên, các vị trí trong trường đại học, nghiên cứu tại viện, …&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Làm cho chính phủ/NGO&lt;/em&gt;. Tuy nhiên hướng này hơi khó vì ít vị trí nên trong bài điều tra này xin phép không bàn về con đường này.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Làm cho công ty lớn&lt;/em&gt;. Đây là hướng chủ đạo của mọi anh em không đi hai con đường trên: kiếm doanh nghiệp kiểu quốc doanh, phúc lợi dồi dào ổn định để mưu cầu sự nghiệp … an bài.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Làm cho công ty nhỏ/ khởi nghiệp&lt;/em&gt;. Đây là con đường mạo hiểm bởi không biết nó sập lúc nào. Chúng ta sẽ đi sâu vào sau, nhưng nhìn chung cty nhỏ hay khởi nghiệp vốn không thể lớn như công ty lớn. Có một dạng thú vị là các công ty khởi nghiệp xuất phát từ 1 nghiên cứu trong academics mà tôi sẽ bàn trong bài viết này.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Khởi nghiệp&lt;/strong&gt;: tức là không làm thuê cho người khác mà tự mở ra mà làm lấy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/careers.png&quot; alt=&quot;careers&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;làm-thuê&quot;&gt;Làm thuê&lt;/h1&gt;

&lt;p&gt;Thực ra mấy cái mảng này thì cũng có nhiều surveys có sẵn rồi và với các bạn trẻ cũng chỉ quan trọng là nhìn thấy mấy con số đáng tin cậy để quyết định hướng đi sự nghiệp nên tôi cứ phang thẳng các bài survey có nhiều citations vào nhé.&lt;/p&gt;

&lt;h2 id=&quot;làm-cho-khởi-nghiệp-và-công-ty-nhỏ-vừa&quot;&gt;Làm cho khởi nghiệp và công ty nhỏ vừa&lt;/h2&gt;

&lt;p&gt;Hướng đi mang tính entrepreneur này có vẻ mạo hiểm với cả chủ lẫn tớ: công ty chả biết lúc nào “sập”?
Một bài survey dưới góc độ employee xem tại sao họ lại chọn hướng đi làm thuê cho khởi nghiệp mà tôi thích là bài của Nystrӧm &lt;a class=&quot;citation&quot; href=&quot;#nystrom2021working&quot;&gt;[1]&lt;/a&gt; ngay vừa 2021 rồi.
Hiểm nguy đi cùng khởi nghiệp sẽ bao gồm:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Great uncertainty&lt;/strong&gt; tức là bấp bênh, lúc có việc, lúc rảnh thôi rồi.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Low wages&lt;/strong&gt; lương thấp tẹt vì bản thân khởi nghiệp cũng đang chờ vốn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Và khả năng công ty không sống sót&lt;/strong&gt;. Có suy thoái kiểu 2008 hay dịch Covid 19 là một loạt chết trắng bụng!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vậy &lt;em&gt;những tên dở người (insane) nào lại đi làm cho khởi nghiệp và các công ty nhỏ?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Đầu tiên chúng ta cũng cần hiểu quy trình tuyển dụng dù công ty nhỏ hay lớn thì cũng sẽ đại loại là trước khi hai bên bắt tay nhau thì phải có một quá trình &lt;strong&gt;matching&lt;/strong&gt;.
Vấn đề là employer cũng không có ngay được thông tin về skill cũng như abilities của employee và ngược lại employee cũng không biết được là cái khởi nghiệp này nó có skill và tiềm năng ra sao.
Cái này trong lĩnh vực này gọi là &lt;strong&gt;information assymmetry&lt;/strong&gt;, tức là sự thiếu thông tin về nhau của hai bên khi tiếp xúc.
Tất nhiên khi hai bên đang thiếu thông tin về nhau như vậy, việc tìm điểm cân bằng equibilirum là cần thiết.&lt;/p&gt;

&lt;p&gt;Theo &lt;a class=&quot;citation&quot; href=&quot;#nystrom2021working&quot;&gt;[1]&lt;/a&gt; thì trong một survey ở labor market Thụy Điển thì sinh viên đại học bên đó đặt các yếu tố sau lên đầu khi tìm việc:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lương khởi điểm cao&lt;/li&gt;
  &lt;li&gt;Cơ hội phát triển bản thân và có training&lt;/li&gt;
  &lt;li&gt;Cơ hội làm việc lâu dài&lt;/li&gt;
  &lt;li&gt;Môi trường làm việc năng động và phát triển tốt.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Một điểm thú vị &lt;a class=&quot;citation&quot; href=&quot;#nystrom2021working&quot;&gt;[1]&lt;/a&gt; là trong những người làm việc R&amp;amp;D (cả chuyển việc lẫn mới tốt nghiệp) thì yếu tố quan trọng nhất với họ không phải lương hay job security mà là &lt;strong&gt;independence và responsibilty (sự độc lập và trách nhiệm)&lt;/strong&gt;.
Tức là lương cao và chuyện không hợp hoặc công ty chết thì nhảy việc thì không quan trọng với người làm R&amp;amp;D, mà quan trọng với họ là được làm việc độc lập (được làm chủ nghiên cứu của mình và toàn quyền quyết định) và trách nhiệm rõ ràng.&lt;/p&gt;

&lt;p&gt;Ngoài việc lương bèo thì những nguy cơ tiềm ẩn như làm việc nhiều giờ, stress cũng dai dẳng với các nhân viên của công ty khởi nghiệp.&lt;/p&gt;

&lt;p&gt;Và Nystrӧm cũng chỉ ra kết quả thú vị: hầu như &lt;strong&gt;khởi nghiệp và cty vừa nhỏ chỉ thuê được các vị trí yếu hơn so với các công ty lớn&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Doner và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#dorner2017wages&quot;&gt;[2]&lt;/a&gt; cũng nghiên cứu về lương khởi nghiệp có thể trả nhưng tập trung vào &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;academics spin-offs&lt;/code&gt; tức là khởi nghiệp xuất phát từ academics.
Những nhà sáng lập xuất thân từ academics hoặc từ 1 nghiên cứu nào đó trong academics.
Câu hỏi nghiên cứu ở đây là liệu spin-offs có sẵn sàng trả cao cho nhân viên không?
Các tác giả tập trung vào bộ dữ liệu spin-offs ở Đức và thật thú vị sau khi họ làm mấy cái regression test linh tinh và đưa ra kết quả cũng không mới lắm: &lt;strong&gt;spin-offs dù nhận vốn academics (có thể dồi dào hay không biết) nhưng cũng không trả cao đâu!&lt;/strong&gt;
Họ cũng chỉ ra rằng một nhận định cũ rằng spin-offs có tốc độ mở rộng quy mô nhân sự cao hơn các khởi nghiệp khác là sai vì số liệu của họ cũng không chỉ ra significance nào.
&lt;em&gt;Mất công làm một đống regression tests mà kết quả là lương cũng không cao hơn là bao thì quả cũng của đáng tội!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nhìn con số từ Đức và Thụy Điển có vẻ không khả quan lắm, nên tôi chuyển hướng nhìn sang Mỹ xem sao.
Thế là có ngay bài điều tra của Kim &lt;a class=&quot;citation&quot; href=&quot;#kim2018there&quot;&gt;[3]&lt;/a&gt; ngay năm 2018 thôi nên số liệu thực tế và rất mới.
Dữ liệu dựa trên khởi nghiệp xuất phát từ OB của MIT.
Thì thật thú vị là nhóm VC-backed tức là được nhận đầu tư của VC thì có thu nhập khá hơn 10% (non-founder employees) so với mặt bằng chung.
Sau khi thêm hiệu ứng cố định (fixed-effect) dựa trên quan sát là nhiều employees nhận multi-offers thì con số lại không significant lắm, chứng tỏ là việc được VC đầu tư sẽ dẫn tới lương trả cao lên.
Và điều đó cho thấy ở Mỹ thì nên tìm vào khởi nghiệp có VC!&lt;/p&gt;

&lt;p&gt;Một bài điều tra thú vị khác về &lt;strong&gt;chất lượng của job&lt;/strong&gt; từ khởi nghiệp được Block và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#block2018quantity&quot;&gt;[4]&lt;/a&gt; hoàn thành cũng vào năm 2018.
Chủ yếu là vì khởi nghiệp thì tạo ra công ăn việc làm, nhưng công ăn việc làm ấy chất lượng có cao không?
Thì kết quả là &lt;strong&gt;chất lượng thấp&lt;/strong&gt;: phần lớn là lương thấp, bonus bèo hoặc không có, bảo hiểm cũng bấp bênh, công việc cũng không biết công ty chết lúc nào.
Block nói chung cũng không đưa ra được dấu hiệu cho thấy đãi ngộ khấm khá hơn là bao.
Tôi nghĩ mấy cái Block này chắc cũng là tình hình chung của mảng này rồi nên thôi chắc mình cũng không nên mất công vào kẻo lại của đáng tội!&lt;/p&gt;

&lt;p&gt;Thôi điều tra một hồi thấy đến cả mấy cái work của Block &lt;a class=&quot;citation&quot; href=&quot;#block2018quantity&quot;&gt;[4]&lt;/a&gt; mà cũng chỉ được bèo như vậy thì chắc làm thuê cho khởi nghiệp mình đọc đến đây thôi!
Đổi mới sáng tạo thì cũng thú vị lắm nhưng nghèo thì gượm để suy nghĩ đã :))&lt;/p&gt;

&lt;h2 id=&quot;làm-cho-hàn-lâm&quot;&gt;Làm cho hàn lâm&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Làm cho hàn lâm có mong giàu không?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nhìn chung là cũng có lý do tại sao đi dạy đi nghiên cứu lại mong giàu: Nghiên cứu của Currall và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#currall2005pay&quot;&gt;[5]&lt;/a&gt; vào năm 2005 trên tập giáo viên từ gần 7,000 trường học ở Mỹ cho thấy, độ thỏa mãn về lương lậu có tương quan dương với hiệu suất làm việc ở trường và tương quan âm với ý định bỏ việc của giáo viên.
Tức là lương lậu OK thì sẽ không bỏ việc và hiệu suất lại tốt hơn. 
Tất nhiên độ thỏa mãn về lương lậu cũng chỉ là survey kiểu &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lương có đủ sống không em?&lt;/code&gt; chứ chưa nói đến giàu, nhưng qua đây cũng thấy là lương mà không cao là giáo viên cũng bỏ dạy đấy.&lt;/p&gt;

&lt;p&gt;Đấy là ngành dạy dỗ nhé, nhưng còn ngạch nghiên cứu thì sao?
Lại có bài của Stern &lt;a class=&quot;citation&quot; href=&quot;#stern2004scientists&quot;&gt;[6]&lt;/a&gt; năm 2004.
Ô đọc xong mà lại thấy nghèo quá!
Kết quả chỉ ra 1 tương quan âm giữa lương và độ khoa học trong tổ chức: những cơ quan nghiên cứu mà cho phép nhà nghiên cứu xuất bản bài báo và hội nghị thì nhận được 25% discount trong lương.
&lt;strong&gt;Tức là nhà nghiên cứu sẵn sàng nhận lương thấp đi 25% nếu cho phép họ xuất bản bài báo lên tạp chí khoa học uy tín hoặc hội nghị lớn.&lt;/strong&gt;
Thế thì nghèo!
&lt;em&gt;Thế nên xuất bản papers là chấp nhận nghèo đi đấy anh em ạ!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mấy cái nghiên cứu này cũng dựa trên giả thuyết multi-offers rồi lại làm mấy cái fixed-effect regression tests, thế mà xong lại ra lương không cao hơn mà lại thấp đi 25% thế này thì có lẽ cũng không cần điều tra hướng này sâu thêm nữa!
Mà lại còn nhiều thử thách, ví dụ như muốn hợp tác nghiên cứu với industry mà hay gọi là &lt;strong&gt;University-Industry (UI) collaboration&lt;/strong&gt;, thì cái UI này cũng tù lắm: vừa bị control của trường đại học (university administration) lại còn vấn đề sở hữu trí tuệ (IP) của doanh nghiệp.
Nói chung là có nhiều thử thách, mà thế mới đúng vì &lt;strong&gt;có làm thì mới có ăn&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;làm-cho-công-ty-lớn&quot;&gt;Làm cho công ty lớn&lt;/h2&gt;

&lt;p&gt;Đây là hướng đi còn lại trong nhánh &lt;strong&gt;Làm thuê&lt;/strong&gt; và cũng là phổ biến nhất trong nhóm lao động trẻ mới tốt nghiệp ra trường.
Nhìn chung vì nó là đại trà nên lương ở mức mặt bằng chung, phúc lợi thì công ty này công ty kia nó cũng same same.
Anh em trải qua hết rồi cũng chả có thấy điểm ưu trội nào mấy.&lt;/p&gt;

&lt;p&gt;Thôi thì xem thử có bài nào hay hay thì thấy có bài của Andersson và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#andersson2009reaching&quot;&gt;[7]&lt;/a&gt; xuất bản năm 2009 về câu hỏi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;những công ty nào sẽ trả tiền thuê talents?&lt;/code&gt;
Thì thu nhập cũng phải hiểu là có thu nhập khởi điểm và thu nhập khi đã có kinh nghiệm.
Ngoài ra còn stock options nữa cũng là một yếu tốt trong thu nhập khi đi làm thuê.
Hay là nghiên cứu này lại tổ chức trên đúng tập dữ liệu là các công ty software ở Mỹ nên có lẽ rất phù hợp cho ngành IT.
Thì câu hỏi là &lt;strong&gt;những công ty như thế nào sẽ trả lương cao và họ sẽ trả cao cho nhân viên như thế nào?&lt;/strong&gt;.
Nhóm của Andersson chỉ ra rằng những công ty ở khu vực đầu tư mạo hiểm (thắng thì sẽ thắng lớn nhưng thua thì cũng dặt dẹo) thì sẵn sàng trả cao để thu hút nhân tài.
Thế nên là anh em thấy những dự án thử thách mạo hiểm thì cứ nhảy vào chiến thôi, đừng lo “sập”!
Chỉ cần lo chiến đấu thôi!&lt;/p&gt;

&lt;h1 id=&quot;khởi-nghiệp&quot;&gt;Khởi nghiệp&lt;/h1&gt;

&lt;p&gt;Làm thuê thì có vẻ cũng hòm hòm vậy, đi vào chỗ nào lương sẽ như thế nào là đã có hình dung trong đầu nên tôi nghĩ chắc cũng chả có gì mới nữa.
&lt;strong&gt;Xem khởi nghiệp tức là mình mở hẳn công ty của mình xem có khá khẩm hơn không nào?&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;đặc-điểm-của-khởi-nghiệp&quot;&gt;Đặc điểm của khởi nghiệp&lt;/h2&gt;

&lt;p&gt;Đầu tiên là vấn đề ý tưởng thì có bài của Homfeldt và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#homfeldt2019suppliers&quot;&gt;[8]&lt;/a&gt; ngay năm 2019.
Họ trả lời câu hỏi &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;liệu ý tưởng của startups có thực sự mới mẻ hơn của các công ty lớn không?&lt;/code&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We find that start-ups’ ideas are characterized by a higher degree of novelty and to some extent higher benefit for end customers but, on the downside, are less likely to be implemented than suppliers’ ideas.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Câu trả lời là ý tưởng thì đúng là có vẻ tân tiến nhưng lại kiểu ít có khả năng thực thi hơn của công ty lớn.
Tức là mới nhưng mà hơi mơ mộng.
Thì đúng là vậy, ý tưởng mới sẽ khó được đón nhận ngay và thường bị xem là mơ mộng, viển vông.
Nhưng qua thời gian nếu startup trụ vững và chứng minh được thì nó lại thành hiện thực và startup sẽ cất cánh.
Rủi ro lớn nhất là startup không thực hiện được thôi, nhưng đó không phải vấn đề mới mà là vấn đề chung của mọi startup hiện nay!&lt;/p&gt;

&lt;p&gt;Khi khởi nghiệp thì nếu xuất phát từ 1 nghiên cứu hàn lâm thì startups sẽ được gọi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spin-off&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#rodriguez2018role&quot;&gt;[9]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#dianez2021drivers&quot;&gt;[10]&lt;/a&gt;.
Khi xây dựng spin-off thì sẽ phải chú ý vấn đề &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hướng&lt;/code&gt; và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;chiến lược&lt;/code&gt; (EO) cũng như xây dựng đội ngũ.
&lt;a class=&quot;citation&quot; href=&quot;#dianez2021drivers&quot;&gt;[10]&lt;/a&gt; cũng chỉ ra một số yếu tố hỗ trợ vấn đề &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hướng&lt;/code&gt; là&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sự có mặt của 1 quản lý không phải academics (non-academics manager) trong đội ngũ quản lý&lt;/li&gt;
  &lt;li&gt;Có vốn của VC&lt;/li&gt;
  &lt;li&gt;Hỗ trợ của tổ chức chuyên tài trợ spin-off trong chuyển giao công nghệ
Trên đây có vẻ là yếu tố chiến lược hỗ trợ spin-off phát triển.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Rồi làm khởi nghiệp còn phải đau đầu lo vấn đề hợp tác mà thực ra là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mua&lt;/code&gt; affiliation &lt;a class=&quot;citation&quot; href=&quot;#hsu2004entrepreneurs&quot;&gt;[11]&lt;/a&gt;.
Không có affiliation thì chẳng có publish gì cả vì reputation thấp.
Muốn reputation cao thì lại phải dựa vào những affiliation là VC có reputation cao.&lt;/p&gt;

&lt;p&gt;Đấy là nếu gọi vốn VC, còn nếu gọi vốn crowdfunding &lt;a class=&quot;citation&quot; href=&quot;#mollick2014dynamics&quot;&gt;[12]&lt;/a&gt; thì cũng là hướng mới mà nhiều anh em đang làm nhưng nói chung &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gian nan&lt;/code&gt; lắm.&lt;/p&gt;

&lt;h2 id=&quot;thu-nhập-của-nhà-khởi-nghiệp&quot;&gt;Thu nhập của nhà khởi nghiệp&lt;/h2&gt;

&lt;p&gt;Quay lại chủ để chính là &lt;strong&gt;thu nhập&lt;/strong&gt;.
Thì có bài của Åstebro và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#aastebro2013does&quot;&gt;[13]&lt;/a&gt; năm 2013 cũng nghiên cứu trên tập các nhà sáng lập xuất thân từ academics xem &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thu nhập của họ có khá lên sau khi rời academics không?&lt;/code&gt;
Đau cái là nó lại không khá lên, qua thống kê của Åstebro và cộng sự thì thu nhập before/after khởi nghiệp nó lại như nhau: trước khi khởi nghiệp thì thu nhập trung bình là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;397,000 (39.7 man)&lt;/code&gt; còn sau khi khởi nghiệp bình quân là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;450,000 (45 man)&lt;/code&gt;.
Mà phương sai lại cao gấp 3 lần tức là sau khi khởi nghiệp thu nhập lại “bấp bênh” hơn.
Ngoài ra 60% những nhà khởi nghiệp này lại từ bỏ khởi nghiệp trong vòng 2 năm và 66% lại quay trở về academics!&lt;/p&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Nói chung &lt;strong&gt;Có làm thì mới có ăn&lt;/strong&gt; là chân lý của lao động rồi.
Bài điều tra điểm qua hầu hết mọi hướng đi khả thi, trừ hướng chính phủ/NGO chưa muốn động vào.
Nhưng nhìn chung là làm thuê thì chấp nhận là lương sẽ chỉ cao nếu đi đúng đường.
Còn tự thân khởi nghiệp thì nghĩ béo bở nhưng đau đầu và thực tế là cũng chả tăng thêm thu nhập bao nhiêu (từ 39.7 man lên 45 man), mà phần lớn (60%) là thất bại sau 2 năm &lt;a class=&quot;citation&quot; href=&quot;#aastebro2013does&quot;&gt;[13]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thế nên quay lại câu hỏi là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nếu làm R&amp;amp;D khởi nghiệp mà không béo bằng đi làm thuê phát triển thì có nên làm nghiên cứu không?&lt;/code&gt; thì tôi nghĩ là tùy mỗi người thôi, nhưng nhìn quan điểm là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thu nhập&lt;/code&gt; thì có lẽ con đường đi nghiên cứu nó không thông minh lắm đâu!&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;nystrom2021working&quot;&gt;Nyström, K. 2021. Working for an entrepreneur: heaven or hell? &lt;i&gt;Small Business Economics&lt;/i&gt;. 56, 2 (2021), 919–931.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nystrom2021working/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dorner2017wages&quot;&gt;Dorner, M., Fryges, H. and Schopen, K. 2017. Wages in high-tech start-ups–Do academic spin-offs pay a wage premium? &lt;i&gt;Research Policy&lt;/i&gt;. 46, 1 (2017), 1–18.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dorner2017wages/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kim2018there&quot;&gt;Kim, J.D. 2018. Is there a startup wage premium? Evidence from MIT graduates. &lt;i&gt;Research Policy&lt;/i&gt;. 47, 3 (2018), 637–649.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kim2018there/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;block2018quantity&quot;&gt;Block, J.H., Fisch, C.O. and Van Praag, M. 2018. Quantity and quality of jobs by entrepreneurial firms. &lt;i&gt;Oxford Review of Economic Policy&lt;/i&gt;. 34, 4 (2018), 565–583.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/block2018quantity/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;currall2005pay&quot;&gt;Currall, S.C., Towler, A.J., Judge, T.A. and Kohn, L. 2005. Pay satisfaction and organizational outcomes. &lt;i&gt;Personnel psychology&lt;/i&gt;. 58, 3 (2005), 613–640.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/currall2005pay/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;stern2004scientists&quot;&gt;Stern, S. 2004. Do scientists pay to be scientists? &lt;i&gt;Management science&lt;/i&gt;. 50, 6 (2004), 835–853.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/stern2004scientists/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;andersson2009reaching&quot;&gt;Andersson, F., Freedman, M., Haltiwanger, J., Lane, J. and Shaw, K. 2009. Reaching for the stars: who pays for talent in innovative industries? &lt;i&gt;The Economic Journal&lt;/i&gt;. 119, 538 (2009), F308–F332.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/andersson2009reaching/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;homfeldt2019suppliers&quot;&gt;Homfeldt, F., Rese, A. and Simon, F. 2019. Suppliers versus start-ups: Where do better innovation ideas come from? &lt;i&gt;Research policy&lt;/i&gt;. 48, 7 (2019), 1738–1757.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/homfeldt2019suppliers/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;rodriguez2018role&quot;&gt;Rodrı́guez-Gulı́as Marı́a Jesús, Fernández-López, S., Rodeiro-Pazos, D., Corsi, C. and Prencipe, A. 2018. The role of knowledge spillovers on the university spin-offs innovation. &lt;i&gt;Science and Public Policy&lt;/i&gt;. 45, 6 (2018), 875–883.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/rodriguez2018role/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dianez2021drivers&quot;&gt;Diánez-González, J.P., Camelo-Ordaz, C. and Fernández-Alles, M. 2021. Drivers and implications of entrepreneurial orientation for academic spin-offs. &lt;i&gt;International Entrepreneurship and Management Journal&lt;/i&gt;. 17, 2 (2021), 1007–1035.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dianez2021drivers/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hsu2004entrepreneurs&quot;&gt;Hsu, D.H. 2004. What do entrepreneurs pay for venture capital affiliation? &lt;i&gt;The journal of finance&lt;/i&gt;. 59, 4 (2004), 1805–1844.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hsu2004entrepreneurs/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mollick2014dynamics&quot;&gt;Mollick, E. 2014. The dynamics of crowdfunding: An exploratory study. &lt;i&gt;Journal of business venturing&lt;/i&gt;. 29, 1 (2014), 1–16.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/mollick2014dynamics/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;aastebro2013does&quot;&gt;Åstebro, T., Braunerhjelm, P. and Broström, A. 2013. Does academic entrepreneurship pay? &lt;i&gt;Industrial and Corporate Change&lt;/i&gt;. 22, 1 (2013), 281–311.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/aastebro2013does/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Mon, 10 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/pay/</link>
        <guid isPermaLink="true">https://wanted2.github.io/pay/</guid>
        
        <category>Career</category>
        
        <category>academics</category>
        
        <category>industry</category>
        
        <category>government</category>
        
        <category>entrepreneurship</category>
        
        <category>startups</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
      </item>
    
      <item>
        <title>RapidAPI and RapidAPI Hub</title>
        <description>&lt;p&gt;&lt;em&gt;Image Credit: &lt;a href=&quot;https://financefeeds.com/rakuten-launches-api-marketplace/&quot;&gt;FinanceFeeds&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Rakuten launched RapidAPI Marketplace in 2018 as a result of the collaboration between Japan’s Rakuten Inc and San Francisco-based startup RapidAPI.
&lt;a href=&quot;https://api.rakuten.co.jp/en/&quot;&gt;The API marketplace&lt;/a&gt; aims to provide software developers in Japan and Asia unified access to more than 8,000 APIs with localized documentation and resources in Japan’s language and English.
The API marketplace platform will connect API providers and developers.
Developers in Japan and across Asia will be able to find, test, and connect to thousands of APIs for their applications.
The marketplace will also allow API providers to connect with the global developer community through personalized API portals.
 &lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-is-rapidapi&quot;&gt;What is RapidAPI?&lt;/h1&gt;

&lt;p&gt;Let us assume that you have an API that is ready for production.
You need to add authentication like API key, OAuth 2, or something else.
You need to deploy your API to somewhere that is stable and reliable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the &lt;strong&gt;shortest path&lt;/strong&gt; to achieving your goal?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You are an application developer, and you need to manage the records of some data for the app.
For example, you need to maintain the list of public holidays in your app.
You don’t want to hardcode those things in the code.
Note that the public holidays change between countries and sometimes due to the law it will change between years.
It is somewhat troublesome to maintain the records in your database as it will make you allocate some effort and human resources there.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the most convenient way to maintain such data?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In both scenarios, Rakuten RapidAPI Marketplace gives you excellent solutions.
Either maintenance of the data (public holidays) or publishing a new API, you can do all of the lifecycles in one platform.&lt;/p&gt;

&lt;p&gt;For example, when you want to check a day is a holiday or not, you can thus search for a free API like this &lt;a href=&quot;https://english.api.rakuten.net/theapiguy/api/public-holiday&quot;&gt;one&lt;/a&gt; and make a request.
Because all maintenance is up on the providers, this solution costs you nothing: you don’t need to worry about maintaining the records of holidays data (which shouldn’t be your matter in any way) and focus on your own application logic.
Note that the &lt;a href=&quot;https://english.api.rakuten.net/theapiguy/api/public-holiday&quot;&gt;Public Holidays API&lt;/a&gt; has low latency (59ms) and is completely free.
Another solution is to build an endpoint in your own API like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/api/v1/holidays&lt;/code&gt; to validate the holidays, but while such a ready-to-use solution is there, why should you waste time and money to build/manage/maintain on your own?&lt;/p&gt;

&lt;p&gt;RapidAPI helps your API to distribute and monetize.
Adding your API to the RapidAPI Hub gets you instant exposure to our growing user base, a search-engine-optimized profile page for your API, as well as features like user management and billing services.
RapidAPI also serves functional testings, API monitoring dashboards, and many other premiere features like API authentication.&lt;/p&gt;

&lt;h1 id=&quot;rapidapi-for-api-vendors&quot;&gt;RapidAPI for API Vendors&lt;/h1&gt;

&lt;p&gt;The workflow between an app developer’s client to a vendor API can be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rapidapi.svg&quot; alt=&quot;rapidapi&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An &lt;a href=&quot;https://docs.rapidapi.com/docs/keys&quot;&gt;API Key&lt;/a&gt; is generated and appended to the request’s header to RapidAPI servers.&lt;/li&gt;
  &lt;li&gt;RapidAPI authenticate the request (using API Key and optionally a configured authentication method like OAuth 2). Then it modifies the requests header to append &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X-RapidAPI-*&lt;/code&gt; headers.&lt;/li&gt;
  &lt;li&gt;The vendor API (destination API in the diagram) checks the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X-RapidAPI-*&lt;/code&gt; headers and authenticates the modified requests.&lt;/li&gt;
  &lt;li&gt;A response is generated according to the requested information and is then returned to RapidAPI.&lt;/li&gt;
  &lt;li&gt;RapidAPI modifies the response from vendor servers. It appends Rapid API headers (for example, headers about rate limits) or generates a new response.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As you can see, RapidAPI Marketplace acts as a proxy between app servers (client in the diagram) and the vendor API servers.
The vendors &lt;a href=&quot;https://docs.rapidapi.com/docs/add-an-api-basics&quot;&gt;register&lt;/a&gt; their APIs and &lt;a href=&quot;https://docs.rapidapi.com/docs/add-an-api-advanced-settings&quot;&gt;fine-tune&lt;/a&gt; the settings in RapidAPI dashboard.
All API endpoints are relative to a base URL, which is added as a “prefix” to all API endpoints.
This approach avoids the need to define absolute URLs for endpoints every time and increases API portability by changing the base URL.&lt;/p&gt;

&lt;p&gt;API vendors can &lt;a href=&quot;https://docs.rapidapi.com/docs/configuring-api-authentication&quot;&gt;add&lt;/a&gt; basic authentication or OAuth 2 to their APIs.&lt;/p&gt;

&lt;p&gt;RapidAPI supports &lt;a href=&quot;https://docs.rapidapi.com/docs/automating-api-provisioning&quot;&gt;automatic API provisioning using OpenAPI&lt;/a&gt; and &lt;a href=&quot;https://docs.rapidapi.com/docs/transformations&quot;&gt;custom transformations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;RapidAPI has basic plan options so app developers can choose among these options to pay:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;API Type&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Free APIs&lt;/td&gt;
      &lt;td&gt;APIs that do not require a credit card or subscription to consume.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pay Per Use&lt;/td&gt;
      &lt;td&gt;APIs that don’t have a subscription fee associated with them. A credit card is required as you pay for what you use on the API.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Freemium APIs&lt;/td&gt;
      &lt;td&gt;Paid APIs that also include a limited free tier. These require a credit card, even for the free plan.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Paid APIs&lt;/td&gt;
      &lt;td&gt;APIs that require a paid subscription plan and credit card to consume.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;some-notes-on-security&quot;&gt;Some notes on security&lt;/h2&gt;

&lt;p&gt;RapidAPI supports &lt;a href=&quot;https://docs.rapidapi.com/docs/secret-headers-parameters&quot;&gt;secret headers and parameters&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;RapidAPI allows you to add secret headers and/or query string parameters to API requests. The RapidAPI proxy adds these secrets to every request but is &lt;strong&gt;hidden from the API consumers&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that even the consumers who make the requests do not know about these secrets.
This differs from header and query authentication methods where consumers know all secrets in the requests they make to RapidAPI.&lt;/p&gt;

&lt;p&gt;Users should configure RapidAPI &lt;a href=&quot;https://docs.rapidapi.com/docs/security-threat-protection&quot;&gt;security&lt;/a&gt; features like firewalls, threat protection, schema validation, and request size limit (which returns error code 413).&lt;/p&gt;

&lt;p&gt;Vendors can set their API to &lt;a href=&quot;https://docs.rapidapi.com/docs/private-apis-api-logo&quot;&gt;private&lt;/a&gt; where only invited users can access.&lt;/p&gt;

&lt;h2 id=&quot;audit-and-marketing-tools&quot;&gt;Audit and marketing tools&lt;/h2&gt;

&lt;p&gt;RapidAPI provides &lt;a href=&quot;https://docs.rapidapi.com/docs/provider-dashboard&quot;&gt;Provider Dashboard&lt;/a&gt; where vendors can monitor their API usages.
Another nice thing is that as a vendor, you can make your monetization more useful using &lt;a href=&quot;https://docs.rapidapi.com/docs/ive-added-my-api-to-rapidapi-now-what&quot;&gt;Marketing API&lt;/a&gt;.
When you have an API, you should make sure you don’t miss a checklist when publishing your solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://api.rakuten.co.jp/docs/ja-images/ProviderWelcome_1.png&quot; alt=&quot;RapidAPI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This checklist helps you have a better SEO for your API.&lt;/p&gt;

&lt;h2 id=&quot;api-testing&quot;&gt;API Testing&lt;/h2&gt;

&lt;p&gt;Testing is quite tedious!
RapidAPI helps vendors reduce testing costs with their &lt;a href=&quot;https://docs.rapidapi.com/docs/rapidapi-testing-overview&quot;&gt;API testing feature&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.readme.io/726dc84-run-code.png&quot; alt=&quot;RapidAPI testing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you are already familiar with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postman-tool&lt;/code&gt; you are ready to go with RapidAPI &lt;a href=&quot;https://docs.rapidapi.com/docs/create-a-test-advanced&quot;&gt;advanced testing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.readme.io/fabfeb1-Screen_Shot_2020-12-03_at_4.00.53_PM.png&quot; alt=&quot;Advanced testing&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;rapidapi-for-app-developers&quot;&gt;RapidAPI for App Developers&lt;/h1&gt;

&lt;p&gt;As an app developer, you can find that &lt;a href=&quot;https://rapidapi.com/hub&quot;&gt;RapidAPI Hub&lt;/a&gt; now has more than 10,000 APIs.
Even you want to develop an OCR app or a Translation app, you can find your API right away.&lt;/p&gt;

&lt;p&gt;All you need is to register a RapidAPI account, choose your API and then &lt;strong&gt;make a payment&lt;/strong&gt;.
Finally, you can &lt;a href=&quot;https://docs.rapidapi.com/docs/connecting-to-an-api&quot;&gt;connect&lt;/a&gt; to your paid API using the API key.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rapidapi-vin.png&quot; alt=&quot;RapidAPI VIN&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It is worth noting that RapidAPI supports not only REST API but also &lt;a href=&quot;https://docs.rapidapi.com/docs/graphql-apis&quot;&gt;GraphQL&lt;/a&gt;, &lt;a href=&quot;https://docs.rapidapi.com/docs/adding-soap-apis&quot;&gt;SOAP&lt;/a&gt;, and &lt;a href=&quot;https://docs.rapidapi.com/docs/kafka-apis&quot;&gt;Kafka&lt;/a&gt; APIs.
We did not touch &lt;a href=&quot;https://docs.rapidapi.com/docs/what-is-rapidapi-for-teams&quot;&gt;RapidAPI for Teams&lt;/a&gt;, but it might be useful at the organization level.&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/rapidapi/</link>
        <guid isPermaLink="true">https://wanted2.github.io/rapidapi/</guid>
        
        <category>api development</category>
        
        <category>backend</category>
        
        <category>infrastructure</category>
        
        <category>rapidapi</category>
        
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
        <category>Site Reliable Engineering</category>
        
      </item>
    
      <item>
        <title>Chào 2022 và câu chuyện khoảng cách công nghệ giữa các quốc gia, tổ chức</title>
        <description>&lt;p&gt;&lt;a href=&quot;/year-end/&quot;&gt;Năm 2021&lt;/a&gt; đã kết thúc và cũng là lúc để nghĩ đến dự định cho năm 2022.
Bản thân AiFi luôn nhắm đến mục tiêu lấy chứng chỉ Project Manager (PM) trong thời gian ngắn nhất có thể.
Ngoài ra, tất nhiên nâng cao năng lực quản lý cũng là một bước chuẩn bị tốt nhưng quan trọng hơn vẫn là nhanh chóng xây dựng roadmap để sớm launch một cái gì đó thú vị, ví dụ như 1 công ty chẳng hạn.
Có lẽ thời điểm đầu xuân cũng là lúc phù hợp để chốt các mục tiêu cho năm 2022 và lên kế hoạch.
Tất nhiên cũng cần hiểu là &lt;strong&gt;năm 2022 vẫn chỉ là một năm chuẩn bị&lt;/strong&gt; thôi.
Cuối cùng là một chút tản mạn về chủ để &lt;strong&gt;khoảng cách công nghệ&lt;/strong&gt; giữa các quốc gia, firm.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;dự-định-2022&quot;&gt;Dự định 2022&lt;/h1&gt;

&lt;h2 id=&quot;trong-công-việc&quot;&gt;Trong công việc&lt;/h2&gt;

&lt;p&gt;Trong công việc thì cứ tiến hành như bình thường.
Cố gắng duy trì nhịp độ làm việc và sớm chuyển sang Nhật Bản.&lt;/p&gt;

&lt;h2 id=&quot;phát-triển-bản-thân&quot;&gt;Phát triển bản thân&lt;/h2&gt;

&lt;h3 id=&quot;về-mặt-quản-lý&quot;&gt;Về mặt quản lý&lt;/h3&gt;
&lt;p&gt;Bản thân AiFi luôn nhắm đến mục tiêu lấy chứng chỉ Project Manager (PM) trong thời gian ngắn nhất có thể.
Tuy nhiên, đó là một việc không thể nhanh chóng vì PMI &lt;a class=&quot;citation&quot; href=&quot;#PMPCerti18:online&quot;&gt;[1]&lt;/a&gt; đòi hỏi:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;36 tháng (3 năm) kinh nghiệm PM (PM experience, không nhất thiết vị trí PM).&lt;/strong&gt; Tuy nhiên, có vẻ là thường những người dự thi đều có &lt;strong&gt;khoảng 7-10 năm kinh nghiệm PM.&lt;/strong&gt; Tức là các bác có nhiều năm kinh nghiệm mới đi thi. Nên một giá trị phù hợp là làm PM khoảng 3 năm thì nghĩ đến chuẩn bị PMP, rồi khoảng năm thứ 4 hoặc thứ 5 thì đi thi là tốt, còn bình thường chắc tầm năm thứ 7 trở ra đi thi thì chắc ăn hơn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;35 giờ học hướng dẫn (contact hours, hay 35 PDUs)&lt;/strong&gt;. Việc lấy 35 PDUs có thể hoàn thành thông qua 1 khóa khoảng chục triệu của 1 đối tác của PMI ngay tại Hà Nội nên cũng sẽ nhanh chóng thôi và lấy lúc nào cũng được.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Có nhiều bạn sẽ thắc mắc là cứ làm việc thôi chứ cần gì phải lấy chứng chỉ PMP?
Thực ra thì cũng đúng vì nếu đã có kinh nghiệm 7-10 năm làm PM rồi thì chứng chỉ mang tính chất vật biểu trưng việc mình là professional nhiều hơn.
&lt;em&gt;Tuy nhiên, theo thống kê của PMI bên Mỹ thì những PM có chứng chỉ PMP thì thu nhập tăng lên 25% so với các bạn PM không có PMP.&lt;/em&gt;
Do đó, theo tôi nghĩ vẫn nên có chứng chỉ PMP!
Bạn cũng nên biết là PMI cũng khá khắt khe trong tiêu chí dự thi, nếu 3 năm kinh nghiệm mà không phải kinh nghiệm PM thì họ cũng không chấp nhận cho thi.
Vì vậy, lấy được chứng chỉ PMP của PMI tôi nghĩ là một achievement không nhỏ!
Ít nhất là chúng ta cũng đã hiểu được góc nhìn của PM rất khác so với Dev và vì vậy, bắt buộc phải chuẩn bị kinh nghiệm PM thì mới được thi PMP.
Bản thân tôi ở vị trí BrSE là một vị trí mà nghiệp vụ thì nói chung là overlap khá nhiều với PM, tức là có PM experience.&lt;/p&gt;

&lt;p&gt;Năm 2022 chắc sẽ là 1 năm dài chuẩn bị cho PMP.
Mục tiêu là hè 2023 có thể bắt đầu tính đến dự thi PMP.
Năm 2021 cũng đã là 1 năm khá dài và cũng có ích:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Việc học sách PMBok mất khoảng nửa năm cũng có giá trị.&lt;/li&gt;
  &lt;li&gt;Việc thực hành quản lý các dự án nội bộ của công ty cũ cũng có giá trị kinh nghiệm nhất định.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;về-mặt-kinh-doanh-khởi-nghiệp&quot;&gt;Về mặt kinh doanh, khởi nghiệp&lt;/h3&gt;

&lt;p&gt;Làm dev thì nói chung là “tù”, vì đôi khi kiểu “chỉ đâu đánh đấy”, bao nằm nghiêng thì nghiêng mà nằm ngửa thì ngửa.
Tức là không có bức tranh lớn, nghe theo sự sai bảo của quản lý.
Có câu chuyện hài hước là có lần tôi làm quản lý mà có cậu dev làm lead mà cứ kiên quyết đòi làm 1 chức năng mà &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thực ra có lẽ không làm thì tốt hơn&lt;/code&gt;.
Bởi vì là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bỏ thì thương mà vương thì tội&lt;/code&gt;, làm thì mất công mà thu lại chả được mấy đồng bạc.
Nhưng vì cậu là lead nên cũng ậm ừ để cho cậu có thể diện mà nói chuyện với đàn em, chứ còn nói thực là có khi không sửa cứ để thế nó còn tốt hơn.
Khổ nỗi nếu sửa thì em lại phải theo nó đến cùng em ạ!
Những trường hợp này ấy thì theo kinh nghiệm của tôi cứ đẩy hết về owner, bởi &lt;strong&gt;trong hệ thống tự nhiên có 1 cái gì đó nó bất hợp lý thì chắc chắn là nên để cái ông nắm business chịu trách nhiệm&lt;/strong&gt;.
&lt;em&gt;Đấy, nên nói chung làm dev là “tù” lắm, không đi đến đâu cả đâu&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Năm 2022, mặt quản lý thì chắc lại tiếp tục thực hành thực chiến tích lũy tự tin để năm sau nữa đi thi PMP thôi.
Còn làm dev thì không cần.
Vì vậy, chỉ còn một chuẩn bị là &lt;strong&gt;mặt kinh doanh, khởi nghiệp&lt;/strong&gt;.
Có lẽ bước chuẩn bị trong năm 2022 sẽ là tập dượt xây dựng một community theo chủ đề nhất định.
Có lẽ trong năm 2022 sẽ khám phá “lại” một lĩnh vực công nghệ nào đó, theo kiểu đào sâu và tìm mới.&lt;/p&gt;

&lt;h1 id=&quot;khoảng-cách-công-nghệ&quot;&gt;Khoảng cách công nghệ&lt;/h1&gt;

&lt;h2 id=&quot;câu-chuyện-7-năm-trước&quot;&gt;Câu chuyện 7 năm trước&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Knowledge is inherently a public good. (Jaffe, 1986 &lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&gt;[2]&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tri thức về cơ bản là một tài sản chung (trích Jaffe &amp;lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&amp;gt;[2]&amp;lt;/a&amp;gt;)&lt;/code&gt;.
Nó mang hai hàm nghĩa:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Thứ nhất, không thể giấu giếm tri thức làm của riêng (nó khác với &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;trí tuệ&lt;/code&gt; là thứ có thể được đăng ký bảo hộ).&lt;/li&gt;
  &lt;li&gt;Thứ hai, tri thức có thể lan tỏa như thông tin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Câu chuyện về khoảng cách công nghệ có nguồn gốc từ 7 năm trước, khi tôi còn tham gia một chương trình lãnh đạo trẻ.
Thật thú vị là trong một bài thuyết trình cuối khóa về thành tích thực tập tại các công ty.
Khi đó một điều kiện bắt buộc tham gia chương trình lãnh đạo trẻ là sẽ phải tham gia thực tập vài tháng tại 1 doanh nghiệp ở nước ngoài (nhưng không được là doanh nghiệp ở quốc gia của chính mình, tránh cảnh tranh thủ về nước).
Có nhiều người xin sang châu Âu, Mỹ để làm intern ngắn hạn.
Trong các bạn lúc đó phát biểu có một sinh viên PhD khi đó lên thuyết trình, nhưng chỉ sau 2 slide giới thiệu cậu ta kết thúc rằng vì đã ký NDA với doanh nghiệp lớn ở Mỹ về lĩnh vực phần mềm (lớn nhất thế giới) nên không thể trình bày tiếp.
Tất nhiên là thanh niên đó đã lĩnh đủ của các giáo sư vì ở trong trường đại học &lt;strong&gt;cái quan trọng nhất chính là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tri thức&lt;/code&gt;&lt;/strong&gt;, mà tri thức thì là public good.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Đến khi nào anh hiểu được rằng những tri thức vĩ đại và to lớn mà người ta hay gọi là trí tuệ ấy, luôn tồn tại dưới vỏ bọc tầm thường và vớ vẩn, thì anh mới hành động như người có tri thức được.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bây giờ tôi thấy cái trường hợp này đúng là cần phải hiểu là chỉ cần làm mấy cái vớ vẩn, cũng không cần phải to tát rồi không dám chia sẻ mất đi ý nghĩa &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tri thức&lt;/code&gt; trong hàn lâm.
Và có lẽ cũng đúng như lời người thày lúc ấy nói, thực ra những công nghệ to tát và vĩ đại có khi thực ra là những thứ vớ vẩn thôi mà phải không?&lt;/p&gt;

&lt;h2 id=&quot;đến-câu-chuyện-rd-spillovers-và-knowledge-diffusion&quot;&gt;Đến câu chuyện R&amp;amp;D spillovers và knowledge diffusion&lt;/h2&gt;

&lt;p&gt;Các nghiên cứu về mối tương quan giữa không gian công nghệ, lợi nhuận và giá trị thị trường của các firm đã có từ khá lâu.
Thật thú vị là câu chuyện kể trên lại khá liên quan bởi công nghệ chốt trong phần nghiên cứu PhD đó lại vốn là một tri thức đã phổ biến và được đào sâu nghiên cứu ở trong hàn lâm.
Do vậy, ở mức độ nào đó dù là đăng ký bản quyền cho một thứ mà giới hàn lâm đã coi là public good thì nó rất là … vớ vẩn.
Tất nhiên chuyện đăng ký bản quyền là thể hiện giá trị R&amp;amp;D trong các doanh nghiệp, và là một phần quan trọng trong cái gọi là tương tác khoa học giữa hàn lâm và giới công nghiệp.&lt;/p&gt;

&lt;p&gt;Trong nhánh nghiên cứu này có thể kể đến công trình của Jaffe &lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&gt;[2]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt; và rất nhiều công trình sau đó của các nhà nghiên cứu khác.
Thì có lẽ có một khái niệm khá quan trọng là &lt;strong&gt;technological space&lt;/strong&gt; tức là &lt;strong&gt;không gian công nghệ&lt;/strong&gt;.
Ở đây, Jaffe đã xây dựng 1 vector 49 chiều (thu gọn từ 328 classes trong hệ thống patent toàn cầu) để biểu diễn vị trí của 1 firm trong không gian công nghệ.
Ví dụ, hệ thống patent toàn cầu có phân nhóm lĩnh vực thành 328 classes, thì tác giả Jaffe đã group lại thành 49 categories bao quát nhất mọi lĩnh vực công nghệ.
Giả sử công ty $A$ có $N$ patent thuộc 49 categories trên (đánh số từ 1) thì $f_i(A),~i=1,2, \ldots $ là số lượng pattern của $A$ trong lĩnh vực $i$, thì&lt;/p&gt;

&lt;p&gt;$\sum_{i=1}^{49} f_i(A)=N$&lt;/p&gt;

&lt;p&gt;và vector biểu diễn của công ty A (&lt;strong&gt;technological vector&lt;/strong&gt;) là&lt;/p&gt;

&lt;p&gt;$F_A=\begin{bmatrix}
f_1 \newline
f_2 \newline
\vdots \newline
f_{49}
\end{bmatrix}$&lt;/p&gt;

&lt;p&gt;với $f_i=\frac{f_i(A)}{N},~i=1, 2, \ldots $.&lt;/p&gt;

&lt;p&gt;Cứ như vậy, các điểm vector đều nằm trên một mặt phẳng (hyperplane) 49 chiều $\sum_{i=1}^{49}x_i=1$.
Khi 2 công ty có cùng một mối quan tâm về 1 công nghệ nào đó thì bằng chứng sẽ là về mặt R&amp;amp;D ở lĩnh vực đó ($i$ chả hạn) chỉ số sẽ cao lên.&lt;/p&gt;

&lt;p&gt;Jaffe đã nghiên cứu về một hiện tượng thú vị trong innovation đó là &lt;strong&gt;R&amp;amp;D spillovers&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&gt;[2]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt;, hay là sự khuếch tán của tri thức R&amp;amp;D, hay là &lt;strong&gt;knowledge diffusion&lt;/strong&gt;.
Nghiên cứu của Jaffe vào năm 1986 chủ yếu dựa trên quan sát của những người đi trước rằng khi có một công ty trong một lĩnh vực nào đó nâng cao đầu tư vào R&amp;amp;D thì đột nhiên những công ty xung quanh cũng bắt đầu phải chạy theo đầu tư vào mảng đó.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Firms whose research is in areas where there is much research by other firms have, on average, more patents per dollar of R&amp;amp;D and a higher return to R&amp;amp;D in terms of accounting profits or market value, though firms with very low own—R&amp;amp;D suffer lower profits and market value if their neighbors do a lot.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tức là nếu trong cùng 1 mảng mà cty đầu tư nhiều hơn vào R&amp;amp;D so với các công ty khác cùng lĩnh vực thì sẽ có giá trị thị thường lợi nhuận cao hơn.
Và ngược lại nếu đầu tư ít hơn các công ty neighbors thì sẽ bị thụt lùi về lợi nhuận lẫn giá trị thị trường.
Điều này cũng giải thích tại sao các tập đoàn công nghệ lớn không tiếc tiền đầu tư để đứng đầu một lĩnh vực nào đó so với các công ty lân cận.&lt;/p&gt;

&lt;p&gt;Ở đây chúng ta cũng cần hiểu khái niệm &lt;strong&gt;neighbor (lân cận)&lt;/strong&gt; là hiểu theo nghĩa địa lý (geographical).
Rất nhiều nghiên cứu theo sau Jaffe về sau đều chỉ ra mối correlation mạnh giữa khoảng cách địa lý và khoảng cách trong không gian công nghệ (đo bằng vector 49 chiều) &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#audretsch1996r&quot;&gt;[4]&lt;/a&gt;.
R&amp;amp;D spillovers không chỉ tồn tại trong các firms cùng lĩnh vực mà còn tồn tại giữa hàn lâm và công nghiệp theo Jaffe, 1989 &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt;.
Thì nghiên cứu hàn lâm lại có vẻ thúc đẩy tiến bộ trong các công ty xung quanh (gần về địa lý) hoặc có hợp tác với trường đại học ấy &lt;a class=&quot;citation&quot; href=&quot;#jaffe1989real&quot;&gt;[3]&lt;/a&gt;.
Khá nhiều nghiên cứu về &lt;strong&gt;spillovers (knowledge diffusion)&lt;/strong&gt; đã trải thành một mạch dài từ sau Jaffe, 1986 &lt;a class=&quot;citation&quot; href=&quot;#jaffe1986technological&quot;&gt;[2]&lt;/a&gt;:
David và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#david2000public&quot;&gt;[5]&lt;/a&gt; thì nghiên cứu về tác động của public R&amp;amp;D trong việc &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kích hoạt&lt;/code&gt; private R&amp;amp;D.
Baptista và Swann &lt;a class=&quot;citation&quot; href=&quot;#baptista1998firms&quot;&gt;[6]&lt;/a&gt; cũng chỉ ra khá nhiều bằng chứng cho thấy “tốt nhất là đi cùng nhau” giữa các firm trong cùng cluster.
Những nghiên cứu về &lt;strong&gt;open innovation&lt;/strong&gt; &lt;a class=&quot;citation&quot; href=&quot;#salter2001economic&quot;&gt;[7]&lt;/a&gt;, &lt;a class=&quot;citation&quot; href=&quot;#perkmann2007university&quot;&gt;[8]&lt;/a&gt; cũng cho thấy benefits của kinh tế từ R&amp;amp;D trong các trường đại học.&lt;/p&gt;

&lt;p&gt;Trên đây là để hiểu rõ nguồn gốc của đống nghiên cứu đã có. Tại sao lại có cái đống này, và có căn bản.
Gần đây từ những năm 2020 trở ra có cái gì thì cũng chịu khó xem thì thấy vẫn có người làm theo hướng này kha khá:
Sun và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#sun2021energy&quot;&gt;[9]&lt;/a&gt; thì tập trung vào vấn đề tối ưu hiệu suất sử dụng năng lượng như 1 benefit của R&amp;amp;D spillover.
Fini và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#fini2021attention&quot;&gt;[10]&lt;/a&gt; thì cho thấy là nếu nhà nghiên cứu &lt;strong&gt;khởi nghiệp&lt;/strong&gt; và đóng góp cho việc diffuse tri thức thì sẽ tăng hiệu suất nghiên cứu lên.
Ufuk và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#akcigit2021back&quot;&gt;[11]&lt;/a&gt; thì nghiên cứu trên tập dữ liệu của Pháp thì thấy rõ spillovers đang bị lệch về phía nghiên cứu ứng dụng mà ít về phía nghiên cứu cơ bản.
Hsu và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#hsu2021rich&quot;&gt;[12]&lt;/a&gt; thì nghiên cứu về vấn đề Trung Quốc: spillovers tại Trung Quốc.
Martinez và cộng sự &lt;a class=&quot;citation&quot; href=&quot;#martinez2021knowledge&quot;&gt;[13]&lt;/a&gt; thì trả lời vấn đề crowdfunding ảnh hưởng tới spillovers ra sao.&lt;/p&gt;

&lt;p&gt;Nhìn chung là&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Đến khi nào anh hiểu được rằng những tri thức vĩ đại và to lớn mà người ta hay gọi là trí tuệ ấy, luôn tồn tại dưới vỏ bọc tầm thường và vớ vẩn, thì anh mới hành động như người có tri thức được.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Chào 2022! Và coi như bắt đầu một năm mới nhiều dự định cũng như kế hoạch.
Luôn có những quy luật xã hội nhất định chi phối cộng đồng của chúng ta: ví dụ như các công ty trong giới công nghiệp cũng như trường đại học sẽ có những tương tác qua lại, tạo nên một mô hình spillover (diffusion) tồn tại mãi mãi trong chúng ta.
Tương tác qua lại giữa các firm trong cùng một cluster đôi khi có ích: 1 tiến bộ R&amp;amp;D từ 1 cty có thể tạo ra lợi ích cho các công ty khác.
Điều đó có cái tốt mà cũng đôi khi không: khi có một công ty gia nhập vào 1 lĩnh vực, nếu công ty đó cùng stack công nghệ với cty bạn thì nên welcome, còn không thì phải đối xử với nó như rival.
Bởi vì nếu cùng stack công nghệ thì R&amp;amp;D của công ty đó có tiến bộ gì thì mình sẽ được hưởng lợi.
Ngược lại, nếu khác stack công nghệ và lại cùng bán 1 sản phẩm thì rõ ràng tiến bộ của họ không hề có ích gì cho mình mà lại tranh giành thị trường với mình.
Do đó, phần lớn các công ty giống nhau cả về sản phẩm lẫn stack công nghệ thì cuối cùng cứ tập trung lại thành 1 cluster và spillover (diffuse) cùng nhau.&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;PMPCerti18:online&quot;&gt;PMA PMP Certification Requirements | Project Management Certification &amp;amp; PMP Certification Eligibility.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/PMPCerti18_online/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jaffe1986technological&quot;&gt;Jaffe, A.B. 1986. Technological Opportunity and Spillovers of R&amp;amp;D: Evidence from Firms’ Patents, Profits, and Market Value. &lt;i&gt;The American Economic Review&lt;/i&gt;. 76, 5 (1986), 984–1001.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/jaffe1986technological/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;jaffe1989real&quot;&gt;Jaffe, A.B. 1989. Real effects of academic research. &lt;i&gt;The American economic review&lt;/i&gt;. (1989), 957–970.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/jaffe1989real/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;audretsch1996r&quot;&gt;Audretsch, D.B. and Feldman, M.P. 1996. R&amp;amp;D spillovers and the geography of innovation and production. &lt;i&gt;The American economic review&lt;/i&gt;. 86, 3 (1996), 630–640.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/audretsch1996r/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;david2000public&quot;&gt;David, P.A., Hall, B.H. and Toole, A.A. 2000. Is public R&amp;amp;D a complement or substitute for private R&amp;amp;D? A review of the econometric evidence. &lt;i&gt;Research policy&lt;/i&gt;. 29, 4-5 (2000), 497–529.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/david2000public/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;baptista1998firms&quot;&gt;Baptista, R. and Swann, P. 1998. Do firms in clusters innovate more? &lt;i&gt;Research policy&lt;/i&gt;. 27, 5 (1998), 525–540.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/baptista1998firms/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;salter2001economic&quot;&gt;Salter, A.J. and Martin, B.R. 2001. The economic benefits of publicly funded basic research: a critical review. &lt;i&gt;Research policy&lt;/i&gt;. 30, 3 (2001), 509–532.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/salter2001economic/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;perkmann2007university&quot;&gt;Perkmann, M. and Walsh, K. 2007. University–industry relationships and open innovation: Towards a research agenda. &lt;i&gt;International journal of management reviews&lt;/i&gt;. 9, 4 (2007), 259–280.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/perkmann2007university/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;sun2021energy&quot;&gt;Sun, H., Edziah, B.K., Kporsu, A.K., Sarkodie, S.A. and Taghizadeh-Hesary, F. 2021. Energy efficiency: The role of technological innovation and knowledge spillover. &lt;i&gt;Technological Forecasting and Social Change&lt;/i&gt;. 167, (2021), 120659.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/sun2021energy/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;fini2021attention&quot;&gt;Fini, R., Perkmann, M. and Michael Ross, J. 2021. Attention to exploration: The effect of academic entrepreneurship on the production of scientific knowledge. &lt;i&gt;Organization Science&lt;/i&gt;. (2021).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/fini2021attention/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;akcigit2021back&quot;&gt;Akcigit, U., Hanley, D. and Serrano-Velarde, N. 2021. Back to basics: Basic research spillovers, innovation policy, and growth. &lt;i&gt;The Review of Economic Studies&lt;/i&gt;. 88, 1 (2021), 1–43.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/akcigit2021back/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;hsu2021rich&quot;&gt;Hsu, D.H., Hsu, P.-H. and Zhao, Q. 2021. Rich on paper? Chinese firms’ academic publications, patents, and market value. &lt;i&gt;Research Policy&lt;/i&gt;. 50, 9 (2021), 104319.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/hsu2021rich/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;martinez2021knowledge&quot;&gt;Martı́nez-Climent Carla, Mastrangelo, L. and Ribeiro-Soriano, D. 2021. The knowledge spillover effect of crowdfunding. &lt;i&gt;Knowledge Management Research &amp;amp; Practice&lt;/i&gt;. 19, 1 (2021), 106–116.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/martinez2021knowledge/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Sat, 01 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/new-year/</link>
        <guid isPermaLink="true">https://wanted2.github.io/new-year/</guid>
        
        <category>Event</category>
        
        <category>Newyear event</category>
        
        <category>technological distance</category>
        
        <category>innovation</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>Chia tay 2021!</title>
        <description>&lt;p&gt;Đây là bài post thứ 61 của blog AiFi trong năm 2021, cũng là bài viết chia tay 2021, trong tâm thế đón chờ 2022 tươi mới hơn.
Theo quan điểm làm việc scrum, thì coi như đây là thời điểm kết thúc 1 chu kỳ, cũng là lúc làm một số việc để nhìn lại một năm đã qua (bao gồm cả GKPT hay &lt;em&gt;Good, Keep, Problem, Try&lt;/em&gt;).
2021年中61番目の投稿です．
2021年と別れて，2022年を迎える時期の投稿です．
一年間を1スプリントとすると，いろいろなことができたと思いますので，スクラムの行事として，レビューとレトロ会をここで開催したいと思います．
&lt;em&gt;Good, Keep, Problem, Try&lt;/em&gt; も含めてやります．
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;nhìn-lại-năm-2021-của-blog-aifi&quot;&gt;Nhìn lại năm 2021 của blog AiFi&lt;/h1&gt;

&lt;h2 id=&quot;nhìn-từ-thống-kê-người-dùng&quot;&gt;Nhìn từ thống kê người dùng&lt;/h2&gt;

&lt;p&gt;Hiện tại AiFi blog sử dụng Google Analytics để track và lấy thống kê người dùng.
Các sự kiện như view, scroll, referal, … được báo cáo theo phút lên server của Google.&lt;/p&gt;

&lt;p&gt;Đầu tiên là thống kê về người dùng và nguồn giới thiệu.
Trong năm 2021, blog tuy mới ra mắt và còn nhiều khó khăn vất vả nhưng đã thu hút được 552 user mới từ khắp nơi trên thế giới.
&lt;strong&gt;552 người dùng này đã ghi lại 7309 sự kiện.&lt;/strong&gt;
Một con số đáng khích lệ với blog mới 1 năm tuổi đời.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Một điểm đáng chú ý là dù facebook.com là nơi tác giả hay chia sẻ bài viết, nhưng &lt;strong&gt;user lại phần lớn đến từ 2 nguồn: google và direct&lt;/strong&gt;.
Về yếu tố địa lý thì đa phần người dùng đến từ &lt;strong&gt;Việt Nam, Mỹ và Nhật Bản&lt;/strong&gt;.
Các nước khác vẫn chưa đóng tỷ trọng lớn trong cơ cấu người dùng của AiFi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tỷ lệ người dùng của AiFi gia tăng tính từ tháng &lt;strong&gt;7&lt;/strong&gt;.
Trong năm 2021, &lt;strong&gt;số lượng sự kiện &lt;em&gt;user engagement&lt;/em&gt; là 1852, và số &lt;em&gt;page view&lt;/em&gt; là 2622 lượt&lt;/strong&gt;.
Ngoài ra, 3 bài viết đạt số lượng truy cập cao nhất (không tính trang chủ) là:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/mOCR-mlkit-androidx-example/&quot;&gt;mOCR: A real-time application of OCR with Google MLKit and Android CameraX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/adobe-creative-cloud/&quot;&gt;Adobe Creative Cloud: An All-in-One Platform for Creators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/aws-lambda-spacy-mxnet-possible-but-shouldnt/&quot;&gt;Implementing a complex system in AWS Lambda: Should or shouldn’t?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sự “vùng lên” của bài viết &lt;a href=&quot;/adobe-creative-cloud/&quot;&gt;Adobe Creative Cloud: An All-in-One Platform for Creators&lt;/a&gt; thật thú vị vì bài viết được xuất bản trên blog AiFi vào tháng cuối năm nhưng lại đứng thứ nhì.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-03.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Về hệ điều hành, trình duyệt và ngôn ngữ đầu vẫn là &lt;strong&gt;Windows, Chrome và English&lt;/strong&gt;.
Theo sau lần lượt là &lt;strong&gt;MacOS, Safari và tiếng Nhật&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-04.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nhìn-từ-kết-quả-tìm-kiếm&quot;&gt;Nhìn từ kết quả tìm kiếm&lt;/h2&gt;

&lt;p&gt;Kết quả tìm kiếm về “AiFi Caineng” trên google.com và Bing Search trong ngày 31 tháng 12 năm 2021 như sau:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aifi-search-engines-2021.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả tìm kiếm từ khóa “aifi” và thậm chí “aifi caineng” quả là hơi nghèo nàn và dễ bị lẫn vào các từ khóa tìm kiếm khác như “wifi” chẳng hạn.
Đây cũng là 1 thiếu sót do blog mới chỉ 1 năm, và tác giả vẫn đang bận bịu công việc chính cuả tác giả.
Tuy nhiên, từ năm 2022, ở mức độ nhất định việc nâng rank trong các cỗ máy tìm kiếm từ khóa sẽ được &lt;strong&gt;tối ưu hóa&lt;/strong&gt; nhằm đưa tri thức của AiFi đến với đông đảo bạn đọc và nâng cao chất lượng phục vụ.&lt;/p&gt;

&lt;h1 id=&quot;good-keep-problem-try&quot;&gt;Good, Keep, Problem, Try&lt;/h1&gt;

&lt;p&gt;Việc chạy sprint kéo dài 1 năm quả là hơi lạ, tuy nhiên là cũng dễ hiểu vì viết blog chỉ là việc phụ làm trong thời gian rảnh rỗi của tác giả.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Good&lt;/th&gt;
      &lt;th&gt;Keep&lt;/th&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Try&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Đã tạo được và thu hút lượng người dùng nhất định.&lt;/td&gt;
      &lt;td&gt;Duy trì tần suất chia sẻ bài viết.&lt;/td&gt;
      &lt;td&gt;Thứ hạng trên search engine chưa cao.&lt;/td&gt;
      &lt;td&gt;Tối ưu hóa SEO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Tối ưu hóa từ khóa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Tối ưu thẻ HTML, …&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Chưa tạo ra thu nhập từ blog&lt;/td&gt;
      &lt;td&gt;Xem xét đưa vào và tối ưu hóa quảng cáo.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Các nguồn Google và Facebook đã đem đến lượng người dùng nhất định.&lt;/td&gt;
      &lt;td&gt;Tiếp tục duy trì quảng bá trên Google và Facebook.&lt;/td&gt;
      &lt;td&gt;Nguồn Facebook chưa đem lại nhiều người dùng mới.&lt;/td&gt;
      &lt;td&gt;Tối ưu hóa quảng bá blog trên Facebook.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Một số nguồn cấp khác như Twitter và LinkedIn vẫn chưa đem lại nhiều người dùng.&lt;/td&gt;
      &lt;td&gt;Lên chiến lược quảng bá trên các nền tảng này.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;tổng-kết&quot;&gt;Tổng kết&lt;/h1&gt;

&lt;p&gt;Kết thúc Sprint 2021, hướng tới Sprint 2022, blog AiFi xin cám ơn đông đảo bạn đọc, đặc biệt là 552 người dùng đã có, vì sự quan tâm và thịnh tình trong năm qua.
Trong năm 2022, AiFi sẽ tiếp tục cập nhật và mong muốn lan tỏa tri thức cho anh em, với phương châm, troll trước học sau.&lt;/p&gt;
</description>
        <pubDate>Fri, 31 Dec 2021 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/year-end/</link>
        <guid isPermaLink="true">https://wanted2.github.io/year-end/</guid>
        
        <category>Event</category>
        
        <category>Year-end event</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
      </item>
    
      <item>
        <title>Nên chọn gói subscription Adobe/Games nào cho nhóm phát triển và nghiên cứu?</title>
        <description>&lt;p&gt;Ở các quốc gia tôn trọng và thực thi nghiêm chỉnh luật bản quyền, thì vị trí leader có một “đặc sản” khá thú vị là &lt;strong&gt;kỹ năng quản lý license&lt;/strong&gt;, hay là &lt;strong&gt;tối ưu hóa license (サブスクリプションの最適化, optimize subscriptions)&lt;/strong&gt;.
Giải thích thì cũng mất thời gian, tôi xin lấy ví dụ bài toán như sau:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Giả sử bạn là chủ nhiệm (hoặc người đứng đầu) một phòng nghiên cứu và phát triển có lượng nhân sự ổn định là 40 người.
Hàng năm có 20 người tốt nghiệp và đi chỗ khác, và bạn sẽ lại đón khoảng 20 người mới vào để phân đề tài hướng dẫn.
Và vì vậy nhân sự luôn ổn định tầm 40 người.
Thế mạnh của phòng là xử lý đồ họa, thiết kế web, và mấy cái game ghiếc, truyền hình (TV).
Những gì trong thế mạnh thì là cái bạn sẽ hướng dẫn (chứ không có chuyện là nhận vào rồi cho làm mấy cái text tiếc vớ vẩn hay mấy cái random là không có đâu).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Vậy bây giờ là phát sinh ra trong hướng nghiên cứu của phòng bạn, sẽ phải mua máy móc như màn hình cỡ lớn, màn hình đặc dụng cho ngành truyền hình, … và đăng ký mua license phần mềm như Adobe Photoshop, Illustrator, XD, … rồi lại còn games, …&lt;/strong&gt;.
&lt;strong&gt;Thế kế hoạch quản lý license của bạn như thế nào? Với giả định bạn là đứng đầu lab và trách nhiệm quản lý sẽ thuộc về bạn.&lt;/strong&gt;
&lt;!--more--&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;bản-chất-của-bài-toán-quản-lý-license-software-license-management&quot;&gt;Bản chất của bài toán quản lý license (Software License Management)&lt;/h1&gt;

&lt;p&gt;Thực ra nói là tôn trọng bản quyền là 1 vấn đề nhưng nó chỉ là một cái mang tính chất bề nổi của tảng băng chìm.
&lt;strong&gt;Ẩn đằng sau kỹ năng quản lý license chính là vấn đề cost (giá thành) và security (bảo mật).&lt;/strong&gt;
6 bước cần phải làm với quản lý license (SLM) chính là:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Quy trình&lt;/th&gt;
      &lt;th&gt;Mô tả&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Usage analysis&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Phân tích nhu cầu sử dụng của nhóm phát triển, tổ chức dưới quyền, nhân viên. Nhóm đứng đầu sẽ phải phân tích nhu cầu sử dụng thực sự của nhân viên là gì. Ở bước này chưa cần tối ưu ngay, nhưng cần phải ngăn chặn những request mua đồ không cần thiết.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;License optimization&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Sau khi đã tối ưu hóa requests từ phía end-users, thì việc tiếp theo là với bộ request đã tối ưu ấy rồi, thì phải lựa chọn mua (hay không mua) những phần mềm, thiết bị nào. License nào sẽ bỏ tiền ra để subscribe, và sẽ subscribe theo gói nào?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Maintenance analysis&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Cái này cũng nói đi nói lại nhiều lần rồi, nếu bước này không tính toán dài hạn dựa trên usage analysis thì ví dụ phiên bản phần mềm nâng cấp thì lại phải mua lại license, dẫn đến tốn tiền nâng cấp license. Ngoài ra, nếu không kịp thời nâng cấp mua mới license thì khi hết hạn nhân viên lại phải chờ sếp mua license mới, rất tốn công.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Standardized data collection&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Với các tổ chức lớn như viện nghiên cứu hay trường đại học có hàng trăm thậm chí hàng ngàn nhân viên, việc quản lý tập trung, thu thập thông tin sử dụng phần mềm của nhân viên (cài phiên bản nào, license thế nào) là vô cùng quan trọng. Về mặt bảo mật, thì khi có license hết hạn, người dùng vi phạm license agreement, nhà quản lý sẽ được thông báo trực tiếp từ data, và như thế đối ứng sự cố bảo mật mới tốt.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Cyber-threat detection&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Thực ra thì thu thập dữ liệu mà muốn lên cảnh báo thì phải có một feature là threat detection. Tức là phát hiện nguy cơ liên quan tới license ngay từ dữ liệu usage của nhân viên.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Audit Prevention&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Phòng chống các vấn đề khi đã phát hiện ra từ dữ liệu ra là cần thiết. Auditor sẽ phải đưa ra những suggestions để tổ chức và nhà quản lý đưa thành policy để implement trong tổ chức.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Đấy, nghe thì có vẻ đơn giản là làm 1 bảng Excel để quản lý license một cách thủ công, nhưng breakdown ra thì cũng đủ thứ.
Làm nhà quản lý cho 1 team nhỏ tầm 5-6 người, hay dù quản lý cả 1 department hay là hiệu trưởng một trường đại học danh tiếng có hàng vạn sinh viên thì những vấn đề như thế này không được lơ là.&lt;/p&gt;

&lt;p&gt;Theo kinh nghiệm của tôi thì cứ tốt nhất mua luôn gói nào nó có sẵn hết các chức năng trên.
Ví dụ thu thập dữ liệu và detect nguy cơ là Adobe nó cũng có hết trong gói Business rồi đấy.
Tuy nhiên, nếu trong nhóm chỉ có 1-2 người cần mua thì bài toán giá thành cân đối bảo mật sẽ phát sinh vì ít nhu cầu dùng quá (cost vs. security trade-off).
Tuy nhiên, nếu là lãnh đạo của 1 department hàng trăm nhân khẩu dưới quyền thì cứ thoải mái thôi, vì khi số lượng đặt hàng lên đến hàng trăm, hàng ngàn thì chắc chắn Adobe sẽ có discount.
Vào mấy cái viện ít người mới lo chứ còn vào những chỗ quân đông tướng lắm như trường đại học, tập đoàn, cty lớn thì chủ yếu là kỹ năng quản lý thôi.&lt;/p&gt;

&lt;h1 id=&quot;quay-lại-ví-dụ-lab-40-người&quot;&gt;Quay lại ví dụ lab 40 người&lt;/h1&gt;

&lt;h2 id=&quot;đại-học-lớn-vs-đại-học-nhỏ-và-viện-ít-người&quot;&gt;Đại học lớn vs. đại học nhỏ (và viện ít người)&lt;/h2&gt;

&lt;p&gt;Ví dụ nhân sự 40 người nói ở trên là một trường đại học và tập đoàn lớn, nơi cứ mỗi năm có hàng ngàn người mới, hết thế hệ này tới thế hệ khác.
Vì vậy làm giáo sư hay teamlead ở một tập đoàn hay trường đại học lớn, thì vấn đề nhân sự lại không phải việc khó.
Cứ lên làm lead thì thiếu người muốn nhận bao nhiêu chả được.
&lt;strong&gt;Cái quan trọng là năng lực quản lý của anh được bao nhiêu người?&lt;/strong&gt;
Thường thì chuyện quản 40 người thậm chí hàng trăm người là chuyện bình thường ở các cơ sở lớn như vậy.&lt;/p&gt;

&lt;p&gt;Còn đối với các viện nhỏ, đại học nhỏ thì đôi khi mỗi leader một năm chỉ quản có vài ba nhân viên.
Chuyện quản lý có khi dễ dàng, người đứng đầu chả thèm quan tâm xây dựng văn hóa đội nhóm, văn hóa team, …
Nếu là nhà nghiên cứu thì đi vào những nơi này họ sẽ có nhiều thời gian làm chuyên môn hơn là lo quản lý hàng trăm hàng ngàn miệng ăn phía dưới.
&lt;strong&gt;Chỉ cần lo cho bản thân mình và mấy cậu phía dưới thôi.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Tuy nhiên, cái gì cũng có mặt trái của nó, nếu cái kiểu vài cơ sở nhỏ như trên thì gánh nặng quản lý không có nhưng giá thành sẽ đắt vì hầu như các gói subscription của các phần mềm như Office hay Adobe thì chỉ rẻ khi có nhiều trên 100 người dùng.
Ông làm nghiên cứu mà lab ông chỉ có 2 anh postdocs thì dù là viện nghiên cứu cấp quốc gia thì khi mua license thì cũng chỉ tính được cho 3 người (ông và 2 anh postdocs).
Mà thế thì hầu như muốn mua phần mềm gì cũng chỉ có vài khả năng:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;là mua gói personal subscription, giá sẽ đắt trên đầu người. Ngoài ra, sẽ phải mua đứng tên từng cá nhân nên control của tổ chức sẽ không có. Tức là như đại học lớn họ mua hàng loạt cho ngàn người dùng thì họ sẽ được tặng kèm gói audit để thực hiện thu thập dữ liệu, detect nguy cơ và prevention.&lt;/li&gt;
  &lt;li&gt;là ông phải chung với các lab lớn khác. Nhiều lab nhỏ cùng có nhu cầu dùng hợp với nhau để mua. Thì nó lại có cái dở là ông admin ở lab khác cũng sẽ quản lý được dữ liệu thu thập được từ nhân viên lab ông. Thành ra là có vấn đề về riêng tư.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thế nên là vào mấy cái startups hay đội nhóm nhỏ, đại học nhỏ ấy thì thường mọi người chấp nhận không dùng bản có phí, dùng &lt;strong&gt;open source&lt;/strong&gt; để tránh việc phải xài những cái đòi licenses như MATLAB, Office, Adobe, …
Cái này cũng có cái hay cái dở.
Thì thực ra tình huống nó bắt buộc là mình ít người mình chịu.
Nhưng nó lại dở vì đa số người ta dùng Adobe, MATLAB, … mình lại không mua được thì thành ra là lại outliers.
Ví dụ nghiên cứu của lĩnh vực đều dùng Adobe Photoshop, thì mình lại tái hiện trên GIMP thì ôi thôi!&lt;/p&gt;

&lt;p&gt;Mà cái quan trọng nhất lại không chỉ giá tiền mà là &lt;strong&gt;audit&lt;/strong&gt;. Dù ông nhà nghiên cứu ở viện nhỏ, ông có mua personal subscription hay open source thì chắc chắn sẽ không có control của tổ chức.
Nên nếu ông đề nghị mua phần mềm cho 2 cậu postdocs thì chắc chắn ông lãnh đạo viện sẽ bắt ông phải mua theo option 2.
Mà như thế là dữ liệu của nhóm ông tự dưng lại phải chia sẻ ra cho các nhóm khác trong viện mà thậm chí ngoài viện.
Còn nếu ông không đề nghị mua, tức là ông giấu lãnh đạo viện thì sẽ không cân đối kế toán được.
Còn ông mà bảo postdocs nó dùng open source thì lại xảy ra vấn đề là không minh bạch vì không có control trong audit.&lt;/p&gt;

&lt;p&gt;Nên lúc cân nhắc làm việc ở viện lớn hay viện nhỏ, đại học danh tiếng hay ít danh tiếng, tập cty lớn hay mấy startups, theo kinh nghiệm của tôi nên cân nhắc bài toán &lt;strong&gt;quản lý license&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;lựa-chọn-subscription-cho-phần-mềm-đồ-họa-adobe-software&quot;&gt;Lựa chọn subscription cho phần mềm đồ họa: Adobe software&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Bây giờ chúng ta sẽ bắt đầu thực chiến quản lý license.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Đầu tiên, về nhu cầu sử dụng, &lt;strong&gt;usage analysis&lt;/strong&gt;, thì quan điểm là người đứng đầu có quyền chỉ định đề tài nghiên cứu cho nhân viên.
Đặc biệt nếu là công ty lớn, đại học lớn thì chuyện này là hiển nhiên.
&lt;strong&gt;Bạn nghĩ (a) nhân viên tự đề xuất đề tài nghiên cứu rồi tự ý làm hay (b) lãnh đạo chỉ đạo chọn đề tài nghiên cứu thì tốt hơn?&lt;/strong&gt;
Về mặt quản lý license, chắc chắn (b) sẽ tốt hơn với trường hợp có tới 40 nhân viên như chúng ta đang giả định.
Vì sao? bởi vì là công nghệ lõi của lab vẫn là xử lý đồ họa, nên chắc chắn lúc đầu năm lúc xin kinh phí, lãnh đạo phải có kế hoạch để mua license nào phục vụ hướng nghiên cứu của lab.
Thế giờ tự cho nhân viên tự tìm đề tài, nó lại tìm ra cái đề tài lệch 180 độ so với hướng đi của lab, rồi đòi mua 1 cái license mà ko có tác dụng gì cho nhóm nghiên cứu thì kế hoạch kinh phí đã xin là phá sản?
Thế nên những trường hợp cũng không làm gì được vì tự do nghiên cứu phát triển, chống harrassment thì không thể bắt ép được.
Lại phải delay thôi vì &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;em đánh đố thế thì thày cũng chịu nhá&lt;/code&gt;.
Ngoài ra không chỉ vấn đề tiền mà cả về bảo mật thì nhân viên tự ý chọn đề tài ra ngoài rồi đòi mua license ngoài dự tính như vậy, thì chắc chắn là nếu đề xuất mua lên trên, thì lãnh đạo trường chắc chắn bắt mua chung với 1 lab khác, cho đủ con số người dùng để được giảm giá, thì lại xuất hiện bài toán phải chia sẻ dữ liệu với lab khác.
Mà hợp tác với lab khác mà chuyện chia sẻ dữ liệu nhập nhằng cũng sẽ nhiều vấn đề.
Nên cái thằng nhân viên nó chọn đề tài mang tính &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outliers&lt;/code&gt; là phải xử lý.&lt;/p&gt;

&lt;p&gt;Thì thực ra cái gì cũng có thể giải quyết được, đề tài lệch 180 độ mà hai bên thống nhất thì cũng “bẻ lái” được thôi, nên thày trò lãnh đạo nhân viên chịu khó nói chuyện rồi mà giải quyết với nhau là êm đẹp nhất.
Còn nếu vướng mắc quá thì lại phải thuyên chuyển sang nơi khác mà nhiều nhân viên dùng cái license ấy thôi.&lt;/p&gt;

&lt;p&gt;Theo kinh nghiệm của tôi, thì lãnh đạo &lt;strong&gt;mát tay&lt;/strong&gt; là có khi ông ấy control luôn cả usage của 40 nhân viên.
Tức là không nói thẳng ra là chỉ đạo nhưng mà nhìn chung là cứ có cái gì lệch lạc tổ chức họp “bẻ lái”, làm sao cho cả 40 nhân viên đều làm đồ họa.
Thằng nào thích làm NLP thì chủ động dùng open source đi vì NLP phần lớn open source chứ ít kiểu license nhiều như bên đồ họa, vision.
Tôi từng gặp một trường hợp lãnh đạo rất &lt;strong&gt;mát tay&lt;/strong&gt;: chọn đề tài từ trên xuống dưới cách thể hiện qua ngôn ngữ thì nhìn qua tên đề tài thấy chả có liên quan tới vision đồ họa mấy, nhưng đến lúc bóc tách ra thì toàn các tính năng của Adobe Creative Suite.
Thế là 40 con người vẫn đều phải phụ thuộc vào Adobe, mà thế thì cả phòng dùng Adobe thì viết đơn xin mua Adobe cho 40 người bên trên làm sao từ chối được?
&lt;strong&gt;Cả phòng làm về Adobe thì mua sản phẩm Adobe là đúng rồi còn gì.&lt;/strong&gt;
Thế nên, nếu mà control usage ngay từ đầu thì lúc đề xuất mua cái gì cũng dễ pass, cái bước usage analysis cũng chả phải làm vì cả phòng làm Adobe thì còn phân tích cái gì nữa.
&lt;strong&gt;Mát tay&lt;/strong&gt; nó là như thế đấy.&lt;/p&gt;

&lt;p&gt;Bây giờ, thày trò họp hành, rồi thảo luận trao đổi, “bẻ lái” thế nào đó để cả 40 con người đồng bộ nhu cầu rồi nhé.
Bước tiếp theo là &lt;strong&gt;License optimization&lt;/strong&gt;, tức là chúng ta sẽ phải lựa chọn gói tối ưu nhất của Adobe để mua đảm bảo 40 con người có đồ mà dùng.
Thực ra bước này làm cùng với phân tích nhu cầu sử dụng cũng được.
Thì cứ giả định là thời gian sử dụng là 24/7 luôn đi.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tham khảo các gói sử dụng của Adobe Creative Cloud tại &lt;a href=&quot;https://www.adobe.com/creativecloud/plans.html&quot;&gt;https://www.adobe.com/creativecloud/plans.html&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bây giờ có 40 người thì có hai cases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Chúng ta là phòng nghiên cứu phát triển của 1 cty lớn và cả 40 nhân viên đều chính thức&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Nếu mua cho 40 nhân viên 40 gói individuals thì sẽ tốn mỗi tháng là $40\times 52.99=2199.6$ USD. Một năm sẽ tốn tầm 25k USD.
        &lt;ul&gt;
          &lt;li&gt;Bất lợi: là không có quản lý license đi kèm.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Nếu mua theo gói Business thì
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.adobe.com/creativecloud/business/teams.html&quot;&gt;https://www.adobe.com/creativecloud/business/teams.html&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;Bạn sẽ trả 79.99 đô/tháng cho mỗi license, thì nhu cầu thay đổi chỉ cần hủy license và lấy license mới.&lt;/li&gt;
          &lt;li&gt;Tuy nhiên là nếu license dừng lại giữa tháng thì sẽ xuất hiện lãng phí và lại phải chia sẻ account để đỡ phí. Mà về mặt bảo mật thì chia sẻ account lại không nên. Thế nên là lại phải quản lý nhân viên để không có ai bỏ việc giữa tháng.&lt;/li&gt;
          &lt;li&gt;Lợi điểm là các tính năng để collaborate và quản lý license có đi kèm.&lt;/li&gt;
          &lt;li&gt;Trong trường hợp không chia sẻ tài khoản thì tốn $79.99\times 40=3199.6$ đô môĩ tháng, 1 năm mất 38-39K.&lt;/li&gt;
          &lt;li&gt;Trong trường hợp chia sẻ tài khoản thì có quy định sau:
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;https://helpx.adobe.com/download-install/using/install-apps-number-of-computers.html&quot;&gt;Số lượng máy có thể cùng lúc dùng cho 1 license&lt;/a&gt; thì chỉ tối đa 2 máy được login cùng license.&lt;/li&gt;
              &lt;li&gt;Thế nên coi như giảm đi 1 nửa so với không chia sẻ tài khoản thì mỗi tháng mất 1600 đô và mỗi năm tầm 19k đô. &lt;strong&gt;Vẫn rẻ hơn so với mua gói cá nhân hoặc không chia sẻ tài khoản&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chúng ta là ở trong đại học lớn và cả 40 nhân viên đều là students&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Thì kể cả gói cá nhân cho 40 sinh viên cũng chỉ mất 19.99 đô/tháng. vậy mỗi tháng mất $19.99\times 40=800$ đô. Mỗi năm mất dưới 10k.
        &lt;ul&gt;
          &lt;li&gt;Tuy nhiên cũng sẽ không có chức năng quản lý đi kèm hoặc team nhóm.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Nếu mua theo đơn vị department hoặc institution.
        &lt;ul&gt;
          &lt;li&gt;Nếu mua theo department thì vì chỉ có 40 người mà yêu cầu là 100 người nên sẽ phải mua chung với nhóm khác. Trong trường hợp có mua chung thì mỗi sinh viên chỉ tốn 12 đô, 1 tháng 40 sinh viên mất có 480 đô, 1 năm mất có 5-6k.&lt;/li&gt;
          &lt;li&gt;Nếu mua theo named-user license thì cứ mỗi user mỗi tháng mất 34.99 đô, 1 tháng 40 users mất $34.99\times 40=1399.6$ đô, 1 năm mất tầm 16-17k.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Nếu mua theo device thì giả sử cứ 5 người dùng chung 1 device (1 tuần luân phiên nhau dùng) thì cần 8 máy, mỗi máy 1 năm mất 330 đô, cả nhóm phải chi cho 8 máy 1 năm tầm $330\times 8=2640$ đô.&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nhìn chung là nếu cài theo máy thì sẽ rẻ nhất (cả năm mất có dưới 3k đô).
Thì vì có người cần dùng khi đi công tác nên sẽ phải có cả máy bàn và máy laptop: 4 máy bàn và 4 máy laptop có cài Adobe Creative Cloud.
Một máy giờ mua cũng tầm 2500 đô, dùng tầm 5 năm thì thay. Thì tổng tiền máy cần đầu tư là $2500\times 8=20000$ đô cho 5 năm.
Vấn đề là sẽ phát sinh thêm công quản lý máy móc: ví dụ khi đi công tác mượn máy lap có cài Adobe thì quy chế ra sao, …
Thì vì lab có 40 sinh viên, năm nào cũng có sinh viên mới nên chuyện có 4-5 cái máy bàn và mỗi sinh viên có 1 cái laptop là chuyện bình thường.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ca khó khăn là trường hợp lab trong cty thôi&lt;/strong&gt; vì Adobe không cung cấp gói device cho bên cty nên phòng nghiên cứu bên cty mà 40 người đều dùng thì mất 20k/năm là chuyện bình thường.
Thế nên lại đòi hỏi giải pháp ở đây:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Đẩy phần nghiên cứu đồ họa này về các trường đại học để giảm giá&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Thì nếu đẩy được thì trong phòng bạn lãnh đạo chỉ cần giữ lại khoảng 4-5 ông làm về đồ họa nhưng cũng chỉ là các ông kiểu PM/Techlead chứ dev là để bên đại học họ lấy sinh viên làm cho (có khi các em nó còn làm không công để tốt nghiệp ấy chứ), còn lại cho đi làm dự án khác.&lt;/li&gt;
      &lt;li&gt;Phần dự án cần nhiều license của Adobe thì thôi đẩy cho mấy ông thày bên đại học lo. Mình hợp tác thôi vì bên đại học họ mua license được số lượng lớn giá rẻ.&lt;/li&gt;
      &lt;li&gt;Chuyện hợp tác đương nhiên cũng phải có trả lương nhất định cho các bên, nhưng chắc chắn bên đại học thì mình sẽ phải chi ít hơn (sinh viên bỏ vào thì có phải trả đâu mà có trả cũng ít hơn nhân viên chuyên nghiệp trong các cty tập đoàn).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Tóm lại là nên đẩy những mảng nghiên cứu này về phía đại học vì nhiều lý do&lt;/strong&gt; và phương châm là thúc đẩy hợp tác giữa hai giới cty và học thuật.
Vầ tất nhiên người được lợi cuối cùng chỉ có bên giới công nghiệp và các sếp thôi.
Các bạn thử ngẫm mà xem:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bên Adobe bán license thì dù thế nào tiền vẫn vào túi họ nhé.&lt;/li&gt;
  &lt;li&gt;Bên cty muốn hợp tác thì tiền chắc chắn họ tiết kiệm được một khoản khá nhờ đẩy việc vào đại học nhé.&lt;/li&gt;
  &lt;li&gt;Các sếp cũng được thêm tiền dự án.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thế nên là tốt nhất cứ đi làm, còn đi nghiên cứu thì phải đòi thù lao nó xứng đáng các em ạ.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Về mặt maintain thì nhìn chung nên dùng gói Cloud, có gì nó cập nhật luôn mà vẫn tính trong tiền hàng tháng như vậy.&lt;/strong&gt;
Còn các mục audit quản lý khác thì nằm trong gói do Adobe cung cấp rồi.&lt;/p&gt;

&lt;h2 id=&quot;lựa-chọn-gói-nào-cho-games&quot;&gt;Lựa chọn gói nào cho games&lt;/h2&gt;
&lt;p&gt;Thi thoảng vẫn có một số nghiên cứu liên quan tới games.
Ví dụ như nghiên cứu để tìm ra 1 framework mới giúp việc thiết kế games trở nên mô phỏng gần với hành vi người dùng hơn:&lt;/p&gt;

&lt;iframe src=&quot;https://view.officeapps.live.com/op/embed.aspx?src=http%3A%2F%2Falgorithmancy%2E8kindsoffun%2Ecom%3A80%2FMDAnwu%2Eppt&amp;amp;wdAr=1.3333333333333333&quot; width=&quot;100%&quot; height=&quot;480px&quot; frameborder=&quot;0&quot;&gt;This is an embedded &lt;a target=&quot;_blank&quot; href=&quot;https://office.com&quot;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&quot;_blank&quot; href=&quot;https://office.com/webapps&quot;&gt;Office&lt;/a&gt;.&lt;/iframe&gt;

&lt;p&gt;Thì về phân tích nhu cầu thì nó lại thế này: nhân viên của mình thì không dùng mà mua về để cho &lt;strong&gt;những người tham gia thí nghiệm&lt;/strong&gt; dùng.
Những người tham gia thí nghiệm sẽ được tuyển chọn dựa trên base của thí nghiệm thôi.
Thì lại phải mua cho những người tham gia ấy chơi.
Tức là không phải mua cho nhân viên mình chơi mà là cho &lt;strong&gt;đối tượng thí nghiệm chơi&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Thì tùy vào &lt;strong&gt;setup của thí nghiệm&lt;/strong&gt; mà có khi phải mua cả bộ Xbox hoặc Playstation.
Có khi phải mua bản Gold hoặc bản Ultimate chứ bản Standard cũng ko được.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://store.ubi.com/us/game/?lang=en_US&amp;amp;pid=5e84a5065cdf9a21c0b4e737&amp;amp;dwvar_5e84a5065cdf9a21c0b4e737_Platform=pcdl&amp;amp;edition=Ultimate%20Edition&amp;amp;source=detail&quot;&gt;Bảng giá Assassin’s Creed Valhalla&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tuy nhiên, cũng cần nhớ nếu mình chọn đối tượng thí nghiệm là students ấy thì sẽ được discount 15%: &lt;a href=&quot;https://store.ubi.com/us/student-discount.html?lang=en_US&quot;&gt;https://store.ubi.com/us/student-discount.html?lang=en_US&lt;/a&gt;
Bao giờ student cũng được discount nên chọn loại đối tượng này thì sẽ tốt hơn.
Hoặc có khi mình có thể setup thí nghiệm là chỉ khảo sát &lt;strong&gt;những đối tượng nào đã chơi game trên thôi&lt;/strong&gt; thì chuyện mua game là của họ, mình chỉ cần tìm ra họ thôi.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuy nhiên là đôi khi là nhu cầu thí nghiệm đòi monitor cả quá trình chơi game của đối tượng thì lại phải mua cho họ games để họ chơi.&lt;/strong&gt;
Trong trường hợp phải mua cho họ chơi ấy, thì nên chọn student để có discount.&lt;/p&gt;

&lt;h2 id=&quot;lựa-chọn-gói-truyền-hình-gì&quot;&gt;Lựa chọn gói truyền hình gì?&lt;/h2&gt;

&lt;p&gt;Đồ họa và games coi như là hai nhánh chính thống của Computer Vision.
Một phần nào đó thì cũng là nơi mà CV thực sự được ứng dụng nghiêm chỉnh.&lt;/p&gt;

&lt;p&gt;Chúng ta ít khi nghĩ đến những đề tài nghiên cứu về truyền hình nhưng thực ra là cũng liên quan kha khá đấy.
Đơn cử là có những nghiên cứu về &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interactive television&lt;/code&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Truyền hình tương tác&lt;/strong&gt; là một dạng truyền hình cho phép người xem tham gia, điều khiển các chương trình truyền hình. Với dạng truyền hình truyền thống, đường truyền truyền hình là một chiều. Các nhà đài cho phép khán giả xem gì, vào giờ nào, trên kênh nào là quyền của họ.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://b-i.forbesimg.com/onmarketing/files/2013/05/300x201.jpg&quot; alt=&quot;interactive TV&quot; /&gt;
&lt;em&gt;Source: &lt;a href=&quot;https://www.forbes.com/sites/onmarketing/2013/05/20/guess-what-marketers-interactive-tv-is-actually-here/?sh=2ac8a4f0778a&quot;&gt;Forbes&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Thì cũng như đồ họa và games, chủ yếu là nghiên cứu nhận kinh phí từ các nguồn dồi dào có sẵn, được đặt hàng hẳn hoi.
Và họ cũng chủ yếu nghiên cứu về &lt;strong&gt;trải nghiệm người dùng&lt;/strong&gt; thôi.
Không cần code kiếc gì đâu, chỉ cần làm mấy thí nghiệm để kiểm chứng giả thuyết về trải nghiệm người dùng.&lt;/p&gt;

&lt;p&gt;Sẽ phải mua sắm màn hình cảm ứng touch, có kết nối truyền hình.
Sau đó là setup một căn phòng và cho người dùng trải nghiệm theo kịch bản cho sẵn.
Thế là sẽ phải mua một gói truyền hình cho thí nghiệm, nhưng như thế thì sẽ không dùng thường xuyên, và phải trả theo tháng thì mua kèm Internet cáp.
Tốt nhất là nên lên kế hoạch thí nghiệm và chốt thời gian tổ chức để chỉ mua gói truyền hình tại tháng ấy thôi.&lt;/p&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;
&lt;p&gt;Nhìn chung, &lt;strong&gt;kỹ năng quản lý license&lt;/strong&gt; là một kỹ năng không thể thiếu để quản trị một nhóm tầm trong quy mô 50-100 nhân viên.
Ở quy mô này trở lên, bắt đầu phải có những điểm lưu ý và thảo luận để quản lý linh hoạt chủ động mà lại tối ưu.&lt;/p&gt;

</description>
        <pubDate>Sat, 25 Dec 2021 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/subcription-optimize/</link>
        <guid isPermaLink="true">https://wanted2.github.io/subcription-optimize/</guid>
        
        <category>Adobe</category>
        
        <category>Adobe Creative Cloud</category>
        
        <category>Games</category>
        
        <category>Subscriptions</category>
        
        <category>teamlead</category>
        
        <category>techlead</category>
        
        <category>supervisor</category>
        
        <category>assistant</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Software Engineering</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>Adobe Creative Cloud: An All-in-One Platform for Creators</title>
        <description>&lt;p&gt;Adobe provides three different cloud solutions for customers: Adobe Creative Cloud for creators, Adobe Experience Cloud for marketers, and Adobe Document Cloud for business.
Creative Cloud is a collection of 20+ apps for photography, video, design, web, UX, and social media — plus integrated essentials like font families and the power to collaborate with anyone, anywhere. 
Many things can be done with this solution.
We take a look at the features.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;adobe-creative-cloud&quot;&gt;Adobe Creative Cloud&lt;/h1&gt;

&lt;p&gt;Adobe Creative Cloud (ACC) is an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all-in-one&lt;/code&gt; platform for creators, marketers, visual artists, and designers.
Let’s hear words about the roadmap of ACC from Scott Belsky.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/BzFY4pzb8cA?start=730&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This was highlighted in Adobe MAX 2020, a trade show event for creative users held by Adobe from October 20 to 22, 2020.
You can access the online tutorials from &lt;a href=&quot;https://experienceleague.adobe.com/docs/creative-cloud-enterprise-learn/cce-learning-hub/max2020/tutorials/maxtutorials.html?lang=en&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Creativity for All&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3-goals-of-adobe-creative-cloud&quot;&gt;3 Goals of Adobe Creative Cloud&lt;/h2&gt;

&lt;h3 id=&quot;enable-connected-creativity&quot;&gt;Enable Connected Creativity&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Creativity $\times$ Organization = Impact&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Putting creativity into an organization makes an impact!
Three pillars of your organization are: &lt;strong&gt;Assets, Apps, and Team&lt;/strong&gt;.
Let’s talk about the assets first.
How can you share your assets with your collaborators?
Adobe Behance, Adobe Bridge, and Adobe Stock make your assets sharable and organized.
You can share your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.psd&lt;/code&gt; files with your teammates, and you can search for your interested images from million of shared assets in Adobe Stock and Bridge.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Bringing 30 years of Photoshop features and code to a new platform like the web is a journey.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Photoshop has more than 30 years of history.
That is so long to have an impact, but now Adobe Photoshop is not only a desktop app.
Adobe Photoshop for Web is in Beta now and will be available in ACC soon.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Work better together with Adobe Creative Cloud.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, let’s talk about the team, the last pillar.
Adobe provides Spaces and Canvas for collaboration.
You can not only share the PSD and AI files with your collaborators but also work with them directly in ACC.
Adobe Cloud Canvases provide an overview of the whole team’s work.
Users can make comments, can get &lt;strong&gt;real-time&lt;/strong&gt; feedbacks in Canvases.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;And don’t forget the devices. &lt;strong&gt;Adobe apps like Photoshop in Apple’s M1 chip run twice faster than in older chips.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;With ACC, your assets and apps can be synchronized between your devices.
You can start your design from your iPhone and then grab it on your iPad or other devices.
And you can collaborate with your co-workers from any device as well.&lt;/p&gt;

&lt;h3 id=&quot;unleash-creative-potential&quot;&gt;Unleash Creative Potential&lt;/h3&gt;

&lt;p&gt;Web designers can use Adobe XD, Illustrators, and so on to create ideas.
Artists can use Fresco, Design to create a lot of drawings.
Photographers and video makers can capture and use Adobe Photoshop and After Effects to create images and videos for social posts.
Cartoon artists can use Character Animator to create a live anime.
Other people can use Lightroom or Premiere Rush to edit photos/videos on the mobile phone.
Other people can search for photos, AI files, videos, and even anime and manga in Stock and Behance.
Some can scan document images and get the content indexed and managed in Adobe Acrobat.
So many possibilities can be done with this suite of apps.&lt;/p&gt;

&lt;p&gt;And now, all of them are available in the Cloud.&lt;/p&gt;

&lt;h3 id=&quot;empower-creative-careers&quot;&gt;Empower Creative Careers&lt;/h3&gt;

&lt;p&gt;Adobe also launched &lt;strong&gt;Content Authenticity Initiate (CAI)&lt;/strong&gt; in 2018 to ensure artists get credits for their work.
Any content made in Adobe today will be attached metadata about the project they belong to, and the credits always go to the creators.
&lt;strong&gt;Non Fungible Tokens (NFT)&lt;/strong&gt; are generally used by artists recently to access marketplaces of collectors.
However, many artists have seen their works were minted without attribution to the original artists.
Adobe has worked with many NFT partners such as OpenSea, KnownOrigin, SuperRare, and so on to ensure the credits will be given to the right people.
When an artist creates a project in Photoshop with Content Credentials, then that information will be displayed in any related NFT partners.&lt;/p&gt;

&lt;h1 id=&quot;toolsets&quot;&gt;Toolsets&lt;/h1&gt;

&lt;p&gt;Now, we understand the goals and roles of ACC in creative activities.
We will take a closer look at each of the app in this section.&lt;/p&gt;

&lt;h2 id=&quot;image-editing&quot;&gt;Image editing&lt;/h2&gt;

&lt;h3 id=&quot;photoshop-vs-lightroom-lightroom-classic&quot;&gt;Photoshop vs. Lightroom (Lightroom classic)&lt;/h3&gt;

&lt;p&gt;Photoshop has more than 30 years of history.
A brief overview of released versions can be found &lt;a href=&quot;https://adobe.fandom.com/wiki/Adobe_Photoshop_release_history&quot;&gt;here&lt;/a&gt;.
The latest version is CC 2021, and we mostly use it.
Adobe Photoshop is specialized to photo enthusiasts who want to make beautiful images from camera-captured inputs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://learningavphotoshop.weebly.com/uploads/2/6/4/6/26460741/4580840_orig.jpg&quot; alt=&quot;pscc&quot; /&gt;
&lt;em&gt;Source: &lt;a href=&quot;https://learningavphotoshop.weebly.com/uploads/2/6/4/6/26460741/4580840_orig.jpg&quot;&gt;learningavphotoshop&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There are some resources in YouTube for you to learn Photoshop (especially, CC 2020):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLyGOksFjYo-nzGtyi6wFs68NpDic1lg9a&quot;&gt;Adobe Photoshop CC 2020 Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL9pkETrdJ0rb-BsDHwE0gmsj0duEXqbQ3&quot;&gt;Adobe Photoshop Tutorials&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLttcEXjN1UcFpeLZXSLtNU5iT2OxkO2nF&quot;&gt;Photoshop CC Essentials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on a rough estimation, there is a requirement of at least two years to comprehend this Photoshop CC.
There are too many features to learn, but think about the 30-year history of Photoshop: you only spend two years to learn 30 years of Photoshop features and code.
What a benefit!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Photoshop is a comprehensive photo editing app for photographers, web designers, visual artists, and photo enthusiasts.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, what’s about Lightroom?
While &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tools&lt;/code&gt; is essential in Photoshop, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Presets&lt;/code&gt; is primary in Lightroom.
In Photoshop, you can tune your images at a detailed level using fine-grained tools.
But it is completely different in Lightroom, where the presets hold only common features which are known to sufficiently produce good quality images.
Because Photoshop is specific for details, it is often used by professional photographers.
But with Lightroom, the user segment can be large.
Because presets were tuned by professionals, and the tuned parameters can be huge (millions to billions of parameters).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Photoshop is best for professionals who want to tune every detail of a single photo, while Lightroom presets are good for unknowledgeable users who want to tune a batch of hundred images without specialized knowledge about graphics.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Adobe-Lightroom.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While Lightroom Classic is a desktop app, Lightroom is a web app with the address: &lt;a href=&quot;https://lightroom.adobe.com/&quot;&gt;lightroom.adobe.com&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;illustrator&quot;&gt;Illustrator&lt;/h3&gt;

&lt;p&gt;If Photoshop CC is a graphic editing app, Illustrator CC is a creator app.
It can be used to create industry-standard vector artworks like logos, illustrations, and posters.
A beginner course is available on YouTube:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLYfCBK8IplO4X-jM1Rp43wAIdpP2XNGwP&quot;&gt;Learn Adobe Illustrator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLwnXQvUxjbNsvCG_rJJ8vzhbqtEjphKsf&quot;&gt;Adobe Illustrator Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://clubsintel.weebly.com/uploads/1/2/3/8/123847818/575683558.jpg&quot; alt=&quot;illustrator cs 5 tools&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Based on a rough estimation, it may take six months to a year to comprehend Adobe Illustrator (AI).
Then you can create professional artworks which can be used in web designs or anywhere they need AI.&lt;/p&gt;

&lt;h3 id=&quot;fresco-for-fun-drawing&quot;&gt;Fresco for fun drawing&lt;/h3&gt;

&lt;p&gt;Similar to Adobe Illustrator but with a friendlier interface and limited toolset, Fresco provides basic features for drawing raster artworks.
Fresco can be used in mobile devices like the iPhone and iPad.
Because it drops the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;presets&lt;/code&gt; notion, Fresco is close to end-users who are unknowledgeable about art.
In contrast, Illustrator is closer to artists and graphics professionals.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/YmPZtFl_j50&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;There are some free courses available on YouTube about Fresco.
If you are a normal end-user, you should like Lightroom and Fresco.
But if you are a professional, you may need Photoshop and Illustrator.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL_dhPga7ruufKDzs1yCPfNUb6Ud5M58so&quot;&gt;Adobe Fresco Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dimension-for-3d-photography&quot;&gt;Dimension for 3D photography&lt;/h3&gt;

&lt;p&gt;In this era of VR/AR applications, artists and producers need a tool to create 3D graphical models.
The models can be saved and viewed from different perspectives.&lt;/p&gt;
&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/bM-3PydkMBg&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL431Fy2ZWI6fqRGdUsKT5KBCmmI3A55dR&quot;&gt;Adobe Dimension Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Needless to say, you take advantage if you are already familiar with &lt;strong&gt;object-oriented design (OOD)&lt;/strong&gt;, because everything here is only 3D &lt;strong&gt;objects&lt;/strong&gt;!
Then you can be quite familiar with the workflow like selecting a template object, changing the &lt;strong&gt;Properties&lt;/strong&gt; of the object, then defining its &lt;strong&gt;Scene&lt;/strong&gt; and &lt;strong&gt;Actions&lt;/strong&gt;.
Adobe Dimension integrates Adobe Stock and Behance as its cloud storage and management solutions.
You can save your models to Stock or share assets to Behance.&lt;/p&gt;

&lt;h2 id=&quot;video-editing&quot;&gt;Video editing&lt;/h2&gt;

&lt;p&gt;Unlike photos, videos need care about motions, perspectives, and so on.&lt;/p&gt;

&lt;h3 id=&quot;premiere-pro-and-rush&quot;&gt;Premiere Pro and Rush&lt;/h3&gt;
&lt;p&gt;With Premiere Pro, users can edit their video durations, change replaying orders, image processing, edit audio, and so on with videos.
Rush is for mobile.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/f7prDfwtMCo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLttcEXjN1UcGF_PCDUcQw2WOoh6MZtvWt&quot;&gt;Adobe Premiere Pro Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some nice features can be &lt;strong&gt;multi-perspective videos&lt;/strong&gt; and &lt;strong&gt;video-audio alignment&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;after-effects&quot;&gt;After Effects&lt;/h3&gt;

&lt;p&gt;Unlike Premiere Pro is for professionals, After Effects has a smaller video editing toolset but yet enough for non-professional users.
Some straight modifications can be quite clunky with Premiere Pro.
In these scenarios, After Effects seems to be a better fit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://d2436y6oj07al2.cloudfront.net/assets/vbblog/2016/09/4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FuJMHF510mc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLYfCBK8IplO77FDDLnS06qEMoVLD7Qyib&quot;&gt;Affter Effects course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;audio-editing&quot;&gt;Audio editing&lt;/h2&gt;

&lt;p&gt;We had a look on image/video editing softwares.
But now we want to edit, mix, enhance audio, we should think about Adobe Audition.
Adobe Audition can be integrated into After Effects or Premiere Pro.
The output audios can be published to Behance and Stock.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/adobe-audition.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0njJIOjxS4c&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL-ssKAXBXMQRzqN5sc1eaR_Q6nACRQDF3&quot;&gt;Adobe Audition course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;storage-and-management&quot;&gt;Storage and Management&lt;/h2&gt;

&lt;h3 id=&quot;adobe-stock-and-adobe-bridge&quot;&gt;Adobe Stock and Adobe Bridge&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Adobe Stock&lt;/strong&gt;, formerly known as Adobe Stock Photos, is a stock image database that was originally integrated with Adobe Bridge in Adobe Creative Suite 2 and 3. It is presently available through certain subscription packages from Adobe Creative Cloud.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Users can access Adobe Stock from &lt;a href=&quot;https://stock.adobe.com/&quot;&gt;https://stock.adobe.com/&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Adobe Bridge&lt;/strong&gt; is a digital asset management tool developed by Adobe. It is freely included as part of Adobe Creative Cloud and was first made available with Creative Suite 2. Its primary purpose is to link the parts of the Creative Suite together using a format similar to the file browser found in previous versions of Adobe Photoshop. It is accessible from all other components of the Creative Suite (except for the standalone version of Adobe Acrobat 8).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bridge is also included with the stand-alone Photoshop application, and can perform certain Photoshop processing functions separately (and simultaneously) with Photoshop itself.&lt;/p&gt;

&lt;h3 id=&quot;media-encoder&quot;&gt;Media Encoder&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Adobe Media Encoder (AME)&lt;/strong&gt; is a video media transcoding utility developed and marketed by Adobe through Adobe Creative Cloud. 
It is designed for integration into workflows with other video applications such as Adobe Premiere Pro and After Effects.&lt;/p&gt;

&lt;h2 id=&quot;animation&quot;&gt;Animation&lt;/h2&gt;

&lt;h3 id=&quot;adobe-animate-cc&quot;&gt;Adobe Animate CC&lt;/h3&gt;

&lt;p&gt;Adobe Animate CC, formerly Adobe Flash Professional CC, is the current series of the Animate application offered by subscription through Adobe Creative Cloud.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/fFRtT-wDZlE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL3atG_J6wz0Vv9AoXqXOgc63X_8D5OBHE&quot;&gt;2D Animation - Free Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adobe-character-animator&quot;&gt;Adobe Character Animator&lt;/h3&gt;

&lt;p&gt;Adobe Character Animator is a 2D puppet animation program developed by Adobe that is included with Adobe After Effects CC through Adobe Creative Cloud.
It uses a &lt;strong&gt;multi-track motion capture recording system&lt;/strong&gt; to apply behaviors to puppets.&lt;/p&gt;

&lt;p&gt;Some nice thing can be done with Character Animator:&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/gWBLAcTWH5Q&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;web-design-tools&quot;&gt;Web design tools&lt;/h2&gt;

&lt;h3 id=&quot;adobe-dreamweaver&quot;&gt;Adobe Dreamweaver&lt;/h3&gt;

&lt;p&gt;Adobe Dreamweaver, formerly Macromedia Dreamweaver, is a web development application originally based on the codebase of Backstage, which was acquired by Macromedia in March 1996.
Dreamweaver has been owned and marketed by Adobe since December 2005.&lt;/p&gt;

&lt;p&gt;Dreamweaver is available for both Mac and Windows. Recent versions have incorporated support for web technologies such as CSS, JavaScript, and various server-side scripting languages and frameworks, including ASP.NET, ColdFusion, JavaServer Pages, and PHP.&lt;/p&gt;

&lt;p&gt;As a WYSIWYG Presto-based editor, Dreamweaver can hide the HTML code details of pages from the user, &lt;strong&gt;making it possible for non-coders to create web pages and sites&lt;/strong&gt;. One criticism of this approach is that it can produce HTML pages whose file size and amount of HTML code can be larger than an optimally hand-coded page would be, which can cause web browsers to perform poorly. This can be particularly true because the application makes it very straightforward to create table-based layouts. In addition, some website developers have criticized Dreamweaver in the past for producing code that often does not comply with W3C standards, though recent versions have been more compliant. Dreamweaver 8.0 performed poorly on the Acid2 Test, developed by the Web Standards Project. However, Adobe has increased the support for CSS and other ways to layout a page without tables in later versions of the application, with the ability to convert tables to layers and vice versa.&lt;/p&gt;

&lt;p&gt;Dreamweaver allows users to preview websites in many browsers, provided that they are installed on their computers.
It also has some site management tools, such as the ability to find and replace lines of text or code by whatever parameters specified across the entire site and a templatization feature for creating multiple pages with similar structures. 
The behaviors panel also enables the use of basic JavaScript without any coding knowledge.&lt;/p&gt;

&lt;p&gt;Dreamweaver can use “Extensions” – small programs, which any web developer can write (usually in HTML and JavaScript). Extensions provide added functionality to the software for whoever wants to download and install them. Dreamweaver is supported by a large community of extension developers who make extensions available (both commercial and free) for most web development tasks, from straight rollover effects to full-featured shopping carts.&lt;/p&gt;

&lt;p&gt;Like other HTML editors, Dreamweaver edits files locally, then uploads all edited files to the remote web server using FTP, SFTP, or WebDAV. Dreamweaver CS4 now supports the Subversion (software) (SVN) version control system.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Ok60gdJif0U&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLttcEXjN1UcHi3Rc-FSNZOaFSsSjFWCBS&quot;&gt;Dreamweaver Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adobe-xd&quot;&gt;Adobe XD&lt;/h3&gt;
&lt;p&gt;Adobe XD is a vector-based user experience design and prototyping system.
It is also a presentation tool for web apps and mobile apps, which is developed and published by Adobe.
It is available for free but requires an account registered through Adobe Creative Cloud.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/68w2VwalD5w&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLOTakqGKI6lcIRHkjCsVyxmg8lweyewmR&quot;&gt;Adobe XD Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;content-creation&quot;&gt;Content Creation&lt;/h2&gt;
&lt;h3 id=&quot;adobe-indesign&quot;&gt;Adobe InDesign&lt;/h3&gt;
&lt;p&gt;Adobe InDesign is a desktop publishing and page layout designing software application produced by Adobe Inc. It can be used to create works such as posters, flyers, brochures, magazines, newspapers, presentations, books, and ebooks. InDesign can also publish content suitable for tablet devices in conjunction with Adobe Digital Publishing Suite. Graphic designers and production artists are the principal users, creating and laying out periodical publications, posters, and print media. It also supports export to EPUB and SWF formats to create ebooks and digital publications, including digital magazines and content suitable for consumption on tablet computers. In addition, InDesign supports XML, style sheets, and other coding markups, making it suitable for exporting tagged text content for use in other digital and online formats. The Adobe InCopy word processor uses the same formatting engine as InDesign.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/RXRT3dHu6_o&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;adobe-incopy&quot;&gt;Adobe InCopy&lt;/h3&gt;
&lt;p&gt;Adobe InCopy is a professional word processor made by Adobe Inc. that integrates with Adobe InDesign. While InDesign is used to publish printed material, including newspapers and magazines, InCopy is used for general word processing. The software enables editors to write, edit, and design documents. The software includes standard word processing features such as spell check, track changes, and word count and has various viewing modes that allow editors to visually inspect design elements — only as it looks to the designer working in Adobe InDesign.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/zq5YpOIA13E&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;miscellaneous&quot;&gt;Miscellaneous&lt;/h2&gt;

&lt;h3 id=&quot;acrobat&quot;&gt;Acrobat&lt;/h3&gt;
&lt;p&gt;Adobe Acrobat is a family of computer programs developed by Adobe Systems, designed to view, create, manipulate and manage files in Adobe’s Portable Document Format (PDF). Some software in the family is commercial, and some are free of charge. Adobe Reader (formerly Acrobat Reader) is available as a no-charge download from Adobe’s website and allows the viewing and printing of PDF files. Acrobat and Reader are widely used as a way to present information with a fixed layout similar to a paper publication.&lt;/p&gt;

&lt;p&gt;As of June 2020, the main members of the Adobe Acrobat family are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Adobe Acrobat DC, sold only by subscription through Adobe Document Cloud with mobile app support.
    &lt;ul&gt;
      &lt;li&gt;Adobe Acrobat Standard DC&lt;/li&gt;
      &lt;li&gt;Adobe Acrobat Pro DC includes the ability to convert scanned documents and add media files&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adobe Acrobat 2020, the last version sold through a one-time perpetual license, but without cloud features.
    &lt;ul&gt;
      &lt;li&gt;Adobe Acrobat Standard 2020&lt;/li&gt;
      &lt;li&gt;Adobe Acrobat Pro 2020&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adobe Acrobat Reader DC, a free client version with the ability to print, sign, and annotate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;aero&quot;&gt;Aero&lt;/h3&gt;
&lt;p&gt;Adobe Aero is an augmented reality authoring and publishing tool developed by Adobe and marketed through Adobe Creative Cloud.
Aero was originally announced as a private beta for iOS users at Adobe MAX 2018.
It was officially launched during Adobe MAX 2019.&lt;/p&gt;

&lt;p&gt;Aero is part of Adobe’s 3D &amp;amp; AR series, which includes Adobe Dimension, Fuse, and Substance.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/fo8aG0vCY7k&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;adobe-sensei-the-graphical-ai&quot;&gt;Adobe Sensei: the graphical AI&lt;/h1&gt;
&lt;p&gt;Adobe Sensei is an artificial intelligence and machine learning technology being developed by Adobe. It is being applied to Adobe Analytics, Campaign, and Target. Sensei technology is also used in subject selection and removal, as seen in recent versions of Adobe Photoshop and Photoshop on iPad.&lt;/p&gt;

&lt;p&gt;Many Adobe products like Photoshop, Premiere Pro, Illustrator, and Stock use somewhat Sensei’s insight to enhance user experience.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/r9E0_lM5l48&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;We took a look at the Adobe Creative Cloud ecosystem.
Like most other commercial clouds like Oracle Cloud, AWS, Google GCP, or M$ Azure, it has many products and services which make our life better.
And Adobe Sensei AI is impressive as it integrates many of the latest technologies in Computer Vision, Machine Learning, and Artificial Intelligence.
Hope to use it in our own products soon.&lt;/p&gt;
</description>
        <pubDate>Sun, 19 Dec 2021 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/adobe-creative-cloud/</link>
        <guid isPermaLink="true">https://wanted2.github.io/adobe-creative-cloud/</guid>
        
        <category>adobe acrobat</category>
        
        <category>adobe creative cloud</category>
        
        <category>design</category>
        
        <category>illustrator</category>
        
        <category>adobe sensei</category>
        
        <category>adobe photoshop</category>
        
        <category>adobe lightroom</category>
        
        <category>adobe premiere pro</category>
        
        <category>premiere rush</category>
        
        <category>charactor animation</category>
        
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
        <category>Computer Vision</category>
        
      </item>
    
  </channel>
</rss>
