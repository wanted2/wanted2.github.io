<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AiFi</title>
    <description>An AI Engineer&apos;s blog (This is a staging site, so the content may be imprecise, refer to official AiFi)</description>
    <link>https://wanted2.github.io/</link>
    <atom:link href="https://wanted2.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 08 May 2022 18:19:41 +0900</pubDate>
    <lastBuildDate>Sun, 08 May 2022 18:19:41 +0900</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      <item>
        <title>Ăn gì ở Hà Nội: Luận về thói quen ăn uống của người Hà Nội</title>
        <description>&lt;p&gt;Hà Nội là điểm đến cho nhiều nhà kinh doanh cũng như du khách trong những năm gần đây.
Nếu chỉ là một chuyến du lịch ngắn ngày việc trải nghiệm ẩm thực Hà thành có thể rất đơn giản như đi theo lịch trình của tour ăn một vài bữa đơn giản (lại phở rồi nem cuốn Harumaki, xoay đi xoay lại có vài món mà hay đem ra quảng cáo cho nước ngoài xem).
Tuy nhiên, nếu là khách kinh doanh phải ghé lại lâu dài (nửa năm tới vài năm) thì việc tìm hiểu về nếp sống, bao gồm thói quen ăn uống, của dân địa phương là điều nên làm nhằm &lt;strong&gt;chủ động&lt;/strong&gt; trong ăn uống sinh hoạt.
Trong bài viết này tôi sẽ điểm qua vài bài báo khoa học có thống kê chuẩn chỉnh về thói quen ăn uống của người Hà Nội, trả lời những câu hỏi sau làm cơ sở để lựa chọn cách hòa nhập vào ẩm thực Hà thành cho du khách tứ phương.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Người Hà Nội đi chợ ở đâu? (Chợ truyền thống, cửa hàng mặt phố, siêu thị, hay đâu?)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Người Hà Nội thích ăn tại nhà (eat-in), ra quán (eat-out), hay đặt đồ ăn sẵn (ready-made meals) về?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cuối bài báo sẽ có thêm một danh sách mà tôi nghĩ nên ăn khi đến Hà Nội sống lâu dài nhằm trả lời câu hỏi &lt;strong&gt;Ăn gì nếu ra phố ở Hà Nội?&lt;/strong&gt;.
Nói chung đều có luận cứ khoa học hẳn hoi, nên hy vọng sẽ là bài tham khảo bổ ích cho các đồng chí (dân tứ xứ trong nước cũng như nước ngoài) đến định cư ở Hà Nội lâu dài.&lt;/p&gt;

&lt;h1 id=&quot;giới-thiệu&quot;&gt;Giới thiệu&lt;/h1&gt;
&lt;p&gt;Thực ra, nếu bạn chỉ ghé thăm vài ba ngày, theo tour hoặc tự đi theo sở thích, thì tôi đồng ý các bạn cứ Google ra mấy địa điểm quán ăn, các món nổi tiếng mà nhiều khách du lịch đến mà ăn.
Đặc biệt, nếu đi theo tour du lịch, thì cứ nó cho gì ăn nấy, không phải suy nghĩ gì cho mệt!
Nói chung sẽ bị động nhưng du lịch có vài ngày vài tuần thì quan trọng gì!&lt;/p&gt;

&lt;p&gt;Nhưng nếu mà bạn đến Hà Nội và dự định ở lại ít nhất nửa năm, thì tôi khuyên các bạn nên đọc bài viết này, do chính tôi viết.
Chứ ở Hà Nội 6 tháng, thậm chí vài năm, mà chỉ quay đi quay lại vài món có trong sách du lịch, thì cũng oái oăm.
&lt;strong&gt;Tất nhiên, là với du khách nước ngoài thì tùy lòng, có người thích tìm hiểu sâu và hòa nhập sâu vào văn hóa địa phương.&lt;/strong&gt;
&lt;strong&gt;Có người thậm chí phi xe máy vù vù ra chợ mua đồ về nấu như người Việt mà dân Việt còn phải trầm trồ.&lt;/strong&gt;
&lt;strong&gt;Nhưng cũng có người ở cả chục năm, cũng chỉ nhớ được mấy món cơ bản với thi thoảng có dịp cuối tuần đi ăn thử ẩm thực Hà Nội.&lt;/strong&gt;
Thì nói chung không ép, nói chung &lt;strong&gt;không cần phải nấu, chỉ cần bỏ công ăn cho biết món Hà Nội là được rồi&lt;/strong&gt;.
Nấu thì các anh cứ nấu mấy món quen miệng các anh là được rồi, còn thi thoảng ra quán ăn ủng hộ ẩm thực địa phương là OK rồi!&lt;/p&gt;

&lt;p&gt;Thì cũng gọi là “nói có sách, mách có chứng”, tôi cũng mạn phép trích dẫn khá nhiều bài báo để trả lời câu hỏi về thói quen ăn uống của người Hà Nội.
Những bài báo do chính người Việt viết cho máu.
Mục tiêu của bài viết thì như đã nói ở trên là để đưa ra 1 góc nhìn khoa học có luận cứ đã được kiểm chứng, về thói quen ăn uống của người Hà Nội trong giai đoạn gần đây (từ 2008 trở lại) để du khách ở lâu tại Hà Nội có thể nắm được để ước lượng xem mình có thể hòa nhập đến đâu.
Vì về cơ bản, thì cũng là du khách thôi nên không ép, có lòng tìm hiểu và hòa nhập một “tí” (nhưng không hòa tan) là OK!&lt;/p&gt;

&lt;h1 id=&quot;thói-quen-ăn-uống-ở-hà-nội&quot;&gt;Thói quen ăn uống ở Hà Nội&lt;/h1&gt;

&lt;h2 id=&quot;người-hà-nội-đi-chợ-ở-đâu&quot;&gt;Người Hà Nội đi chợ ở đâu?&lt;/h2&gt;
&lt;p&gt;Ngoài chợ truyền thống, siêu thị ra thì &lt;strong&gt;bán hàng rong (roving street vendors, &lt;a class=&quot;citation&quot; href=&quot;#jensen2007food&quot;&gt;[1]&lt;/a&gt;)&lt;/strong&gt; là một đặc trưng về những nơi mà người dân Việt Nam nói chung, cũng như Hà Nội nói riêng có thể đi chợ.
Những gánh hàng rong này chính là 1 nét đặc trưng của phố phường Hà Nội, nhưng do khó quản lý nên có khá nhiều chính sách ra đời trong những năm 1980s và 1990s nhằm cấm hoặc giảm bớt hàng rong đã được đưa ra.
Cộng thêm việc nở rộ của nhiều loại thực phẩm từ siêu thị, các chuỗi đại lý đã tác động lớn vào đối tượng này.
Một bài điều tra từ giai đoạn 2000-2008 &lt;a class=&quot;citation&quot; href=&quot;#jensen2007food&quot;&gt;[1]&lt;/a&gt; cho thấy, trung bình người Hà Nội đi chợ nhiều nhất ở chợ truyền thống (7.0 lần/tuần, tức là hầu như ngày nào cũng đi), sau đó là các quán hàng rong (3.4 lần/tuần).
Vào giai đoạn trước 2010s, thì tần suất đi siêu thị (supermarket) của người Hà Nội khá thấp (0.9 lần/tuần).&lt;/p&gt;

&lt;p&gt;Bẵng đi chục năm, tới năm 2020, có tiếp bài điều tra của &lt;a class=&quot;citation&quot; href=&quot;#tran2020shopping&quot;&gt;[2]&lt;/a&gt; về thói quen đi chợ của người Hà Nội.
Thì so với bài năm 2008, tất nhiên là 12 năm trôi qua rồi nên tình hình nó cũng khác, ví dụ như sự xuất hiện các chuỗi &lt;strong&gt;minimarket như VinMart&lt;/strong&gt; hay chợ con (chợ họp ở góc phố, lề đường chứ không vào địa điểm do cơ quan quản lý quy định), các hệ thống cửa hàng rau sạch, thịt sạch chuyên môn.
Sự xuất hiện của các cửa hàng rau sạch, thịt sạch hay VinMart được cho là xuất phát từ sự nâng cao về thu nhập, dẫn đến ý thức về &lt;strong&gt;an toàn thực phẩm&lt;/strong&gt; được nâng cao theo.
Bài này tôi thấy khá thú vị, vì đã đưa ra được những yếu tố quan trọng trong việc đưa ra quyết định mua sắm của người tiêu dùng.
Thứ nhất, với những đối tượng nhiều kinh nghiệm mua sắm, họ không ngại chợ truyền thống, chợ con, chợ cóc hay thậm chí hàng rong.
Họ tự tin về quyết định của mình!
Nghiên cứu cũng cho thấy tầm quan trọng của &lt;strong&gt;sự tin tưởng (trust)&lt;/strong&gt; và &lt;strong&gt;an toàn thực phẩm&lt;/strong&gt; khi mua sắm ở những địa điểm đi chợ như vây.
Tuy nhiên, với những thế hệ trẻ hơn (younger generation), họ ít kinh nghiệm hơn nên thường chọn siêu thị hoặc VinMart để đảm bảo trust cũng như an toàn thực phẩm.
Và một đặc điểm của người Hà Nội đi chợ nhiều kinh nghiệm là họ sẵn sàng mặc cả (khi đi chợ truyền thống):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Indeed, the ability to bargain for better price is confirmed as a skill developed by Vietnamese consumers (Wertheim-heck et al., 2015), but this skill seems linked to generation, since it is less present in the younger generations, who by default turn to supermarkets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kết luận lại thì về việc đi chợ, nếu không phải là dân địa phương, sống quen ở vùng đấy thì chuyện sử dụng các hệ thống siêu thị hay VinMart (nay là WinMart) là cách yên tâm hơn để đảm bảo về &lt;strong&gt;trust và an toàn thực phẩm&lt;/strong&gt;.
Còn nếu muốn tìm hiểu cho biết thì đi tham quan chợ truyền thống, chợ cóc, hay ăn hàng rong xem sao thì cũng được, thể hiện niềm yêu thích với văn hóa địa phương thắt chặt tình đoàn kết quốc tế, nhưng &lt;strong&gt;không cần thiết lắm&lt;/strong&gt;, đặc biệt với các thành phần tứ xứ, gọi chung là &lt;strong&gt;khách&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ăn-ngoài-hay-ăn-tại-nhà-hay-đặt-đồ-ăn-sẵn&quot;&gt;Ăn ngoài, hay ăn tại nhà, hay đặt đồ ăn sẵn?&lt;/h2&gt;
&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#nguyen2015factors&quot;&gt;[3, 4]&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ra-quán-ăn-gì&quot;&gt;Ra quán ăn gì?&lt;/h2&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;jensen2007food&quot;&gt;Jensen, R.W. and Peppard, D. 2007. Food-buying habits in Hanoi. &lt;i&gt;Sojourn: Journal of Social Issues in Southeast Asia&lt;/i&gt;. 22, 2 (2007), 230–254.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/jensen2007food/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tran2020shopping&quot;&gt;Tran, V.H. and Sirieix, L. 2020. Shopping and cross-shopping practices in Hanoi Vietnam: An emerging urban market context. &lt;i&gt;Journal of Retailing and Consumer Services&lt;/i&gt;. 56, (2020), 102178.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/tran2020shopping/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nguyen2015factors&quot;&gt;Nguyen, T.T.P., Zhu, D. and Le, N.P. 2015. Factors influencing waste separation intention of residential households in a developing country: Evidence from Hanoi, Vietnam. &lt;i&gt;Habitat International&lt;/i&gt;. 48, (2015), 169–176.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2015factors/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;liu2020evaluation&quot;&gt;Liu, C. and Nguyen, T.T. 2020. Evaluation of household food waste generation in hanoi and policy implications towards SDGs target 12.3. &lt;i&gt;Sustainability&lt;/i&gt;. 12, 16 (2020), 6565.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/liu2020evaluation/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Sun, 08 May 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/hanoi-eating-habits/</link>
        <guid isPermaLink="true">https://wanted2.github.io/hanoi-eating-habits/</guid>
        
        <category>Hà Nội</category>
        
        <category>Ẩm thực</category>
        
        <category>Thói quen hàng ngày</category>
        
        <category>Ăn uống</category>
        
        <category>Thói quen ăn uống</category>
        
        <category>Eating habits</category>
        
        <category>Eat-out</category>
        
        <category>Eat-in</category>
        
        <category>Ready-made meals</category>
        
        <category>Food</category>
        
        <category>Street foods</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
      </item>
    
      <item>
        <title>ハノイではリモートワークが本当に人気でしょうか？</title>
        <description>&lt;p&gt;ちらほら海外からお誘いがありました．
「どうかリモートワークをしましょうか」という誘いです．
なんかコロナ禍で海外では皆がよくリモートワークをしているかと思います．
おー，本当にベトナムでもリモートワークが人気だろうなと思っていたが，周りを見ると&lt;strong&gt;ハノイではそうではない&lt;/strong&gt;気もしました．
業界はそれぞれで，現場にいかないと仕事を行えない土木系の仕事なども多いという原因があります．
IT業界でも実際はコロナ禍は主要な原因でリモートワークをしているが，コロナ禍がなくなると，IT業界もみんな本気でオフィスに戻っている気もしています．
そういう疑問を持ちながら，解答を探しております．&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;コロナ禍の後にハノイではリモートワークが生きるか？&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;リモートワークを促進する要因は何か？支障する要因は何か？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;結論としては，ハノイではコロナ禍後リモートワークが基本的になくなるだろう．&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;コロナ禍後のリモートワーク&quot;&gt;コロナ禍後のリモートワーク&lt;/h1&gt;

&lt;p&gt;本日2022年4月27日，&lt;a href=&quot;https://ncov.vnanet.vn/tin-tuc/viet-nam-tam-dung-khai-bao-y-te-covid-19-voi-nguoi-nhap-canh-tu-0h-ngay-27-4/3c085198-7e9f-48a1-9e09-e97ad4ca3513&quot;&gt;ベトナム医療省が海外から入国者に対してコロナ検査など水際対策が停止する方針を発表しました．&lt;/a&gt;
また，2022年3月中には，3月から全面で観光や貿易などの活動を再開するよう方針を定められました．
今は，もうハノイではコロナ禍後という時代を考えたほうが良いでしょうか？&lt;/p&gt;

&lt;p&gt;コロナ禍を理由として，リモートワークを採用した会社は少なくないと思います．
特にIT業界では，去年10月ごろでほぼリモートワークだったそうです．
しかし，コロナ禍後なので，もうリモートワークをする背景はないでしょう．
もうこれから，ハノイでは，さすがに毎日リモートワークをするのがあまりないだろう．
全市には完全に否定できないが，どこかにリモートワークをしている方がいるとしても，まわりから見ると，ちょっと変な人だと思われるかもしれません．&lt;/p&gt;

&lt;h1 id=&quot;ハノイでのリモートワーク&quot;&gt;ハノイでのリモートワーク&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;The first concept of working far from a workplace (i.e., telecommuting) was introduced in the USA in the 1970s to handle transport-related issues such as traffic congestion and air pollution by reducing commuting between home and the workplace.&lt;a class=&quot;citation&quot; href=&quot;#nguyen2021perception&quot;&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;テレワークはなんとアメリカで1970年代から人気になったそうです．
その背景は，通勤時間を短縮したくて，空気汚染の影響も削減したいという動機がありました．
コロナ禍がなかった時代で，この2つはリモートワークの主な動機でした．&lt;/p&gt;

&lt;p&gt;ハノイではね，空気もちょっとあまり良くないし，渋滞も多いです．
しかし，それでも，コロナ禍前に，皆がきちんと出社していました．
ほぼの会社は勤怠管理システムを導入しています．
指紋認証や顔認証などで勤怠登録する場合もありました．&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Regarding the perception of HBT, while the fear of COVID-19 was a strong positive factor, difficulties in focusing on work and accessing data were negative factors. &lt;a class=&quot;citation&quot; href=&quot;#nguyen2021factors&quot;&gt;[2]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;そうですね．ハノイでは，なぜコロナ禍でリモートワークをしているかというと，コロナ禍は怖いからだけの理由です．
それ以外，リモートワークを促進する強い理由はないだろう．
主な支障は，&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;仕事に集中できない&lt;/strong&gt;．うん，まあ，家にいるから家族もいるし，集中しにくいですね．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;仕事に必要なデータと機器を入手できない&lt;/strong&gt;．そうですね．アクセス制限もあるので，多くの場合，出社しないといけないですね．&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;やはり，仕事を考えると，コロナ禍がなくなると出社しましょうよ！&lt;/p&gt;

&lt;p&gt;他にも，コロナ禍中で子供の通学の動機についての研究とオンラインショッピング活動におけるコロナ禍の影響についての研究&lt;a class=&quot;citation&quot; href=&quot;#nguyen2021factorsaffecting&quot;&gt;[3, 4]&lt;/a&gt;もあります．&lt;/p&gt;

&lt;h1 id=&quot;結論&quot;&gt;結論&lt;/h1&gt;

&lt;p&gt;もうベトナム政府が全面で活動を再開する方針も出したし，コロナ禍はなくなったため，ハノイにいる場合，出社することは避けられないと思いますね．
ハノイなら，出社しましょう！&lt;/p&gt;

&lt;h1 id=&quot;参考文献&quot;&gt;参考文献&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;nguyen2021perception&quot;&gt;Nguyen, M.H. and Armoogum, J. 2021. Perception and preference for home-based telework in the covid-19 era: A gender-based analysis in Hanoi, Vietnam. &lt;i&gt;Sustainability&lt;/i&gt;. 13, 6 (2021), 3179.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021perception/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nguyen2021factors&quot;&gt;Nguyen, M.H. 2021. Factors influencing home-based telework in Hanoi (Vietnam) during and after the COVID-19 era. &lt;i&gt;Transportation&lt;/i&gt;. 48, 6 (2021), 3207–3238.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021factors/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nguyen2021factorsaffecting&quot;&gt;Nguyen, M.H., Armoogum, J. and Nguyen Thi, B. 2021. Factors affecting the growth of e-shopping over the covid-19 era in hanoi, vietnam. &lt;i&gt;Sustainability&lt;/i&gt;. 13, 16 (2021), 9205.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021factorsaffecting/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nguyen2021impact&quot;&gt;Nguyen, M.H., Pojani, D., Nguyen, T.C. and Ha, T.T. 2021. The impact of Covid-19 on children’s active travel to school in Vietnam. &lt;i&gt;Journal of Transport Geography&lt;/i&gt;. 96, (2021), 103191.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021impact/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Wed, 27 Apr 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/hanoi-remote-work/</link>
        <guid isPermaLink="true">https://wanted2.github.io/hanoi-remote-work/</guid>
        
        <category>勤務条件</category>
        
        <category>ハノイ</category>
        
        <category>リモートワーク</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>Edge-Cloud architectures &amp; TPU resources</title>
        <description>&lt;p&gt;&lt;strong&gt;Edge-Cloud (EC) architectures&lt;/strong&gt; is an emerging computing paradigm recently, which takes the AI computation to the edge devices and only aggregates processed important data to the cloud.
&lt;strong&gt;Tensor Processing Unit (TPU)&lt;/strong&gt; is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google’s own TensorFlow software.
Google started selling the Cloud TPUv3 in 2018 for third-party users.
Their edge TPU version (Coral)&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; was started in the same year.
Both cloud and edge TPU support Tensorflow and its Lite version but do not support other deep learning frameworks like PyTorch or MXNet.
TPU is useful for EC architectures.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;edge-computing&quot;&gt;Edge computing&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Edge computing&lt;/strong&gt; is a new paradigm of distributed computing on resource-constrained IoT devices, such as sensors and actuators, which has become ubiquitous recently &lt;a class=&quot;citation&quot; href=&quot;#murshed2021machine&quot;&gt;[1, 2]&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;More than 25 billion IoT devices all over the world have been used by 2020.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One crucial need is to aggregate the information from these devices and make decisions from the data.
Thus, a machine learning (ML) system is used to aggregate such data.
But the IoT devices have only low resources, which makes almost all on-device ML systems is impossible.
A typical solution for this problem is to offload them into an external data processing system like cloud computing.
It worsens latency, increases communication costs, and introduces additional privacy concerns.&lt;/p&gt;

&lt;div style=&quot;width: 50%; float: right; margin: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/images/edge-server.png&quot; /&gt;
&lt;p&gt;&lt;i&gt;Edge servers. (Source: Murshed et al., 2021)&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The solution requires allocation of a new &lt;strong&gt;edge server&lt;/strong&gt; near the IoT devices, which aggregates local information and makes quick decisions at the local edge (&lt;strong&gt;distributed inference&lt;/strong&gt;).
Only processed and meaningful information, which is advantageous to the global model, is sent to cloud computing servers to update global model parameters (&lt;strong&gt;distributed training&lt;/strong&gt;).
This decentralized architecture has many advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Reducing cost&lt;/em&gt;: The volume of data needed to be transferred to a central computing location is reduced because some of it is processed by
edge-devices.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Improving latency&lt;/em&gt;: The physical proximity of edge-devices to the data sources makes it possible to achieve lower latency which improves real-time
data processing performance.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Better privacy&lt;/em&gt;: For the cases where data must be processed remotely, edge devices can be used to discard personally identifiable information
(PII) prior to data transfer, thus enhancing user privacy and security.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Better failure handling&lt;/em&gt;: Decentralization can make systems more robust by providing transient services during network failures or cyberattacks.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;But everything comes with a cost.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And edge computing is no exception!
While the applications range from real-time video analytics, image recognition, automatic speech recognition, user privacy, fraud detection, creating new datasets, autonomous vehicles, smart home/cities, human safety to augmented reality, the power to run has many problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;How to adapt existing ML algorithms to edge devices?&lt;/strong&gt;: For this problem, we need to consider more distributed-computing specific algorithms, care about hardware constraints, and build lighter/faster model architectures (which also requires pruning techniques and compression).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How to improve latency of distributed training and inference systems while preserving privacy?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What are the common hardware devices used to enable edge intelligence?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the nature of the emerging software ecosystem that supports this new end-edge-cloud architecture for real-time intelligent systems?&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the third question, we will consider a choice in the art: the &lt;strong&gt;Tensor Processing Unit (TPU)&lt;/strong&gt; along with other options like &lt;strong&gt;Central Processing Unit (CPU)&lt;/strong&gt; and &lt;strong&gt;Graphical Processing Unit (GPU)&lt;/strong&gt;.
It has both cloud and edge versions that make it possible for new end-edge-cloud architecture.&lt;/p&gt;

&lt;h1 id=&quot;cpu-gpu-and-tpu&quot;&gt;CPU, GPU, and TPU&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321751_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Central Processing Unit (CPU)&lt;/strong&gt; is the main processing unit in a computer.
It takes inputs, referring to data in memory units, and produces outputs in every task.
It takes control of the computation and does arithmetic/logic computations.&lt;/p&gt;

&lt;div style=&quot;width: 100%; margin: 0.5em;&quot;&gt;
&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321849_11.gif&quot; /&gt;
&lt;p&gt;&lt;i&gt;How CPU works (Source: Tinhte.vn)&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;CPU in the von-Neumann architectures stores computation results in L1 cache or register due to the fact that the next computation is unknown (thus, the current results must be stored somewhere for future computing).
However, such L1 cache and registers are limited in their memory capacity, and sometimes, cache misses happen, which we call as &lt;em&gt;von-Neumann bottleneck&lt;/em&gt;.
Because there are many matrix computations (multiplication and addition) in deep learning, bottleneck problems arise more and more.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321867_main-qimg-ee7d4757d5c7cdd1105f9e4aa267db22.png&quot; style=&quot;width: 50%; margin: 0.5em; float: left;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Graphical Processing Unit (GPU)&lt;/strong&gt; can compute more than one arithmetic logic at a time, then it has higher parallelism than CPU (which is a sequential processing unit).
While a CPU only has a few &lt;em&gt;Arithmetic Logic Unit (ALU)&lt;/em&gt;, a GPU can have some thousands of ALUs, making GPU parallel computing is fast, especially for &lt;strong&gt;arithmetic computations&lt;/strong&gt;.
GPU maybe is not good for non-arithmetic computations such as Office tasks, but in the case of deep learning, it is a dominant choice by now.&lt;/p&gt;

&lt;p&gt;However, although GPU is faster for deep learning, it does not solve the &lt;em&gt;bottleneck&lt;/em&gt; problem!
That’s why Google had to design TPU!&lt;/p&gt;

&lt;div style=&quot;width: 100%; margin: 0.5em;&quot;&gt;
&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321851_22.gif&quot; /&gt;
&lt;p&gt;&lt;i&gt;How GPU works (Source: Tinhte.vn)&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Tensor Processing Unit (TPU)&lt;/strong&gt; is the neural network hardware designed by Google.
It means it is good ONLY for neural network computing, not for all.
It cannot do Office tasks, control rockets, or monitor bank transactions.
&lt;strong&gt;It can only do arithmetic computations!&lt;/strong&gt;&lt;/p&gt;

&lt;div style=&quot;width: 100%; margin: 0.5em;&quot;&gt;
&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321852_33.gif&quot; /&gt;
&lt;p&gt;&lt;i&gt;How TPU works (Source: Tinhte.vn)&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In essence, TPU only access memory once.
Then for each multiplication, the result is passed to the next multiplication when performing the additions at the same time.
Therefore, no more memory access is needed.
This is how TPU solved the von-Neumann bottleneck!&lt;/p&gt;
&lt;div style=&quot;width: 50%; margin: 0.5em;float: right;&quot;&gt;
&lt;img src=&quot;/assets/images/tpu-gpu-comparison.png&quot; /&gt;
&lt;p&gt;&lt;i&gt;Source: Wang et al., 2019&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In a recent benchmark &lt;a class=&quot;citation&quot; href=&quot;#wang2019benchmarking&quot;&gt;[3]&lt;/a&gt;, TPU showed that it is a better choice than GPU for large CNNs, suggesting that TPU is highly-optimized for CNNs.
While TPU is a better choice for Recurrent Neural Networks (RNNs), it is not as flexible as GPU for embedding computations.
The smallest gain perhaps is when TPU does fully-connected computations (FC).&lt;/p&gt;

&lt;h1 id=&quot;pricing&quot;&gt;Pricing&lt;/h1&gt;

&lt;p&gt;Now, we already understand that TPU is fast and has been designed for neural nets.
We may want to add these resources to our project, but the question here is &lt;strong&gt;how does it cost?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Google somewhat offers two different choices for computing: a cloud version&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and an edge version&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;cloud-tpu&quot;&gt;Cloud TPU&lt;/h2&gt;

&lt;p&gt;You have two choices: you can rent a single TPU or a cluster of TPUs (TPU Pod)&lt;sup id=&quot;fnref:3:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.
Unfortunately, TPU is only available in three zones:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Iowa (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-central1&lt;/code&gt;);&lt;/li&gt;
  &lt;li&gt;Netherlands (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;europe-west4&lt;/code&gt;); and&lt;/li&gt;
  &lt;li&gt;Taiwan (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asia-east1&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And Taiwan zone does not have Pods, while the Iowa zone does not have TPUv3 Pods!&lt;/p&gt;

&lt;p&gt;For a single GPU device with 8 TPUv3 cores, you must pay 8USD/hour for on-demand options or 2.4USD/hour for spot options.
Pods with 32 TPUv3 cores require a 1-year or 3-year commitment with an &lt;strong&gt;Evaluation price&lt;/strong&gt; is 32USD/hour.
If you afford a 3-year plan, then the monthly pricing is &lt;strong&gt;10,512USD/month&lt;/strong&gt;.
The total price for annual options in the Netherlands is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pod-pricing.png&quot; alt=&quot;pods&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;edge-tpu&quot;&gt;Edge TPU&lt;/h2&gt;

&lt;p&gt;Google also offered an Edge TPU version with up to 4 TOPS.
Edge TPU has been a new device for edge recently &lt;a class=&quot;citation&quot; href=&quot;#murshed2021machine&quot;&gt;[1, 4]&lt;/a&gt;.
If you already knew NVIDIA Jetson Nano&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, then this is the same version for TPU.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/4AseEI_s6QWM75QwncKhPgMPi0P7YHume5O7Njx8FovMze22nblcLP8zOU-s5qhpwMgzVmYe9VPLiwrHecNYbjtNCKgHLOSO_w9X1NkEzEDjxdWRM7g=w2000-rw&quot; alt=&quot;edge tpu&quot; /&gt;
&lt;em&gt;Source: &lt;a href=&quot;https://coral.ai/products/&quot;&gt;Google Coral&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In Vietnam, I can buy a 1GB RAM DevBoard with a budget that approximates five million bucks (tax included)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.
Google Coral announced the price is 129.99USD/board. 
It is equipped with 4TOPS and only favors Tensorflow Lite models.&lt;/p&gt;

&lt;h1 id=&quot;conslusion&quot;&gt;Conslusion&lt;/h1&gt;

&lt;p&gt;Edge-Cloud architectures will be the next generation of computing platforms.
TPUs are good for neural network computations and then are good for ML as well.
We need to allocate the budget for our project first, and it is good to know both cloud and edge versions of TPU.
They differ about 1100x in terms of annual cost (Edge TPU 129.99USD/lifetime vs. 132,000USD?year Cloud Pod TPUv2)!&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;murshed2021machine&quot;&gt;Murshed, M.G.S., Murphy, C., Hou, D., Khan, N., Ananthanarayanan, G. and Hussain, F. 2021. Machine learning at the network edge: A survey. &lt;i&gt;ACM Computing Surveys (CSUR)&lt;/i&gt;. 54, 8 (2021), 1–37.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/murshed2021machine/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;shi2020communication&quot;&gt;Shi, Y., Yang, K., Jiang, T., Zhang, J. and Letaief, K.B. 2020. Communication-efficient edge AI: Algorithms and systems. &lt;i&gt;IEEE Communications Surveys &amp;amp; Tutorials&lt;/i&gt;. 22, 4 (2020), 2167–2191.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/shi2020communication/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wang2019benchmarking&quot;&gt;Wang, Y.E., Wei, G.-Y. and Brooks, D. 2019. Benchmarking tpu, gpu, and cpu platforms for deep learning. &lt;i&gt;arXiv preprint arXiv:1907.10701&lt;/i&gt;. (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/wang2019benchmarking/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cass2019taking&quot;&gt;Cass, S. 2019. Taking AI to the edge: Google’s TPU now comes in a maker-friendly package. &lt;i&gt;IEEE Spectrum&lt;/i&gt;. 56, 5 (2019), 16–17.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/cass2019taking/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://cloud.google.com/edge-tpu/&quot;&gt;https://cloud.google.com/edge-tpu/&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://cloud.google.com/tpu/pricing&quot;&gt;https://cloud.google.com/tpu/pricing&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://coral.ai/products/&quot;&gt;https://coral.ai/products/&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/embedded/jetson-nano-developer-kit&quot;&gt;NVIDIA Jetson Nano&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://pivietnam.com.vn/coral-dev-board-edge-tpu-module-som-pivietnam-com-vn.html&quot;&gt;Coral DevBoard Edge TPU SoM at PiVietnam&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 05 Mar 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/edge-cloud-tpu/</link>
        <guid isPermaLink="true">https://wanted2.github.io/edge-cloud-tpu/</guid>
        
        <category>Machine learning</category>
        
        <category>TPU</category>
        
        <category>edge computing</category>
        
        <category>edge server</category>
        
        <category>edge devices</category>
        
        <category>edge-cloud architectures</category>
        
        <category>cloud computing</category>
        
        <category>6g</category>
        
        <category>federated learning</category>
        
        <category>distributed computing</category>
        
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices</title>
        <description>&lt;p&gt;Trong các bài viết trước, chúng ta đã xem xét kha khá về &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cho NLP/Vision-Language&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; và âm thanh&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.
Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự án cần &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;vision-transformers&quot;&gt;Vision Transformers&lt;/h1&gt;

&lt;h2 id=&quot;vit&quot;&gt;ViT&lt;/h2&gt;

&lt;p&gt;Về nguyên lý chung của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; thì chúng ta có hai bài viết trước nói khá nhiều rồi&lt;sup id=&quot;fnref:4:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:5:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; nên các bạn tham khảo nhé.
Nhà cũng đang có cái “eo hẹp” là chỉ được tối đa 7 citations và 9 phút đọc thôi nên các bạn cần thì đọc lại hai bài viết&lt;sup id=&quot;fnref:4:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:5:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; để xem thêm kiến thức về attention, Transformer, …
Vision Transformer (ViT, &lt;a class=&quot;citation&quot; href=&quot;#dosovitskiy2020image&quot;&gt;[1]&lt;/a&gt;) đưa khái niệm &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; vào vision.
Nếu các bạn đã quen &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; thì hiểu ngay là ta cần chuyển hình ảnh thành 1 chuỗi:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/ATsWtUsuuFRfq8OhZ7/source.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Source: Google&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Để làm được việc này, các tác giả đề xuất:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Image patches&lt;/strong&gt;: Hình ảnh được chia ra thành patches và đánh số thự tự để input vào transformer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Position embedding&lt;/strong&gt;: thứ tự chỉ đơn giản là chuỗi 1D.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MLP với GELU activation&lt;/strong&gt;: MLP sử dụng GELU để kích hoạt. Model không chứa CNN, có tối thiểu 12 tầng, với kích cỡ hidden size từ 768, ngoài ra có 12-16 heads.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Large-scale pre-training and fine-tuning&lt;/strong&gt;: Một điểm đáng chú ý khác là pre-training dạng supervised trên một dataset 300 triệu ảnh tạo ra 1 model rất mạnh.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Các tác giả báo cáo cải tiến trên khá nhiều bộ dữ liệu lớn, và ngoài ra cả con số &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TPUv3-core-days = số lượng TPUv3 cores x sô lượng ngày train&lt;/code&gt;.
Cái con số &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TPUv3-core-days&lt;/code&gt; thì cứ mỗi thí nghiệm là vài ngàn tới vài chục ngàn, mà mỗi core thì cứ 10$/ngày thì các bạn cứ nhẩm tính xem budget của hội &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;con nhà giàu&lt;/code&gt; này đầu tư vào nó lớn cỡ nào đấy.&lt;/p&gt;

&lt;h2 id=&quot;detr&quot;&gt;DETR&lt;/h2&gt;

&lt;p&gt;DETR &lt;a class=&quot;citation&quot; href=&quot;#carion2020end&quot;&gt;[2, 3]&lt;/a&gt; tiếp tục ứng dụng transformer vào Object Detection và Segmentation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/detr.png&quot; alt=&quot;detr&quot; /&gt;
&lt;em&gt;Source: &lt;a class=&quot;citation&quot; href=&quot;#carion2020end&quot;&gt;[2]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Backbone&lt;/strong&gt;: DETR dùng CNN quen thuộc như resnet-50 hoặc 101.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: DETR dùng \(1\times 1\) convolution để dimention reduction các feature maps rồi input vào. Position embedding là cố định vì transformer là không phụ thuộc vào permutation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: thay vì decode từng object query, thì DETR decode song song cùng lúc tất cả các queries.&lt;/li&gt;
  &lt;li&gt;Sau khi decode thì dùng FFN để predict vị trí và class. Để tính hàm loss thì dùng thuật toán Hungarian để matching.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Về mặt giá cả thì DETR tốn 300 epochs để hội tụ trên COCO, nên về sau gần đây có khá nhiều nghiên cứu để giảm giá thành hộ tụ (chỉ cần 50 epochs thôi chả hạn).&lt;/p&gt;

&lt;h2 id=&quot;yolox&quot;&gt;YOLOX&lt;/h2&gt;

&lt;p&gt;So với phiên bản cũ &lt;a class=&quot;citation&quot; href=&quot;#redmon2018yolov3&quot;&gt;[4]&lt;/a&gt; thì YOLOX &lt;a class=&quot;citation&quot; href=&quot;#ge2021yolox&quot;&gt;[5]&lt;/a&gt; ứng dụng khá nhiều kỹ thuật mới như decoupled heads, strong augmentation (Moáic và Mixup), anchor-free, pulti-positives, và SimOTA.
Backbone thì ngoài Darknet ra cũng dùng thêm những backbone nhỏ hơn như Tiny.&lt;/p&gt;

&lt;p&gt;Dưới đây là kết quả inference của model YOLOX-Tiny:&lt;/p&gt;
&lt;iframe width=&quot;100%&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/_5inpa6ruUY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;xây-dựng-tài-nguyên-gpu&quot;&gt;Xây dựng tài nguyên GPU&lt;/h1&gt;

&lt;h2 id=&quot;giá-thành-khi-train-model-state-of-the-art-sota&quot;&gt;Giá thành khi train model State-of-the-art (SOTA)&lt;/h2&gt;

&lt;p&gt;Nhìn chung là nếu chỉ hình ảnh với bộ dữ liệu nhỏ nhỏ như COCO &lt;a class=&quot;citation&quot; href=&quot;#lin2014microsoft&quot;&gt;[6]&lt;/a&gt; tầm trăm ngàn ảnh thì có bảng giá dưới đây: chúng ta lấy ví dụ từ báo cáo của 1 state-of-the-art thì họ dùng 8 cái V100, train tầm 6 ngày liên tục (\(6\times 24\) giờ) thì tổng tiền cho một lượt trên tầm ngàn Mỹ kim cho 6 ngày, 1 tháng cứ tầm 5 ngàn Mỹ kim.
Mà các bạn cũng nhớ giá này là giá &lt;a href=&quot;https://aws.amazon.com/ec2/spot/pricing/&quot;&gt;&lt;strong&gt;Spot&lt;/strong&gt;&lt;/a&gt; tức là có thể bị interrupt giữa chừng nên mới rẻ thế.
Chứ nếu bạn mà chọn &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand/&quot;&gt;&lt;strong&gt;on-demand&lt;/strong&gt;&lt;/a&gt; thì có mà gấp 10 lần.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/detr-cost-analysis.jpg&quot; alt=&quot;pricing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nhưng ở trên mới chỉ là giá bộ COCO có hơn 100k ảnh nhé.
Bộ Open Images &lt;a class=&quot;citation&quot; href=&quot;#kuznetsova2020open&quot;&gt;[7]&lt;/a&gt; với 1.7 triệu ảnh thì còn máu nữa.
Search trên Kaggle mà có đồng chí chịu khó bỏ tiền ra ngồi train và báo cáo kết quả cho anh biết (xin cám ơn đồng chí): &lt;a href=&quot;https://www.kaggle.com/c/open-images-2019-object-detection/discussion/110953&quot;&gt;Kaggle Open Images 2019 challenge 6th place solution&lt;/a&gt;.
Thì kết quả là đồng chí ấy báo cáo:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;train 8 models trên V100 (chắc lại EC2 P3 thôi thì mình cứ dùng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3.xlarge&lt;/code&gt; để làm phân tích giá nhé) rồi ensemble.&lt;/li&gt;
  &lt;li&gt;mỗi model train mất 18-36 ngày (tùy model). Thì đồng chí này train 8 GPUs khác nhau.&lt;/li&gt;
  &lt;li&gt;sau khi train xong các model thì mất thêm 1 ngày nữa để inference và 1 ngày nữa để ensemble (dùng NMS).&lt;/li&gt;
  &lt;li&gt;Vậy tổng thể đã tiêu tốn \(36\times 8+1\times 8+1=297\) ngày train, tức là \(297\times 24=7128\) giờ train.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/open-images-price.jpg&quot; alt=&quot;pricing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3.xlarge&lt;/code&gt; thì giá mềm nhất là &lt;a href=&quot;https://aws.amazon.com/ec2/spot/pricing/&quot;&gt;&lt;strong&gt;Spot&lt;/strong&gt;&lt;/a&gt; cũng tầm $0.918/h.&lt;/p&gt;

&lt;p&gt;Tức là để train được accuracy tầm 60% đã mất \(7128\times 0.918\) tức là tầm 6543 Mỹ kim và hơn tháng ngồi monitor màn hình train.&lt;/p&gt;

&lt;h2 id=&quot;xây-dựng-hệ-thống-816-gpu&quot;&gt;Xây dựng hệ thống 8~16 GPU&lt;/h2&gt;

&lt;p&gt;Nhìn chung thì theo dòng lịch sử có 3 loại NVIDIA GPU dành cho cloud khá thông dụng như sau (tôi không nói tới hai dòng GTX và RTX nhé):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NVIDIA V100 hay &lt;strong&gt;Volta&lt;/strong&gt;: nói đến dòng này chúng ta có những sự lựa chọn chủ yếu liên quan tới V100 Tensor Core mà đại diện cho thuê là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3.16xlarge&lt;/code&gt; và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3dn.24xlarge&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Với băng thông mạng của phiên bản P3.16xlarge cao hơn tới 4 lần, phiên bản P3dn.24xlarge của Amazon EC2 là sự bổ sung mới nhất cho dòng phiên bản P3, được tối ưu hóa cho machine learning phân tán và các ứng dụng HPC. Các phiên bản này cung cấp thông lượng kết nối mạng lên tới 100 Gbps, 96 vCPU Intel® Xeon® Có thể mở rộng (Skylake) tùy chỉnh, 8 GPU NVIDIA® V100 Tensor Core với 32 GB bộ nhớ mỗi GPU và 1,8 TB ổ lưu trữ SSD cục bộ chuẩn NVMe. Các phiên bản P3dn.24xlarge cũng hỗ trợ Elastic Fabric Adapter (EFA). Giao diện này tăng tốc các ứng dụng machine learning phân tán sử dụng Thư viện giao tiếp chung NVIDIA (NCCL). EFA có thể mở rộng quy mô lên đến hàng nghìn GPU, cải thiện đáng kể thông lượng và khả năng mở rộng của các mô hình huấn luyện deep learning, từ đó cho kết quả nhanh hơn.
Source: Amazon Web Service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;NVIDIA T4 hay &lt;strong&gt;Turing&lt;/strong&gt;: với AWS EC2 thì bạn có thể thuê &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g4dn.metal&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;NVIDIA A100 hay &lt;strong&gt;Ampere&lt;/strong&gt;: Với AWS EC2 thì có thể thuê &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p4d.24xlarge&lt;/code&gt;, với Azure HPC thì có thể thuê &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Standard_ND96amsr_A100_v4&lt;/code&gt;. GCP thì có &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a2-highgpu-8g&lt;/code&gt; hoặc bản 16 GPU là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a2-highgpu-16g&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thì về mặt spec Ampere là khỏe nhất nếu nói về TFLOPS.
Dưới đây là bảng giá thành của NVIDIA 8x A100 Tensor Core.
Trong bảng này có 2 cột mà các bạn nên để ý là giá thành thuê theo giờ (&lt;strong&gt;Hourly cost&lt;/strong&gt;) và tỷ lệ GFLOPS/USD (đáng giá thế nào).
Giả định chung là hệ thống được xây dựng tối thiểu 4x GPU và được dùng ít nhất 24 tháng, mỗi tháng dùng 22 ngày (T7/CN nghỉ ngơi).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ampere.png&quot; alt=&quot;ampere&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nói chung tự build thì các bạn có thể tham khảo cấu hình của DGX-1&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, DGX-2&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, và DGX-A100&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; để mua các bộ phận về tự ráp thì sẽ tiết kiệm công lắp ráp, nhưng nhìn chung tôi nghĩ cũng phải 50 ngàn Mỹ Kim.&lt;/p&gt;

&lt;h2 id=&quot;các-cloud-solutions&quot;&gt;Các cloud solutions&lt;/h2&gt;
&lt;p&gt;Trong trường hợp bạn có bài toán train dữ liệu mà mất hàng tháng trời train với GTX/RTX thì bạn sẽ nghĩ phải thuê GPUs trên data center (8x-16x GPU).
Thì ngoài AWS/Azure/GCP là khá cùng rank nên bảng giá không chênh lệch nhau mấy, bạn có thể tham khảo thêm các trang cho thuê GPU bên ngoài để tìm được chỗ thuê hợp lý hơn.
Như kết quả tìm kiếm của AIFI thì hiện tại có trang &lt;a href=&quot;https://vast.ai&quot;&gt;vast.ai&lt;/a&gt; cung cấp khá nhiều sự lựa chọn cho thuê ở mức giá thấp hơn 5 USD/hour.&lt;/p&gt;

&lt;h2 id=&quot;còn-lời-giải-nào-khác&quot;&gt;Còn lời giải nào khác?&lt;/h2&gt;

&lt;p&gt;Nhìn chung tự build thì có hai khả năng:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mua đồ sẵn&lt;/strong&gt; như DGX&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:3:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; thì các bạn cứ chuẩn bị 100k Mỹ kim trở lên.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mua bộ phận về tự ráp&lt;/strong&gt; thì các bác tham khảo cấu hình của DGX rồi độ lại tùy theo nhu cầu. Tuy nhiên, chắc chỉ giảm được tiền công, và tối ưu một chút kiểu DGX dùng nhiều RAM thì mình giảm RAM xuống. Nói chung chắc cũng phải 50K Mỹ Kim.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Về cá nhân, tôi thiên về thuê!
Nếu tự build thì mua mấy cái RTX/GTX dòng Ti là ổn rồi.
Tuy nhiên nếu bài toán lớn thì bạn bắt buộc phải dùng data center GPU thì lúc ấy phải có &lt;strong&gt;TIỀN&lt;/strong&gt;!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Vấn đề của đi thuê là tiền tính theo giờ nên bạn cần phải ước lượng được số giờ sử dụng.&lt;/strong&gt; Nếu tầm trên 200h/tháng, tôi nghĩ nên thuê theo năm hoặc 3 năm.
Spot price thì cũng tàm tạm thôi, vì mất công chờ với nó ngắt điện (interupt) mình cũng phải chịu ấy, nên là rẻ nhưng lại mất thời gian chờ và bị ngắt.
Mà vấn đề với Spot là &lt;strong&gt;nó không có luôn ấy (phải chờ đến khi cái server ấy nó open mình mới được dùng)&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hơi buồn nhưng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;đầu tiên vẫn là ... tiền đâu&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Như vậy chúng ta đã điểm qua một số SOTAs của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cho Vision và nhìn chung các &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; vẫn đang nắm vị trí số 1.
Tuy vậy, vấn đề lớn khi “đua đòi” vào mảng này thì vẫn là tài nguyên thôi.
Nếu chuẩn bị được budget và plan nghiên cứu nghiêm chỉnh (mà đầu tiên là tiền đâu) thì về mặt nghiệp vụ PM tôi nghĩ không nên triển khai làm gì mất time anh em.
Ít nhất là cần vốn 200k Mỹ Kim thì cũng phải có tầm 100k trong túi hãy nghĩ!&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;dosovitskiy2020image&quot;&gt;Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. &lt;i&gt;arXiv preprint arXiv:2010.11929&lt;/i&gt;. (2020).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dosovitskiy2020image/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;carion2020end&quot;&gt;Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. &lt;i&gt;European conference on computer vision&lt;/i&gt; (2020), 213–229.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/carion2020end/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gao2021fast&quot;&gt;Gao, P., Zheng, M., Wang, X., Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. &lt;i&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/i&gt; (2021), 3621–3630.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/gao2021fast/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;redmon2018yolov3&quot;&gt;Redmon, J. and Farhadi, A. 2018. Yolov3: An incremental improvement. &lt;i&gt;arXiv preprint arXiv:1804.02767&lt;/i&gt;. (2018).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/redmon2018yolov3/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ge2021yolox&quot;&gt;Ge, Z., Liu, S., Wang, F., Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. &lt;i&gt;arXiv preprint arXiv:2107.08430&lt;/i&gt;. (2021).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ge2021yolox/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lin2014microsoft&quot;&gt;Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L. 2014. Microsoft coco: Common objects in context. &lt;i&gt;European conference on computer vision&lt;/i&gt; (2014), 740–755.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lin2014microsoft/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kuznetsova2020open&quot;&gt;Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T. and Ferrari, V. 2020. The open images dataset v4. &lt;i&gt;International Journal of Computer Vision&lt;/i&gt;. 128, 7 (2020), 1956–1981.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kuznetsova2020open/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://wanted2.github.io/seq2seq/&quot;&gt;https://wanted2.github.io/seq2seq/&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:4:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:4:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://wanted2.github.io/speech/&quot;&gt;https://wanted2.github.io/speech/&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:5:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:5:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-1/&quot;&gt;https://www.nvidia.com/en-us/data-center/dgx-1/&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-2/&quot;&gt;https://www.nvidia.com/en-us/data-center/dgx-2/&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-a100/&quot;&gt;https://www.nvidia.com/en-us/data-center/dgx-a100/&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 12 Feb 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/seq2seq-cv/</link>
        <guid isPermaLink="true">https://wanted2.github.io/seq2seq-cv/</guid>
        
        <category>Object Detection</category>
        
        <category>Object Recognition</category>
        
        <category>Image Segmentation</category>
        
        <category>Temporal Segmentation</category>
        
        <category>Sequence-to-sequence</category>
        
        <category>seq2seq</category>
        
        <category>Transformer</category>
        
        <category>Self-supervised learning</category>
        
        <category>BERT</category>
        
        <category>Vision Transformer</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Artificial Intelligence</category>
        
        <category>Computer Vision</category>
        
      </item>
    
      <item>
        <title>Machine Learning for Network Intrusion Detection: From Local to Production</title>
        <description>&lt;h1 id=&quot;network-intrusion-detection-system&quot;&gt;Network Intrusion Detection System&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Simon-Enoch/publication/319637372/figure/fig1/AS:543679434510336@1506634686111/Configuration-of-the-enterprise-network.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Network intrusion detection system (NIDS)&lt;/strong&gt; is an independent platform that examines network traffic patterns to identify intrusions for an entire network. It needs to be placed at a choke point where all traffic traverses. A good location for this is in the DMZ.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An IDS is &lt;em&gt;reactive&lt;/em&gt; in nature: it only monitors and sends alerts to a group of specific people like administrators.
The above figure shows a common NIDS architecture, where a DMZ is placed in between &lt;em&gt;external firewall&lt;/em&gt; and &lt;em&gt;internal firewall (to an internal network)&lt;/em&gt;.
Here, in the DMZ, a NIDS can be set up to monitor the traffic of the whole corporation and identify the anomalies.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Vivek-Singh-70/publication/328572055/figure/fig1/AS:686824250945536@1540763070945/Major-Components-of-Snort-IDS-and-Bro-IDS.ppm&quot; style=&quot;float: right; margin: 10px; width: 50%;&quot; /&gt;
A NIDS can have the following architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;streaming engine&lt;/strong&gt; which ingests packet stream into the NIDS&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;package decoder&lt;/strong&gt; which turns packet content into visibility&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;detection engine&lt;/strong&gt; which identify intrusions&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;policy engine&lt;/strong&gt; which decide and suggest what to do with an intrusion&lt;/li&gt;
  &lt;li&gt;Finally, the intrusion details are collected and &lt;strong&gt;logged&lt;/strong&gt;. &lt;strong&gt;Alerts&lt;/strong&gt; will be sent to admins and optionally, scripted &lt;strong&gt;actions&lt;/strong&gt; can be performed in according to policies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two typical examples of NIDS are &lt;a href=&quot;https://www.snort.org/&quot;&gt;Snort IDS&lt;/a&gt; and &lt;a href=&quot;https://bricata.com/blog/zeek-ids-threat-detection/&quot;&gt;Bro IDS&lt;/a&gt;.
A nicer example that can be integrated into network monitoring can be &lt;a href=&quot;https://www.zabbix.com/features#smart_thresholds&quot;&gt;Zabbix’s Problem Detection Engine&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;machine-learning-at-scale-some-solutions-in-aws&quot;&gt;Machine Learning at scale: some solutions in AWS&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The theory about NIDS perhaps is a huge bundle of knowledge!
Corporations have set up their own NIDS (in most cases in DMZ) for years.
We will not talk about such solutions anymore.
The most interesting part is in the cloud platforms like AWS.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;While companies are moving their resources to the cloud, where is the NIDS?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the &lt;a href=&quot;https://d1.awsstatic.com/Marketplace/scenarios/security/SEC_01_TSB_Final.pdf&quot;&gt;&lt;strong&gt;Shared Responsibility Model (SRM)&lt;/strong&gt;&lt;/a&gt;.
The &lt;strong&gt;responsibility&lt;/strong&gt; of protecting cloud resources like computing instances (EC2) and networking is of AWS.
Users have the responsibility to tune the best configurations of firewall, instances, load balancers, and other resources in AWS.
So a platform IDS is already managed at AWS, and the at the users (application developers), the remaining task is to &lt;strong&gt;implement a best Network/HIDS (endpoint protection)&lt;/strong&gt;.
There are several choices for a HIDS in the AWS Marketplace like &lt;a href=&quot;https://www.trendmicro.com/en_us/business/products/hybrid-cloud/security-data-center-virtualization.html&quot;&gt;TrendMicro’s Deep Security&lt;/a&gt;.
For a custom NIDS, users can implement a &lt;a href=&quot;https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-inspection-architecture-with-aws-gateway-load-balancer-and-aws-transit-gateway/&quot;&gt;Transit DMZ Gateway&lt;/a&gt;. 
An examplar implementation can be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/aws-samples/aws-gateway-load-balancer-code-samples/raw/964874069c0a90d0b6758b2612c2de44a43f2a21/aws-cloudformation/centralized_architecture/images/gwlb_centralized_architecture.jpg&quot; alt=&quot;AWS Transit gateway&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenges-in-nids-operations&quot;&gt;Challenges in NIDS operations&lt;/h2&gt;

&lt;p&gt;A standard DMZ with NIDS can be implemented at the cloud or data center.
However, the &lt;strong&gt;hurdle&lt;/strong&gt; only comes when we operate our solution: this is the hard part!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When it comes to operations (運用保守), it often require an enormous amount of manual work!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We need a large number of low-paid workers who will sit in front of the monitoring screen and then manually mark each access as legal or not.
It is the real-world hurdle!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;However, the technical issues start when we want &lt;strong&gt;automation&lt;/strong&gt;: how to reduce false alarms and misses?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Good automated solutions will reduce manual work a lot.
But it is not straightforward!
Rule-based systems can have many misses or false alarms.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A weak rule misses many, but a strong rule alerts too much! (So both strong and weak ones are useless!)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;machine-learning-at-local&quot;&gt;Machine Learning at local&lt;/h1&gt;

&lt;p&gt;You have a dataset and perform some analytics at your local or edge PC.
You don’t use any server or cloud solution at all.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You should ask us why do you need to care about these works while cloud platforms already prepare so many ready-to-use solutions for you?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Agree! Machine learning engineers behind the platforms already do such works (model engineering).
Then when you do these works, that means you are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A student who is learning ML at an educational place (like university);&lt;/li&gt;
  &lt;li&gt;A researcher (or engineer) who perform a project for NGO, government, or a big company (who is building a platform solution); and&lt;/li&gt;
  &lt;li&gt;The worst luck: you’re only an ML enthusiast who is looking into ML when you have free time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not so many people do model engineering on their own laptop (or desktop), so in my experience, they fall into one of these three categories.
For the second category, people in that category is ML engineer/scientist who will make the model for thousands to million application developers over the world (who uses the platforms).
Such category is quite a few, and to be in, you need &lt;strong&gt;qualification&lt;/strong&gt;!
The most common cases are in the first.
The third category is possible but is rare and complex: while the first and second category have their own goals with ML (for studying and for works), the third category has no particular purpose.
They only do it in their free time and for fun (like doing a hobby)!
The first and second ones will have outcomes (successes and non-successes), but the third one is only for fun!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;No one can ban the hobby of a man, and we only do the hobby: we start when we want and stop when we don’t like it anymore!
That’s why I call it (the third category) the worst luck!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Actually, people in the third category already have another job (but that job does not relate to ML or even AI).
They do ML as hobbies but don’t complete anything (because ML is not their business, even more, ML does not give them bread and butter)!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is said, but in practice, if a thing doesn’t give any benefit, it won’t be done properly!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nevertheless, whatever your category is, when you do these things in your local computing environment, ML matters with you in some ways!
We will see what a &lt;strong&gt;local IDS model would look like&lt;/strong&gt;.
And, in a synthetic way!&lt;/p&gt;

&lt;h2 id=&quot;a-kaggle-synthetic-dataset&quot;&gt;A Kaggle synthetic dataset&lt;/h2&gt;

&lt;p&gt;We start with a synthetic dataset from &lt;a href=&quot;https://www.kaggle.com/sampadab17/network-intrusion-detection&quot;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The dataset to be audited was provided that consists of a wide variety of intrusions simulated in a military network environment. It created an environment to acquire raw TCP/IP dump data for a network by simulating a typical US Air Force LAN. The LAN was focused like a real environment and blasted with multiple attacks. A connection is a sequence of TCP packets starting and ending at some time duration between which data flows to and from a source IP address to a target IP address under some well-defined protocol. Also, each connection is labeled as either normal or as an attack with exactly one specific attack type. Each connection record consists of about 100 bytes.&lt;/p&gt;

  &lt;p&gt;For each TCP/IP connection, 41 quantitative and qualitative features are obtained from normal and attack data (3 qualitative and 38 quantitative features). The class variable has two categories:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Normal&lt;/li&gt;
    &lt;li&gt;Anomalous&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;explorative-analysis&quot;&gt;Explorative analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/ids-kaggle-feature-selection.png&quot; alt=&quot;feature selection&quot; /&gt;
&lt;em&gt;We notice that not all features are useful. Some features like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_Host_login&lt;/code&gt; have only a constant value.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our dataset is big enough (25K entries) and contains both categorical and numerical features:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;RangeIndex: 25192 entries, 0 to 25191
Data columns (total 42 columns):
 #   Column                       Non-Null Count  Dtype  
---  ------                       --------------  -----  
 0   duration                     25192 non-null  int64  
 1   protocol_type                25192 non-null  object 
 2   service                      25192 non-null  object 
 3   flag                         25192 non-null  object 
 4   src_bytes                    25192 non-null  int64  
 5   dst_bytes                    25192 non-null  int64  
 6   land                         25192 non-null  int64  
 7   wrong_fragment               25192 non-null  int64  
 8   urgent                       25192 non-null  int64  
 9   hot                          25192 non-null  int64  
 10  num_failed_logins            25192 non-null  int64  
 11  logged_in                    25192 non-null  int64  
 12  num_compromised              25192 non-null  int64  
 13  root_shell                   25192 non-null  int64  
 14  su_attempted                 25192 non-null  int64  
 15  num_root                     25192 non-null  int64  
 16  num_file_creations           25192 non-null  int64  
 17  num_shells                   25192 non-null  int64  
 18  num_access_files             25192 non-null  int64  
 19  num_outbound_cmds            25192 non-null  int64  
 20  is_Host_login                25192 non-null  int64  
 21  is_guest_login               25192 non-null  int64  
 22  count                        25192 non-null  int64  
 23  srv_count                    25192 non-null  int64  
 24  serror_rate                  25192 non-null  float64
 25  srv_serror_rate              25192 non-null  float64
 26  rerror_rate                  25192 non-null  float64
 27  srv_rerror_rate              25192 non-null  float64
 28  same_srv_rate                25192 non-null  float64
 29  diff_srv_rate                25192 non-null  float64
 30  srv_diff_Host_rate           25192 non-null  float64
 31  dst_Host_count               25192 non-null  int64  
 32  dst_Host_srv_count           25192 non-null  int64  
 33  dst_Host_same_srv_rate       25192 non-null  float64
 34  dst_Host_diff_srv_rate       25192 non-null  float64
 35  dst_Host_same_src_port_rate  25192 non-null  float64
 36  dst_Host_srv_diff_Host_rate  25192 non-null  float64
 37  dst_Host_serror_rate         25192 non-null  float64
 38  dst_Host_srv_serror_rate     25192 non-null  float64
 39  dst_Host_rerror_rate         25192 non-null  float64
 40  dst_Host_srv_rerror_rate     25192 non-null  float64
 41  class                        25192 non-null  object 
dtypes: float64(15), int64(23), object(4)
memory usage: 8.1+ MB
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/ids.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;
Anyway, this is a synthetic dataset with some characteristics based on simulation.
However, it is close to real-world examples enough.
Next, we will try some machine learning models for predicting anomalies.&lt;/p&gt;

&lt;p&gt;With a visualization technique, like T-SNE, we have a diagram of data distribution.
A green point is a normal data point, and a red one is an anomaly.
We can see that it would be hard to draw a linear boundary between normal points and anomalies.&lt;/p&gt;

&lt;h3 id=&quot;play-with-some-machine-learners&quot;&gt;Play with some machine learners&lt;/h3&gt;

&lt;p&gt;In this section, we will try two different ML models with two different training/inference strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Generative model with a reconstruction strategy&lt;/strong&gt;: the Auto-Encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# creating the autoencoder model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;sigmoid&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Discriminative model with a classification strategy&lt;/strong&gt;: the quite classical Random Forest!&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;gini&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;training-and-validation&quot;&gt;Training and validation&lt;/h4&gt;

&lt;p&gt;We need to transform the data a little bit.
It is usual to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MinMaxScaler&lt;/code&gt; to perform data transformation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Auto-Encoder is trained in a reconstruction manner, i. e., it learns to reconstruct the input:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But the Random Forest learns to classify data as usual:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;results-of-auto-encoder&quot;&gt;Results of Auto-Encoder&lt;/h4&gt;

&lt;p&gt;Since the prediction of AE relies on how good it can reconstruct the input:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the reconstruction error is over a threshold, then the reconstruction fails, and the input is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anomaly&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Otherwise, the input is normal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we must define a &lt;strong&gt;threshold&lt;/strong&gt; to separate anomalies and normal inputs.
To find such threshold, we can compute from the training dataset:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# reconstructing the train set
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# finding the mean reconstruction error
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since the mean of construction error is 0.0018520295581492838, we can choose the threshold &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thres = 0.0018&lt;/code&gt;.
Validation in validation set:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# classifying the samples based on threshold
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;anomaly&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thres&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;normal&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Acc. = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[%], F1 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f1_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;anomaly&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[[2901   69]
 [ 563 2765]]
Acc. = 89.96506827564306[%], F1 = 90.17718371153248[%]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;~90%&lt;/strong&gt; of F1 score. Hmm, not so bad!&lt;/p&gt;
&lt;h4 id=&quot;results-of-random-forest&quot;&gt;Results of Random Forest&lt;/h4&gt;

&lt;p&gt;Let’s see the results with Random Forest:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[[3321    7]
 [  14 2956]]
Acc. = 99.66656081295649[%], F1 = 99.64604753076016[%]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;99.6%&lt;/strong&gt;, it is quite good!&lt;/p&gt;

&lt;h2 id=&quot;some-methods-for-machine-learning-based-ids&quot;&gt;Some methods for Machine Learning based IDS&lt;/h2&gt;
&lt;p&gt;There are many ways to identify anomalous access (i. e., intrusion) in the network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rule-based methods&lt;/strong&gt; tend to find a “good” heuristic that can be generalized to a global policy for all feature space.
The same policy can be applied to every input.
For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;every connection which lasted for more than 7 minutes are anomalies&lt;/code&gt; is a policy to identify intrusions.
The problem with this approach is that many false negatives (misses) can be raised.
Because the rule is clear, counterfactuals try to make themselves legal (try to connect faster) and overcome the 7-minute rule.
So a fixed rule is not enough.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The attacks become more and more advanced, but the rules cannot be changed so fast, making an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Achilles&apos; heel&lt;/code&gt; in the defense system.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Machine Learning-based methods&lt;/strong&gt; try to define a soft boundary that can be learned and improved over time.
The main advantage of the Machine Learning approach is that since the boundary is soft and there is no clear rule, the intruders cannot know the rule exactly (how many minutes should they make for a successful intrusion?).
Another important aspect is that since the soft rule is not fixed, it can &lt;strong&gt;evolve&lt;/strong&gt; with the intrusions: the more advanced the intrusion is, the more advanced the defender is.
When this sounds &lt;em&gt;ambiguous&lt;/em&gt;, but for the security systems, it becomes exactly a common strategy to overcome incidents.
The Machine Learning methods can be generative or discriminative, but they must be somewhat non-linear and ambiguous enough to hide details of the system to hackers.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;We have reviewed several views of NIDS: at a corporation network view, cloud solutions, and machine learning models.&lt;/p&gt;

&lt;p&gt;On the synthetic examples, we observed that a quite classical model like &lt;strong&gt;Random Forest&lt;/strong&gt; can outperform neural nets.
It is not a new thing: we already &lt;em&gt;empirically&lt;/em&gt; knew that Random Forest is good at this problem, especially when it is in synthetic environments.
Somebody can argue that there is a chance for overfitting with Random Forest: hmm, I don’t think so.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;And even if it overfits, this is quite good, because &lt;strong&gt;that’s what we want&lt;/strong&gt;: &lt;strong&gt;We want our model to overfit to this dataset&lt;/strong&gt;.&lt;/p&gt;

  &lt;p&gt;Anonymous&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Source code for this article can be found at:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/AIFI-INC/ml-ids&quot;&gt;AIFI-INC’s ML-based IDS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 29 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/ml-ids/</link>
        <guid isPermaLink="true">https://wanted2.github.io/ml-ids/</guid>
        
        <category>Machine learning</category>
        
        <category>network intrusion detection</category>
        
        <category>threat intelligence</category>
        
        <category>edge computing</category>
        
        <category>iot</category>
        
        
        <category>Site Reliable Engineering</category>
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>責任者</title>
        <description>&lt;p&gt;ベトナムに帰ってからベトナムの職場文化になってからはもはや2年間になっております．
ベトナム職場でいうと，恥ずかしいけど，楽しい経験もあるし，悲しい経験もありました．
仕事の責任者として働いた経験もあり，悲しい時で，部下に怒られて，そろそろ殴られる経験もありました．
なぜなら，背景から考えると，文化の違いかなと思います．
ベトナム職場では上下関係は社会的に存在するけど，きちんと働かないとね，上下関係なく殴られるそうです．
「殴られる」は厳しい言葉ですが，主にいうと，ベトナム職場で下記の三大原則はあります．&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則1：&lt;strong&gt;Có làm thì mới có ăn&lt;/strong&gt;. You must work to be fed. 職あり食ある．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則2：&lt;strong&gt;Có lỗi thì sửa là được&lt;/strong&gt;. Don’t be panic, just fix bugs, then it’s OK. 問題にダラダラしないで，修正すれば問題なし．（ただ何もしないなら，↑の原則１をみてください）．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則3：&lt;strong&gt;Làm thì làm cho nghiêm chỉnh.&lt;/strong&gt; You should do the work thoroughly. 徹底的にやりましょう．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ベトナム職場の約2年間の経験をまとめて，一度整理したいと思っております．
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;第一原則職と食&quot;&gt;第一原則：「職」と「食」&lt;/h1&gt;

&lt;p&gt;我々は職場で「仕事」を大事にしております．
仕事が順調であれば，食感もよくなるという発想です．
仕事をしなければ，食えるもんがなく，大変ですね．
ですが，仕事（職）があって，就いたけど何もしないことで組織には不満が引き起こされたことが多いようです．&lt;/p&gt;

&lt;p&gt;ですので，組織全体で皆は統一して，職務をしてから食べるということを理解したらよかったです．
もちろん，海外から来る要員だと，現地の文化がわからず，仕事とは何か現地の人の考え方は最初でわからないこともあります．
しかし，時間が経過すると，その意義を理解すれば，原則を守る姿勢を整い，結局「労働」は第一だという考え方に帰着しました．
組織全体といえば，作業人だけではなく，管理職でもきちんと働き姿勢を見せればよいと思います．&lt;/p&gt;

&lt;p&gt;ここで，一番気になることは，この原則にはもう一つの意味の層があります．
仕事をやるといっても，何をやっても認められるのではないです．
「すごいことをやる」のも間違いです．
ある「職」に就く時，自分の役割と権限は決まっております．
ある場合，期待もあります．
しかし，開発の現場では，ほとんどの場合，役割と権限と責任範囲・仕事内容はすでに決まっております．
（※申し訳ないが，それぐらい曖昧に契約してしまえば，ちょっとそれがまずい職になるかもしれません）．
ですので，「自分の役割と権限・責任範囲の中に行動し，仕事を仕上げる」ことで職を行っているねといいます．
まあ，「職あり食ある」という句です．&lt;/p&gt;

&lt;p&gt;※曖昧に定めてしまった職に入った場合，もちろん仕事をしたから食べることがあるけど，ちょっと食べ物は美味しくないかもしれないです．
おいしい食をどうやって作るか山ほど研究がありますので，それらをGoogleして参考にすればと思います．&lt;/p&gt;

&lt;h1 id=&quot;第二原則チームは障害にだらだらしないで乗り越えることは大事&quot;&gt;第二原則：チームは障害にだらだらしないで，乗り越えることは大事！&lt;/h1&gt;
&lt;p&gt;チーム運営の中に，一緒に働くので，楽しい時も，悲しい時も一緒に乗り越えています．
楽しい時はいいんだけど，悲しい時はどんな時かな？
自分の経験では，パニック状態になるときです．
多くの場合，急に障害が起きるときとかですね．&lt;/p&gt;

&lt;p&gt;確かに，パニック状態のハンドルをうまく取り込めているチームはすごいねとおもいます．
パニック状態になった場合，よいチームは取り込むけど，悪いチームは責任追及ゲームを遊びます．
なぜ責任追及ゲームは悪いのかというと，緊急対応なのに，問題対応をせずに，ゲームをしているからです．
よいチームは問題を見極めて，対処法を第一優先し対応を取り込むのです．&lt;/p&gt;

&lt;p&gt;※ちなみに，チームで開発する場合には，運営中で時間がかかってしまう状態があります．
それが，&lt;strong&gt;パニック状態&lt;/strong&gt;と，&lt;strong&gt;会議状態&lt;/strong&gt;です．
なぜなら，この2つだけは，一人の時間を取るだけではなく，チーム皆の時間を取っているからです．
だらだらして，仕事が進まない罠に落ちやすいのです．
チームのパフォーマンスを上げたい場合に，責任者として，回避と対応策を計画しなければなりません．&lt;/p&gt;

&lt;p&gt;ベトナム職場では，なぜこの原則が取り込まれているかというと，チームがどこかにだらだらすると，めっちゃ時間がかかっているから，まずは乗り越えることを第一優先したいからです．
人がミスをすることは人間の根性ですので，それよりももっと悪いことは，&lt;strong&gt;「なにもしないこと」&lt;/strong&gt;です．
直さないことや修正しないことなどはまた，原則1で処分されると思います．
いつもチームを前向きに進行させることができればと思います．&lt;/p&gt;

&lt;h1 id=&quot;第三原則自分の仕事へのこだわりも重視完成したらもう一度見直そう&quot;&gt;第三原則：自分の仕事へのこだわりも重視！完成したら，もう一度見直そう！&lt;/h1&gt;

&lt;p&gt;2年間ベトナム職場で感じたもう一つの原則です．
第一原則と第二原則を相互に働かせているため，「作成」と「修正」を交互に行っています．
しかし，これらを交互に働かせると，永遠に修正のループに入る可能性はまだ残っています．
ですので，途中でもっと修正してもあまり報われないと感じるときとか，早く止めた方がよいと感じるときとか，もう一つのコントローラーが必要でしょうか．
それは第三原則です．
必ず，（再）作成と（再）修正が完成したら，もう一度&lt;strong&gt;客観的&lt;/strong&gt;にレビューしましょう．
狭い視野で詳細をレビューするだけではなく，広い視野で，この作成と修正は長期的によいか悪いか一回考えるべきというステップがあれば，なおよいです．
ここで，責任者として，技術部分だけではなく，スコープ管理とか要望管理とかスケジュール管理とかうまく併せてやる必要があります．&lt;/p&gt;

&lt;h1 id=&quot;結論&quot;&gt;結論&lt;/h1&gt;

&lt;p&gt;これが，おそらく私が2年間で観測したベトナム職場で支配されている三大原則かと思います．
これらは，なぜか職場をコントロールして，各PJを進行させることが多いそうです．&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/pm-sekininsha/</link>
        <guid isPermaLink="true">https://wanted2.github.io/pm-sekininsha/</guid>
        
        <category>責任者</category>
        
        <category>担当者</category>
        
        <category>権限管理</category>
        
        <category>計画管理</category>
        
        <category>進捗管理</category>
        
        <category>スコープ管理</category>
        
        <category>リスク管理</category>
        
        <category>コミュニケーション管理</category>
        
        <category>リソース管理</category>
        
        <category>プロジェクトマネジメント</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>RapidAPI and RapidAPI Hub</title>
        <description>&lt;p&gt;&lt;em&gt;Image Credit: &lt;a href=&quot;https://financefeeds.com/rakuten-launches-api-marketplace/&quot;&gt;FinanceFeeds&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Rakuten launched RapidAPI Marketplace in 2018 as a result of the collaboration between Japan’s Rakuten Inc and San Francisco-based startup RapidAPI.
&lt;a href=&quot;https://api.rakuten.co.jp/en/&quot;&gt;The API marketplace&lt;/a&gt; aims to provide software developers in Japan and Asia unified access to more than 8,000 APIs with localized documentation and resources in Japan’s language and English.
The API marketplace platform will connect API providers and developers.
Developers in Japan and across Asia will be able to find, test, and connect to thousands of APIs for their applications.
The marketplace will also allow API providers to connect with the global developer community through personalized API portals.
 &lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-is-rapidapi&quot;&gt;What is RapidAPI?&lt;/h1&gt;

&lt;p&gt;Let us assume that you have an API that is ready for production.
You need to add authentication like API key, OAuth 2, or something else.
You need to deploy your API to somewhere that is stable and reliable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the &lt;strong&gt;shortest path&lt;/strong&gt; to achieving your goal?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You are an application developer, and you need to manage the records of some data for the app.
For example, you need to maintain the list of public holidays in your app.
You don’t want to hardcode those things in the code.
Note that the public holidays change between countries and sometimes due to the law it will change between years.
It is somewhat troublesome to maintain the records in your database as it will make you allocate some effort and human resources there.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the most convenient way to maintain such data?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In both scenarios, Rakuten RapidAPI Marketplace gives you excellent solutions.
Either maintenance of the data (public holidays) or publishing a new API, you can do all of the lifecycles in one platform.&lt;/p&gt;

&lt;p&gt;For example, when you want to check a day is a holiday or not, you can thus search for a free API like this &lt;a href=&quot;https://english.api.rakuten.net/theapiguy/api/public-holiday&quot;&gt;one&lt;/a&gt; and make a request.
Because all maintenance is up on the providers, this solution costs you nothing: you don’t need to worry about maintaining the records of holidays data (which shouldn’t be your matter in any way) and focus on your own application logic.
Note that the &lt;a href=&quot;https://english.api.rakuten.net/theapiguy/api/public-holiday&quot;&gt;Public Holidays API&lt;/a&gt; has low latency (59ms) and is completely free.
Another solution is to build an endpoint in your own API like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/api/v1/holidays&lt;/code&gt; to validate the holidays, but while such a ready-to-use solution is there, why should you waste time and money to build/manage/maintain on your own?&lt;/p&gt;

&lt;p&gt;RapidAPI helps your API to distribute and monetize.
Adding your API to the RapidAPI Hub gets you instant exposure to our growing user base, a search-engine-optimized profile page for your API, as well as features like user management and billing services.
RapidAPI also serves functional testings, API monitoring dashboards, and many other premiere features like API authentication.&lt;/p&gt;

&lt;h1 id=&quot;rapidapi-for-api-vendors&quot;&gt;RapidAPI for API Vendors&lt;/h1&gt;

&lt;p&gt;The workflow between an app developer’s client to a vendor API can be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rapidapi.svg&quot; alt=&quot;rapidapi&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An &lt;a href=&quot;https://docs.rapidapi.com/docs/keys&quot;&gt;API Key&lt;/a&gt; is generated and appended to the request’s header to RapidAPI servers.&lt;/li&gt;
  &lt;li&gt;RapidAPI authenticate the request (using API Key and optionally a configured authentication method like OAuth 2). Then it modifies the requests header to append &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X-RapidAPI-*&lt;/code&gt; headers.&lt;/li&gt;
  &lt;li&gt;The vendor API (destination API in the diagram) checks the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X-RapidAPI-*&lt;/code&gt; headers and authenticates the modified requests.&lt;/li&gt;
  &lt;li&gt;A response is generated according to the requested information and is then returned to RapidAPI.&lt;/li&gt;
  &lt;li&gt;RapidAPI modifies the response from vendor servers. It appends Rapid API headers (for example, headers about rate limits) or generates a new response.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As you can see, RapidAPI Marketplace acts as a proxy between app servers (client in the diagram) and the vendor API servers.
The vendors &lt;a href=&quot;https://docs.rapidapi.com/docs/add-an-api-basics&quot;&gt;register&lt;/a&gt; their APIs and &lt;a href=&quot;https://docs.rapidapi.com/docs/add-an-api-advanced-settings&quot;&gt;fine-tune&lt;/a&gt; the settings in RapidAPI dashboard.
All API endpoints are relative to a base URL, which is added as a “prefix” to all API endpoints.
This approach avoids the need to define absolute URLs for endpoints every time and increases API portability by changing the base URL.&lt;/p&gt;

&lt;p&gt;API vendors can &lt;a href=&quot;https://docs.rapidapi.com/docs/configuring-api-authentication&quot;&gt;add&lt;/a&gt; basic authentication or OAuth 2 to their APIs.&lt;/p&gt;

&lt;p&gt;RapidAPI supports &lt;a href=&quot;https://docs.rapidapi.com/docs/automating-api-provisioning&quot;&gt;automatic API provisioning using OpenAPI&lt;/a&gt; and &lt;a href=&quot;https://docs.rapidapi.com/docs/transformations&quot;&gt;custom transformations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;RapidAPI has basic plan options so app developers can choose among these options to pay:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;API Type&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Free APIs&lt;/td&gt;
      &lt;td&gt;APIs that do not require a credit card or subscription to consume.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pay Per Use&lt;/td&gt;
      &lt;td&gt;APIs that don’t have a subscription fee associated with them. A credit card is required as you pay for what you use on the API.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Freemium APIs&lt;/td&gt;
      &lt;td&gt;Paid APIs that also include a limited free tier. These require a credit card, even for the free plan.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Paid APIs&lt;/td&gt;
      &lt;td&gt;APIs that require a paid subscription plan and credit card to consume.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;some-notes-on-security&quot;&gt;Some notes on security&lt;/h2&gt;

&lt;p&gt;RapidAPI supports &lt;a href=&quot;https://docs.rapidapi.com/docs/secret-headers-parameters&quot;&gt;secret headers and parameters&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;RapidAPI allows you to add secret headers and/or query string parameters to API requests. The RapidAPI proxy adds these secrets to every request but is &lt;strong&gt;hidden from the API consumers&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that even the consumers who make the requests do not know about these secrets.
This differs from header and query authentication methods where consumers know all secrets in the requests they make to RapidAPI.&lt;/p&gt;

&lt;p&gt;Users should configure RapidAPI &lt;a href=&quot;https://docs.rapidapi.com/docs/security-threat-protection&quot;&gt;security&lt;/a&gt; features like firewalls, threat protection, schema validation, and request size limit (which returns error code 413).&lt;/p&gt;

&lt;p&gt;Vendors can set their API to &lt;a href=&quot;https://docs.rapidapi.com/docs/private-apis-api-logo&quot;&gt;private&lt;/a&gt; where only invited users can access.&lt;/p&gt;

&lt;h2 id=&quot;audit-and-marketing-tools&quot;&gt;Audit and marketing tools&lt;/h2&gt;

&lt;p&gt;RapidAPI provides &lt;a href=&quot;https://docs.rapidapi.com/docs/provider-dashboard&quot;&gt;Provider Dashboard&lt;/a&gt; where vendors can monitor their API usages.
Another nice thing is that as a vendor, you can make your monetization more useful using &lt;a href=&quot;https://docs.rapidapi.com/docs/ive-added-my-api-to-rapidapi-now-what&quot;&gt;Marketing API&lt;/a&gt;.
When you have an API, you should make sure you don’t miss a checklist when publishing your solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://api.rakuten.co.jp/docs/ja-images/ProviderWelcome_1.png&quot; alt=&quot;RapidAPI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This checklist helps you have a better SEO for your API.&lt;/p&gt;

&lt;h2 id=&quot;api-testing&quot;&gt;API Testing&lt;/h2&gt;

&lt;p&gt;Testing is quite tedious!
RapidAPI helps vendors reduce testing costs with their &lt;a href=&quot;https://docs.rapidapi.com/docs/rapidapi-testing-overview&quot;&gt;API testing feature&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.readme.io/726dc84-run-code.png&quot; alt=&quot;RapidAPI testing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you are already familiar with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postman-tool&lt;/code&gt; you are ready to go with RapidAPI &lt;a href=&quot;https://docs.rapidapi.com/docs/create-a-test-advanced&quot;&gt;advanced testing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.readme.io/fabfeb1-Screen_Shot_2020-12-03_at_4.00.53_PM.png&quot; alt=&quot;Advanced testing&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;rapidapi-for-app-developers&quot;&gt;RapidAPI for App Developers&lt;/h1&gt;

&lt;p&gt;As an app developer, you can find that &lt;a href=&quot;https://rapidapi.com/hub&quot;&gt;RapidAPI Hub&lt;/a&gt; now has more than 10,000 APIs.
Even you want to develop an OCR app or a Translation app, you can find your API right away.&lt;/p&gt;

&lt;p&gt;All you need is to register a RapidAPI account, choose your API and then &lt;strong&gt;make a payment&lt;/strong&gt;.
Finally, you can &lt;a href=&quot;https://docs.rapidapi.com/docs/connecting-to-an-api&quot;&gt;connect&lt;/a&gt; to your paid API using the API key.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rapidapi-vin.png&quot; alt=&quot;RapidAPI VIN&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It is worth noting that RapidAPI supports not only REST API but also &lt;a href=&quot;https://docs.rapidapi.com/docs/graphql-apis&quot;&gt;GraphQL&lt;/a&gt;, &lt;a href=&quot;https://docs.rapidapi.com/docs/adding-soap-apis&quot;&gt;SOAP&lt;/a&gt;, and &lt;a href=&quot;https://docs.rapidapi.com/docs/kafka-apis&quot;&gt;Kafka&lt;/a&gt; APIs.
We did not touch &lt;a href=&quot;https://docs.rapidapi.com/docs/what-is-rapidapi-for-teams&quot;&gt;RapidAPI for Teams&lt;/a&gt;, but it might be useful at the organization level.&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/rapidapi/</link>
        <guid isPermaLink="true">https://wanted2.github.io/rapidapi/</guid>
        
        <category>api development</category>
        
        <category>backend</category>
        
        <category>infrastructure</category>
        
        <category>rapidapi</category>
        
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
        <category>Site Reliable Engineering</category>
        
      </item>
    
      <item>
        <title>Chia tay 2021!</title>
        <description>&lt;p&gt;Đây là bài post thứ 61 của blog AiFi trong năm 2021, cũng là bài viết chia tay 2021, trong tâm thế đón chờ 2022 tươi mới hơn.
Theo quan điểm làm việc scrum, thì coi như đây là thời điểm kết thúc 1 chu kỳ, cũng là lúc làm một số việc để nhìn lại một năm đã qua (bao gồm cả GKPT hay &lt;em&gt;Good, Keep, Problem, Try&lt;/em&gt;).
2021年中61番目の投稿です．
2021年と別れて，2022年を迎える時期の投稿です．
一年間を1スプリントとすると，いろいろなことができたと思いますので，スクラムの行事として，レビューとレトロ会をここで開催したいと思います．
&lt;em&gt;Good, Keep, Problem, Try&lt;/em&gt; も含めてやります．
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;nhìn-lại-năm-2021-của-blog-aifi&quot;&gt;Nhìn lại năm 2021 của blog AiFi&lt;/h1&gt;

&lt;h2 id=&quot;nhìn-từ-thống-kê-người-dùng&quot;&gt;Nhìn từ thống kê người dùng&lt;/h2&gt;

&lt;p&gt;Hiện tại AiFi blog sử dụng Google Analytics để track và lấy thống kê người dùng.
Các sự kiện như view, scroll, referal, … được báo cáo theo phút lên server của Google.&lt;/p&gt;

&lt;p&gt;Đầu tiên là thống kê về người dùng và nguồn giới thiệu.
Trong năm 2021, blog tuy mới ra mắt và còn nhiều khó khăn vất vả nhưng đã thu hút được 552 user mới từ khắp nơi trên thế giới.
&lt;strong&gt;552 người dùng này đã ghi lại 7309 sự kiện.&lt;/strong&gt;
Một con số đáng khích lệ với blog mới 1 năm tuổi đời.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Một điểm đáng chú ý là dù facebook.com là nơi tác giả hay chia sẻ bài viết, nhưng &lt;strong&gt;user lại phần lớn đến từ 2 nguồn: google và direct&lt;/strong&gt;.
Về yếu tố địa lý thì đa phần người dùng đến từ &lt;strong&gt;Việt Nam, Mỹ và Nhật Bản&lt;/strong&gt;.
Các nước khác vẫn chưa đóng tỷ trọng lớn trong cơ cấu người dùng của AiFi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tỷ lệ người dùng của AiFi gia tăng tính từ tháng &lt;strong&gt;7&lt;/strong&gt;.
Trong năm 2021, &lt;strong&gt;số lượng sự kiện &lt;em&gt;user engagement&lt;/em&gt; là 1852, và số &lt;em&gt;page view&lt;/em&gt; là 2622 lượt&lt;/strong&gt;.
Ngoài ra, 3 bài viết đạt số lượng truy cập cao nhất (không tính trang chủ) là:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/mOCR-mlkit-androidx-example/&quot;&gt;mOCR: A real-time application of OCR with Google MLKit and Android CameraX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/adobe-creative-cloud/&quot;&gt;Adobe Creative Cloud: An All-in-One Platform for Creators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/aws-lambda-spacy-mxnet-possible-but-shouldnt/&quot;&gt;Implementing a complex system in AWS Lambda: Should or shouldn’t?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sự “vùng lên” của bài viết &lt;a href=&quot;/adobe-creative-cloud/&quot;&gt;Adobe Creative Cloud: An All-in-One Platform for Creators&lt;/a&gt; thật thú vị vì bài viết được xuất bản trên blog AiFi vào tháng cuối năm nhưng lại đứng thứ nhì.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-03.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Về hệ điều hành, trình duyệt và ngôn ngữ đầu vẫn là &lt;strong&gt;Windows, Chrome và English&lt;/strong&gt;.
Theo sau lần lượt là &lt;strong&gt;MacOS, Safari và tiếng Nhật&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-04.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nhìn-từ-kết-quả-tìm-kiếm&quot;&gt;Nhìn từ kết quả tìm kiếm&lt;/h2&gt;

&lt;p&gt;Kết quả tìm kiếm về “AiFi Caineng” trên google.com và Bing Search trong ngày 31 tháng 12 năm 2021 như sau:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aifi-search-engines-2021.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả tìm kiếm từ khóa “aifi” và thậm chí “aifi caineng” quả là hơi nghèo nàn và dễ bị lẫn vào các từ khóa tìm kiếm khác như “wifi” chẳng hạn.
Đây cũng là 1 thiếu sót do blog mới chỉ 1 năm, và tác giả vẫn đang bận bịu công việc chính cuả tác giả.
Tuy nhiên, từ năm 2022, ở mức độ nhất định việc nâng rank trong các cỗ máy tìm kiếm từ khóa sẽ được &lt;strong&gt;tối ưu hóa&lt;/strong&gt; nhằm đưa tri thức của AiFi đến với đông đảo bạn đọc và nâng cao chất lượng phục vụ.&lt;/p&gt;

&lt;h1 id=&quot;good-keep-problem-try&quot;&gt;Good, Keep, Problem, Try&lt;/h1&gt;

&lt;p&gt;Việc chạy sprint kéo dài 1 năm quả là hơi lạ, tuy nhiên là cũng dễ hiểu vì viết blog chỉ là việc phụ làm trong thời gian rảnh rỗi của tác giả.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Good&lt;/th&gt;
      &lt;th&gt;Keep&lt;/th&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Try&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Đã tạo được và thu hút lượng người dùng nhất định.&lt;/td&gt;
      &lt;td&gt;Duy trì tần suất chia sẻ bài viết.&lt;/td&gt;
      &lt;td&gt;Thứ hạng trên search engine chưa cao.&lt;/td&gt;
      &lt;td&gt;Tối ưu hóa SEO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Tối ưu hóa từ khóa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Tối ưu thẻ HTML, …&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Chưa tạo ra thu nhập từ blog&lt;/td&gt;
      &lt;td&gt;Xem xét đưa vào và tối ưu hóa quảng cáo.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Các nguồn Google và Facebook đã đem đến lượng người dùng nhất định.&lt;/td&gt;
      &lt;td&gt;Tiếp tục duy trì quảng bá trên Google và Facebook.&lt;/td&gt;
      &lt;td&gt;Nguồn Facebook chưa đem lại nhiều người dùng mới.&lt;/td&gt;
      &lt;td&gt;Tối ưu hóa quảng bá blog trên Facebook.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Một số nguồn cấp khác như Twitter và LinkedIn vẫn chưa đem lại nhiều người dùng.&lt;/td&gt;
      &lt;td&gt;Lên chiến lược quảng bá trên các nền tảng này.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;tổng-kết&quot;&gt;Tổng kết&lt;/h1&gt;

&lt;p&gt;Kết thúc Sprint 2021, hướng tới Sprint 2022, blog AiFi xin cám ơn đông đảo bạn đọc, đặc biệt là 552 người dùng đã có, vì sự quan tâm và thịnh tình trong năm qua.
Trong năm 2022, AiFi sẽ tiếp tục cập nhật và mong muốn lan tỏa tri thức cho anh em, với phương châm, troll trước học sau.&lt;/p&gt;
</description>
        <pubDate>Fri, 31 Dec 2021 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/year-end/</link>
        <guid isPermaLink="true">https://wanted2.github.io/year-end/</guid>
        
        <category>Event</category>
        
        <category>Year-end event</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
      </item>
    
      <item>
        <title>Nên chọn gói subscription Adobe/Games nào cho nhóm phát triển và nghiên cứu?</title>
        <description>&lt;p&gt;Ở các quốc gia tôn trọng và thực thi nghiêm chỉnh luật bản quyền, thì vị trí leader có một “đặc sản” khá thú vị là &lt;strong&gt;kỹ năng quản lý license&lt;/strong&gt;, hay là &lt;strong&gt;tối ưu hóa license (サブスクリプションの最適化, optimize subscriptions)&lt;/strong&gt;.
Giải thích thì cũng mất thời gian, tôi xin lấy ví dụ bài toán như sau:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Giả sử bạn là chủ nhiệm (hoặc người đứng đầu) một phòng nghiên cứu và phát triển có lượng nhân sự ổn định là 40 người.
Hàng năm có 20 người tốt nghiệp và đi chỗ khác, và bạn sẽ lại đón khoảng 20 người mới vào để phân đề tài hướng dẫn.
Và vì vậy nhân sự luôn ổn định tầm 40 người.
Thế mạnh của phòng là xử lý đồ họa, thiết kế web, và mấy cái game ghiếc, truyền hình (TV).
Những gì trong thế mạnh thì là cái bạn sẽ hướng dẫn (chứ không có chuyện là nhận vào rồi cho làm mấy cái text tiếc vớ vẩn hay mấy cái random là không có đâu).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Vậy bây giờ là phát sinh ra trong hướng nghiên cứu của phòng bạn, sẽ phải mua máy móc như màn hình cỡ lớn, màn hình đặc dụng cho ngành truyền hình, … và đăng ký mua license phần mềm như Adobe Photoshop, Illustrator, XD, … rồi lại còn games, …&lt;/strong&gt;.
&lt;strong&gt;Thế kế hoạch quản lý license của bạn như thế nào? Với giả định bạn là đứng đầu lab và trách nhiệm quản lý sẽ thuộc về bạn.&lt;/strong&gt;
&lt;!--more--&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;bản-chất-của-bài-toán-quản-lý-license-software-license-management&quot;&gt;Bản chất của bài toán quản lý license (Software License Management)&lt;/h1&gt;

&lt;p&gt;Thực ra nói là tôn trọng bản quyền là 1 vấn đề nhưng nó chỉ là một cái mang tính chất bề nổi của tảng băng chìm.
&lt;strong&gt;Ẩn đằng sau kỹ năng quản lý license chính là vấn đề cost (giá thành) và security (bảo mật).&lt;/strong&gt;
6 bước cần phải làm với quản lý license (SLM) chính là:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Quy trình&lt;/th&gt;
      &lt;th&gt;Mô tả&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Usage analysis&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Phân tích nhu cầu sử dụng của nhóm phát triển, tổ chức dưới quyền, nhân viên. Nhóm đứng đầu sẽ phải phân tích nhu cầu sử dụng thực sự của nhân viên là gì. Ở bước này chưa cần tối ưu ngay, nhưng cần phải ngăn chặn những request mua đồ không cần thiết.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;License optimization&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Sau khi đã tối ưu hóa requests từ phía end-users, thì việc tiếp theo là với bộ request đã tối ưu ấy rồi, thì phải lựa chọn mua (hay không mua) những phần mềm, thiết bị nào. License nào sẽ bỏ tiền ra để subscribe, và sẽ subscribe theo gói nào?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Maintenance analysis&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Cái này cũng nói đi nói lại nhiều lần rồi, nếu bước này không tính toán dài hạn dựa trên usage analysis thì ví dụ phiên bản phần mềm nâng cấp thì lại phải mua lại license, dẫn đến tốn tiền nâng cấp license. Ngoài ra, nếu không kịp thời nâng cấp mua mới license thì khi hết hạn nhân viên lại phải chờ sếp mua license mới, rất tốn công.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Standardized data collection&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Với các tổ chức lớn như viện nghiên cứu hay trường đại học có hàng trăm thậm chí hàng ngàn nhân viên, việc quản lý tập trung, thu thập thông tin sử dụng phần mềm của nhân viên (cài phiên bản nào, license thế nào) là vô cùng quan trọng. Về mặt bảo mật, thì khi có license hết hạn, người dùng vi phạm license agreement, nhà quản lý sẽ được thông báo trực tiếp từ data, và như thế đối ứng sự cố bảo mật mới tốt.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Cyber-threat detection&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Thực ra thì thu thập dữ liệu mà muốn lên cảnh báo thì phải có một feature là threat detection. Tức là phát hiện nguy cơ liên quan tới license ngay từ dữ liệu usage của nhân viên.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Audit Prevention&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Phòng chống các vấn đề khi đã phát hiện ra từ dữ liệu ra là cần thiết. Auditor sẽ phải đưa ra những suggestions để tổ chức và nhà quản lý đưa thành policy để implement trong tổ chức.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Đấy, nghe thì có vẻ đơn giản là làm 1 bảng Excel để quản lý license một cách thủ công, nhưng breakdown ra thì cũng đủ thứ.
Làm nhà quản lý cho 1 team nhỏ tầm 5-6 người, hay dù quản lý cả 1 department hay là hiệu trưởng một trường đại học danh tiếng có hàng vạn sinh viên thì những vấn đề như thế này không được lơ là.&lt;/p&gt;

&lt;p&gt;Theo kinh nghiệm của tôi thì cứ tốt nhất mua luôn gói nào nó có sẵn hết các chức năng trên.
Ví dụ thu thập dữ liệu và detect nguy cơ là Adobe nó cũng có hết trong gói Business rồi đấy.
Tuy nhiên, nếu trong nhóm chỉ có 1-2 người cần mua thì bài toán giá thành cân đối bảo mật sẽ phát sinh vì ít nhu cầu dùng quá (cost vs. security trade-off).
Tuy nhiên, nếu là lãnh đạo của 1 department hàng trăm nhân khẩu dưới quyền thì cứ thoải mái thôi, vì khi số lượng đặt hàng lên đến hàng trăm, hàng ngàn thì chắc chắn Adobe sẽ có discount.
Vào mấy cái viện ít người mới lo chứ còn vào những chỗ quân đông tướng lắm như trường đại học, tập đoàn, cty lớn thì chủ yếu là kỹ năng quản lý thôi.&lt;/p&gt;

&lt;h1 id=&quot;quay-lại-ví-dụ-lab-40-người&quot;&gt;Quay lại ví dụ lab 40 người&lt;/h1&gt;

&lt;h2 id=&quot;đại-học-lớn-vs-đại-học-nhỏ-và-viện-ít-người&quot;&gt;Đại học lớn vs. đại học nhỏ (và viện ít người)&lt;/h2&gt;

&lt;p&gt;Ví dụ nhân sự 40 người nói ở trên là một trường đại học và tập đoàn lớn, nơi cứ mỗi năm có hàng ngàn người mới, hết thế hệ này tới thế hệ khác.
Vì vậy làm giáo sư hay teamlead ở một tập đoàn hay trường đại học lớn, thì vấn đề nhân sự lại không phải việc khó.
Cứ lên làm lead thì thiếu người muốn nhận bao nhiêu chả được.
&lt;strong&gt;Cái quan trọng là năng lực quản lý của anh được bao nhiêu người?&lt;/strong&gt;
Thường thì chuyện quản 40 người thậm chí hàng trăm người là chuyện bình thường ở các cơ sở lớn như vậy.&lt;/p&gt;

&lt;p&gt;Còn đối với các viện nhỏ, đại học nhỏ thì đôi khi mỗi leader một năm chỉ quản có vài ba nhân viên.
Chuyện quản lý có khi dễ dàng, người đứng đầu chả thèm quan tâm xây dựng văn hóa đội nhóm, văn hóa team, …
Nếu là nhà nghiên cứu thì đi vào những nơi này họ sẽ có nhiều thời gian làm chuyên môn hơn là lo quản lý hàng trăm hàng ngàn miệng ăn phía dưới.
&lt;strong&gt;Chỉ cần lo cho bản thân mình và mấy cậu phía dưới thôi.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Tuy nhiên, cái gì cũng có mặt trái của nó, nếu cái kiểu vài cơ sở nhỏ như trên thì gánh nặng quản lý không có nhưng giá thành sẽ đắt vì hầu như các gói subscription của các phần mềm như Office hay Adobe thì chỉ rẻ khi có nhiều trên 100 người dùng.
Ông làm nghiên cứu mà lab ông chỉ có 2 anh postdocs thì dù là viện nghiên cứu cấp quốc gia thì khi mua license thì cũng chỉ tính được cho 3 người (ông và 2 anh postdocs).
Mà thế thì hầu như muốn mua phần mềm gì cũng chỉ có vài khả năng:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;là mua gói personal subscription, giá sẽ đắt trên đầu người. Ngoài ra, sẽ phải mua đứng tên từng cá nhân nên control của tổ chức sẽ không có. Tức là như đại học lớn họ mua hàng loạt cho ngàn người dùng thì họ sẽ được tặng kèm gói audit để thực hiện thu thập dữ liệu, detect nguy cơ và prevention.&lt;/li&gt;
  &lt;li&gt;là ông phải chung với các lab lớn khác. Nhiều lab nhỏ cùng có nhu cầu dùng hợp với nhau để mua. Thì nó lại có cái dở là ông admin ở lab khác cũng sẽ quản lý được dữ liệu thu thập được từ nhân viên lab ông. Thành ra là có vấn đề về riêng tư.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thế nên là vào mấy cái startups hay đội nhóm nhỏ, đại học nhỏ ấy thì thường mọi người chấp nhận không dùng bản có phí, dùng &lt;strong&gt;open source&lt;/strong&gt; để tránh việc phải xài những cái đòi licenses như MATLAB, Office, Adobe, …
Cái này cũng có cái hay cái dở.
Thì thực ra tình huống nó bắt buộc là mình ít người mình chịu.
Nhưng nó lại dở vì đa số người ta dùng Adobe, MATLAB, … mình lại không mua được thì thành ra là lại outliers.
Ví dụ nghiên cứu của lĩnh vực đều dùng Adobe Photoshop, thì mình lại tái hiện trên GIMP thì ôi thôi!&lt;/p&gt;

&lt;p&gt;Mà cái quan trọng nhất lại không chỉ giá tiền mà là &lt;strong&gt;audit&lt;/strong&gt;. Dù ông nhà nghiên cứu ở viện nhỏ, ông có mua personal subscription hay open source thì chắc chắn sẽ không có control của tổ chức.
Nên nếu ông đề nghị mua phần mềm cho 2 cậu postdocs thì chắc chắn ông lãnh đạo viện sẽ bắt ông phải mua theo option 2.
Mà như thế là dữ liệu của nhóm ông tự dưng lại phải chia sẻ ra cho các nhóm khác trong viện mà thậm chí ngoài viện.
Còn nếu ông không đề nghị mua, tức là ông giấu lãnh đạo viện thì sẽ không cân đối kế toán được.
Còn ông mà bảo postdocs nó dùng open source thì lại xảy ra vấn đề là không minh bạch vì không có control trong audit.&lt;/p&gt;

&lt;p&gt;Nên lúc cân nhắc làm việc ở viện lớn hay viện nhỏ, đại học danh tiếng hay ít danh tiếng, tập cty lớn hay mấy startups, theo kinh nghiệm của tôi nên cân nhắc bài toán &lt;strong&gt;quản lý license&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;lựa-chọn-subscription-cho-phần-mềm-đồ-họa-adobe-software&quot;&gt;Lựa chọn subscription cho phần mềm đồ họa: Adobe software&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Bây giờ chúng ta sẽ bắt đầu thực chiến quản lý license.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Đầu tiên, về nhu cầu sử dụng, &lt;strong&gt;usage analysis&lt;/strong&gt;, thì quan điểm là người đứng đầu có quyền chỉ định đề tài nghiên cứu cho nhân viên.
Đặc biệt nếu là công ty lớn, đại học lớn thì chuyện này là hiển nhiên.
&lt;strong&gt;Bạn nghĩ (a) nhân viên tự đề xuất đề tài nghiên cứu rồi tự ý làm hay (b) lãnh đạo chỉ đạo chọn đề tài nghiên cứu thì tốt hơn?&lt;/strong&gt;
Về mặt quản lý license, chắc chắn (b) sẽ tốt hơn với trường hợp có tới 40 nhân viên như chúng ta đang giả định.
Vì sao? bởi vì là công nghệ lõi của lab vẫn là xử lý đồ họa, nên chắc chắn lúc đầu năm lúc xin kinh phí, lãnh đạo phải có kế hoạch để mua license nào phục vụ hướng nghiên cứu của lab.
Thế giờ tự cho nhân viên tự tìm đề tài, nó lại tìm ra cái đề tài lệch 180 độ so với hướng đi của lab, rồi đòi mua 1 cái license mà ko có tác dụng gì cho nhóm nghiên cứu thì kế hoạch kinh phí đã xin là phá sản?
Thế nên những trường hợp cũng không làm gì được vì tự do nghiên cứu phát triển, chống harrassment thì không thể bắt ép được.
Lại phải delay thôi vì &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;em đánh đố thế thì thày cũng chịu nhá&lt;/code&gt;.
Ngoài ra không chỉ vấn đề tiền mà cả về bảo mật thì nhân viên tự ý chọn đề tài ra ngoài rồi đòi mua license ngoài dự tính như vậy, thì chắc chắn là nếu đề xuất mua lên trên, thì lãnh đạo trường chắc chắn bắt mua chung với 1 lab khác, cho đủ con số người dùng để được giảm giá, thì lại xuất hiện bài toán phải chia sẻ dữ liệu với lab khác.
Mà hợp tác với lab khác mà chuyện chia sẻ dữ liệu nhập nhằng cũng sẽ nhiều vấn đề.
Nên cái thằng nhân viên nó chọn đề tài mang tính &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;outliers&lt;/code&gt; là phải xử lý.&lt;/p&gt;

&lt;p&gt;Thì thực ra cái gì cũng có thể giải quyết được, đề tài lệch 180 độ mà hai bên thống nhất thì cũng “bẻ lái” được thôi, nên thày trò lãnh đạo nhân viên chịu khó nói chuyện rồi mà giải quyết với nhau là êm đẹp nhất.
Còn nếu vướng mắc quá thì lại phải thuyên chuyển sang nơi khác mà nhiều nhân viên dùng cái license ấy thôi.&lt;/p&gt;

&lt;p&gt;Theo kinh nghiệm của tôi, thì lãnh đạo &lt;strong&gt;mát tay&lt;/strong&gt; là có khi ông ấy control luôn cả usage của 40 nhân viên.
Tức là không nói thẳng ra là chỉ đạo nhưng mà nhìn chung là cứ có cái gì lệch lạc tổ chức họp “bẻ lái”, làm sao cho cả 40 nhân viên đều làm đồ họa.
Thằng nào thích làm NLP thì chủ động dùng open source đi vì NLP phần lớn open source chứ ít kiểu license nhiều như bên đồ họa, vision.
Tôi từng gặp một trường hợp lãnh đạo rất &lt;strong&gt;mát tay&lt;/strong&gt;: chọn đề tài từ trên xuống dưới cách thể hiện qua ngôn ngữ thì nhìn qua tên đề tài thấy chả có liên quan tới vision đồ họa mấy, nhưng đến lúc bóc tách ra thì toàn các tính năng của Adobe Creative Suite.
Thế là 40 con người vẫn đều phải phụ thuộc vào Adobe, mà thế thì cả phòng dùng Adobe thì viết đơn xin mua Adobe cho 40 người bên trên làm sao từ chối được?
&lt;strong&gt;Cả phòng làm về Adobe thì mua sản phẩm Adobe là đúng rồi còn gì.&lt;/strong&gt;
Thế nên, nếu mà control usage ngay từ đầu thì lúc đề xuất mua cái gì cũng dễ pass, cái bước usage analysis cũng chả phải làm vì cả phòng làm Adobe thì còn phân tích cái gì nữa.
&lt;strong&gt;Mát tay&lt;/strong&gt; nó là như thế đấy.&lt;/p&gt;

&lt;p&gt;Bây giờ, thày trò họp hành, rồi thảo luận trao đổi, “bẻ lái” thế nào đó để cả 40 con người đồng bộ nhu cầu rồi nhé.
Bước tiếp theo là &lt;strong&gt;License optimization&lt;/strong&gt;, tức là chúng ta sẽ phải lựa chọn gói tối ưu nhất của Adobe để mua đảm bảo 40 con người có đồ mà dùng.
Thực ra bước này làm cùng với phân tích nhu cầu sử dụng cũng được.
Thì cứ giả định là thời gian sử dụng là 24/7 luôn đi.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Tham khảo các gói sử dụng của Adobe Creative Cloud tại &lt;a href=&quot;https://www.adobe.com/creativecloud/plans.html&quot;&gt;https://www.adobe.com/creativecloud/plans.html&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bây giờ có 40 người thì có hai cases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Chúng ta là phòng nghiên cứu phát triển của 1 cty lớn và cả 40 nhân viên đều chính thức&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Nếu mua cho 40 nhân viên 40 gói individuals thì sẽ tốn mỗi tháng là $40\times 52.99=2199.6$ USD. Một năm sẽ tốn tầm 25k USD.
        &lt;ul&gt;
          &lt;li&gt;Bất lợi: là không có quản lý license đi kèm.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Nếu mua theo gói Business thì
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.adobe.com/creativecloud/business/teams.html&quot;&gt;https://www.adobe.com/creativecloud/business/teams.html&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;Bạn sẽ trả 79.99 đô/tháng cho mỗi license, thì nhu cầu thay đổi chỉ cần hủy license và lấy license mới.&lt;/li&gt;
          &lt;li&gt;Tuy nhiên là nếu license dừng lại giữa tháng thì sẽ xuất hiện lãng phí và lại phải chia sẻ account để đỡ phí. Mà về mặt bảo mật thì chia sẻ account lại không nên. Thế nên là lại phải quản lý nhân viên để không có ai bỏ việc giữa tháng.&lt;/li&gt;
          &lt;li&gt;Lợi điểm là các tính năng để collaborate và quản lý license có đi kèm.&lt;/li&gt;
          &lt;li&gt;Trong trường hợp không chia sẻ tài khoản thì tốn $79.99\times 40=3199.6$ đô môĩ tháng, 1 năm mất 38-39K.&lt;/li&gt;
          &lt;li&gt;Trong trường hợp chia sẻ tài khoản thì có quy định sau:
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;https://helpx.adobe.com/download-install/using/install-apps-number-of-computers.html&quot;&gt;Số lượng máy có thể cùng lúc dùng cho 1 license&lt;/a&gt; thì chỉ tối đa 2 máy được login cùng license.&lt;/li&gt;
              &lt;li&gt;Thế nên coi như giảm đi 1 nửa so với không chia sẻ tài khoản thì mỗi tháng mất 1600 đô và mỗi năm tầm 19k đô. &lt;strong&gt;Vẫn rẻ hơn so với mua gói cá nhân hoặc không chia sẻ tài khoản&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chúng ta là ở trong đại học lớn và cả 40 nhân viên đều là students&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Thì kể cả gói cá nhân cho 40 sinh viên cũng chỉ mất 19.99 đô/tháng. vậy mỗi tháng mất $19.99\times 40=800$ đô. Mỗi năm mất dưới 10k.
        &lt;ul&gt;
          &lt;li&gt;Tuy nhiên cũng sẽ không có chức năng quản lý đi kèm hoặc team nhóm.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Nếu mua theo đơn vị department hoặc institution.
        &lt;ul&gt;
          &lt;li&gt;Nếu mua theo department thì vì chỉ có 40 người mà yêu cầu là 100 người nên sẽ phải mua chung với nhóm khác. Trong trường hợp có mua chung thì mỗi sinh viên chỉ tốn 12 đô, 1 tháng 40 sinh viên mất có 480 đô, 1 năm mất có 5-6k.&lt;/li&gt;
          &lt;li&gt;Nếu mua theo named-user license thì cứ mỗi user mỗi tháng mất 34.99 đô, 1 tháng 40 users mất $34.99\times 40=1399.6$ đô, 1 năm mất tầm 16-17k.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Nếu mua theo device thì giả sử cứ 5 người dùng chung 1 device (1 tuần luân phiên nhau dùng) thì cần 8 máy, mỗi máy 1 năm mất 330 đô, cả nhóm phải chi cho 8 máy 1 năm tầm $330\times 8=2640$ đô.&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nhìn chung là nếu cài theo máy thì sẽ rẻ nhất (cả năm mất có dưới 3k đô).
Thì vì có người cần dùng khi đi công tác nên sẽ phải có cả máy bàn và máy laptop: 4 máy bàn và 4 máy laptop có cài Adobe Creative Cloud.
Một máy giờ mua cũng tầm 2500 đô, dùng tầm 5 năm thì thay. Thì tổng tiền máy cần đầu tư là $2500\times 8=20000$ đô cho 5 năm.
Vấn đề là sẽ phát sinh thêm công quản lý máy móc: ví dụ khi đi công tác mượn máy lap có cài Adobe thì quy chế ra sao, …
Thì vì lab có 40 sinh viên, năm nào cũng có sinh viên mới nên chuyện có 4-5 cái máy bàn và mỗi sinh viên có 1 cái laptop là chuyện bình thường.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ca khó khăn là trường hợp lab trong cty thôi&lt;/strong&gt; vì Adobe không cung cấp gói device cho bên cty nên phòng nghiên cứu bên cty mà 40 người đều dùng thì mất 20k/năm là chuyện bình thường.
Thế nên lại đòi hỏi giải pháp ở đây:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Đẩy phần nghiên cứu đồ họa này về các trường đại học để giảm giá&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Thì nếu đẩy được thì trong phòng bạn lãnh đạo chỉ cần giữ lại khoảng 4-5 ông làm về đồ họa nhưng cũng chỉ là các ông kiểu PM/Techlead chứ dev là để bên đại học họ lấy sinh viên làm cho (có khi các em nó còn làm không công để tốt nghiệp ấy chứ), còn lại cho đi làm dự án khác.&lt;/li&gt;
      &lt;li&gt;Phần dự án cần nhiều license của Adobe thì thôi đẩy cho mấy ông thày bên đại học lo. Mình hợp tác thôi vì bên đại học họ mua license được số lượng lớn giá rẻ.&lt;/li&gt;
      &lt;li&gt;Chuyện hợp tác đương nhiên cũng phải có trả lương nhất định cho các bên, nhưng chắc chắn bên đại học thì mình sẽ phải chi ít hơn (sinh viên bỏ vào thì có phải trả đâu mà có trả cũng ít hơn nhân viên chuyên nghiệp trong các cty tập đoàn).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Tóm lại là nên đẩy những mảng nghiên cứu này về phía đại học vì nhiều lý do&lt;/strong&gt; và phương châm là thúc đẩy hợp tác giữa hai giới cty và học thuật.
Vầ tất nhiên người được lợi cuối cùng chỉ có bên giới công nghiệp và các sếp thôi.
Các bạn thử ngẫm mà xem:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bên Adobe bán license thì dù thế nào tiền vẫn vào túi họ nhé.&lt;/li&gt;
  &lt;li&gt;Bên cty muốn hợp tác thì tiền chắc chắn họ tiết kiệm được một khoản khá nhờ đẩy việc vào đại học nhé.&lt;/li&gt;
  &lt;li&gt;Các sếp cũng được thêm tiền dự án.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thế nên là tốt nhất cứ đi làm, còn đi nghiên cứu thì phải đòi thù lao nó xứng đáng các em ạ.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Về mặt maintain thì nhìn chung nên dùng gói Cloud, có gì nó cập nhật luôn mà vẫn tính trong tiền hàng tháng như vậy.&lt;/strong&gt;
Còn các mục audit quản lý khác thì nằm trong gói do Adobe cung cấp rồi.&lt;/p&gt;

&lt;h2 id=&quot;lựa-chọn-gói-nào-cho-games&quot;&gt;Lựa chọn gói nào cho games&lt;/h2&gt;
&lt;p&gt;Thi thoảng vẫn có một số nghiên cứu liên quan tới games.
Ví dụ như nghiên cứu để tìm ra 1 framework mới giúp việc thiết kế games trở nên mô phỏng gần với hành vi người dùng hơn:&lt;/p&gt;

&lt;iframe src=&quot;https://view.officeapps.live.com/op/embed.aspx?src=http%3A%2F%2Falgorithmancy%2E8kindsoffun%2Ecom%3A80%2FMDAnwu%2Eppt&amp;amp;wdAr=1.3333333333333333&quot; width=&quot;100%&quot; height=&quot;480px&quot; frameborder=&quot;0&quot;&gt;This is an embedded &lt;a target=&quot;_blank&quot; href=&quot;https://office.com&quot;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&quot;_blank&quot; href=&quot;https://office.com/webapps&quot;&gt;Office&lt;/a&gt;.&lt;/iframe&gt;

&lt;p&gt;Thì về phân tích nhu cầu thì nó lại thế này: nhân viên của mình thì không dùng mà mua về để cho &lt;strong&gt;những người tham gia thí nghiệm&lt;/strong&gt; dùng.
Những người tham gia thí nghiệm sẽ được tuyển chọn dựa trên base của thí nghiệm thôi.
Thì lại phải mua cho những người tham gia ấy chơi.
Tức là không phải mua cho nhân viên mình chơi mà là cho &lt;strong&gt;đối tượng thí nghiệm chơi&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Thì tùy vào &lt;strong&gt;setup của thí nghiệm&lt;/strong&gt; mà có khi phải mua cả bộ Xbox hoặc Playstation.
Có khi phải mua bản Gold hoặc bản Ultimate chứ bản Standard cũng ko được.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://store.ubi.com/us/game/?lang=en_US&amp;amp;pid=5e84a5065cdf9a21c0b4e737&amp;amp;dwvar_5e84a5065cdf9a21c0b4e737_Platform=pcdl&amp;amp;edition=Ultimate%20Edition&amp;amp;source=detail&quot;&gt;Bảng giá Assassin’s Creed Valhalla&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tuy nhiên, cũng cần nhớ nếu mình chọn đối tượng thí nghiệm là students ấy thì sẽ được discount 15%: &lt;a href=&quot;https://store.ubi.com/us/student-discount.html?lang=en_US&quot;&gt;https://store.ubi.com/us/student-discount.html?lang=en_US&lt;/a&gt;
Bao giờ student cũng được discount nên chọn loại đối tượng này thì sẽ tốt hơn.
Hoặc có khi mình có thể setup thí nghiệm là chỉ khảo sát &lt;strong&gt;những đối tượng nào đã chơi game trên thôi&lt;/strong&gt; thì chuyện mua game là của họ, mình chỉ cần tìm ra họ thôi.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tuy nhiên là đôi khi là nhu cầu thí nghiệm đòi monitor cả quá trình chơi game của đối tượng thì lại phải mua cho họ games để họ chơi.&lt;/strong&gt;
Trong trường hợp phải mua cho họ chơi ấy, thì nên chọn student để có discount.&lt;/p&gt;

&lt;h2 id=&quot;lựa-chọn-gói-truyền-hình-gì&quot;&gt;Lựa chọn gói truyền hình gì?&lt;/h2&gt;

&lt;p&gt;Đồ họa và games coi như là hai nhánh chính thống của Computer Vision.
Một phần nào đó thì cũng là nơi mà CV thực sự được ứng dụng nghiêm chỉnh.&lt;/p&gt;

&lt;p&gt;Chúng ta ít khi nghĩ đến những đề tài nghiên cứu về truyền hình nhưng thực ra là cũng liên quan kha khá đấy.
Đơn cử là có những nghiên cứu về &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;interactive television&lt;/code&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Truyền hình tương tác&lt;/strong&gt; là một dạng truyền hình cho phép người xem tham gia, điều khiển các chương trình truyền hình. Với dạng truyền hình truyền thống, đường truyền truyền hình là một chiều. Các nhà đài cho phép khán giả xem gì, vào giờ nào, trên kênh nào là quyền của họ.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://b-i.forbesimg.com/onmarketing/files/2013/05/300x201.jpg&quot; alt=&quot;interactive TV&quot; /&gt;
&lt;em&gt;Source: &lt;a href=&quot;https://www.forbes.com/sites/onmarketing/2013/05/20/guess-what-marketers-interactive-tv-is-actually-here/?sh=2ac8a4f0778a&quot;&gt;Forbes&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Thì cũng như đồ họa và games, chủ yếu là nghiên cứu nhận kinh phí từ các nguồn dồi dào có sẵn, được đặt hàng hẳn hoi.
Và họ cũng chủ yếu nghiên cứu về &lt;strong&gt;trải nghiệm người dùng&lt;/strong&gt; thôi.
Không cần code kiếc gì đâu, chỉ cần làm mấy thí nghiệm để kiểm chứng giả thuyết về trải nghiệm người dùng.&lt;/p&gt;

&lt;p&gt;Sẽ phải mua sắm màn hình cảm ứng touch, có kết nối truyền hình.
Sau đó là setup một căn phòng và cho người dùng trải nghiệm theo kịch bản cho sẵn.
Thế là sẽ phải mua một gói truyền hình cho thí nghiệm, nhưng như thế thì sẽ không dùng thường xuyên, và phải trả theo tháng thì mua kèm Internet cáp.
Tốt nhất là nên lên kế hoạch thí nghiệm và chốt thời gian tổ chức để chỉ mua gói truyền hình tại tháng ấy thôi.&lt;/p&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;
&lt;p&gt;Nhìn chung, &lt;strong&gt;kỹ năng quản lý license&lt;/strong&gt; là một kỹ năng không thể thiếu để quản trị một nhóm tầm trong quy mô 50-100 nhân viên.
Ở quy mô này trở lên, bắt đầu phải có những điểm lưu ý và thảo luận để quản lý linh hoạt chủ động mà lại tối ưu.&lt;/p&gt;

</description>
        <pubDate>Sat, 25 Dec 2021 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/subcription-optimize/</link>
        <guid isPermaLink="true">https://wanted2.github.io/subcription-optimize/</guid>
        
        <category>Adobe</category>
        
        <category>Adobe Creative Cloud</category>
        
        <category>Games</category>
        
        <category>Subscriptions</category>
        
        <category>teamlead</category>
        
        <category>techlead</category>
        
        <category>supervisor</category>
        
        <category>assistant</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Software Engineering</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>Adobe Creative Cloud: An All-in-One Platform for Creators</title>
        <description>&lt;p&gt;Adobe provides three different cloud solutions for customers: Adobe Creative Cloud for creators, Adobe Experience Cloud for marketers, and Adobe Document Cloud for business.
Creative Cloud is a collection of 20+ apps for photography, video, design, web, UX, and social media — plus integrated essentials like font families and the power to collaborate with anyone, anywhere. 
Many things can be done with this solution.
We take a look at the features.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;adobe-creative-cloud&quot;&gt;Adobe Creative Cloud&lt;/h1&gt;

&lt;p&gt;Adobe Creative Cloud (ACC) is an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all-in-one&lt;/code&gt; platform for creators, marketers, visual artists, and designers.
Let’s hear words about the roadmap of ACC from Scott Belsky.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/BzFY4pzb8cA?start=730&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This was highlighted in Adobe MAX 2020, a trade show event for creative users held by Adobe from October 20 to 22, 2020.
You can access the online tutorials from &lt;a href=&quot;https://experienceleague.adobe.com/docs/creative-cloud-enterprise-learn/cce-learning-hub/max2020/tutorials/maxtutorials.html?lang=en&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Creativity for All&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3-goals-of-adobe-creative-cloud&quot;&gt;3 Goals of Adobe Creative Cloud&lt;/h2&gt;

&lt;h3 id=&quot;enable-connected-creativity&quot;&gt;Enable Connected Creativity&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Creativity $\times$ Organization = Impact&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Putting creativity into an organization makes an impact!
Three pillars of your organization are: &lt;strong&gt;Assets, Apps, and Team&lt;/strong&gt;.
Let’s talk about the assets first.
How can you share your assets with your collaborators?
Adobe Behance, Adobe Bridge, and Adobe Stock make your assets sharable and organized.
You can share your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.psd&lt;/code&gt; files with your teammates, and you can search for your interested images from million of shared assets in Adobe Stock and Bridge.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Bringing 30 years of Photoshop features and code to a new platform like the web is a journey.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Photoshop has more than 30 years of history.
That is so long to have an impact, but now Adobe Photoshop is not only a desktop app.
Adobe Photoshop for Web is in Beta now and will be available in ACC soon.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Work better together with Adobe Creative Cloud.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Finally, let’s talk about the team, the last pillar.
Adobe provides Spaces and Canvas for collaboration.
You can not only share the PSD and AI files with your collaborators but also work with them directly in ACC.
Adobe Cloud Canvases provide an overview of the whole team’s work.
Users can make comments, can get &lt;strong&gt;real-time&lt;/strong&gt; feedbacks in Canvases.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;And don’t forget the devices. &lt;strong&gt;Adobe apps like Photoshop in Apple’s M1 chip run twice faster than in older chips.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;With ACC, your assets and apps can be synchronized between your devices.
You can start your design from your iPhone and then grab it on your iPad or other devices.
And you can collaborate with your co-workers from any device as well.&lt;/p&gt;

&lt;h3 id=&quot;unleash-creative-potential&quot;&gt;Unleash Creative Potential&lt;/h3&gt;

&lt;p&gt;Web designers can use Adobe XD, Illustrators, and so on to create ideas.
Artists can use Fresco, Design to create a lot of drawings.
Photographers and video makers can capture and use Adobe Photoshop and After Effects to create images and videos for social posts.
Cartoon artists can use Character Animator to create a live anime.
Other people can use Lightroom or Premiere Rush to edit photos/videos on the mobile phone.
Other people can search for photos, AI files, videos, and even anime and manga in Stock and Behance.
Some can scan document images and get the content indexed and managed in Adobe Acrobat.
So many possibilities can be done with this suite of apps.&lt;/p&gt;

&lt;p&gt;And now, all of them are available in the Cloud.&lt;/p&gt;

&lt;h3 id=&quot;empower-creative-careers&quot;&gt;Empower Creative Careers&lt;/h3&gt;

&lt;p&gt;Adobe also launched &lt;strong&gt;Content Authenticity Initiate (CAI)&lt;/strong&gt; in 2018 to ensure artists get credits for their work.
Any content made in Adobe today will be attached metadata about the project they belong to, and the credits always go to the creators.
&lt;strong&gt;Non Fungible Tokens (NFT)&lt;/strong&gt; are generally used by artists recently to access marketplaces of collectors.
However, many artists have seen their works were minted without attribution to the original artists.
Adobe has worked with many NFT partners such as OpenSea, KnownOrigin, SuperRare, and so on to ensure the credits will be given to the right people.
When an artist creates a project in Photoshop with Content Credentials, then that information will be displayed in any related NFT partners.&lt;/p&gt;

&lt;h1 id=&quot;toolsets&quot;&gt;Toolsets&lt;/h1&gt;

&lt;p&gt;Now, we understand the goals and roles of ACC in creative activities.
We will take a closer look at each of the app in this section.&lt;/p&gt;

&lt;h2 id=&quot;image-editing&quot;&gt;Image editing&lt;/h2&gt;

&lt;h3 id=&quot;photoshop-vs-lightroom-lightroom-classic&quot;&gt;Photoshop vs. Lightroom (Lightroom classic)&lt;/h3&gt;

&lt;p&gt;Photoshop has more than 30 years of history.
A brief overview of released versions can be found &lt;a href=&quot;https://adobe.fandom.com/wiki/Adobe_Photoshop_release_history&quot;&gt;here&lt;/a&gt;.
The latest version is CC 2021, and we mostly use it.
Adobe Photoshop is specialized to photo enthusiasts who want to make beautiful images from camera-captured inputs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://learningavphotoshop.weebly.com/uploads/2/6/4/6/26460741/4580840_orig.jpg&quot; alt=&quot;pscc&quot; /&gt;
&lt;em&gt;Source: &lt;a href=&quot;https://learningavphotoshop.weebly.com/uploads/2/6/4/6/26460741/4580840_orig.jpg&quot;&gt;learningavphotoshop&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There are some resources in YouTube for you to learn Photoshop (especially, CC 2020):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLyGOksFjYo-nzGtyi6wFs68NpDic1lg9a&quot;&gt;Adobe Photoshop CC 2020 Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL9pkETrdJ0rb-BsDHwE0gmsj0duEXqbQ3&quot;&gt;Adobe Photoshop Tutorials&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLttcEXjN1UcFpeLZXSLtNU5iT2OxkO2nF&quot;&gt;Photoshop CC Essentials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on a rough estimation, there is a requirement of at least two years to comprehend this Photoshop CC.
There are too many features to learn, but think about the 30-year history of Photoshop: you only spend two years to learn 30 years of Photoshop features and code.
What a benefit!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Photoshop is a comprehensive photo editing app for photographers, web designers, visual artists, and photo enthusiasts.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, what’s about Lightroom?
While &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tools&lt;/code&gt; is essential in Photoshop, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Presets&lt;/code&gt; is primary in Lightroom.
In Photoshop, you can tune your images at a detailed level using fine-grained tools.
But it is completely different in Lightroom, where the presets hold only common features which are known to sufficiently produce good quality images.
Because Photoshop is specific for details, it is often used by professional photographers.
But with Lightroom, the user segment can be large.
Because presets were tuned by professionals, and the tuned parameters can be huge (millions to billions of parameters).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Photoshop is best for professionals who want to tune every detail of a single photo, while Lightroom presets are good for unknowledgeable users who want to tune a batch of hundred images without specialized knowledge about graphics.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Adobe-Lightroom.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While Lightroom Classic is a desktop app, Lightroom is a web app with the address: &lt;a href=&quot;https://lightroom.adobe.com/&quot;&gt;lightroom.adobe.com&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;illustrator&quot;&gt;Illustrator&lt;/h3&gt;

&lt;p&gt;If Photoshop CC is a graphic editing app, Illustrator CC is a creator app.
It can be used to create industry-standard vector artworks like logos, illustrations, and posters.
A beginner course is available on YouTube:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLYfCBK8IplO4X-jM1Rp43wAIdpP2XNGwP&quot;&gt;Learn Adobe Illustrator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLwnXQvUxjbNsvCG_rJJ8vzhbqtEjphKsf&quot;&gt;Adobe Illustrator Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://clubsintel.weebly.com/uploads/1/2/3/8/123847818/575683558.jpg&quot; alt=&quot;illustrator cs 5 tools&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Based on a rough estimation, it may take six months to a year to comprehend Adobe Illustrator (AI).
Then you can create professional artworks which can be used in web designs or anywhere they need AI.&lt;/p&gt;

&lt;h3 id=&quot;fresco-for-fun-drawing&quot;&gt;Fresco for fun drawing&lt;/h3&gt;

&lt;p&gt;Similar to Adobe Illustrator but with a friendlier interface and limited toolset, Fresco provides basic features for drawing raster artworks.
Fresco can be used in mobile devices like the iPhone and iPad.
Because it drops the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;presets&lt;/code&gt; notion, Fresco is close to end-users who are unknowledgeable about art.
In contrast, Illustrator is closer to artists and graphics professionals.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/YmPZtFl_j50&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;There are some free courses available on YouTube about Fresco.
If you are a normal end-user, you should like Lightroom and Fresco.
But if you are a professional, you may need Photoshop and Illustrator.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL_dhPga7ruufKDzs1yCPfNUb6Ud5M58so&quot;&gt;Adobe Fresco Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dimension-for-3d-photography&quot;&gt;Dimension for 3D photography&lt;/h3&gt;

&lt;p&gt;In this era of VR/AR applications, artists and producers need a tool to create 3D graphical models.
The models can be saved and viewed from different perspectives.&lt;/p&gt;
&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/bM-3PydkMBg&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL431Fy2ZWI6fqRGdUsKT5KBCmmI3A55dR&quot;&gt;Adobe Dimension Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Needless to say, you take advantage if you are already familiar with &lt;strong&gt;object-oriented design (OOD)&lt;/strong&gt;, because everything here is only 3D &lt;strong&gt;objects&lt;/strong&gt;!
Then you can be quite familiar with the workflow like selecting a template object, changing the &lt;strong&gt;Properties&lt;/strong&gt; of the object, then defining its &lt;strong&gt;Scene&lt;/strong&gt; and &lt;strong&gt;Actions&lt;/strong&gt;.
Adobe Dimension integrates Adobe Stock and Behance as its cloud storage and management solutions.
You can save your models to Stock or share assets to Behance.&lt;/p&gt;

&lt;h2 id=&quot;video-editing&quot;&gt;Video editing&lt;/h2&gt;

&lt;p&gt;Unlike photos, videos need care about motions, perspectives, and so on.&lt;/p&gt;

&lt;h3 id=&quot;premiere-pro-and-rush&quot;&gt;Premiere Pro and Rush&lt;/h3&gt;
&lt;p&gt;With Premiere Pro, users can edit their video durations, change replaying orders, image processing, edit audio, and so on with videos.
Rush is for mobile.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/f7prDfwtMCo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLttcEXjN1UcGF_PCDUcQw2WOoh6MZtvWt&quot;&gt;Adobe Premiere Pro Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some nice features can be &lt;strong&gt;multi-perspective videos&lt;/strong&gt; and &lt;strong&gt;video-audio alignment&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;after-effects&quot;&gt;After Effects&lt;/h3&gt;

&lt;p&gt;Unlike Premiere Pro is for professionals, After Effects has a smaller video editing toolset but yet enough for non-professional users.
Some straight modifications can be quite clunky with Premiere Pro.
In these scenarios, After Effects seems to be a better fit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://d2436y6oj07al2.cloudfront.net/assets/vbblog/2016/09/4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FuJMHF510mc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLYfCBK8IplO77FDDLnS06qEMoVLD7Qyib&quot;&gt;Affter Effects course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;audio-editing&quot;&gt;Audio editing&lt;/h2&gt;

&lt;p&gt;We had a look on image/video editing softwares.
But now we want to edit, mix, enhance audio, we should think about Adobe Audition.
Adobe Audition can be integrated into After Effects or Premiere Pro.
The output audios can be published to Behance and Stock.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/adobe-audition.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/0njJIOjxS4c&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL-ssKAXBXMQRzqN5sc1eaR_Q6nACRQDF3&quot;&gt;Adobe Audition course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;storage-and-management&quot;&gt;Storage and Management&lt;/h2&gt;

&lt;h3 id=&quot;adobe-stock-and-adobe-bridge&quot;&gt;Adobe Stock and Adobe Bridge&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Adobe Stock&lt;/strong&gt;, formerly known as Adobe Stock Photos, is a stock image database that was originally integrated with Adobe Bridge in Adobe Creative Suite 2 and 3. It is presently available through certain subscription packages from Adobe Creative Cloud.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Users can access Adobe Stock from &lt;a href=&quot;https://stock.adobe.com/&quot;&gt;https://stock.adobe.com/&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Adobe Bridge&lt;/strong&gt; is a digital asset management tool developed by Adobe. It is freely included as part of Adobe Creative Cloud and was first made available with Creative Suite 2. Its primary purpose is to link the parts of the Creative Suite together using a format similar to the file browser found in previous versions of Adobe Photoshop. It is accessible from all other components of the Creative Suite (except for the standalone version of Adobe Acrobat 8).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bridge is also included with the stand-alone Photoshop application, and can perform certain Photoshop processing functions separately (and simultaneously) with Photoshop itself.&lt;/p&gt;

&lt;h3 id=&quot;media-encoder&quot;&gt;Media Encoder&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Adobe Media Encoder (AME)&lt;/strong&gt; is a video media transcoding utility developed and marketed by Adobe through Adobe Creative Cloud. 
It is designed for integration into workflows with other video applications such as Adobe Premiere Pro and After Effects.&lt;/p&gt;

&lt;h2 id=&quot;animation&quot;&gt;Animation&lt;/h2&gt;

&lt;h3 id=&quot;adobe-animate-cc&quot;&gt;Adobe Animate CC&lt;/h3&gt;

&lt;p&gt;Adobe Animate CC, formerly Adobe Flash Professional CC, is the current series of the Animate application offered by subscription through Adobe Creative Cloud.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/fFRtT-wDZlE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL3atG_J6wz0Vv9AoXqXOgc63X_8D5OBHE&quot;&gt;2D Animation - Free Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adobe-character-animator&quot;&gt;Adobe Character Animator&lt;/h3&gt;

&lt;p&gt;Adobe Character Animator is a 2D puppet animation program developed by Adobe that is included with Adobe After Effects CC through Adobe Creative Cloud.
It uses a &lt;strong&gt;multi-track motion capture recording system&lt;/strong&gt; to apply behaviors to puppets.&lt;/p&gt;

&lt;p&gt;Some nice thing can be done with Character Animator:&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/gWBLAcTWH5Q&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;web-design-tools&quot;&gt;Web design tools&lt;/h2&gt;

&lt;h3 id=&quot;adobe-dreamweaver&quot;&gt;Adobe Dreamweaver&lt;/h3&gt;

&lt;p&gt;Adobe Dreamweaver, formerly Macromedia Dreamweaver, is a web development application originally based on the codebase of Backstage, which was acquired by Macromedia in March 1996.
Dreamweaver has been owned and marketed by Adobe since December 2005.&lt;/p&gt;

&lt;p&gt;Dreamweaver is available for both Mac and Windows. Recent versions have incorporated support for web technologies such as CSS, JavaScript, and various server-side scripting languages and frameworks, including ASP.NET, ColdFusion, JavaServer Pages, and PHP.&lt;/p&gt;

&lt;p&gt;As a WYSIWYG Presto-based editor, Dreamweaver can hide the HTML code details of pages from the user, &lt;strong&gt;making it possible for non-coders to create web pages and sites&lt;/strong&gt;. One criticism of this approach is that it can produce HTML pages whose file size and amount of HTML code can be larger than an optimally hand-coded page would be, which can cause web browsers to perform poorly. This can be particularly true because the application makes it very straightforward to create table-based layouts. In addition, some website developers have criticized Dreamweaver in the past for producing code that often does not comply with W3C standards, though recent versions have been more compliant. Dreamweaver 8.0 performed poorly on the Acid2 Test, developed by the Web Standards Project. However, Adobe has increased the support for CSS and other ways to layout a page without tables in later versions of the application, with the ability to convert tables to layers and vice versa.&lt;/p&gt;

&lt;p&gt;Dreamweaver allows users to preview websites in many browsers, provided that they are installed on their computers.
It also has some site management tools, such as the ability to find and replace lines of text or code by whatever parameters specified across the entire site and a templatization feature for creating multiple pages with similar structures. 
The behaviors panel also enables the use of basic JavaScript without any coding knowledge.&lt;/p&gt;

&lt;p&gt;Dreamweaver can use “Extensions” – small programs, which any web developer can write (usually in HTML and JavaScript). Extensions provide added functionality to the software for whoever wants to download and install them. Dreamweaver is supported by a large community of extension developers who make extensions available (both commercial and free) for most web development tasks, from straight rollover effects to full-featured shopping carts.&lt;/p&gt;

&lt;p&gt;Like other HTML editors, Dreamweaver edits files locally, then uploads all edited files to the remote web server using FTP, SFTP, or WebDAV. Dreamweaver CS4 now supports the Subversion (software) (SVN) version control system.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Ok60gdJif0U&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLttcEXjN1UcHi3Rc-FSNZOaFSsSjFWCBS&quot;&gt;Dreamweaver Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adobe-xd&quot;&gt;Adobe XD&lt;/h3&gt;
&lt;p&gt;Adobe XD is a vector-based user experience design and prototyping system.
It is also a presentation tool for web apps and mobile apps, which is developed and published by Adobe.
It is available for free but requires an account registered through Adobe Creative Cloud.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/68w2VwalD5w&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLOTakqGKI6lcIRHkjCsVyxmg8lweyewmR&quot;&gt;Adobe XD Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;content-creation&quot;&gt;Content Creation&lt;/h2&gt;
&lt;h3 id=&quot;adobe-indesign&quot;&gt;Adobe InDesign&lt;/h3&gt;
&lt;p&gt;Adobe InDesign is a desktop publishing and page layout designing software application produced by Adobe Inc. It can be used to create works such as posters, flyers, brochures, magazines, newspapers, presentations, books, and ebooks. InDesign can also publish content suitable for tablet devices in conjunction with Adobe Digital Publishing Suite. Graphic designers and production artists are the principal users, creating and laying out periodical publications, posters, and print media. It also supports export to EPUB and SWF formats to create ebooks and digital publications, including digital magazines and content suitable for consumption on tablet computers. In addition, InDesign supports XML, style sheets, and other coding markups, making it suitable for exporting tagged text content for use in other digital and online formats. The Adobe InCopy word processor uses the same formatting engine as InDesign.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/RXRT3dHu6_o&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;adobe-incopy&quot;&gt;Adobe InCopy&lt;/h3&gt;
&lt;p&gt;Adobe InCopy is a professional word processor made by Adobe Inc. that integrates with Adobe InDesign. While InDesign is used to publish printed material, including newspapers and magazines, InCopy is used for general word processing. The software enables editors to write, edit, and design documents. The software includes standard word processing features such as spell check, track changes, and word count and has various viewing modes that allow editors to visually inspect design elements — only as it looks to the designer working in Adobe InDesign.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/zq5YpOIA13E&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;miscellaneous&quot;&gt;Miscellaneous&lt;/h2&gt;

&lt;h3 id=&quot;acrobat&quot;&gt;Acrobat&lt;/h3&gt;
&lt;p&gt;Adobe Acrobat is a family of computer programs developed by Adobe Systems, designed to view, create, manipulate and manage files in Adobe’s Portable Document Format (PDF). Some software in the family is commercial, and some are free of charge. Adobe Reader (formerly Acrobat Reader) is available as a no-charge download from Adobe’s website and allows the viewing and printing of PDF files. Acrobat and Reader are widely used as a way to present information with a fixed layout similar to a paper publication.&lt;/p&gt;

&lt;p&gt;As of June 2020, the main members of the Adobe Acrobat family are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Adobe Acrobat DC, sold only by subscription through Adobe Document Cloud with mobile app support.
    &lt;ul&gt;
      &lt;li&gt;Adobe Acrobat Standard DC&lt;/li&gt;
      &lt;li&gt;Adobe Acrobat Pro DC includes the ability to convert scanned documents and add media files&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adobe Acrobat 2020, the last version sold through a one-time perpetual license, but without cloud features.
    &lt;ul&gt;
      &lt;li&gt;Adobe Acrobat Standard 2020&lt;/li&gt;
      &lt;li&gt;Adobe Acrobat Pro 2020&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adobe Acrobat Reader DC, a free client version with the ability to print, sign, and annotate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;aero&quot;&gt;Aero&lt;/h3&gt;
&lt;p&gt;Adobe Aero is an augmented reality authoring and publishing tool developed by Adobe and marketed through Adobe Creative Cloud.
Aero was originally announced as a private beta for iOS users at Adobe MAX 2018.
It was officially launched during Adobe MAX 2019.&lt;/p&gt;

&lt;p&gt;Aero is part of Adobe’s 3D &amp;amp; AR series, which includes Adobe Dimension, Fuse, and Substance.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/fo8aG0vCY7k&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;adobe-sensei-the-graphical-ai&quot;&gt;Adobe Sensei: the graphical AI&lt;/h1&gt;
&lt;p&gt;Adobe Sensei is an artificial intelligence and machine learning technology being developed by Adobe. It is being applied to Adobe Analytics, Campaign, and Target. Sensei technology is also used in subject selection and removal, as seen in recent versions of Adobe Photoshop and Photoshop on iPad.&lt;/p&gt;

&lt;p&gt;Many Adobe products like Photoshop, Premiere Pro, Illustrator, and Stock use somewhat Sensei’s insight to enhance user experience.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/r9E0_lM5l48&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;We took a look at the Adobe Creative Cloud ecosystem.
Like most other commercial clouds like Oracle Cloud, AWS, Google GCP, or M$ Azure, it has many products and services which make our life better.
And Adobe Sensei AI is impressive as it integrates many of the latest technologies in Computer Vision, Machine Learning, and Artificial Intelligence.
Hope to use it in our own products soon.&lt;/p&gt;
</description>
        <pubDate>Sun, 19 Dec 2021 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/adobe-creative-cloud/</link>
        <guid isPermaLink="true">https://wanted2.github.io/adobe-creative-cloud/</guid>
        
        <category>adobe acrobat</category>
        
        <category>adobe creative cloud</category>
        
        <category>design</category>
        
        <category>illustrator</category>
        
        <category>adobe sensei</category>
        
        <category>adobe photoshop</category>
        
        <category>adobe lightroom</category>
        
        <category>adobe premiere pro</category>
        
        <category>premiere rush</category>
        
        <category>charactor animation</category>
        
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
        <category>Computer Vision</category>
        
      </item>
    
  </channel>
</rss>
