<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AiFi</title>
    <description>An AI Researcher&apos;s blog (This is a staging site, so the content may be imprecise, refer to official AiFi)</description>
    <link>https://wanted2.github.io/</link>
    <atom:link href="https://wanted2.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 06 Jun 2022 00:44:14 +0900</pubDate>
    <lastBuildDate>Mon, 06 Jun 2022 00:44:14 +0900</lastBuildDate>
    <generator>Jekyll v4.2.1</generator>
    
      <item>
        <title>Federated Learning: concerns about user data privacy and security</title>
        <description>&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#aledhari2020federated&quot;&gt;[1, 2, 3, 4, 5]&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;aledhari2020federated&quot;&gt;Aledhari, M., Razzak, R., Parizi, R.M. and Saeed, F. 2020. Federated learning: A survey on enabling technologies, protocols, and applications. &lt;i&gt;IEEE Access&lt;/i&gt;. 8, (2020), 140699–140725.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/aledhari2020federated/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;liu2020federated&quot;&gt;Liu, Y., Yuan, X., Xiong, Z., Kang, J., Wang, X. and Niyato, D. 2020. Federated learning for 6G communications: Challenges, methods, and future directions. &lt;i&gt;China Communications&lt;/i&gt;. 17, 9 (2020), 105–118.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/liu2020federated/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;khan2021federated&quot;&gt;Khan, L.U., Saad, W., Han, Z., Hossain, E. and Hong, C.S. 2021. Federated learning for internet of things: Recent advances, taxonomy, and open challenges. &lt;i&gt;IEEE Communications Surveys &amp;amp; Tutorials&lt;/i&gt;. (2021).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/khan2021federated/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kaissis2020secure&quot;&gt;Kaissis, G.A., Makowski, M.R., Rückert, D. and Braren, R.F. 2020. Secure, privacy-preserving and federated machine learning in medical imaging. &lt;i&gt;Nature Machine Intelligence&lt;/i&gt;. 2, 6 (2020), 305–311.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kaissis2020secure/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nguyen2021federated&quot;&gt;Nguyen, D.C., Ding, M., Pathirana, P.N., Seneviratne, A., Li, J. and Poor, H.V. 2021. Federated learning for internet of things: A comprehensive survey. &lt;i&gt;IEEE Communications Surveys &amp;amp; Tutorials&lt;/i&gt;. (2021).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021federated/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Sun, 05 Jun 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/federated-learning/</link>
        <guid isPermaLink="true">https://wanted2.github.io/federated-learning/</guid>
        
        <category>Machine learning</category>
        
        <category>TPU</category>
        
        <category>edge computing</category>
        
        <category>edge server</category>
        
        <category>edge devices</category>
        
        <category>edge-cloud architectures</category>
        
        <category>cloud computing</category>
        
        <category>6g</category>
        
        <category>federated learning</category>
        
        <category>distributed computing</category>
        
        <category>internet of things</category>
        
        <category>iot</category>
        
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>Ăn gì ở Hà Nội: Luận về thói quen ăn uống của người Hà Nội</title>
        <description>&lt;p&gt;Hà Nội là điểm đến cho nhiều nhà kinh doanh cũng như du khách trong những năm gần đây.
Nếu chỉ là một chuyến du lịch ngắn ngày việc trải nghiệm ẩm thực Hà thành có thể rất đơn giản như đi theo lịch trình của tour ăn một vài bữa đơn giản (lại phở rồi nem cuốn Harumaki, xoay đi xoay lại có vài món mà hay đem ra quảng cáo cho nước ngoài xem).
Tuy nhiên, nếu là khách kinh doanh phải ghé lại lâu dài (nửa năm tới vài năm) thì việc tìm hiểu về nếp sống, bao gồm thói quen ăn uống, của dân địa phương là điều nên làm nhằm &lt;strong&gt;chủ động&lt;/strong&gt; trong ăn uống sinh hoạt.
Trong bài viết này tôi sẽ điểm qua vài bài báo khoa học có thống kê chuẩn chỉnh về thói quen ăn uống của người Hà Nội, trả lời những câu hỏi sau làm cơ sở để lựa chọn cách hòa nhập vào ẩm thực Hà thành cho du khách tứ phương.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Người Hà Nội đi chợ ở đâu? (Chợ truyền thống, cửa hàng mặt phố, siêu thị, hay đâu?)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Người Hà Nội thích ăn tại nhà (eat-in), ra quán (eat-out), hay đặt đồ ăn sẵn (ready-made meals) về?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cuối bài báo sẽ có thêm một danh sách mà tôi nghĩ nên ăn khi đến Hà Nội sống lâu dài nhằm trả lời câu hỏi &lt;strong&gt;Ăn gì nếu ra phố ở Hà Nội?&lt;/strong&gt;.
Nói chung đều có luận cứ khoa học hẳn hoi, nên hy vọng sẽ là bài tham khảo bổ ích cho các đồng chí (dân tứ xứ trong nước cũng như nước ngoài) đến định cư ở Hà Nội lâu dài.&lt;/p&gt;

&lt;h1 id=&quot;giới-thiệu&quot;&gt;Giới thiệu&lt;/h1&gt;
&lt;p&gt;Thực ra, nếu bạn chỉ ghé thăm vài ba ngày, theo tour hoặc tự đi theo sở thích, thì tôi đồng ý các bạn cứ Google ra mấy địa điểm quán ăn, các món nổi tiếng mà nhiều khách du lịch đến mà ăn.
Đặc biệt, nếu đi theo tour du lịch, thì cứ nó cho gì ăn nấy, không phải suy nghĩ gì cho mệt!
Nói chung sẽ bị động nhưng du lịch có vài ngày vài tuần thì quan trọng gì!&lt;/p&gt;

&lt;p&gt;Nhưng nếu mà bạn đến Hà Nội và dự định ở lại ít nhất nửa năm, thì tôi khuyên các bạn nên đọc bài viết này, do chính tôi viết.
Chứ ở Hà Nội 6 tháng, thậm chí vài năm, mà chỉ quay đi quay lại vài món có trong sách du lịch, thì cũng oái oăm.
&lt;strong&gt;Tất nhiên, là với du khách nước ngoài thì tùy lòng, có người thích tìm hiểu sâu và hòa nhập sâu vào văn hóa địa phương.&lt;/strong&gt;
&lt;strong&gt;Có người thậm chí phi xe máy vù vù ra chợ mua đồ về nấu như người Việt mà dân Việt còn phải trầm trồ.&lt;/strong&gt;
&lt;strong&gt;Nhưng cũng có người ở cả chục năm, cũng chỉ nhớ được mấy món cơ bản với thi thoảng có dịp cuối tuần đi ăn thử ẩm thực Hà Nội.&lt;/strong&gt;
Thì nói chung không ép, nói chung &lt;strong&gt;không cần phải nấu, chỉ cần bỏ công ăn cho biết món Hà Nội là được rồi&lt;/strong&gt;.
Nấu thì các anh cứ nấu mấy món quen miệng các anh là được rồi, còn thi thoảng ra quán ăn ủng hộ ẩm thực địa phương là OK rồi!&lt;/p&gt;

&lt;p&gt;Thì cũng gọi là “nói có sách, mách có chứng”, tôi cũng mạn phép trích dẫn khá nhiều bài báo để trả lời câu hỏi về thói quen ăn uống của người Hà Nội.
Những bài báo do chính người Việt viết cho máu.
Mục tiêu của bài viết thì như đã nói ở trên là để đưa ra 1 góc nhìn khoa học có luận cứ đã được kiểm chứng, về thói quen ăn uống của người Hà Nội trong giai đoạn gần đây (từ 2008 trở lại) để du khách ở lâu tại Hà Nội có thể nắm được để ước lượng xem mình có thể hòa nhập đến đâu.
Vì về cơ bản, thì cũng là du khách thôi nên không ép, có lòng tìm hiểu và hòa nhập một “tí” (nhưng không hòa tan) là OK!&lt;/p&gt;

&lt;h1 id=&quot;thói-quen-ăn-uống-ở-hà-nội&quot;&gt;Thói quen ăn uống ở Hà Nội&lt;/h1&gt;

&lt;h2 id=&quot;người-hà-nội-đi-chợ-ở-đâu&quot;&gt;Người Hà Nội đi chợ ở đâu?&lt;/h2&gt;
&lt;p&gt;Ngoài chợ truyền thống, siêu thị ra thì &lt;strong&gt;bán hàng rong (roving street vendors, &lt;a class=&quot;citation&quot; href=&quot;#jensen2007food&quot;&gt;[1]&lt;/a&gt;)&lt;/strong&gt; là một đặc trưng về những nơi mà người dân Việt Nam nói chung, cũng như Hà Nội nói riêng có thể đi chợ.
Những gánh hàng rong này chính là 1 nét đặc trưng của phố phường Hà Nội, nhưng do khó quản lý nên có khá nhiều chính sách ra đời trong những năm 1980s và 1990s nhằm cấm hoặc giảm bớt hàng rong đã được đưa ra.
Cộng thêm việc nở rộ của nhiều loại thực phẩm từ siêu thị, các chuỗi đại lý đã tác động lớn vào đối tượng này.
Một bài điều tra từ giai đoạn 2000-2008 &lt;a class=&quot;citation&quot; href=&quot;#jensen2007food&quot;&gt;[1]&lt;/a&gt; cho thấy, trung bình người Hà Nội đi chợ nhiều nhất ở chợ truyền thống (7.0 lần/tuần, tức là hầu như ngày nào cũng đi), sau đó là các quán hàng rong (3.4 lần/tuần).
Vào giai đoạn trước 2010s, thì tần suất đi siêu thị (supermarket) của người Hà Nội khá thấp (0.9 lần/tuần).&lt;/p&gt;

&lt;p&gt;Bẵng đi chục năm, tới năm 2020, có tiếp bài điều tra của &lt;a class=&quot;citation&quot; href=&quot;#tran2020shopping&quot;&gt;[2]&lt;/a&gt; về thói quen đi chợ của người Hà Nội.
Thì so với bài năm 2008, tất nhiên là 12 năm trôi qua rồi nên tình hình nó cũng khác, ví dụ như sự xuất hiện các chuỗi &lt;strong&gt;minimarket như VinMart&lt;/strong&gt; hay chợ con (chợ họp ở góc phố, lề đường chứ không vào địa điểm do cơ quan quản lý quy định), các hệ thống cửa hàng rau sạch, thịt sạch chuyên môn.
Sự xuất hiện của các cửa hàng rau sạch, thịt sạch hay VinMart được cho là xuất phát từ sự nâng cao về thu nhập, dẫn đến ý thức về &lt;strong&gt;an toàn thực phẩm&lt;/strong&gt; được nâng cao theo.
Bài này tôi thấy khá thú vị, vì đã đưa ra được những yếu tố quan trọng trong việc đưa ra quyết định mua sắm của người tiêu dùng.
Thứ nhất, với những đối tượng nhiều kinh nghiệm mua sắm, họ không ngại chợ truyền thống, chợ con, chợ cóc hay thậm chí hàng rong.
Họ tự tin về quyết định của mình!
Nghiên cứu cũng cho thấy tầm quan trọng của &lt;strong&gt;sự tin tưởng (trust)&lt;/strong&gt; và &lt;strong&gt;an toàn thực phẩm&lt;/strong&gt; khi mua sắm ở những địa điểm đi chợ như vây.
Tuy nhiên, với những thế hệ trẻ hơn (younger generation), họ ít kinh nghiệm hơn nên thường chọn siêu thị hoặc VinMart để đảm bảo trust cũng như an toàn thực phẩm.
Và một đặc điểm của người Hà Nội đi chợ nhiều kinh nghiệm là họ sẵn sàng mặc cả (khi đi chợ truyền thống):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Indeed, the ability to bargain for better price is confirmed as a skill developed by Vietnamese consumers (Wertheim-heck et al., 2015), but this skill seems linked to generation, since it is less present in the younger generations, who by default turn to supermarkets.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kết luận lại thì về việc đi chợ, nếu không phải là dân địa phương, sống quen ở vùng đấy thì chuyện sử dụng các hệ thống siêu thị hay VinMart (nay là WinMart) là cách yên tâm hơn để đảm bảo về &lt;strong&gt;trust và an toàn thực phẩm&lt;/strong&gt;.
Còn nếu muốn tìm hiểu cho biết thì đi tham quan chợ truyền thống, chợ cóc, hay ăn hàng rong xem sao thì cũng được, thể hiện niềm yêu thích với văn hóa địa phương thắt chặt tình đoàn kết quốc tế, nhưng &lt;strong&gt;không cần thiết lắm&lt;/strong&gt;, đặc biệt với các thành phần tứ xứ, gọi chung là &lt;strong&gt;khách&lt;/strong&gt;.
Chẳng may cứ khách nước ngoài mà cố ra chợ cóc mua nhầm cái gì về ăn đi bệnh viện ngộ độc thực phẩm thì có &lt;strong&gt;nhục cả nước, nhục cả quốc thể Việt Nam&lt;/strong&gt;, nên tóm lại là thôi cứ recommend các ông cứ ra siêu thị cho chúng con nhờ.&lt;/p&gt;

&lt;h2 id=&quot;ăn-ngoài-hay-ăn-tại-nhà-hay-đặt-đồ-ăn-sẵn&quot;&gt;Ăn ngoài, hay ăn tại nhà, hay đặt đồ ăn sẵn?&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Muốn biết người ta ăn gì thì cách tốt nhất là phân tích rác của người ta.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Không hiểu có phải vì cái “lý” này mà khi tìm hiểu các bài khảo sát về thói quen ăn uống lại thấy khá nhiều bài về rác thải sinh hoạt &lt;a class=&quot;citation&quot; href=&quot;#liu2020evaluation&quot;&gt;[3]&lt;/a&gt;.
Thì bài báo nghiên cứu về thói quen rác thải sinh hoạt (A) đương nhiên cũng phải nghiên cứu về thói quen ăn uống (B) vì cái (B) nó ảnh hưởng đến cái (A)!&lt;/p&gt;

&lt;p&gt;&lt;a class=&quot;citation&quot; href=&quot;#liu2020evaluation&quot;&gt;[3]&lt;/a&gt; thực hiện khảo sát về thói quen rác thải sinh hoạt và chỉ ra rằng, ở vùng đô thị Hà Nội, trung bình 1 hộ thải ra 1192 gram rác thải ăn uống 1 ngày.
Nhưng các hộ ở vùng nông thôn lại thải ra tới 1694 gram/ngày, và tổng tất cả lại lấy trung bình thì toàn Hà Nội, mỗi hộ thải ra 1443 gram/ngày.
Tức là sống ở nông thôn lại thải ra đồ rác ăn uống nhiều hơn ở đô thị!
Thì vì (B) nó ảnh hưởng đến (A) nên tác giả cũng thực hiện điều tra về địa điểm ăn uống của người Hà Nội.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Hình thức ăn&lt;/th&gt;
      &lt;th&gt;Đô thị&lt;/th&gt;
      &lt;th&gt;Nông thôn&lt;/th&gt;
      &lt;th&gt;Trung bình&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Ăn ngoài (eat-out)&lt;/td&gt;
      &lt;td&gt;0.5 lần/ngày&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Order đồ ăn sẵn&lt;/td&gt;
      &lt;td&gt;0.3&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ăn ở nhà riêng (eat-in)&lt;/td&gt;
      &lt;td&gt;1.7&lt;/td&gt;
      &lt;td&gt;2.3&lt;/td&gt;
      &lt;td&gt;2.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Rõ ràng, hiện tại thì người Hà Nội ở các quận đô thị lẫn các huyện nông thôn đều thích &lt;strong&gt;eat-in (ăn ở nhà)&lt;/strong&gt; hơn với tần suất gấp 4-6 lần so với ăn ngoài hay mua đồ ăn sẵn.&lt;/p&gt;

&lt;p&gt;Ngoài ra nghiên cứu cũng chỉ ra người trả lời cũng có khuynh hướng duy trì thói quen ăn uống &lt;strong&gt;tại nhà&lt;/strong&gt; trong tương lai:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;More than half of the respondents plan to maintain their current eating habits in the future. In particular, a greater number of people in the urban area plan to cook and eat in more often. This indicates that most people intend to continue eating in more often in both urban and rural areas.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Phần còn lại của bài báo thì tập trung phân tích vấn đề rác thải ăn uống, không trả lời câu hỏi của chúng ta, và chúng ta cũng đã có câu trả lời cho mình: &lt;strong&gt;người Hà Nội ăn ở nhà&lt;/strong&gt;!
Tuy nhiên, tần suất ăn ngoài 0.4 lần/ngày (tức là 2.8 lần/tuần/người), và order đồ ăn sẵn là 0.2 lần/ngày (hay 1.4 lần/tuần/người), cho thấy là thị trường quán ăn với ship đồ là vẫn có chỗ.
Tổng lại là 0.4 + 0.2 + 2.0 = 2.6 lần/ngày, tức là &lt;strong&gt;trung bình người Hà Nội ăn 2.6 bữa/ngày&lt;/strong&gt; (có người ngày ăn 2 bữa, có người ăn 3 bữa chứ cũng không phải ai cũng ăn 3 bữa).&lt;/p&gt;

&lt;h2 id=&quot;ra-quán-ăn-gì&quot;&gt;Ra quán ăn gì?&lt;/h2&gt;
&lt;p&gt;Cái này thì hơi khó tìm bài nghiên cứu, vì nếu câu hỏi nghiên cứu chỉ là lên danh sách các món hay quán tại Hà Nội thì không phải là câu hỏi nghiên cứu tốt.
Vì đơn giản là có hàng sa số những quảng cáo, thông tin trên Internet.
Tôi xin đưa ra một trang web có giới thiệu nhiều quán tốt tại mà vệ sinh an toàn thực phẩm yên tâm hơn tại Hà Nội: &lt;a href=&quot;https://www.foody.vn/ha-noi/dia-diem&quot;&gt;https://www.foody.vn/ha-noi/dia-diem&lt;/a&gt;
Tuy nhiên, nên có thêm recommend của người Hà Nội thì sẽ tốt hơn.&lt;/p&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;p&gt;Tất nhiên, chúng ta cần hiểu là trong các nghiên cứu này thì &lt;strong&gt;bữa ăn&lt;/strong&gt; và &lt;strong&gt;bữa nhậu&lt;/strong&gt; là hai phạm trù khác nhau. Vì cũng có một số thành phần không nhỏ, ngoài bữa ăn vẫn đi nhậu.
Nên “nhậu” không nằm trong phạm trù nói đến ở đây, và đó là giới hạn của bài viết này. Mà chúng ta chỉ nói đến các bữa ăn chính.
Thì thói quen ăn uống của người dân Hà Nội cũng có nề nếp nhất định: họ thích ăn ở nhà bên gia đình, và người càng già càng thích đi chợ truyền thống (bao gồm cả chợ con, chợ cóc, gánh hàng rong) thích mặc cả, trong khi người trẻ thì thích đi siêu thị, cửa hàng có uy tín.
Thì thôi, &lt;strong&gt;khách&lt;/strong&gt; thì mình cũng không ép, làm sao không ngộ độc thực phẩm, không vướng vào sự vụ tranh cãi ảnh hưởng quan hệ giao lưu các nước thì thôi cứ vào siêu thị, quán ăn sạch sẽ, yên tâm, là anh em được nhờ rồi!&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;jensen2007food&quot;&gt;Jensen, R.W. and Peppard, D. 2007. Food-buying habits in Hanoi. &lt;i&gt;Sojourn: Journal of Social Issues in Southeast Asia&lt;/i&gt;. 22, 2 (2007), 230–254.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/jensen2007food/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;tran2020shopping&quot;&gt;Tran, V.H. and Sirieix, L. 2020. Shopping and cross-shopping practices in Hanoi Vietnam: An emerging urban market context. &lt;i&gt;Journal of Retailing and Consumer Services&lt;/i&gt;. 56, (2020), 102178.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/tran2020shopping/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;liu2020evaluation&quot;&gt;Liu, C. and Nguyen, T.T. 2020. Evaluation of household food waste generation in hanoi and policy implications towards SDGs target 12.3. &lt;i&gt;Sustainability&lt;/i&gt;. 12, 16 (2020), 6565.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/liu2020evaluation/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Sun, 08 May 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/hanoi-eating-habits/</link>
        <guid isPermaLink="true">https://wanted2.github.io/hanoi-eating-habits/</guid>
        
        <category>Hà Nội</category>
        
        <category>Ẩm thực</category>
        
        <category>Thói quen hàng ngày</category>
        
        <category>Ăn uống</category>
        
        <category>Thói quen ăn uống</category>
        
        <category>Eating habits</category>
        
        <category>Eat-out</category>
        
        <category>Eat-in</category>
        
        <category>Ready-made meals</category>
        
        <category>Food</category>
        
        <category>Street foods</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
      </item>
    
      <item>
        <title>CrowdRE</title>
        <description>&lt;p&gt;最も重要な仕事なんですが，すでに&lt;strong&gt;Requirement Engineering (RE，要件工学)&lt;/strong&gt;という応用分野があって，それが近年活発に発展しております．
また，多くのユーザーに製品とサービスを提供するための開発が多くなっているので，そのユーザーの集団からフィードバックを要件として &lt;a class=&quot;citation&quot; href=&quot;#dalpiaz2019re&quot;&gt;[1, 2]&lt;/a&gt;，開発陣を走らせるオーナーとマネージャーが多
くなっています．
ですので，特に最近&lt;strong&gt;Crowd Requirement Engineering（CrowdRE，集合知要件工学 &lt;a class=&quot;citation&quot; href=&quot;#khan2019crowd&quot;&gt;[3, 4]&lt;/a&gt;）&lt;/strong&gt;という分野もできました．
本稿では，CrowdREの最新動向について述べます．&lt;/p&gt;

&lt;h1 id=&quot;集合知の声はどんなに重要なのか&quot;&gt;集合知の声はどんなに重要なのか？&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;事例：&lt;/strong&gt;ベトナムでは，&lt;a href=&quot;https://www.travel.co.jp/guide/howto/296/&quot;&gt;Grab&lt;/a&gt;やGojekなどの配車アプリが普及してきました．
先日，友人が荷物を送ろうとしたときに，配車アプリにも郵便物の機能もあるから，使おうとしたときに，アプリがドライバーが見つけれなかったなんです．
ネットでニュースを確認したら，&lt;a href=&quot;https://baomoi.com/shipper-dong-loat-huy-don-giao-trong-4-gio-sieu-re/c/42566728.epi&quot;&gt;「格安の4時間」キャンペーンに反対　ドライバーが注文をキャンセルする事件が多発&lt;/a&gt;のようなタイトルが見えました．&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;chạy hơn chục đơn giao hàng khắp thành phố như vậy thì chỉ có ứng dụng là vừa có lãi, vừa kéo được người dùng, tài xế chúng tôi thì chịu thiệt．
訳）「4時間に十個のタスクをアサインして，俺らを過労させるでしょう．（十個の場所を4時間に訪れるのも大変ですし，燃料料も自腹なんだけど，配達料金が3万ドンしかないという事態があったそうです）．お金が設けたのがアプリ運営局だけだろう．ドライバー達には損しか見えない．」とドライバーが語った．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;友達が荷物を送ることができないということを気にしないけど，じゃ，「アプリ運営局にはお金が払っているユーザ（注文者）だけがユーザだと考えるなら，実際に仕事を行っているドライバー達はユーザではないから，&lt;strong&gt;ドライバーのフィードバックを聞かなくてもいい&lt;/strong&gt;？」&lt;/p&gt;

&lt;p&gt;「格安の4時間」キャンペーンは，おそらくユーザをもっと引き取れるための手段ですが，ドライバーにはキャンセルの権限を持たせるべきでしょうか？&lt;/p&gt;

&lt;p&gt;そのキャンペーンを実施する前に，ユーザの声，ドライバーの声がきちんと聞けたのでしょうか？&lt;/p&gt;

&lt;h1 id=&quot;requirements-engineering&quot;&gt;Requirements Engineering&lt;/h1&gt;
&lt;p&gt;Requirement Engineering (RE，要件工学）は工学の中で歴史の長い分野です．
今年で&lt;a href=&quot;https://conf.researchr.org/home/RE-2022&quot;&gt;30th IEEE International Requirements Engineering 2022 conference&lt;/a&gt;があるので，少なくても30年間の歴史を持っています．&lt;/p&gt;

&lt;h2 id=&quot;reの定義&quot;&gt;REの定義&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;IEEEによる要件の定義&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;:
a) A condition or capability needed by a user to solve a problem or achieve an objective;
b) A condition or capability that must be met or possessed by a system or system component to satisfy a contract, standard, specification or other formally imposed document; and
c) A documented representation of a condition or capability as in definition (a) or (b).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;MITREによる要件工学の定義&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;:
A requirement is a singular documented need—what a particular product or service should be or how it should perform. It is a statement that identifies a necessary attribute, capability, characteristic, or quality of a system in order for it to have value and utility to a user. &lt;strong&gt;Requirements engineering is the discipline concerned with establishing and managing requirements. It consists of requirements elicitation, analysis, specification, verification, and management&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;良い要件&lt;/strong&gt;を書くためには，いくつかの準備調査とプロトタイプ作成が必要です．また，良い要件にはいくつか求められる質があります．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Correct（正確性）&lt;/strong&gt;：多くの場合，正確性を保証するのが顧客の専門家の責任になります．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Feasibility（実施可能性）&lt;/strong&gt;：開発陣が求める要件ですね．実施可能な要件じゃないと難しいです．また，実施可能性は，現状のツールやテクノロジーなどで判断する想定です．例えば，「多分来年には新しいテクノロジーが公開されるから，可能性がある」をいうときに実装可能性が現状にないですね．現状の可能性ではなく来年の可能性ですね．まあ，来年というなら来年にキックオフしようよ．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Necessary（必要性）&lt;/strong&gt;：本当に必要な要件でしょうか？いらないもんに開発陣のエフォートや顧客のコストを配分する必要がありません．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prioritized（優先順位）&lt;/strong&gt;：スプリントの計画時に，VeryImportant/Important/Optional等の段階的に分けて，要件を分類する必要があります．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unambiguous（明瞭）&lt;/strong&gt;：CLEARでないと開発陣や顧客でもっとクリアになるために余計なコストをかけることになってしまいます．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Concise（単純）&lt;/strong&gt;：あまりに余計な情報が要件に入らないように要件を定めるべきでしょう．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Verifiable（検証可能性）&lt;/strong&gt;：実施可能性とは違って，テストやQA時でその要件が検証できること．&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;もちろん，現場においては，100点満点が取れない要件もあるから，顧客の要望に沿って，進める必要があります．
例えば，現状は実装が無理だけど，顧客がもっと早く開始してほしいという要望があって，100点中70点程度の要件を記入して，開発中にどんどん完成させるというアジャイル開発の進め方もありでしょう．&lt;/p&gt;

&lt;h2 id=&quot;誰がreをやるか&quot;&gt;誰がREをやるか?&lt;/h2&gt;

&lt;p&gt;ビジネスアナリティクスのスキルは，ソフトウェア開発の現場（モノづくりの現場）でどこに使うか，なかなか面白い疑問です．
データ解析や業務理解などのスキルがあると思います．
しかし，オーナーとマネージャーはすでにいるから，彼らも業務解析のスキルを持っています．
オーナーとマネージャー以外でも，自分の経験でいうと，おそらく&lt;strong&gt;主に要件解析などの業務に従事することが多いです．&lt;/strong&gt;
なぜなら，残りのタスクはすべて開発・品質保証と（技術的な）研究ばかりあって，唯一にビジネスのスキルがソフトウェア開発の現場に役に立つのが，要件の理解を促進する仕事です．
それ以外の業務は，オーナー，マネージャーと研究開発陣が理解できたら，自分で行えるからです．
あとは，顧客折衝などのモノづくりの現場以外の仕事です．&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;要件書&quot;&gt;要件書&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/requirements_file.png&quot; style=&quot;width: 50%; float:right; margin: 10px;&quot; /&gt;
また，要件は&lt;strong&gt;要件書&lt;/strong&gt;でまとめる必要があります．
ですので，要件を定める際には，要件ファイルを作成することがあります．
用語説明やスコープやユースケースやステークホルダーやテクノロジーや法律の話などの記載が必要でしょう．
右側にCockburn氏の本&lt;a class=&quot;citation&quot; href=&quot;#cockburn2001writing&quot;&gt;[5]&lt;/a&gt;から引用した一例を掲載します．
長さは6章だけですが，十分でしょう．
これをエクセルで書くときに，機能要件（functional requirements）と非機能要件（non-functional requirements）を分けて記述した書類もあったと思うけど，いくつかの章で書いてもほとんど同じ構造になっている書類でしょう．&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;
実務では，要件書はこんなイメージで利用されていると，皆さんにわかりやすく説明しました．
これから，要件工学のお話にします．
ビッグデータやデータマイニングや自然言語処理などを使って，要件を集めたり，モデリングしたりすることで，オーナーとマネージャーの仕事をよくするお話になれるかと思います．&lt;/p&gt;

&lt;h2 id=&quot;要件策定プロセス&quot;&gt;要件策定プロセス&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;要件策定プロセス&lt;/strong&gt;は様々にあります．下記に典型的な一例を示します．
左側にダイアグラムを示し，右側に&lt;a class=&quot;citation&quot; href=&quot;#khan2019crowd&quot;&gt;[3]&lt;/a&gt;から引用した表を示しています．
左側のダイアグラムを使ってFeasibilityStudyから要件書（RequirementsDocument）の策定まで述べていきましょう．
要件を確定する前に4カ月から半年ぐらいで実施可能性を検討するFeasibilityStudyを行います．
Feasibilityを検討するため，要件になりうる方向性，ストーリー，ユースケースはおおろか，資源やスケジュール調整なども考慮すべきであろう．
可能性を検討し，予備実験，調査，準備，場合によって，専門家の意見もエスカレートします．
最終的にFeasibilityレポートを提出することになります．
&lt;strong&gt;Feasibilityレポートは要件書ではないということに注意してください．&lt;/strong&gt;&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;
FeasibilityStudyで必要な場合，既存テクノロジーを調査し，実現可能性が見えれば十分で，現在，クラウド基盤が多く存在するため，それで実現できることが多いので，ほぼ確実に実現できるといっても間違いないが，稀に実現不能な技術もあります．
今，AWSなどを使っていればほぼ確実に実現できるため，FeasibilityStudyというところはAWSのサービスを使って実現できるかシンプルなレポートになると思います．
また，少しデータを見せるために自分のPCで予備実験を行っても面白いと思います．
経験的に，本当に変な（要件ではなく）要望ではない，かつ，十分に技術を持っている研究開発集団だと，問題ないでしょう．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/crowdre_activities.png&quot; alt=&quot;activities&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FeasibilityStudyでうまく「やる」方針が取れたときに，&lt;strong&gt;要件書&lt;/strong&gt;作成に入ります．
まず，要件を集めて解析する必要があります．ElicitationとAnalysisですね．
CrowdREの場合，自社製品に対するユーザレビューや競合他社の製品に対するコメントなどの情報源から要望を集めて，それらに自社の優位性を確保するのに，要望から良い要件に書き直します．
いっぱい要件がでてくると思うけど，すべてやる必要がないですね．
Specificationのステップで，皆で話し合って有望な要件を選択して，優先順位もつけることがあります．
ValidationでSpecificationの後に残っている要件を検証して，もう一度要件の品質を上げることができます．
システムモデリングも必要です．
要件に合わせるシステムはどんな形かを検討するべきで，要件書に記述することにしましょう．
要件には，ユーザ要件とシステム要件を分けて，Specificationからの要件をさらに分類します．
システムモデルの情報，分類済要件と要件の検証結果をベースにして要件書を書き終わらせてから，要件定義の段階が完了になります．
最終的の成果物は&lt;strong&gt;要件書（要件定義書）&lt;/strong&gt;になります．&lt;/p&gt;

&lt;h1 id=&quot;crowdreの最新動向&quot;&gt;CrowdREの最新動向&lt;/h1&gt;

&lt;p&gt;右側の表を説明しましょう．
各ステップで行ってくれるツールと手法です．
BAの人がやってくれたほうがうまいかもしれないが，オーナーとマネージャーのほうでRun-timeまでモニタリングする責任があるため，要件書作成からオーナーとマネージャーが指導することも多いと思います．
因みに，右側の表で「Run-time」も書いてくれるけど，要件書の次の実施の段階で要件がきちんと実施されたかと，要件が変更する必要な場合，要件を更新するとかの作業です．
オーナーとマネージャーは要件書作成までフォローするだけだと思わないで，Run-timeの時にもずっとフォローしてくれると思うべきであろう．&lt;/p&gt;

&lt;h2 id=&quot;elicitation&quot;&gt;Elicitation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Elicitation（データ準備）&lt;/strong&gt;というステップで，目的に合わせて要件のデータを収集して，要件のデータ標準化のようなETL作業になります．
ここで，データ収集の時，オンラインフォーラムやレビューサイト等からデータをスクレイピングする作業も含めます．
アンケート調査を作成し，MTurkのように多くの回答者が求める仕事もあるので，単純に収集するだけではなく，勧誘の作業もあります．
複数の情報源からデータを取得してデータレイクに置く作業ですね．
&lt;strong&gt;目的&lt;/strong&gt;は事前に決めておくもので，一般的要件か特定の機能に関する要件が求められます．
事前に決めれない目的の場合，Run-timeFeedback（運用時で得たフィードバックで改善する）やトレンド解析などによって求められる要件もあります．&lt;/p&gt;

&lt;h2 id=&quot;modelingspecification&quot;&gt;Modeling/Specification&lt;/h2&gt;
&lt;p&gt;システムモデリングと要件策定については，要件に基づいて適切なシステムモデルを組み合わせて，再度要件を改善することもできます．
システムモデルにはドメインドリブンデベロップメント（D3）というもので，&lt;strong&gt;ユースケースモデル&lt;/strong&gt;がよく用いられます．
&lt;strong&gt;&lt;em&gt;i*&lt;/em&gt;&lt;/strong&gt;モデルを利用することもあります．
&lt;strong&gt;&lt;em&gt;i*&lt;/em&gt;&lt;/strong&gt;について説明しましょう．
下記の図のように，OpenOME&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;ツールを使って&lt;strong&gt;&lt;em&gt;i*&lt;/em&gt;&lt;/strong&gt;モデルを生成する作業を紹介します．
&lt;strong&gt;&lt;em&gt;i*&lt;/em&gt;&lt;/strong&gt;モデルでは，ユーザストーリーを書くときに必ず&lt;em&gt;who, what, why, how&lt;/em&gt;を答える必要があります．
収集した数千件以上の要件をステークホルダーにリンクして，依存関係が解析できます．
そのために，要件の不完全性と欠陥を発見することができます．
ですので，&lt;strong&gt;&lt;em&gt;i*&lt;/em&gt;&lt;/strong&gt;モデルはシステムと要件モデリングには相性良いモデルとなります．
OpenOMEでは，&lt;strong&gt;&lt;em&gt;i*&lt;/em&gt;&lt;/strong&gt;モデルを評価するために&lt;em&gt;forward&lt;/em&gt;解析でwhat-if分析を利用して，代替案を比較することができます．
一方，&lt;em&gt;backward&lt;/em&gt;解析で達成度を推定できます．
このような&lt;em&gt;forward-backward&lt;/em&gt;解析で，&lt;strong&gt;&lt;em&gt;i*&lt;/em&gt;&lt;/strong&gt;モデルを最適化し，要件も一緒に向上できます．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/i-asterick-model.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;他には，業務フローをベースにしたモデリングや目的をベースにしたモデリングなどもあります．
詳細は&lt;a class=&quot;citation&quot; href=&quot;#khan2019crowd&quot;&gt;[3]&lt;/a&gt;を参照してほしいです．
本稿で，7個の参考文献までしか引用できない制約があるので，短縮させていただきます．&lt;/p&gt;

&lt;h2 id=&quot;analysisvalidation&quot;&gt;Analysis/Validation&lt;/h2&gt;
&lt;p&gt;解析と評価ですね．
解析は，データレイクに収集した要件をパーシングして，理解する作業です．
評価は，解析した要件をステークホルダーのニーズにアラインメントする（合わせる）作業です．
さらに，解析した要件の正確性，一貫性と検証可能性も確保します．
いくつかの方法があります．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;クラウドソーシング&lt;/strong&gt;：クラウドのエフォートを利用して，解析と評価を行います．例えば，「この要件を改善しようとしたら，あなたがどう書き直しますか？」のような調査をネットで意見を集めることで，解析が可能です．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;テキストデータ解析&lt;/strong&gt;：自然言語処理，データマイニング，統計解析と機械学習を使って，自動解析を可能にする方法です．メリットとしては，潜在的要件を発見することが可能になることです．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;画像と音声解析&lt;/strong&gt;：テキスト以外のデータ解析です．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;プロトタイプ&lt;/strong&gt;：デモを作成して，デモからわかった要件もあります．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;感情分析&lt;/strong&gt;：レビューデータの感情を解析することで，潜在的要件を発見することができます．&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;priorization&quot;&gt;Priorization&lt;/h2&gt;
&lt;p&gt;要件に優先順位をつけるために，ポイント制ゲーム，統計解析法，投票とユーザの評価を使った手法があります．
要件がいっぱい来ている時に，固い内容ですし，読むだけで順位を決定するのはつまらないですね．
なんで，ゲームをやらせるとか，投票させるとか，つまらない作業をもっと面白い方にするというのもオーナーとマネージャーの責任です．&lt;/p&gt;

&lt;h2 id=&quot;run-time-handling&quot;&gt;Run-time handling&lt;/h2&gt;

&lt;p&gt;運用時で，定期的に調査を行い，要件とコンテキストを監視しながら，ユーザ層の変化に要件を適応させることができます．
また，状況によって，要件を進化させていくこともできます．
新たに新規要件を発見し，よりユーザのニーズを先読みして，サービスを改善することもできます．&lt;/p&gt;

&lt;h2 id=&quot;crowdreにおけるaiの応用&quot;&gt;CrowdREにおけるAIの応用&lt;/h2&gt;

&lt;p&gt;機械学習やデータマイニングを使って，解析を行うことができます．
要件は議論から生成されたので，AIによる議論マイニング&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;は役に立つそうです．
運用時に，監視データとユーザフィードバックの関係を解析することで，より新たな知見を得ることもできます．
新聞を毎日読んで，&lt;strong&gt;世界知識&lt;/strong&gt;を充実している方と特定の&lt;strong&gt;コンテキスト知識&lt;/strong&gt;が精通している方と議論すると，とてもいいけど，自然言語処理による議論マイニングの場合，機械学習モデルはそんなに精通できるかという疑問です．
世界知識はおそらくCommonSenseの知識で，米大統領選挙のデベイトで，ある話が出るとそんなに理解しているでしょうか？
コンテキスト知識というのが，この建物を監視した結果でその建物の来客について議論できるかという狭いコンテキスト（CommonSenseより狭い）で議論することです．&lt;/p&gt;

&lt;p&gt;では，最も重要なのは&lt;strong&gt;AIによる自動化 &lt;a class=&quot;citation&quot; href=&quot;#dalpiaz2021value&quot;&gt;[4]&lt;/a&gt;&lt;/strong&gt;の話ですね．
ビジネスアナリティクスの方で，手でやる作業が多いけど，開発の現場で役に立つのに，この要件工学の知見を使って自動化してもらえるといいですね．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/automation_crowdre.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;結論&quot;&gt;結論&lt;/h1&gt;

&lt;p&gt;ビジネスアナリティクスといってもオーナーとマネージャーのアシスタントの仕事に近いですね．
本稿で，もっとも現場に役に立つそうなRequirements Engineering（RE，要件工学）を軽く紹介しました．
興味がある方は，この方向性でもっと調べて自分の業務をもっと充実できるじゃないかと思います．
そのような思いで書き届きました．
&lt;a href=&quot;https://baomoi.com/shipper-dong-loat-huy-don-giao-trong-4-gio-sieu-re/c/42566728.epi&quot;&gt;「格安の4時間」キャンペーンに反対　ドライバーが注文をキャンセルする事件が多発&lt;/a&gt;を再考すると，確かに，ゲームの観点で考えると，不公平だと思っているドライバーもいるため，投票とかで事前に調査すればよいかもしれません．
もうちょっとうまいキャンペーンを考えましょうか．&lt;/p&gt;

&lt;h1 id=&quot;参考文献&quot;&gt;参考文献&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;dalpiaz2019re&quot;&gt;Dalpiaz, F. and Parente, M. 2019. RE-SWOT: from user feedback to requirements via competitor analysis. &lt;i&gt;International working conference on requirements engineering: foundation for software quality&lt;/i&gt; (2019), 55–70.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dalpiaz2019re/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;morales2017exploiting&quot;&gt;Morales-Ramirez, I., Munante, D., Kifetew, F., Perini, A., Susi, A. and Siena, A. 2017. Exploiting user feedback in tool-supported multi-criteria requirements prioritization. &lt;i&gt;2017 IEEE 25th International Requirements Engineering Conference (RE)&lt;/i&gt; (2017), 424–429.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/morales2017exploiting/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;khan2019crowd&quot;&gt;Khan, J.A., Liu, L., Wen, L. and Ali, R. 2019. Crowd intelligence in requirements engineering: Current status and future directions. &lt;i&gt;International working conference on requirements engineering: Foundation for software quality&lt;/i&gt; (2019), 245–261.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/khan2019crowd/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;dalpiaz2021value&quot;&gt;Dalpiaz, F. 2021. On the Value of CrowdRE in Research and Practice. &lt;i&gt;2021 IEEE 29th International Requirements Engineering Conference Workshops (REW)&lt;/i&gt; (2021), 291–292.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dalpiaz2021value/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cockburn2001writing&quot;&gt;Cockburn, A. 2001. &lt;i&gt;Writing effective use cases&lt;/i&gt;. Pearson Education India.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/cockburn2001writing/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://standards.ieee.org/findstds/standard/610.12-1990.html&quot;&gt;http://standards.ieee.org/findstds/standard/610.12-1990.html&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.mitre.org/publications/systems-engineering-guide/se-lifecycle-building-blocks/requirements-engineering&quot;&gt;https://www.mitre.org/publications/systems-engineering-guide/se-lifecycle-building-blocks/requirements-engineering&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;でも，オーナーとマネージャーも同じくそういう業務をやっていますね．むしろ，現場にない役割になっています． &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;ユーザストーリー一覧や要望一覧とは違うものです． &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;きちんとやっているから，FeasibilityStudyも行っていますね．この段階で明瞭な要件は要らず，可能性があればいいです．上部の人たちはまた再検討するため，Priorization（優先順位）をつける処理もあります．また，上部の人が「何をやるかやらないか」を検討するため，この段階で「必ずやるよ」と思わないでください．例えば，自分でチャンスが非常にあるというビジネスだと思っても，積極的に実施を進めてしまうが，皆で話し合ってから，もうやらないよと言われて，せっかく準備したのに無駄になることも多いです．ですので，この段階の出力は，&lt;strong&gt;Feasibilityレポートだけ&lt;/strong&gt;と考え，それのために必要十分な作業だけに取り込んで問題ないはずです．&lt;strong&gt;エクセル一枚で数行を書けば十分です．&lt;/strong&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.cs.toronto.edu/km/openome&quot;&gt;http://www.cs.toronto.edu/km/openome/&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/naoakiokazaki/ss-100603788&quot;&gt;岡崎直観　自然言語処理による議論マイニング&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 08 May 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/crowdre/</link>
        <guid isPermaLink="true">https://wanted2.github.io/crowdre/</guid>
        
        <category>Business Analytics in Development</category>
        
        <category>Requirement Engineering</category>
        
        <category>Crowd Requirement Engineering</category>
        
        <category>要件工学</category>
        
        <category>集合知要件工学</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>ハノイではリモートワークが本当に人気でしょうか？</title>
        <description>&lt;p&gt;ちらほら海外からお誘いがありました．
「どうかリモートワークをしましょうか」という誘いです．
なんかコロナ禍で海外では皆がよくリモートワークをしているかと思います．
おー，本当にベトナムでもリモートワークが人気だろうなと思っていたが，周りを見ると&lt;strong&gt;ハノイではそうではない&lt;/strong&gt;気もしました．
業界はそれぞれで，現場にいかないと仕事を行えない土木系の仕事なども多いという原因があります．
IT業界でも実際はコロナ禍は主要な原因でリモートワークをしているが，コロナ禍がなくなると，IT業界もみんな本気でオフィスに戻っている気もしています．
そういう疑問を持ちながら，解答を探しております．&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;コロナ禍の後にハノイではリモートワークが生きるか？&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;リモートワークを促進する要因は何か？支障する要因は何か？&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;結論としては，ハノイではコロナ禍後リモートワークが基本的になくなるだろう．&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;コロナ禍後のリモートワーク&quot;&gt;コロナ禍後のリモートワーク&lt;/h1&gt;

&lt;p&gt;本日2022年4月27日，&lt;a href=&quot;https://ncov.vnanet.vn/tin-tuc/viet-nam-tam-dung-khai-bao-y-te-covid-19-voi-nguoi-nhap-canh-tu-0h-ngay-27-4/3c085198-7e9f-48a1-9e09-e97ad4ca3513&quot;&gt;ベトナム医療省が海外から入国者に対してコロナ検査など水際対策が停止する方針を発表しました．&lt;/a&gt;
また，2022年3月中には，3月から全面で観光や貿易などの活動を再開するよう方針を定められました．
今は，もうハノイではコロナ禍後という時代を考えたほうが良いでしょうか？&lt;/p&gt;

&lt;p&gt;コロナ禍を理由として，リモートワークを採用した会社は少なくないと思います．
特にIT業界では，去年10月ごろでほぼリモートワークだったそうです．
しかし，コロナ禍後なので，もうリモートワークをする背景はないでしょう．
もうこれから，ハノイでは，さすがに毎日リモートワークをするのがあまりないだろう．
全市には完全に否定できないが，どこかにリモートワークをしている方がいるとしても，まわりから見ると，ちょっと変な人だと思われるかもしれません．&lt;/p&gt;

&lt;h1 id=&quot;ハノイでのリモートワーク&quot;&gt;ハノイでのリモートワーク&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;The first concept of working far from a workplace (i.e., telecommuting) was introduced in the USA in the 1970s to handle transport-related issues such as traffic congestion and air pollution by reducing commuting between home and the workplace.&lt;a class=&quot;citation&quot; href=&quot;#nguyen2021perception&quot;&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;テレワークはなんとアメリカで1970年代から人気になったそうです．
その背景は，通勤時間を短縮したくて，空気汚染の影響も削減したいという動機がありました．
コロナ禍がなかった時代で，この2つはリモートワークの主な動機でした．&lt;/p&gt;

&lt;p&gt;ハノイではね，空気もちょっとあまり良くないし，渋滞も多いです．
しかし，それでも，コロナ禍前に，皆がきちんと出社していました．
ほぼの会社は勤怠管理システムを導入しています．
指紋認証や顔認証などで勤怠登録する場合もありました．&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Regarding the perception of HBT, while the fear of COVID-19 was a strong positive factor, difficulties in focusing on work and accessing data were negative factors. &lt;a class=&quot;citation&quot; href=&quot;#nguyen2021factors&quot;&gt;[2]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;そうですね．ハノイでは，なぜコロナ禍でリモートワークをしているかというと，コロナ禍は怖いからだけの理由です．
それ以外，リモートワークを促進する強い理由はないだろう．
主な支障は，&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;仕事に集中できない&lt;/strong&gt;．うん，まあ，家にいるから家族もいるし，集中しにくいですね．&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;仕事に必要なデータと機器を入手できない&lt;/strong&gt;．そうですね．アクセス制限もあるので，多くの場合，出社しないといけないですね．&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;やはり，仕事を考えると，コロナ禍がなくなると出社しましょうよ！&lt;/p&gt;

&lt;p&gt;他にも，コロナ禍中で子供の通学の動機についての研究とオンラインショッピング活動におけるコロナ禍の影響についての研究&lt;a class=&quot;citation&quot; href=&quot;#nguyen2021factorsaffecting&quot;&gt;[3, 4]&lt;/a&gt;もあります．&lt;/p&gt;

&lt;h1 id=&quot;結論&quot;&gt;結論&lt;/h1&gt;

&lt;p&gt;もうベトナム政府が全面で活動を再開する方針も出したし，コロナ禍はなくなったため，ハノイにいる場合，出社することは避けられないと思いますね．
ハノイなら，出社しましょう！&lt;/p&gt;

&lt;h1 id=&quot;参考文献&quot;&gt;参考文献&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;nguyen2021perception&quot;&gt;Nguyen, M.H. and Armoogum, J. 2021. Perception and preference for home-based telework in the covid-19 era: A gender-based analysis in Hanoi, Vietnam. &lt;i&gt;Sustainability&lt;/i&gt;. 13, 6 (2021), 3179.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021perception/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nguyen2021factors&quot;&gt;Nguyen, M.H. 2021. Factors influencing home-based telework in Hanoi (Vietnam) during and after the COVID-19 era. &lt;i&gt;Transportation&lt;/i&gt;. 48, 6 (2021), 3207–3238.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021factors/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nguyen2021factorsaffecting&quot;&gt;Nguyen, M.H., Armoogum, J. and Nguyen Thi, B. 2021. Factors affecting the growth of e-shopping over the covid-19 era in hanoi, vietnam. &lt;i&gt;Sustainability&lt;/i&gt;. 13, 16 (2021), 9205.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021factorsaffecting/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;nguyen2021impact&quot;&gt;Nguyen, M.H., Pojani, D., Nguyen, T.C. and Ha, T.T. 2021. The impact of Covid-19 on children’s active travel to school in Vietnam. &lt;i&gt;Journal of Transport Geography&lt;/i&gt;. 96, (2021), 103191.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/nguyen2021impact/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
</description>
        <pubDate>Wed, 27 Apr 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/hanoi-remote-work/</link>
        <guid isPermaLink="true">https://wanted2.github.io/hanoi-remote-work/</guid>
        
        <category>勤務条件</category>
        
        <category>ハノイ</category>
        
        <category>リモートワーク</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>Edge-Cloud architectures &amp; TPU resources</title>
        <description>&lt;p&gt;&lt;strong&gt;Edge-Cloud (EC) architectures&lt;/strong&gt; is an emerging computing paradigm recently, which takes the AI computation to the edge devices and only aggregates processed important data to the cloud.
&lt;strong&gt;Tensor Processing Unit (TPU)&lt;/strong&gt; is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google’s own TensorFlow software.
Google started selling the Cloud TPUv3 in 2018 for third-party users.
Their edge TPU version (Coral)&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; was started in the same year.
Both cloud and edge TPU support Tensorflow and its Lite version but do not support other deep learning frameworks like PyTorch or MXNet.
TPU is useful for EC architectures.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1 id=&quot;edge-computing&quot;&gt;Edge computing&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Edge computing&lt;/strong&gt; is a new paradigm of distributed computing on resource-constrained IoT devices, such as sensors and actuators, which has become ubiquitous recently &lt;a class=&quot;citation&quot; href=&quot;#murshed2021machine&quot;&gt;[1, 2]&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;More than 25 billion IoT devices all over the world have been used by 2020.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One crucial need is to aggregate the information from these devices and make decisions from the data.
Thus, a machine learning (ML) system is used to aggregate such data.
But the IoT devices have only low resources, which makes almost all on-device ML systems is impossible.
A typical solution for this problem is to offload them into an external data processing system like cloud computing.
It worsens latency, increases communication costs, and introduces additional privacy concerns.&lt;/p&gt;

&lt;div style=&quot;width: 50%; float: right; margin: 0.5em;&quot;&gt;
&lt;img src=&quot;/assets/images/edge-server.png&quot; /&gt;
&lt;p&gt;&lt;i&gt;Edge servers. (Source: Murshed et al., 2021)&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The solution requires allocation of a new &lt;strong&gt;edge server&lt;/strong&gt; near the IoT devices, which aggregates local information and makes quick decisions at the local edge (&lt;strong&gt;distributed inference&lt;/strong&gt;).
Only processed and meaningful information, which is advantageous to the global model, is sent to cloud computing servers to update global model parameters (&lt;strong&gt;distributed training&lt;/strong&gt;).
This decentralized architecture has many advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Reducing cost&lt;/em&gt;: The volume of data needed to be transferred to a central computing location is reduced because some of it is processed by
edge-devices.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Improving latency&lt;/em&gt;: The physical proximity of edge-devices to the data sources makes it possible to achieve lower latency which improves real-time
data processing performance.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Better privacy&lt;/em&gt;: For the cases where data must be processed remotely, edge devices can be used to discard personally identifiable information
(PII) prior to data transfer, thus enhancing user privacy and security.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Better failure handling&lt;/em&gt;: Decentralization can make systems more robust by providing transient services during network failures or cyberattacks.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;But everything comes with a cost.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And edge computing is no exception!
While the applications range from real-time video analytics, image recognition, automatic speech recognition, user privacy, fraud detection, creating new datasets, autonomous vehicles, smart home/cities, human safety to augmented reality, the power to run has many problems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;How to adapt existing ML algorithms to edge devices?&lt;/strong&gt;: For this problem, we need to consider more distributed-computing specific algorithms, care about hardware constraints, and build lighter/faster model architectures (which also requires pruning techniques and compression).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How to improve latency of distributed training and inference systems while preserving privacy?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What are the common hardware devices used to enable edge intelligence?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the nature of the emerging software ecosystem that supports this new end-edge-cloud architecture for real-time intelligent systems?&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the third question, we will consider a choice in the art: the &lt;strong&gt;Tensor Processing Unit (TPU)&lt;/strong&gt; along with other options like &lt;strong&gt;Central Processing Unit (CPU)&lt;/strong&gt; and &lt;strong&gt;Graphical Processing Unit (GPU)&lt;/strong&gt;.
It has both cloud and edge versions that make it possible for new end-edge-cloud architecture.&lt;/p&gt;

&lt;h1 id=&quot;cpu-gpu-and-tpu&quot;&gt;CPU, GPU, and TPU&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321751_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Central Processing Unit (CPU)&lt;/strong&gt; is the main processing unit in a computer.
It takes inputs, referring to data in memory units, and produces outputs in every task.
It takes control of the computation and does arithmetic/logic computations.&lt;/p&gt;

&lt;div style=&quot;width: 100%; margin: 0.5em;&quot;&gt;
&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321849_11.gif&quot; /&gt;
&lt;p&gt;&lt;i&gt;How CPU works (Source: Tinhte.vn)&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;CPU in the von-Neumann architectures stores computation results in L1 cache or register due to the fact that the next computation is unknown (thus, the current results must be stored somewhere for future computing).
However, such L1 cache and registers are limited in their memory capacity, and sometimes, cache misses happen, which we call as &lt;em&gt;von-Neumann bottleneck&lt;/em&gt;.
Because there are many matrix computations (multiplication and addition) in deep learning, bottleneck problems arise more and more.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321867_main-qimg-ee7d4757d5c7cdd1105f9e4aa267db22.png&quot; style=&quot;width: 50%; margin: 0.5em; float: left;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Graphical Processing Unit (GPU)&lt;/strong&gt; can compute more than one arithmetic logic at a time, then it has higher parallelism than CPU (which is a sequential processing unit).
While a CPU only has a few &lt;em&gt;Arithmetic Logic Unit (ALU)&lt;/em&gt;, a GPU can have some thousands of ALUs, making GPU parallel computing is fast, especially for &lt;strong&gt;arithmetic computations&lt;/strong&gt;.
GPU maybe is not good for non-arithmetic computations such as Office tasks, but in the case of deep learning, it is a dominant choice by now.&lt;/p&gt;

&lt;p&gt;However, although GPU is faster for deep learning, it does not solve the &lt;em&gt;bottleneck&lt;/em&gt; problem!
That’s why Google had to design TPU!&lt;/p&gt;

&lt;div style=&quot;width: 100%; margin: 0.5em;&quot;&gt;
&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321851_22.gif&quot; /&gt;
&lt;p&gt;&lt;i&gt;How GPU works (Source: Tinhte.vn)&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Tensor Processing Unit (TPU)&lt;/strong&gt; is the neural network hardware designed by Google.
It means it is good ONLY for neural network computing, not for all.
It cannot do Office tasks, control rockets, or monitor bank transactions.
&lt;strong&gt;It can only do arithmetic computations!&lt;/strong&gt;&lt;/p&gt;

&lt;div style=&quot;width: 100%; margin: 0.5em;&quot;&gt;
&lt;img src=&quot;https://photo2.tinhte.vn/data/attachment-files/2021/01/5321852_33.gif&quot; /&gt;
&lt;p&gt;&lt;i&gt;How TPU works (Source: Tinhte.vn)&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In essence, TPU only access memory once.
Then for each multiplication, the result is passed to the next multiplication when performing the additions at the same time.
Therefore, no more memory access is needed.
This is how TPU solved the von-Neumann bottleneck!&lt;/p&gt;
&lt;div style=&quot;width: 50%; margin: 0.5em;float: right;&quot;&gt;
&lt;img src=&quot;/assets/images/tpu-gpu-comparison.png&quot; /&gt;
&lt;p&gt;&lt;i&gt;Source: Wang et al., 2019&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In a recent benchmark &lt;a class=&quot;citation&quot; href=&quot;#wang2019benchmarking&quot;&gt;[3]&lt;/a&gt;, TPU showed that it is a better choice than GPU for large CNNs, suggesting that TPU is highly-optimized for CNNs.
While TPU is a better choice for Recurrent Neural Networks (RNNs), it is not as flexible as GPU for embedding computations.
The smallest gain perhaps is when TPU does fully-connected computations (FC).&lt;/p&gt;

&lt;h1 id=&quot;pricing&quot;&gt;Pricing&lt;/h1&gt;

&lt;p&gt;Now, we already understand that TPU is fast and has been designed for neural nets.
We may want to add these resources to our project, but the question here is &lt;strong&gt;how does it cost?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Google somewhat offers two different choices for computing: a cloud version&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and an edge version&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;cloud-tpu&quot;&gt;Cloud TPU&lt;/h2&gt;

&lt;p&gt;You have two choices: you can rent a single TPU or a cluster of TPUs (TPU Pod)&lt;sup id=&quot;fnref:3:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.
Unfortunately, TPU is only available in three zones:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Iowa (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;us-central1&lt;/code&gt;);&lt;/li&gt;
  &lt;li&gt;Netherlands (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;europe-west4&lt;/code&gt;); and&lt;/li&gt;
  &lt;li&gt;Taiwan (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;asia-east1&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And Taiwan zone does not have Pods, while the Iowa zone does not have TPUv3 Pods!&lt;/p&gt;

&lt;p&gt;For a single GPU device with 8 TPUv3 cores, you must pay 8USD/hour for on-demand options or 2.4USD/hour for spot options.
Pods with 32 TPUv3 cores require a 1-year or 3-year commitment with an &lt;strong&gt;Evaluation price&lt;/strong&gt; is 32USD/hour.
If you afford a 3-year plan, then the monthly pricing is &lt;strong&gt;10,512USD/month&lt;/strong&gt;.
The total price for annual options in the Netherlands is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pod-pricing.png&quot; alt=&quot;pods&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;edge-tpu&quot;&gt;Edge TPU&lt;/h2&gt;

&lt;p&gt;Google also offered an Edge TPU version with up to 4 TOPS.
Edge TPU has been a new device for edge recently &lt;a class=&quot;citation&quot; href=&quot;#murshed2021machine&quot;&gt;[1, 4]&lt;/a&gt;.
If you already knew NVIDIA Jetson Nano&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, then this is the same version for TPU.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://lh3.googleusercontent.com/4AseEI_s6QWM75QwncKhPgMPi0P7YHume5O7Njx8FovMze22nblcLP8zOU-s5qhpwMgzVmYe9VPLiwrHecNYbjtNCKgHLOSO_w9X1NkEzEDjxdWRM7g=w2000-rw&quot; alt=&quot;edge tpu&quot; /&gt;
&lt;em&gt;Source: &lt;a href=&quot;https://coral.ai/products/&quot;&gt;Google Coral&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In Vietnam, I can buy a 1GB RAM DevBoard with a budget that approximates five million bucks (tax included)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.
Google Coral announced the price is 129.99USD/board. 
It is equipped with 4TOPS and only favors Tensorflow Lite models.&lt;/p&gt;

&lt;h1 id=&quot;conslusion&quot;&gt;Conslusion&lt;/h1&gt;

&lt;p&gt;Edge-Cloud architectures will be the next generation of computing platforms.
TPUs are good for neural network computations and then are good for ML as well.
We need to allocate the budget for our project first, and it is good to know both cloud and edge versions of TPU.
They differ about 1100x in terms of annual cost (Edge TPU 129.99USD/lifetime vs. 132,000USD?year Cloud Pod TPUv2)!&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;murshed2021machine&quot;&gt;Murshed, M.G.S., Murphy, C., Hou, D., Khan, N., Ananthanarayanan, G. and Hussain, F. 2021. Machine learning at the network edge: A survey. &lt;i&gt;ACM Computing Surveys (CSUR)&lt;/i&gt;. 54, 8 (2021), 1–37.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/murshed2021machine/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;shi2020communication&quot;&gt;Shi, Y., Yang, K., Jiang, T., Zhang, J. and Letaief, K.B. 2020. Communication-efficient edge AI: Algorithms and systems. &lt;i&gt;IEEE Communications Surveys &amp;amp; Tutorials&lt;/i&gt;. 22, 4 (2020), 2167–2191.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/shi2020communication/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;wang2019benchmarking&quot;&gt;Wang, Y.E., Wei, G.-Y. and Brooks, D. 2019. Benchmarking tpu, gpu, and cpu platforms for deep learning. &lt;i&gt;arXiv preprint arXiv:1907.10701&lt;/i&gt;. (2019).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/wang2019benchmarking/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;cass2019taking&quot;&gt;Cass, S. 2019. Taking AI to the edge: Google’s TPU now comes in a maker-friendly package. &lt;i&gt;IEEE Spectrum&lt;/i&gt;. 56, 5 (2019), 16–17.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/cass2019taking/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://cloud.google.com/edge-tpu/&quot;&gt;https://cloud.google.com/edge-tpu/&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://cloud.google.com/tpu/pricing&quot;&gt;https://cloud.google.com/tpu/pricing&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://coral.ai/products/&quot;&gt;https://coral.ai/products/&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://developer.nvidia.com/embedded/jetson-nano-developer-kit&quot;&gt;NVIDIA Jetson Nano&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://pivietnam.com.vn/coral-dev-board-edge-tpu-module-som-pivietnam-com-vn.html&quot;&gt;Coral DevBoard Edge TPU SoM at PiVietnam&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 05 Mar 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/edge-cloud-tpu/</link>
        <guid isPermaLink="true">https://wanted2.github.io/edge-cloud-tpu/</guid>
        
        <category>Machine learning</category>
        
        <category>TPU</category>
        
        <category>edge computing</category>
        
        <category>edge server</category>
        
        <category>edge devices</category>
        
        <category>edge-cloud architectures</category>
        
        <category>cloud computing</category>
        
        <category>6g</category>
        
        <category>federated learning</category>
        
        <category>distributed computing</category>
        
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices</title>
        <description>&lt;p&gt;Trong các bài viết trước, chúng ta đã xem xét kha khá về &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cho NLP/Vision-Language&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; và âm thanh&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.
Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự án cần &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;vision-transformers&quot;&gt;Vision Transformers&lt;/h1&gt;

&lt;h2 id=&quot;vit&quot;&gt;ViT&lt;/h2&gt;

&lt;p&gt;Về nguyên lý chung của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; thì chúng ta có hai bài viết trước nói khá nhiều rồi&lt;sup id=&quot;fnref:4:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:5:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; nên các bạn tham khảo nhé.
Nhà cũng đang có cái “eo hẹp” là chỉ được tối đa 7 citations và 9 phút đọc thôi nên các bạn cần thì đọc lại hai bài viết&lt;sup id=&quot;fnref:4:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:5:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; để xem thêm kiến thức về attention, Transformer, …
Vision Transformer (ViT, &lt;a class=&quot;citation&quot; href=&quot;#dosovitskiy2020image&quot;&gt;[1]&lt;/a&gt;) đưa khái niệm &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; vào vision.
Nếu các bạn đã quen &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; thì hiểu ngay là ta cần chuyển hình ảnh thành 1 chuỗi:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://media.giphy.com/media/ATsWtUsuuFRfq8OhZ7/source.gif&quot; alt=&quot;&quot; /&gt;
&lt;em&gt;Source: Google&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Để làm được việc này, các tác giả đề xuất:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Image patches&lt;/strong&gt;: Hình ảnh được chia ra thành patches và đánh số thự tự để input vào transformer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Position embedding&lt;/strong&gt;: thứ tự chỉ đơn giản là chuỗi 1D.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MLP với GELU activation&lt;/strong&gt;: MLP sử dụng GELU để kích hoạt. Model không chứa CNN, có tối thiểu 12 tầng, với kích cỡ hidden size từ 768, ngoài ra có 12-16 heads.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Large-scale pre-training and fine-tuning&lt;/strong&gt;: Một điểm đáng chú ý khác là pre-training dạng supervised trên một dataset 300 triệu ảnh tạo ra 1 model rất mạnh.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Các tác giả báo cáo cải tiến trên khá nhiều bộ dữ liệu lớn, và ngoài ra cả con số &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TPUv3-core-days = số lượng TPUv3 cores x sô lượng ngày train&lt;/code&gt;.
Cái con số &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TPUv3-core-days&lt;/code&gt; thì cứ mỗi thí nghiệm là vài ngàn tới vài chục ngàn, mà mỗi core thì cứ 10$/ngày thì các bạn cứ nhẩm tính xem budget của hội &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;con nhà giàu&lt;/code&gt; này đầu tư vào nó lớn cỡ nào đấy.&lt;/p&gt;

&lt;h2 id=&quot;detr&quot;&gt;DETR&lt;/h2&gt;

&lt;p&gt;DETR &lt;a class=&quot;citation&quot; href=&quot;#carion2020end&quot;&gt;[2, 3]&lt;/a&gt; tiếp tục ứng dụng transformer vào Object Detection và Segmentation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/detr.png&quot; alt=&quot;detr&quot; /&gt;
&lt;em&gt;Source: &lt;a class=&quot;citation&quot; href=&quot;#carion2020end&quot;&gt;[2]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Backbone&lt;/strong&gt;: DETR dùng CNN quen thuộc như resnet-50 hoặc 101.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: DETR dùng \(1\times 1\) convolution để dimention reduction các feature maps rồi input vào. Position embedding là cố định vì transformer là không phụ thuộc vào permutation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: thay vì decode từng object query, thì DETR decode song song cùng lúc tất cả các queries.&lt;/li&gt;
  &lt;li&gt;Sau khi decode thì dùng FFN để predict vị trí và class. Để tính hàm loss thì dùng thuật toán Hungarian để matching.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Về mặt giá cả thì DETR tốn 300 epochs để hội tụ trên COCO, nên về sau gần đây có khá nhiều nghiên cứu để giảm giá thành hộ tụ (chỉ cần 50 epochs thôi chả hạn).&lt;/p&gt;

&lt;h2 id=&quot;yolox&quot;&gt;YOLOX&lt;/h2&gt;

&lt;p&gt;So với phiên bản cũ &lt;a class=&quot;citation&quot; href=&quot;#redmon2018yolov3&quot;&gt;[4]&lt;/a&gt; thì YOLOX &lt;a class=&quot;citation&quot; href=&quot;#ge2021yolox&quot;&gt;[5]&lt;/a&gt; ứng dụng khá nhiều kỹ thuật mới như decoupled heads, strong augmentation (Moáic và Mixup), anchor-free, pulti-positives, và SimOTA.
Backbone thì ngoài Darknet ra cũng dùng thêm những backbone nhỏ hơn như Tiny.&lt;/p&gt;

&lt;p&gt;Dưới đây là kết quả inference của model YOLOX-Tiny:&lt;/p&gt;
&lt;iframe width=&quot;100%&quot; height=&quot;480&quot; src=&quot;https://www.youtube.com/embed/_5inpa6ruUY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;xây-dựng-tài-nguyên-gpu&quot;&gt;Xây dựng tài nguyên GPU&lt;/h1&gt;

&lt;h2 id=&quot;giá-thành-khi-train-model-state-of-the-art-sota&quot;&gt;Giá thành khi train model State-of-the-art (SOTA)&lt;/h2&gt;

&lt;p&gt;Nhìn chung là nếu chỉ hình ảnh với bộ dữ liệu nhỏ nhỏ như COCO &lt;a class=&quot;citation&quot; href=&quot;#lin2014microsoft&quot;&gt;[6]&lt;/a&gt; tầm trăm ngàn ảnh thì có bảng giá dưới đây: chúng ta lấy ví dụ từ báo cáo của 1 state-of-the-art thì họ dùng 8 cái V100, train tầm 6 ngày liên tục (\(6\times 24\) giờ) thì tổng tiền cho một lượt trên tầm ngàn Mỹ kim cho 6 ngày, 1 tháng cứ tầm 5 ngàn Mỹ kim.
Mà các bạn cũng nhớ giá này là giá &lt;a href=&quot;https://aws.amazon.com/ec2/spot/pricing/&quot;&gt;&lt;strong&gt;Spot&lt;/strong&gt;&lt;/a&gt; tức là có thể bị interrupt giữa chừng nên mới rẻ thế.
Chứ nếu bạn mà chọn &lt;a href=&quot;https://aws.amazon.com/ec2/pricing/on-demand/&quot;&gt;&lt;strong&gt;on-demand&lt;/strong&gt;&lt;/a&gt; thì có mà gấp 10 lần.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/detr-cost-analysis.jpg&quot; alt=&quot;pricing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nhưng ở trên mới chỉ là giá bộ COCO có hơn 100k ảnh nhé.
Bộ Open Images &lt;a class=&quot;citation&quot; href=&quot;#kuznetsova2020open&quot;&gt;[7]&lt;/a&gt; với 1.7 triệu ảnh thì còn máu nữa.
Search trên Kaggle mà có đồng chí chịu khó bỏ tiền ra ngồi train và báo cáo kết quả cho anh biết (xin cám ơn đồng chí): &lt;a href=&quot;https://www.kaggle.com/c/open-images-2019-object-detection/discussion/110953&quot;&gt;Kaggle Open Images 2019 challenge 6th place solution&lt;/a&gt;.
Thì kết quả là đồng chí ấy báo cáo:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;train 8 models trên V100 (chắc lại EC2 P3 thôi thì mình cứ dùng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3.xlarge&lt;/code&gt; để làm phân tích giá nhé) rồi ensemble.&lt;/li&gt;
  &lt;li&gt;mỗi model train mất 18-36 ngày (tùy model). Thì đồng chí này train 8 GPUs khác nhau.&lt;/li&gt;
  &lt;li&gt;sau khi train xong các model thì mất thêm 1 ngày nữa để inference và 1 ngày nữa để ensemble (dùng NMS).&lt;/li&gt;
  &lt;li&gt;Vậy tổng thể đã tiêu tốn \(36\times 8+1\times 8+1=297\) ngày train, tức là \(297\times 24=7128\) giờ train.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/open-images-price.jpg&quot; alt=&quot;pricing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3.xlarge&lt;/code&gt; thì giá mềm nhất là &lt;a href=&quot;https://aws.amazon.com/ec2/spot/pricing/&quot;&gt;&lt;strong&gt;Spot&lt;/strong&gt;&lt;/a&gt; cũng tầm $0.918/h.&lt;/p&gt;

&lt;p&gt;Tức là để train được accuracy tầm 60% đã mất \(7128\times 0.918\) tức là tầm 6543 Mỹ kim và hơn tháng ngồi monitor màn hình train.&lt;/p&gt;

&lt;h2 id=&quot;xây-dựng-hệ-thống-816-gpu&quot;&gt;Xây dựng hệ thống 8~16 GPU&lt;/h2&gt;

&lt;p&gt;Nhìn chung thì theo dòng lịch sử có 3 loại NVIDIA GPU dành cho cloud khá thông dụng như sau (tôi không nói tới hai dòng GTX và RTX nhé):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NVIDIA V100 hay &lt;strong&gt;Volta&lt;/strong&gt;: nói đến dòng này chúng ta có những sự lựa chọn chủ yếu liên quan tới V100 Tensor Core mà đại diện cho thuê là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3.16xlarge&lt;/code&gt; và &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3dn.24xlarge&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Với băng thông mạng của phiên bản P3.16xlarge cao hơn tới 4 lần, phiên bản P3dn.24xlarge của Amazon EC2 là sự bổ sung mới nhất cho dòng phiên bản P3, được tối ưu hóa cho machine learning phân tán và các ứng dụng HPC. Các phiên bản này cung cấp thông lượng kết nối mạng lên tới 100 Gbps, 96 vCPU Intel® Xeon® Có thể mở rộng (Skylake) tùy chỉnh, 8 GPU NVIDIA® V100 Tensor Core với 32 GB bộ nhớ mỗi GPU và 1,8 TB ổ lưu trữ SSD cục bộ chuẩn NVMe. Các phiên bản P3dn.24xlarge cũng hỗ trợ Elastic Fabric Adapter (EFA). Giao diện này tăng tốc các ứng dụng machine learning phân tán sử dụng Thư viện giao tiếp chung NVIDIA (NCCL). EFA có thể mở rộng quy mô lên đến hàng nghìn GPU, cải thiện đáng kể thông lượng và khả năng mở rộng của các mô hình huấn luyện deep learning, từ đó cho kết quả nhanh hơn.
Source: Amazon Web Service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;NVIDIA T4 hay &lt;strong&gt;Turing&lt;/strong&gt;: với AWS EC2 thì bạn có thể thuê &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g4dn.metal&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;NVIDIA A100 hay &lt;strong&gt;Ampere&lt;/strong&gt;: Với AWS EC2 thì có thể thuê &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p4d.24xlarge&lt;/code&gt;, với Azure HPC thì có thể thuê &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Standard_ND96amsr_A100_v4&lt;/code&gt;. GCP thì có &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a2-highgpu-8g&lt;/code&gt; hoặc bản 16 GPU là &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a2-highgpu-16g&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thì về mặt spec Ampere là khỏe nhất nếu nói về TFLOPS.
Dưới đây là bảng giá thành của NVIDIA 8x A100 Tensor Core.
Trong bảng này có 2 cột mà các bạn nên để ý là giá thành thuê theo giờ (&lt;strong&gt;Hourly cost&lt;/strong&gt;) và tỷ lệ GFLOPS/USD (đáng giá thế nào).
Giả định chung là hệ thống được xây dựng tối thiểu 4x GPU và được dùng ít nhất 24 tháng, mỗi tháng dùng 22 ngày (T7/CN nghỉ ngơi).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ampere.png&quot; alt=&quot;ampere&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nói chung tự build thì các bạn có thể tham khảo cấu hình của DGX-1&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, DGX-2&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, và DGX-A100&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; để mua các bộ phận về tự ráp thì sẽ tiết kiệm công lắp ráp, nhưng nhìn chung tôi nghĩ cũng phải 50 ngàn Mỹ Kim.&lt;/p&gt;

&lt;h2 id=&quot;các-cloud-solutions&quot;&gt;Các cloud solutions&lt;/h2&gt;
&lt;p&gt;Trong trường hợp bạn có bài toán train dữ liệu mà mất hàng tháng trời train với GTX/RTX thì bạn sẽ nghĩ phải thuê GPUs trên data center (8x-16x GPU).
Thì ngoài AWS/Azure/GCP là khá cùng rank nên bảng giá không chênh lệch nhau mấy, bạn có thể tham khảo thêm các trang cho thuê GPU bên ngoài để tìm được chỗ thuê hợp lý hơn.
Như kết quả tìm kiếm của AIFI thì hiện tại có trang &lt;a href=&quot;https://vast.ai&quot;&gt;vast.ai&lt;/a&gt; cung cấp khá nhiều sự lựa chọn cho thuê ở mức giá thấp hơn 5 USD/hour.&lt;/p&gt;

&lt;h2 id=&quot;còn-lời-giải-nào-khác&quot;&gt;Còn lời giải nào khác?&lt;/h2&gt;

&lt;p&gt;Nhìn chung tự build thì có hai khả năng:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mua đồ sẵn&lt;/strong&gt; như DGX&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:3:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; thì các bạn cứ chuẩn bị 100k Mỹ kim trở lên.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mua bộ phận về tự ráp&lt;/strong&gt; thì các bác tham khảo cấu hình của DGX rồi độ lại tùy theo nhu cầu. Tuy nhiên, chắc chỉ giảm được tiền công, và tối ưu một chút kiểu DGX dùng nhiều RAM thì mình giảm RAM xuống. Nói chung chắc cũng phải 50K Mỹ Kim.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Về cá nhân, tôi thiên về thuê!
Nếu tự build thì mua mấy cái RTX/GTX dòng Ti là ổn rồi.
Tuy nhiên nếu bài toán lớn thì bạn bắt buộc phải dùng data center GPU thì lúc ấy phải có &lt;strong&gt;TIỀN&lt;/strong&gt;!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Vấn đề của đi thuê là tiền tính theo giờ nên bạn cần phải ước lượng được số giờ sử dụng.&lt;/strong&gt; Nếu tầm trên 200h/tháng, tôi nghĩ nên thuê theo năm hoặc 3 năm.
Spot price thì cũng tàm tạm thôi, vì mất công chờ với nó ngắt điện (interupt) mình cũng phải chịu ấy, nên là rẻ nhưng lại mất thời gian chờ và bị ngắt.
Mà vấn đề với Spot là &lt;strong&gt;nó không có luôn ấy (phải chờ đến khi cái server ấy nó open mình mới được dùng)&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;kết-luận&quot;&gt;Kết luận&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hơi buồn nhưng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;đầu tiên vẫn là ... tiền đâu&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Như vậy chúng ta đã điểm qua một số SOTAs của &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; cho Vision và nhìn chung các &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;seq2seq&lt;/code&gt; vẫn đang nắm vị trí số 1.
Tuy vậy, vấn đề lớn khi “đua đòi” vào mảng này thì vẫn là tài nguyên thôi.
Nếu chuẩn bị được budget và plan nghiên cứu nghiêm chỉnh (mà đầu tiên là tiền đâu) thì về mặt nghiệp vụ PM tôi nghĩ không nên triển khai làm gì mất time anh em.
Ít nhất là cần vốn 200k Mỹ Kim thì cũng phải có tầm 100k trong túi hãy nghĩ!&lt;/p&gt;

&lt;h1 id=&quot;tài-liệu-tham-khảo&quot;&gt;Tài liệu tham khảo&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;dosovitskiy2020image&quot;&gt;Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. &lt;i&gt;arXiv preprint arXiv:2010.11929&lt;/i&gt;. (2020).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/dosovitskiy2020image/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;carion2020end&quot;&gt;Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. &lt;i&gt;European conference on computer vision&lt;/i&gt; (2020), 213–229.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/carion2020end/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;gao2021fast&quot;&gt;Gao, P., Zheng, M., Wang, X., Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. &lt;i&gt;Proceedings of the IEEE/CVF International Conference on Computer Vision&lt;/i&gt; (2021), 3621–3630.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/gao2021fast/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;redmon2018yolov3&quot;&gt;Redmon, J. and Farhadi, A. 2018. Yolov3: An incremental improvement. &lt;i&gt;arXiv preprint arXiv:1804.02767&lt;/i&gt;. (2018).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/redmon2018yolov3/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ge2021yolox&quot;&gt;Ge, Z., Liu, S., Wang, F., Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. &lt;i&gt;arXiv preprint arXiv:2107.08430&lt;/i&gt;. (2021).&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/ge2021yolox/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;lin2014microsoft&quot;&gt;Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L. 2014. Microsoft coco: Common objects in context. &lt;i&gt;European conference on computer vision&lt;/i&gt; (2014), 740–755.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/lin2014microsoft/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;kuznetsova2020open&quot;&gt;Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A., Duerig, T. and Ferrari, V. 2020. The open images dataset v4. &lt;i&gt;International Journal of Computer Vision&lt;/i&gt;. 128, 7 (2020), 1956–1981.&lt;/span&gt;&lt;a class=&quot;details&quot; href=&quot;https://wanted2.github.io/bibliography/kuznetsova2020open/&quot;&gt;Details&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://wanted2.github.io/seq2seq/&quot;&gt;https://wanted2.github.io/seq2seq/&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:4:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:4:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://wanted2.github.io/speech/&quot;&gt;https://wanted2.github.io/speech/&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:5:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;a href=&quot;#fnref:5:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-1/&quot;&gt;https://www.nvidia.com/en-us/data-center/dgx-1/&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-2/&quot;&gt;https://www.nvidia.com/en-us/data-center/dgx-2/&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/data-center/dgx-a100/&quot;&gt;https://www.nvidia.com/en-us/data-center/dgx-a100/&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sat, 12 Feb 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/seq2seq-cv/</link>
        <guid isPermaLink="true">https://wanted2.github.io/seq2seq-cv/</guid>
        
        <category>Object Detection</category>
        
        <category>Object Recognition</category>
        
        <category>Image Segmentation</category>
        
        <category>Temporal Segmentation</category>
        
        <category>Sequence-to-sequence</category>
        
        <category>seq2seq</category>
        
        <category>Transformer</category>
        
        <category>Self-supervised learning</category>
        
        <category>BERT</category>
        
        <category>Vision Transformer</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Artificial Intelligence</category>
        
        <category>Computer Vision</category>
        
      </item>
    
      <item>
        <title>Machine Learning for Network Intrusion Detection: From Local to Production</title>
        <description>&lt;h1 id=&quot;network-intrusion-detection-system&quot;&gt;Network Intrusion Detection System&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Simon-Enoch/publication/319637372/figure/fig1/AS:543679434510336@1506634686111/Configuration-of-the-enterprise-network.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Network intrusion detection system (NIDS)&lt;/strong&gt; is an independent platform that examines network traffic patterns to identify intrusions for an entire network. It needs to be placed at a choke point where all traffic traverses. A good location for this is in the DMZ.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An IDS is &lt;em&gt;reactive&lt;/em&gt; in nature: it only monitors and sends alerts to a group of specific people like administrators.
The above figure shows a common NIDS architecture, where a DMZ is placed in between &lt;em&gt;external firewall&lt;/em&gt; and &lt;em&gt;internal firewall (to an internal network)&lt;/em&gt;.
Here, in the DMZ, a NIDS can be set up to monitor the traffic of the whole corporation and identify the anomalies.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Vivek-Singh-70/publication/328572055/figure/fig1/AS:686824250945536@1540763070945/Major-Components-of-Snort-IDS-and-Bro-IDS.ppm&quot; style=&quot;float: right; margin: 10px; width: 50%;&quot; /&gt;
A NIDS can have the following architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;streaming engine&lt;/strong&gt; which ingests packet stream into the NIDS&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;package decoder&lt;/strong&gt; which turns packet content into visibility&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;detection engine&lt;/strong&gt; which identify intrusions&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;policy engine&lt;/strong&gt; which decide and suggest what to do with an intrusion&lt;/li&gt;
  &lt;li&gt;Finally, the intrusion details are collected and &lt;strong&gt;logged&lt;/strong&gt;. &lt;strong&gt;Alerts&lt;/strong&gt; will be sent to admins and optionally, scripted &lt;strong&gt;actions&lt;/strong&gt; can be performed in according to policies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two typical examples of NIDS are &lt;a href=&quot;https://www.snort.org/&quot;&gt;Snort IDS&lt;/a&gt; and &lt;a href=&quot;https://bricata.com/blog/zeek-ids-threat-detection/&quot;&gt;Bro IDS&lt;/a&gt;.
A nicer example that can be integrated into network monitoring can be &lt;a href=&quot;https://www.zabbix.com/features#smart_thresholds&quot;&gt;Zabbix’s Problem Detection Engine&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;machine-learning-at-scale-some-solutions-in-aws&quot;&gt;Machine Learning at scale: some solutions in AWS&lt;/h1&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The theory about NIDS perhaps is a huge bundle of knowledge!
Corporations have set up their own NIDS (in most cases in DMZ) for years.
We will not talk about such solutions anymore.
The most interesting part is in the cloud platforms like AWS.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;While companies are moving their resources to the cloud, where is the NIDS?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the &lt;a href=&quot;https://d1.awsstatic.com/Marketplace/scenarios/security/SEC_01_TSB_Final.pdf&quot;&gt;&lt;strong&gt;Shared Responsibility Model (SRM)&lt;/strong&gt;&lt;/a&gt;.
The &lt;strong&gt;responsibility&lt;/strong&gt; of protecting cloud resources like computing instances (EC2) and networking is of AWS.
Users have the responsibility to tune the best configurations of firewall, instances, load balancers, and other resources in AWS.
So a platform IDS is already managed at AWS, and the at the users (application developers), the remaining task is to &lt;strong&gt;implement a best Network/HIDS (endpoint protection)&lt;/strong&gt;.
There are several choices for a HIDS in the AWS Marketplace like &lt;a href=&quot;https://www.trendmicro.com/en_us/business/products/hybrid-cloud/security-data-center-virtualization.html&quot;&gt;TrendMicro’s Deep Security&lt;/a&gt;.
For a custom NIDS, users can implement a &lt;a href=&quot;https://aws.amazon.com/blogs/networking-and-content-delivery/centralized-inspection-architecture-with-aws-gateway-load-balancer-and-aws-transit-gateway/&quot;&gt;Transit DMZ Gateway&lt;/a&gt;. 
An examplar implementation can be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/aws-samples/aws-gateway-load-balancer-code-samples/raw/964874069c0a90d0b6758b2612c2de44a43f2a21/aws-cloudformation/centralized_architecture/images/gwlb_centralized_architecture.jpg&quot; alt=&quot;AWS Transit gateway&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;challenges-in-nids-operations&quot;&gt;Challenges in NIDS operations&lt;/h2&gt;

&lt;p&gt;A standard DMZ with NIDS can be implemented at the cloud or data center.
However, the &lt;strong&gt;hurdle&lt;/strong&gt; only comes when we operate our solution: this is the hard part!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When it comes to operations (運用保守), it often require an enormous amount of manual work!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We need a large number of low-paid workers who will sit in front of the monitoring screen and then manually mark each access as legal or not.
It is the real-world hurdle!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;However, the technical issues start when we want &lt;strong&gt;automation&lt;/strong&gt;: how to reduce false alarms and misses?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Good automated solutions will reduce manual work a lot.
But it is not straightforward!
Rule-based systems can have many misses or false alarms.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A weak rule misses many, but a strong rule alerts too much! (So both strong and weak ones are useless!)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;machine-learning-at-local&quot;&gt;Machine Learning at local&lt;/h1&gt;

&lt;p&gt;You have a dataset and perform some analytics at your local or edge PC.
You don’t use any server or cloud solution at all.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You should ask us why do you need to care about these works while cloud platforms already prepare so many ready-to-use solutions for you?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Agree! Machine learning engineers behind the platforms already do such works (model engineering).
Then when you do these works, that means you are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A student who is learning ML at an educational place (like university);&lt;/li&gt;
  &lt;li&gt;A researcher (or engineer) who perform a project for NGO, government, or a big company (who is building a platform solution); and&lt;/li&gt;
  &lt;li&gt;The worst luck: you’re only an ML enthusiast who is looking into ML when you have free time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not so many people do model engineering on their own laptop (or desktop), so in my experience, they fall into one of these three categories.
For the second category, people in that category is ML engineer/scientist who will make the model for thousands to million application developers over the world (who uses the platforms).
Such category is quite a few, and to be in, you need &lt;strong&gt;qualification&lt;/strong&gt;!
The most common cases are in the first.
The third category is possible but is rare and complex: while the first and second category have their own goals with ML (for studying and for works), the third category has no particular purpose.
They only do it in their free time and for fun (like doing a hobby)!
The first and second ones will have outcomes (successes and non-successes), but the third one is only for fun!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;No one can ban the hobby of a man, and we only do the hobby: we start when we want and stop when we don’t like it anymore!
That’s why I call it (the third category) the worst luck!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Actually, people in the third category already have another job (but that job does not relate to ML or even AI).
They do ML as hobbies but don’t complete anything (because ML is not their business, even more, ML does not give them bread and butter)!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is said, but in practice, if a thing doesn’t give any benefit, it won’t be done properly!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nevertheless, whatever your category is, when you do these things in your local computing environment, ML matters with you in some ways!
We will see what a &lt;strong&gt;local IDS model would look like&lt;/strong&gt;.
And, in a synthetic way!&lt;/p&gt;

&lt;h2 id=&quot;a-kaggle-synthetic-dataset&quot;&gt;A Kaggle synthetic dataset&lt;/h2&gt;

&lt;p&gt;We start with a synthetic dataset from &lt;a href=&quot;https://www.kaggle.com/sampadab17/network-intrusion-detection&quot;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The dataset to be audited was provided that consists of a wide variety of intrusions simulated in a military network environment. It created an environment to acquire raw TCP/IP dump data for a network by simulating a typical US Air Force LAN. The LAN was focused like a real environment and blasted with multiple attacks. A connection is a sequence of TCP packets starting and ending at some time duration between which data flows to and from a source IP address to a target IP address under some well-defined protocol. Also, each connection is labeled as either normal or as an attack with exactly one specific attack type. Each connection record consists of about 100 bytes.&lt;/p&gt;

  &lt;p&gt;For each TCP/IP connection, 41 quantitative and qualitative features are obtained from normal and attack data (3 qualitative and 38 quantitative features). The class variable has two categories:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Normal&lt;/li&gt;
    &lt;li&gt;Anomalous&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;explorative-analysis&quot;&gt;Explorative analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/ids-kaggle-feature-selection.png&quot; alt=&quot;feature selection&quot; /&gt;
&lt;em&gt;We notice that not all features are useful. Some features like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_Host_login&lt;/code&gt; have only a constant value.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our dataset is big enough (25K entries) and contains both categorical and numerical features:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;RangeIndex: 25192 entries, 0 to 25191
Data columns (total 42 columns):
 #   Column                       Non-Null Count  Dtype  
---  ------                       --------------  -----  
 0   duration                     25192 non-null  int64  
 1   protocol_type                25192 non-null  object 
 2   service                      25192 non-null  object 
 3   flag                         25192 non-null  object 
 4   src_bytes                    25192 non-null  int64  
 5   dst_bytes                    25192 non-null  int64  
 6   land                         25192 non-null  int64  
 7   wrong_fragment               25192 non-null  int64  
 8   urgent                       25192 non-null  int64  
 9   hot                          25192 non-null  int64  
 10  num_failed_logins            25192 non-null  int64  
 11  logged_in                    25192 non-null  int64  
 12  num_compromised              25192 non-null  int64  
 13  root_shell                   25192 non-null  int64  
 14  su_attempted                 25192 non-null  int64  
 15  num_root                     25192 non-null  int64  
 16  num_file_creations           25192 non-null  int64  
 17  num_shells                   25192 non-null  int64  
 18  num_access_files             25192 non-null  int64  
 19  num_outbound_cmds            25192 non-null  int64  
 20  is_Host_login                25192 non-null  int64  
 21  is_guest_login               25192 non-null  int64  
 22  count                        25192 non-null  int64  
 23  srv_count                    25192 non-null  int64  
 24  serror_rate                  25192 non-null  float64
 25  srv_serror_rate              25192 non-null  float64
 26  rerror_rate                  25192 non-null  float64
 27  srv_rerror_rate              25192 non-null  float64
 28  same_srv_rate                25192 non-null  float64
 29  diff_srv_rate                25192 non-null  float64
 30  srv_diff_Host_rate           25192 non-null  float64
 31  dst_Host_count               25192 non-null  int64  
 32  dst_Host_srv_count           25192 non-null  int64  
 33  dst_Host_same_srv_rate       25192 non-null  float64
 34  dst_Host_diff_srv_rate       25192 non-null  float64
 35  dst_Host_same_src_port_rate  25192 non-null  float64
 36  dst_Host_srv_diff_Host_rate  25192 non-null  float64
 37  dst_Host_serror_rate         25192 non-null  float64
 38  dst_Host_srv_serror_rate     25192 non-null  float64
 39  dst_Host_rerror_rate         25192 non-null  float64
 40  dst_Host_srv_rerror_rate     25192 non-null  float64
 41  class                        25192 non-null  object 
dtypes: float64(15), int64(23), object(4)
memory usage: 8.1+ MB
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/ids.png&quot; style=&quot;float: left; margin: 10px; width: 50%;&quot; /&gt;
Anyway, this is a synthetic dataset with some characteristics based on simulation.
However, it is close to real-world examples enough.
Next, we will try some machine learning models for predicting anomalies.&lt;/p&gt;

&lt;p&gt;With a visualization technique, like T-SNE, we have a diagram of data distribution.
A green point is a normal data point, and a red one is an anomaly.
We can see that it would be hard to draw a linear boundary between normal points and anomalies.&lt;/p&gt;

&lt;h3 id=&quot;play-with-some-machine-learners&quot;&gt;Play with some machine learners&lt;/h3&gt;

&lt;p&gt;In this section, we will try two different ML models with two different training/inference strategies:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Generative model with a reconstruction strategy&lt;/strong&gt;: the Auto-Encoder&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# creating the autoencoder model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;relu&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;sigmoid&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Discriminative model with a classification strategy&lt;/strong&gt;: the quite classical Random Forest!&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;gini&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;training-and-validation&quot;&gt;Training and validation&lt;/h4&gt;

&lt;p&gt;We need to transform the data a little bit.
It is usual to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MinMaxScaler&lt;/code&gt; to perform data transformation.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Auto-Encoder is trained in a reconstruction manner, i. e., it learns to reconstruct the input:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;validation_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But the Random Forest learns to classify data as usual:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_train_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;results-of-auto-encoder&quot;&gt;Results of Auto-Encoder&lt;/h4&gt;

&lt;p&gt;Since the prediction of AE relies on how good it can reconstruct the input:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the reconstruction error is over a threshold, then the reconstruction fails, and the input is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;anomaly&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Otherwise, the input is normal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we must define a &lt;strong&gt;threshold&lt;/strong&gt; to separate anomalies and normal inputs.
To find such threshold, we can compute from the training dataset:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# reconstructing the train set
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# finding the mean reconstruction error
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since the mean of construction error is 0.0018520295581492838, we can choose the threshold &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;thres = 0.0018&lt;/code&gt;.
Validation in validation set:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# classifying the samples based on threshold
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;anomaly&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thres&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;normal&apos;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Acc. = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[%], F1 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f1_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred_class&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos_label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;anomaly&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;[%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[[2901   69]
 [ 563 2765]]
Acc. = 89.96506827564306[%], F1 = 90.17718371153248[%]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;~90%&lt;/strong&gt; of F1 score. Hmm, not so bad!&lt;/p&gt;
&lt;h4 id=&quot;results-of-random-forest&quot;&gt;Results of Random Forest&lt;/h4&gt;

&lt;p&gt;Let’s see the results with Random Forest:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[[3321    7]
 [  14 2956]]
Acc. = 99.66656081295649[%], F1 = 99.64604753076016[%]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;99.6%&lt;/strong&gt;, it is quite good!&lt;/p&gt;

&lt;h2 id=&quot;some-methods-for-machine-learning-based-ids&quot;&gt;Some methods for Machine Learning based IDS&lt;/h2&gt;
&lt;p&gt;There are many ways to identify anomalous access (i. e., intrusion) in the network.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rule-based methods&lt;/strong&gt; tend to find a “good” heuristic that can be generalized to a global policy for all feature space.
The same policy can be applied to every input.
For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;every connection which lasted for more than 7 minutes are anomalies&lt;/code&gt; is a policy to identify intrusions.
The problem with this approach is that many false negatives (misses) can be raised.
Because the rule is clear, counterfactuals try to make themselves legal (try to connect faster) and overcome the 7-minute rule.
So a fixed rule is not enough.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The attacks become more and more advanced, but the rules cannot be changed so fast, making an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Achilles&apos; heel&lt;/code&gt; in the defense system.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Machine Learning-based methods&lt;/strong&gt; try to define a soft boundary that can be learned and improved over time.
The main advantage of the Machine Learning approach is that since the boundary is soft and there is no clear rule, the intruders cannot know the rule exactly (how many minutes should they make for a successful intrusion?).
Another important aspect is that since the soft rule is not fixed, it can &lt;strong&gt;evolve&lt;/strong&gt; with the intrusions: the more advanced the intrusion is, the more advanced the defender is.
When this sounds &lt;em&gt;ambiguous&lt;/em&gt;, but for the security systems, it becomes exactly a common strategy to overcome incidents.
The Machine Learning methods can be generative or discriminative, but they must be somewhat non-linear and ambiguous enough to hide details of the system to hackers.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;We have reviewed several views of NIDS: at a corporation network view, cloud solutions, and machine learning models.&lt;/p&gt;

&lt;p&gt;On the synthetic examples, we observed that a quite classical model like &lt;strong&gt;Random Forest&lt;/strong&gt; can outperform neural nets.
It is not a new thing: we already &lt;em&gt;empirically&lt;/em&gt; knew that Random Forest is good at this problem, especially when it is in synthetic environments.
Somebody can argue that there is a chance for overfitting with Random Forest: hmm, I don’t think so.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;And even if it overfits, this is quite good, because &lt;strong&gt;that’s what we want&lt;/strong&gt;: &lt;strong&gt;We want our model to overfit to this dataset&lt;/strong&gt;.&lt;/p&gt;

  &lt;p&gt;Anonymous&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Source code for this article can be found at:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/AIFI-INC/ml-ids&quot;&gt;AIFI-INC’s ML-based IDS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 29 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/ml-ids/</link>
        <guid isPermaLink="true">https://wanted2.github.io/ml-ids/</guid>
        
        <category>Machine learning</category>
        
        <category>network intrusion detection</category>
        
        <category>threat intelligence</category>
        
        <category>edge computing</category>
        
        <category>iot</category>
        
        
        <category>Site Reliable Engineering</category>
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
      </item>
    
      <item>
        <title>責任者</title>
        <description>&lt;p&gt;ベトナムに帰ってからベトナムの職場文化になってからはもはや2年間になっております．
ベトナム職場でいうと，恥ずかしいけど，楽しい経験もあるし，悲しい経験もありました．
仕事の責任者として働いた経験もあり，悲しい時で，部下に怒られて，そろそろ殴られる経験もありました．
なぜなら，背景から考えると，文化の違いかなと思います．
ベトナム職場では上下関係は社会的に存在するけど，きちんと働かないとね，上下関係なく殴られるそうです．
「殴られる」は厳しい言葉ですが，主にいうと，ベトナム職場で下記の三大原則はあります．&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則1：&lt;strong&gt;Có làm thì mới có ăn&lt;/strong&gt;. You must work to be fed. 職あり食ある．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則2：&lt;strong&gt;Có lỗi thì sửa là được&lt;/strong&gt;. Don’t be panic, just fix bugs, then it’s OK. 問題にダラダラしないで，修正すれば問題なし．（ただ何もしないなら，↑の原則１をみてください）．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;原則3：&lt;strong&gt;Làm thì làm cho nghiêm chỉnh.&lt;/strong&gt; You should do the work thoroughly. 徹底的にやりましょう．&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ベトナム職場の約2年間の経験をまとめて，一度整理したいと思っております．
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;第一原則職と食&quot;&gt;第一原則：「職」と「食」&lt;/h1&gt;

&lt;p&gt;我々は職場で「仕事」を大事にしております．
仕事が順調であれば，食感もよくなるという発想です．
仕事をしなければ，食えるもんがなく，大変ですね．
ですが，仕事（職）があって，就いたけど何もしないことで組織には不満が引き起こされたことが多いようです．&lt;/p&gt;

&lt;p&gt;ですので，組織全体で皆は統一して，職務をしてから食べるということを理解したらよかったです．
もちろん，海外から来る要員だと，現地の文化がわからず，仕事とは何か現地の人の考え方は最初でわからないこともあります．
しかし，時間が経過すると，その意義を理解すれば，原則を守る姿勢を整い，結局「労働」は第一だという考え方に帰着しました．
組織全体といえば，作業人だけではなく，管理職でもきちんと働き姿勢を見せればよいと思います．&lt;/p&gt;

&lt;p&gt;ここで，一番気になることは，この原則にはもう一つの意味の層があります．
仕事をやるといっても，何をやっても認められるのではないです．
「すごいことをやる」のも間違いです．
ある「職」に就く時，自分の役割と権限は決まっております．
ある場合，期待もあります．
しかし，開発の現場では，ほとんどの場合，役割と権限と責任範囲・仕事内容はすでに決まっております．
（※申し訳ないが，それぐらい曖昧に契約してしまえば，ちょっとそれがまずい職になるかもしれません）．
ですので，「自分の役割と権限・責任範囲の中に行動し，仕事を仕上げる」ことで職を行っているねといいます．
まあ，「職あり食ある」という句です．&lt;/p&gt;

&lt;p&gt;※曖昧に定めてしまった職に入った場合，もちろん仕事をしたから食べることがあるけど，ちょっと食べ物は美味しくないかもしれないです．
おいしい食をどうやって作るか山ほど研究がありますので，それらをGoogleして参考にすればと思います．&lt;/p&gt;

&lt;h1 id=&quot;第二原則チームは障害にだらだらしないで乗り越えることは大事&quot;&gt;第二原則：チームは障害にだらだらしないで，乗り越えることは大事！&lt;/h1&gt;
&lt;p&gt;チーム運営の中に，一緒に働くので，楽しい時も，悲しい時も一緒に乗り越えています．
楽しい時はいいんだけど，悲しい時はどんな時かな？
自分の経験では，パニック状態になるときです．
多くの場合，急に障害が起きるときとかですね．&lt;/p&gt;

&lt;p&gt;確かに，パニック状態のハンドルをうまく取り込めているチームはすごいねとおもいます．
パニック状態になった場合，よいチームは取り込むけど，悪いチームは責任追及ゲームを遊びます．
なぜ責任追及ゲームは悪いのかというと，緊急対応なのに，問題対応をせずに，ゲームをしているからです．
よいチームは問題を見極めて，対処法を第一優先し対応を取り込むのです．&lt;/p&gt;

&lt;p&gt;※ちなみに，チームで開発する場合には，運営中で時間がかかってしまう状態があります．
それが，&lt;strong&gt;パニック状態&lt;/strong&gt;と，&lt;strong&gt;会議状態&lt;/strong&gt;です．
なぜなら，この2つだけは，一人の時間を取るだけではなく，チーム皆の時間を取っているからです．
だらだらして，仕事が進まない罠に落ちやすいのです．
チームのパフォーマンスを上げたい場合に，責任者として，回避と対応策を計画しなければなりません．&lt;/p&gt;

&lt;p&gt;ベトナム職場では，なぜこの原則が取り込まれているかというと，チームがどこかにだらだらすると，めっちゃ時間がかかっているから，まずは乗り越えることを第一優先したいからです．
人がミスをすることは人間の根性ですので，それよりももっと悪いことは，&lt;strong&gt;「なにもしないこと」&lt;/strong&gt;です．
直さないことや修正しないことなどはまた，原則1で処分されると思います．
いつもチームを前向きに進行させることができればと思います．&lt;/p&gt;

&lt;h1 id=&quot;第三原則自分の仕事へのこだわりも重視完成したらもう一度見直そう&quot;&gt;第三原則：自分の仕事へのこだわりも重視！完成したら，もう一度見直そう！&lt;/h1&gt;

&lt;p&gt;2年間ベトナム職場で感じたもう一つの原則です．
第一原則と第二原則を相互に働かせているため，「作成」と「修正」を交互に行っています．
しかし，これらを交互に働かせると，永遠に修正のループに入る可能性はまだ残っています．
ですので，途中でもっと修正してもあまり報われないと感じるときとか，早く止めた方がよいと感じるときとか，もう一つのコントローラーが必要でしょうか．
それは第三原則です．
必ず，（再）作成と（再）修正が完成したら，もう一度&lt;strong&gt;客観的&lt;/strong&gt;にレビューしましょう．
狭い視野で詳細をレビューするだけではなく，広い視野で，この作成と修正は長期的によいか悪いか一回考えるべきというステップがあれば，なおよいです．
ここで，責任者として，技術部分だけではなく，スコープ管理とか要望管理とかスケジュール管理とかうまく併せてやる必要があります．&lt;/p&gt;

&lt;h1 id=&quot;結論&quot;&gt;結論&lt;/h1&gt;

&lt;p&gt;これが，おそらく私が2年間で観測したベトナム職場で支配されている三大原則かと思います．
これらは，なぜか職場をコントロールして，各PJを進行させることが多いそうです．&lt;/p&gt;
</description>
        <pubDate>Sun, 16 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/pm-sekininsha/</link>
        <guid isPermaLink="true">https://wanted2.github.io/pm-sekininsha/</guid>
        
        <category>責任者</category>
        
        <category>担当者</category>
        
        <category>権限管理</category>
        
        <category>計画管理</category>
        
        <category>進捗管理</category>
        
        <category>スコープ管理</category>
        
        <category>リスク管理</category>
        
        <category>コミュニケーション管理</category>
        
        <category>リソース管理</category>
        
        <category>プロジェクトマネジメント</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
        <category>Project Management</category>
        
      </item>
    
      <item>
        <title>RapidAPI and RapidAPI Hub</title>
        <description>&lt;p&gt;&lt;em&gt;Image Credit: &lt;a href=&quot;https://financefeeds.com/rakuten-launches-api-marketplace/&quot;&gt;FinanceFeeds&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Rakuten launched RapidAPI Marketplace in 2018 as a result of the collaboration between Japan’s Rakuten Inc and San Francisco-based startup RapidAPI.
&lt;a href=&quot;https://api.rakuten.co.jp/en/&quot;&gt;The API marketplace&lt;/a&gt; aims to provide software developers in Japan and Asia unified access to more than 8,000 APIs with localized documentation and resources in Japan’s language and English.
The API marketplace platform will connect API providers and developers.
Developers in Japan and across Asia will be able to find, test, and connect to thousands of APIs for their applications.
The marketplace will also allow API providers to connect with the global developer community through personalized API portals.
 &lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;what-is-rapidapi&quot;&gt;What is RapidAPI?&lt;/h1&gt;

&lt;p&gt;Let us assume that you have an API that is ready for production.
You need to add authentication like API key, OAuth 2, or something else.
You need to deploy your API to somewhere that is stable and reliable.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the &lt;strong&gt;shortest path&lt;/strong&gt; to achieving your goal?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You are an application developer, and you need to manage the records of some data for the app.
For example, you need to maintain the list of public holidays in your app.
You don’t want to hardcode those things in the code.
Note that the public holidays change between countries and sometimes due to the law it will change between years.
It is somewhat troublesome to maintain the records in your database as it will make you allocate some effort and human resources there.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is the most convenient way to maintain such data?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In both scenarios, Rakuten RapidAPI Marketplace gives you excellent solutions.
Either maintenance of the data (public holidays) or publishing a new API, you can do all of the lifecycles in one platform.&lt;/p&gt;

&lt;p&gt;For example, when you want to check a day is a holiday or not, you can thus search for a free API like this &lt;a href=&quot;https://english.api.rakuten.net/theapiguy/api/public-holiday&quot;&gt;one&lt;/a&gt; and make a request.
Because all maintenance is up on the providers, this solution costs you nothing: you don’t need to worry about maintaining the records of holidays data (which shouldn’t be your matter in any way) and focus on your own application logic.
Note that the &lt;a href=&quot;https://english.api.rakuten.net/theapiguy/api/public-holiday&quot;&gt;Public Holidays API&lt;/a&gt; has low latency (59ms) and is completely free.
Another solution is to build an endpoint in your own API like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/api/v1/holidays&lt;/code&gt; to validate the holidays, but while such a ready-to-use solution is there, why should you waste time and money to build/manage/maintain on your own?&lt;/p&gt;

&lt;p&gt;RapidAPI helps your API to distribute and monetize.
Adding your API to the RapidAPI Hub gets you instant exposure to our growing user base, a search-engine-optimized profile page for your API, as well as features like user management and billing services.
RapidAPI also serves functional testings, API monitoring dashboards, and many other premiere features like API authentication.&lt;/p&gt;

&lt;h1 id=&quot;rapidapi-for-api-vendors&quot;&gt;RapidAPI for API Vendors&lt;/h1&gt;

&lt;p&gt;The workflow between an app developer’s client to a vendor API can be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rapidapi.svg&quot; alt=&quot;rapidapi&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;An &lt;a href=&quot;https://docs.rapidapi.com/docs/keys&quot;&gt;API Key&lt;/a&gt; is generated and appended to the request’s header to RapidAPI servers.&lt;/li&gt;
  &lt;li&gt;RapidAPI authenticate the request (using API Key and optionally a configured authentication method like OAuth 2). Then it modifies the requests header to append &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X-RapidAPI-*&lt;/code&gt; headers.&lt;/li&gt;
  &lt;li&gt;The vendor API (destination API in the diagram) checks the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X-RapidAPI-*&lt;/code&gt; headers and authenticates the modified requests.&lt;/li&gt;
  &lt;li&gt;A response is generated according to the requested information and is then returned to RapidAPI.&lt;/li&gt;
  &lt;li&gt;RapidAPI modifies the response from vendor servers. It appends Rapid API headers (for example, headers about rate limits) or generates a new response.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As you can see, RapidAPI Marketplace acts as a proxy between app servers (client in the diagram) and the vendor API servers.
The vendors &lt;a href=&quot;https://docs.rapidapi.com/docs/add-an-api-basics&quot;&gt;register&lt;/a&gt; their APIs and &lt;a href=&quot;https://docs.rapidapi.com/docs/add-an-api-advanced-settings&quot;&gt;fine-tune&lt;/a&gt; the settings in RapidAPI dashboard.
All API endpoints are relative to a base URL, which is added as a “prefix” to all API endpoints.
This approach avoids the need to define absolute URLs for endpoints every time and increases API portability by changing the base URL.&lt;/p&gt;

&lt;p&gt;API vendors can &lt;a href=&quot;https://docs.rapidapi.com/docs/configuring-api-authentication&quot;&gt;add&lt;/a&gt; basic authentication or OAuth 2 to their APIs.&lt;/p&gt;

&lt;p&gt;RapidAPI supports &lt;a href=&quot;https://docs.rapidapi.com/docs/automating-api-provisioning&quot;&gt;automatic API provisioning using OpenAPI&lt;/a&gt; and &lt;a href=&quot;https://docs.rapidapi.com/docs/transformations&quot;&gt;custom transformations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;RapidAPI has basic plan options so app developers can choose among these options to pay:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;API Type&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Free APIs&lt;/td&gt;
      &lt;td&gt;APIs that do not require a credit card or subscription to consume.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pay Per Use&lt;/td&gt;
      &lt;td&gt;APIs that don’t have a subscription fee associated with them. A credit card is required as you pay for what you use on the API.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Freemium APIs&lt;/td&gt;
      &lt;td&gt;Paid APIs that also include a limited free tier. These require a credit card, even for the free plan.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Paid APIs&lt;/td&gt;
      &lt;td&gt;APIs that require a paid subscription plan and credit card to consume.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;some-notes-on-security&quot;&gt;Some notes on security&lt;/h2&gt;

&lt;p&gt;RapidAPI supports &lt;a href=&quot;https://docs.rapidapi.com/docs/secret-headers-parameters&quot;&gt;secret headers and parameters&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;RapidAPI allows you to add secret headers and/or query string parameters to API requests. The RapidAPI proxy adds these secrets to every request but is &lt;strong&gt;hidden from the API consumers&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that even the consumers who make the requests do not know about these secrets.
This differs from header and query authentication methods where consumers know all secrets in the requests they make to RapidAPI.&lt;/p&gt;

&lt;p&gt;Users should configure RapidAPI &lt;a href=&quot;https://docs.rapidapi.com/docs/security-threat-protection&quot;&gt;security&lt;/a&gt; features like firewalls, threat protection, schema validation, and request size limit (which returns error code 413).&lt;/p&gt;

&lt;p&gt;Vendors can set their API to &lt;a href=&quot;https://docs.rapidapi.com/docs/private-apis-api-logo&quot;&gt;private&lt;/a&gt; where only invited users can access.&lt;/p&gt;

&lt;h2 id=&quot;audit-and-marketing-tools&quot;&gt;Audit and marketing tools&lt;/h2&gt;

&lt;p&gt;RapidAPI provides &lt;a href=&quot;https://docs.rapidapi.com/docs/provider-dashboard&quot;&gt;Provider Dashboard&lt;/a&gt; where vendors can monitor their API usages.
Another nice thing is that as a vendor, you can make your monetization more useful using &lt;a href=&quot;https://docs.rapidapi.com/docs/ive-added-my-api-to-rapidapi-now-what&quot;&gt;Marketing API&lt;/a&gt;.
When you have an API, you should make sure you don’t miss a checklist when publishing your solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://api.rakuten.co.jp/docs/ja-images/ProviderWelcome_1.png&quot; alt=&quot;RapidAPI&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This checklist helps you have a better SEO for your API.&lt;/p&gt;

&lt;h2 id=&quot;api-testing&quot;&gt;API Testing&lt;/h2&gt;

&lt;p&gt;Testing is quite tedious!
RapidAPI helps vendors reduce testing costs with their &lt;a href=&quot;https://docs.rapidapi.com/docs/rapidapi-testing-overview&quot;&gt;API testing feature&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.readme.io/726dc84-run-code.png&quot; alt=&quot;RapidAPI testing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you are already familiar with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postman-tool&lt;/code&gt; you are ready to go with RapidAPI &lt;a href=&quot;https://docs.rapidapi.com/docs/create-a-test-advanced&quot;&gt;advanced testing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://files.readme.io/fabfeb1-Screen_Shot_2020-12-03_at_4.00.53_PM.png&quot; alt=&quot;Advanced testing&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;rapidapi-for-app-developers&quot;&gt;RapidAPI for App Developers&lt;/h1&gt;

&lt;p&gt;As an app developer, you can find that &lt;a href=&quot;https://rapidapi.com/hub&quot;&gt;RapidAPI Hub&lt;/a&gt; now has more than 10,000 APIs.
Even you want to develop an OCR app or a Translation app, you can find your API right away.&lt;/p&gt;

&lt;p&gt;All you need is to register a RapidAPI account, choose your API and then &lt;strong&gt;make a payment&lt;/strong&gt;.
Finally, you can &lt;a href=&quot;https://docs.rapidapi.com/docs/connecting-to-an-api&quot;&gt;connect&lt;/a&gt; to your paid API using the API key.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rapidapi-vin.png&quot; alt=&quot;RapidAPI VIN&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;It is worth noting that RapidAPI supports not only REST API but also &lt;a href=&quot;https://docs.rapidapi.com/docs/graphql-apis&quot;&gt;GraphQL&lt;/a&gt;, &lt;a href=&quot;https://docs.rapidapi.com/docs/adding-soap-apis&quot;&gt;SOAP&lt;/a&gt;, and &lt;a href=&quot;https://docs.rapidapi.com/docs/kafka-apis&quot;&gt;Kafka&lt;/a&gt; APIs.
We did not touch &lt;a href=&quot;https://docs.rapidapi.com/docs/what-is-rapidapi-for-teams&quot;&gt;RapidAPI for Teams&lt;/a&gt;, but it might be useful at the organization level.&lt;/p&gt;
</description>
        <pubDate>Sun, 09 Jan 2022 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/rapidapi/</link>
        <guid isPermaLink="true">https://wanted2.github.io/rapidapi/</guid>
        
        <category>api development</category>
        
        <category>backend</category>
        
        <category>infrastructure</category>
        
        <category>rapidapi</category>
        
        
        <category>Software Engineering</category>
        
        <category>Artificial Intelligence</category>
        
        <category>Site Reliable Engineering</category>
        
      </item>
    
      <item>
        <title>Chia tay 2021!</title>
        <description>&lt;p&gt;Đây là bài post thứ 61 của blog AiFi trong năm 2021, cũng là bài viết chia tay 2021, trong tâm thế đón chờ 2022 tươi mới hơn.
Theo quan điểm làm việc scrum, thì coi như đây là thời điểm kết thúc 1 chu kỳ, cũng là lúc làm một số việc để nhìn lại một năm đã qua (bao gồm cả GKPT hay &lt;em&gt;Good, Keep, Problem, Try&lt;/em&gt;).
2021年中61番目の投稿です．
2021年と別れて，2022年を迎える時期の投稿です．
一年間を1スプリントとすると，いろいろなことができたと思いますので，スクラムの行事として，レビューとレトロ会をここで開催したいと思います．
&lt;em&gt;Good, Keep, Problem, Try&lt;/em&gt; も含めてやります．
&lt;!--more--&gt;&lt;/p&gt;

&lt;h1 id=&quot;nhìn-lại-năm-2021-của-blog-aifi&quot;&gt;Nhìn lại năm 2021 của blog AiFi&lt;/h1&gt;

&lt;h2 id=&quot;nhìn-từ-thống-kê-người-dùng&quot;&gt;Nhìn từ thống kê người dùng&lt;/h2&gt;

&lt;p&gt;Hiện tại AiFi blog sử dụng Google Analytics để track và lấy thống kê người dùng.
Các sự kiện như view, scroll, referal, … được báo cáo theo phút lên server của Google.&lt;/p&gt;

&lt;p&gt;Đầu tiên là thống kê về người dùng và nguồn giới thiệu.
Trong năm 2021, blog tuy mới ra mắt và còn nhiều khó khăn vất vả nhưng đã thu hút được 552 user mới từ khắp nơi trên thế giới.
&lt;strong&gt;552 người dùng này đã ghi lại 7309 sự kiện.&lt;/strong&gt;
Một con số đáng khích lệ với blog mới 1 năm tuổi đời.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Một điểm đáng chú ý là dù facebook.com là nơi tác giả hay chia sẻ bài viết, nhưng &lt;strong&gt;user lại phần lớn đến từ 2 nguồn: google và direct&lt;/strong&gt;.
Về yếu tố địa lý thì đa phần người dùng đến từ &lt;strong&gt;Việt Nam, Mỹ và Nhật Bản&lt;/strong&gt;.
Các nước khác vẫn chưa đóng tỷ trọng lớn trong cơ cấu người dùng của AiFi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-02.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Tỷ lệ người dùng của AiFi gia tăng tính từ tháng &lt;strong&gt;7&lt;/strong&gt;.
Trong năm 2021, &lt;strong&gt;số lượng sự kiện &lt;em&gt;user engagement&lt;/em&gt; là 1852, và số &lt;em&gt;page view&lt;/em&gt; là 2622 lượt&lt;/strong&gt;.
Ngoài ra, 3 bài viết đạt số lượng truy cập cao nhất (không tính trang chủ) là:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/mOCR-mlkit-androidx-example/&quot;&gt;mOCR: A real-time application of OCR with Google MLKit and Android CameraX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/adobe-creative-cloud/&quot;&gt;Adobe Creative Cloud: An All-in-One Platform for Creators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/aws-lambda-spacy-mxnet-possible-but-shouldnt/&quot;&gt;Implementing a complex system in AWS Lambda: Should or shouldn’t?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sự “vùng lên” của bài viết &lt;a href=&quot;/adobe-creative-cloud/&quot;&gt;Adobe Creative Cloud: An All-in-One Platform for Creators&lt;/a&gt; thật thú vị vì bài viết được xuất bản trên blog AiFi vào tháng cuối năm nhưng lại đứng thứ nhì.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-03.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Về hệ điều hành, trình duyệt và ngôn ngữ đầu vẫn là &lt;strong&gt;Windows, Chrome và English&lt;/strong&gt;.
Theo sau lần lượt là &lt;strong&gt;MacOS, Safari và tiếng Nhật&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/analytics-2021-04.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;nhìn-từ-kết-quả-tìm-kiếm&quot;&gt;Nhìn từ kết quả tìm kiếm&lt;/h2&gt;

&lt;p&gt;Kết quả tìm kiếm về “AiFi Caineng” trên google.com và Bing Search trong ngày 31 tháng 12 năm 2021 như sau:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aifi-search-engines-2021.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kết quả tìm kiếm từ khóa “aifi” và thậm chí “aifi caineng” quả là hơi nghèo nàn và dễ bị lẫn vào các từ khóa tìm kiếm khác như “wifi” chẳng hạn.
Đây cũng là 1 thiếu sót do blog mới chỉ 1 năm, và tác giả vẫn đang bận bịu công việc chính cuả tác giả.
Tuy nhiên, từ năm 2022, ở mức độ nhất định việc nâng rank trong các cỗ máy tìm kiếm từ khóa sẽ được &lt;strong&gt;tối ưu hóa&lt;/strong&gt; nhằm đưa tri thức của AiFi đến với đông đảo bạn đọc và nâng cao chất lượng phục vụ.&lt;/p&gt;

&lt;h1 id=&quot;good-keep-problem-try&quot;&gt;Good, Keep, Problem, Try&lt;/h1&gt;

&lt;p&gt;Việc chạy sprint kéo dài 1 năm quả là hơi lạ, tuy nhiên là cũng dễ hiểu vì viết blog chỉ là việc phụ làm trong thời gian rảnh rỗi của tác giả.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Good&lt;/th&gt;
      &lt;th&gt;Keep&lt;/th&gt;
      &lt;th&gt;Problem&lt;/th&gt;
      &lt;th&gt;Try&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Đã tạo được và thu hút lượng người dùng nhất định.&lt;/td&gt;
      &lt;td&gt;Duy trì tần suất chia sẻ bài viết.&lt;/td&gt;
      &lt;td&gt;Thứ hạng trên search engine chưa cao.&lt;/td&gt;
      &lt;td&gt;Tối ưu hóa SEO&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Tối ưu hóa từ khóa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Tối ưu thẻ HTML, …&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Chưa tạo ra thu nhập từ blog&lt;/td&gt;
      &lt;td&gt;Xem xét đưa vào và tối ưu hóa quảng cáo.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Các nguồn Google và Facebook đã đem đến lượng người dùng nhất định.&lt;/td&gt;
      &lt;td&gt;Tiếp tục duy trì quảng bá trên Google và Facebook.&lt;/td&gt;
      &lt;td&gt;Nguồn Facebook chưa đem lại nhiều người dùng mới.&lt;/td&gt;
      &lt;td&gt;Tối ưu hóa quảng bá blog trên Facebook.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;Một số nguồn cấp khác như Twitter và LinkedIn vẫn chưa đem lại nhiều người dùng.&lt;/td&gt;
      &lt;td&gt;Lên chiến lược quảng bá trên các nền tảng này.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;tổng-kết&quot;&gt;Tổng kết&lt;/h1&gt;

&lt;p&gt;Kết thúc Sprint 2021, hướng tới Sprint 2022, blog AiFi xin cám ơn đông đảo bạn đọc, đặc biệt là 552 người dùng đã có, vì sự quan tâm và thịnh tình trong năm qua.
Trong năm 2022, AiFi sẽ tiếp tục cập nhật và mong muốn lan tỏa tri thức cho anh em, với phương châm, troll trước học sau.&lt;/p&gt;
</description>
        <pubDate>Fri, 31 Dec 2021 00:00:00 +0900</pubDate>
        <link>https://wanted2.github.io/year-end/</link>
        <guid isPermaLink="true">https://wanted2.github.io/year-end/</guid>
        
        <category>Event</category>
        
        <category>Year-end event</category>
        
        
        <category>Tiếng Việt, 日本語</category>
        
      </item>
    
  </channel>
</rss>
