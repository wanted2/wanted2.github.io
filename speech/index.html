<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="https://wanted2.github.io/assets/images/favicon.ico">

<title>Speech and Sequence-to-sequence | AiFi</title>

 
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-2CDTCF0EP6" crossorigin="anonymous"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-2CDTCF0EP6');
        </script>
    


<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Speech and Sequence-to-sequence | AiFi</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Speech and Sequence-to-sequence" />
<meta name="author" content="tuan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Như trong bài viết trước thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì seq2seq đều đang làm chủ. Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà seq2seq và các hậu duệ (thuộc dòng dõi Transformer [27] và BERT [27, 27]) cũng đang nắm thế chủ động. Khởi đầu bài viết, tôi định viết lại về i-vector [27, 27] và x-vector [27] là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị seq2seq chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer [27, 27] 12. Với bài toán SI thì cách làm cổ điển sẽ là dùng Gaussian Mixture Models (GMM, [27, 27, 27, 27]), còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì Deep Neural Networks (DNN) với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, [27]) lẫn ASR (DNN-HMM, [27]). Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy. Gần đây thì đến lượt những tiến bộ bắt nguồn từ seq2seq DNN-RNN, rồi đến những model end-to-end, transformers. Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. &#8617; Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn đồ ăn sẵn thì các bạn có thể tham khảo Amazon Lex. &#8617;" />
<meta property="og:description" content="Như trong bài viết trước thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì seq2seq đều đang làm chủ. Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà seq2seq và các hậu duệ (thuộc dòng dõi Transformer [27] và BERT [27, 27]) cũng đang nắm thế chủ động. Khởi đầu bài viết, tôi định viết lại về i-vector [27, 27] và x-vector [27] là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị seq2seq chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer [27, 27] 12. Với bài toán SI thì cách làm cổ điển sẽ là dùng Gaussian Mixture Models (GMM, [27, 27, 27, 27]), còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì Deep Neural Networks (DNN) với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, [27]) lẫn ASR (DNN-HMM, [27]). Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy. Gần đây thì đến lượt những tiến bộ bắt nguồn từ seq2seq DNN-RNN, rồi đến những model end-to-end, transformers. Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. &#8617; Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn đồ ăn sẵn thì các bạn có thể tham khảo Amazon Lex. &#8617;" />
<meta property="og:site_name" content="AiFi" />
<meta property="og:image" content="https://fosspost.org/wp-content/uploads/2019/02/rendered.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-06T00:00:00+09:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://fosspost.org/wp-content/uploads/2019/02/rendered.jpg" />
<meta property="twitter:title" content="Speech and Sequence-to-sequence" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"tuan"},"dateModified":"2022-02-06T00:00:00+09:00","datePublished":"2022-02-06T00:00:00+09:00","description":"Như trong bài viết trước thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì seq2seq đều đang làm chủ. Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà seq2seq và các hậu duệ (thuộc dòng dõi Transformer [27] và BERT [27, 27]) cũng đang nắm thế chủ động. Khởi đầu bài viết, tôi định viết lại về i-vector [27, 27] và x-vector [27] là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị seq2seq chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer [27, 27] 12. Với bài toán SI thì cách làm cổ điển sẽ là dùng Gaussian Mixture Models (GMM, [27, 27, 27, 27]), còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì Deep Neural Networks (DNN) với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, [27]) lẫn ASR (DNN-HMM, [27]). Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy. Gần đây thì đến lượt những tiến bộ bắt nguồn từ seq2seq DNN-RNN, rồi đến những model end-to-end, transformers. Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. &#8617; Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn đồ ăn sẵn thì các bạn có thể tham khảo Amazon Lex. &#8617;","headline":"Speech and Sequence-to-sequence","image":"https://fosspost.org/wp-content/uploads/2019/02/rendered.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"/https://wanted2.github.io/speech/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/https://wanted2.github.io/assets/images/favicon.ico"},"name":"tuan"},"url":"/https://wanted2.github.io/speech/"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="https://wanted2.github.io/assets/css/screen.css" rel="stylesheet">

<link href="https://wanted2.github.io/assets/css/main.css" rel="stylesheet">

<script src="https://wanted2.github.io/assets/js/jquery.min.js"></script>
<script src="https://kit.fontawesome.com/d0b91d895e.js" crossorigin="anonymous"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
    }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" crossorigin="anonymous"></script>
<script src="https://d3js.org/d3.v4.js" crossorigin="anonymous"></script>
<!-- <script src="https://bl.ocks.org/mbostock/raw/4061502/0a200ddf998aa75dfdb1ff32e16b680a15e5cb01/box.js" crossorigin="anonymous"></script> -->
</head>


<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="https://wanted2.github.io/">
    <img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="https://wanted2.github.io/">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/about">About</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/projects">Projects</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-wordpress/"><i class="fab fa-wordpress-simple"></i> WP Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-ghost/"><i class="fab fa-snapchat-ghost"></i> Ghost Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/wowthemesnet/mediumish-theme-jekyll"><i class="fab fa-github"></i> Fork on Github</a>
                </li> -->

                <!-- <script src="https://wanted2.github.io/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="https://wanted2.github.io/assets/js/lunrsearchengine.js"></script> -->

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">AiFi</h1>
    <p class="lead">
        An AI Engineer's blog
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Speech and Sequence-to-sequence&url=https://wanted2.github.io/speech/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=https://wanted2.github.io/speech/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=https://wanted2.github.io/speech/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="mailto:?subject=Speech and Sequence-to-sequence&body=https://wanted2.github.io/speech/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fas fa-envelope"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
    
    <div class="sep">

    </div>
    <ul>
        <li class="small">
        2692
     words</li>
        <li class="small">14 minutes</li>
    </ul>
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-2 col-lg-2 text-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="https://wanted2.github.io/assets/images/favicon.png" alt="AiFi">
                        
                    </div>
                    <div class="col-xs-12 col-md-10 col-lg-10 text-right">
                        <a target="_blank" class="link-dark" href="">AiFi</a>
                        <!-- <a target="_blank" href="" class="btn follow">Follow</a> -->
                        <!-- LikeBtn.com BEGIN -->
                        <span class="likebtn-wrapper" 
                            data-site_id="61cfccd36fd08b2d68c1929e"
                            data-theme="custom" 
                            data-icon_l_url="/assets/images/OK_EM.png" 
                            data-icon_l_url_v="/assets/images/OK_EM_clicked.png" 
                            data-identifier="/speech/" 
                            data-show_like_label="false" 
                            data-like_enabled="false" 
                            data-dislike_enabled="false" 
                            data-icon_dislike_show="false" 
                            data-voting_cancelable="false" 
                            data-counter_show="true"
                            data-counter_frmt="comma"></span>
                        <script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
                        <!-- LikeBtn.com END -->
                        <span class="author-description"></span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Speech and Sequence-to-sequence</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid lazyimg" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=" data-src="https://fosspost.org/wp-content/uploads/2019/02/rendered.jpg" alt="Speech and Sequence-to-sequence">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                    
                    <div class="toc mt-4 mb-4 lead">
                        <h3 class="font-weight-bold">Summary</h3>
                        <ul>
  <li><a href="#giới-thiệu-chung-về-asrsi-và-các-bài-toán-liên-quan">Giới thiệu chung về ASR/SI và các bài toán liên quan</a>
    <ul>
      <li><a href="#automatic-speech-recognition">Automatic Speech Recognition</a></li>
      <li><a href="#sound-classification-và-speaker-identification">Sound classification và Speaker Identification</a></li>
      <li><a href="#voice-conversion-text-to-speech-và-speech-synthesis">Voice Conversion, Text-To-Speech và Speech Synthesis</a></li>
    </ul>
  </li>
  <li><a href="#công-nghệ-chính">Công nghệ chính</a>
    <ul>
      <li><a href="#i-vector-và-x-vector-cho-bài-toán-speaker-identification">i-vector và x-vector cho bài toán Speaker Identification</a></li>
      <li><a href="#seq2seq-và-các-transformers-cho-bài-toán-asr"><code class="language-plaintext highlighter-rouge">seq2seq</code> và các Transformers cho bài toán ASR</a></li>
    </ul>
  </li>
  <li><a href="#lưu-ý-khi-train-model-asrsi">Lưu ý khi train model ASR/SI</a>
    <ul>
      <li><a href="#giá-thành-khi-train-model-state-of-the-art-sota">Giá thành khi train model State-of-the-art (SOTA)</a></li>
    </ul>
  </li>
  <li><a href="#tài-liệu-tham-khảo">Tài liệu tham khảo</a></li>
</ul>
                    </div>
                
                <!-- End Toc -->
                <p>Như trong <a href="/seq2seq/">bài viết trước</a> thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì <code class="language-plaintext highlighter-rouge">seq2seq</code> đều đang làm chủ.
Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà <code class="language-plaintext highlighter-rouge">seq2seq</code> và các hậu duệ (thuộc dòng dõi Transformer <a class="citation" href="#vaswani2017attention">[1]</a> và BERT <a class="citation" href="#devlin2019bert">[2, 3]</a>) cũng đang nắm thế chủ động.
Khởi đầu bài viết, tôi định viết lại về i-vector <a class="citation" href="#kenny2007joint">[4, 5]</a> và x-vector <a class="citation" href="#snyder2018x">[6]</a> là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị <code class="language-plaintext highlighter-rouge">seq2seq</code> chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer <a class="citation" href="#gulati2020conformer">[7, 8]</a> <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.
Với bài toán SI thì cách làm cổ điển sẽ là dùng <strong>Gaussian Mixture Models (GMM, <a class="citation" href="#reynolds1995robust">[9, 10, 4, 5]</a>)</strong>, còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì <strong>Deep Neural Networks (DNN)</strong> với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, <a class="citation" href="#snyder2018x">[6]</a>) lẫn ASR (DNN-HMM, <a class="citation" href="#hinton2012deep">[11]</a>).
Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy.
Gần đây thì đến lượt những tiến bộ bắt nguồn từ <code class="language-plaintext highlighter-rouge">seq2seq</code> DNN-RNN, rồi đến những model end-to-end, transformers.</p>

<!--more-->

<h1 id="giới-thiệu-chung-về-asrsi-và-các-bài-toán-liên-quan">Giới thiệu chung về ASR/SI và các bài toán liên quan</h1>

<h2 id="automatic-speech-recognition">Automatic Speech Recognition</h2>
<p>Automatic Speech Recognition (ASR, <a class="citation" href="#yu2016automatic">[12]</a>) là nhiệm vụ chuyển một chuỗi âm thanh giọng nói (waveform) sang một chuỗi văn bản được chứa trong âm thanh đó.
Cách làm truyền thống nhất là sử dụng <strong>Hidden Markov Model (HMM)</strong> để mô hình thông tin chuỗi, và dùng GMM để fit lượng thông tin của các states trong HMM vào 1 phân bố có sẵn và đưa ra nhận dạng cho âm thanh trong time window tương ứng.
Tuy nhiên, GMM có nhược điểm là khó có thể mô hình được dữ liệu phi tuyến tính (non-linear manifold) dẫn đến nếu lượng dữ liệu ít có thể xảy ra tình trạng overfit hoặc model HMM không mô tả hết manifold dữ liệu (ếch ngồi đáy giếng).
Do đó, thông qua kết quả thực nghiệm <a class="citation" href="#hinton2012deep">[11]</a>, thì nếu thay GMM bởi DNN để xuất ra xác suất của từng âm một từ states của HMM thì kết quả outperform GMM-HMM trong nhiều bộ dữ liệu lớn.
Và quan trọng là càng train nhiều dữ liệu thì model DNN-HMM càng stable.</p>

<p>Nhìn chung DNN-HMM thì cũng là một kiểu model <code class="language-plaintext highlighter-rouge">seq2seq</code> tức là chuyển chuỗi (audio) thành chuỗi (text hoặc phoneme <a class="citation" href="#graves2013speech">[13, 14]</a>).
Vì vậy khá đơn giản để ứng dụng model <code class="language-plaintext highlighter-rouge">seq2seq</code> vào ASR.
Chuỗi âm thanh (waveform) được chuyển thành chuỗi đặc trưng MFCC và rồi input vào <code class="language-plaintext highlighter-rouge">seq2seq</code>.
Và việc này cũng giúp chúng ta loại bỏ luôn HMM để thực hiện training <code class="language-plaintext highlighter-rouge">end-to-end</code>.</p>

<p>Khi decode output của end-to-end (E2E) network, với giả thiết chúng ta có input là chuỗi \(X=\left\{\mathbf{x}_t\right\}_{t=1}^T\subset\mathbb{R}^d\) và output của E2E network là vector xác suất \(\mathbf{p}_t=\left[p_{t,0},p_{t,2},\ldots,p_{t,N}\right]^\top\in [0,1]^N\) với \(p_{t,i}\) là xác suất của từ thứ \(i\) trong vocabulary ở bước thứ \(t\).
Và hiển nhiên \(\sum_{i=1}^Np_{t,i}=1\).
Ở đây ta có vocabulary \(V=\left\{\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_N\right\}\) gồm \(N\) từ.
Ta sẽ phải tính thêm những quãng <code class="language-plaintext highlighter-rouge">ngừng</code> trong output, nên ta thêm từ rỗng vào vocabulary \(\overline{V}=V\cup\phi\).
<strong>Vậy xác suất của một chuỗi phoneme như \(W=\left[\mathbf{w}_{i_1},\mathbf{w}_{i_2},\ldots,\mathbf{w}_{i_M}\right], M\leq T, 1\leq i_j\leq N\) là bao nhiêu?</strong>
Chúng ta cần hiểu là từ chuỗi \(P=\left[\mathbf{p}_1,\ldots,\mathbf{p}_T\right]\) để chuyển sang chuỗi \(W\) thì đã thêm từ rỗng \(\phi\) xen kẽ vào \(W\) để có độ dài \(T\).
Ta định nghĩa một ánh xạ <code class="language-plaintext highlighter-rouge">many-to-one</code> \(f: V^T\rightarrow V^{\leq T}\) là một phép toán xóa bỏ tất cả các từ rỗng khỏi từ có độ dài \(T\).
Vậy phép toán ngược <code class="language-plaintext highlighter-rouge">one-to-many</code> \(f^{-1}: V^{\leq T}\rightarrow 2^{V^T}\).
Và \(f^{-1}(W)\) là tập các từ có độ dài \(T\) mà loại bỏ hết từ rỗng đi thì còn lại \(W\).
Xác xuất của 1 từ \(\pi = \left[\mathbf{w}_{l_1},\mathbf{w}_{l_2},\ldots,\mathbf{w}_{l_T}\right]\in f^{-1}(W) \subset V^T\) là</p>

\[p\left(\pi \mid \mathbf{\theta};X\right) = \sum_{t=1}^T p\left(\mathbf{w}_{l_t}\mid \mathbf{\theta};\mathbf{x}_t\right)=\sum_{t=1}^Tp_{t,l_t}\]

<p>trong đó \(\mathbf{\theta}\) là parameters của model E2E.
Vậy xác xuất để model output ra cụm từ \(W\) là</p>

\[p\left(W\mid \mathbf{\theta};X\right)=\sum_{\pi\in f^{-1}(W)}p\left(\pi\mid \mathbf{\theta};X\right)\]

<p><strong>Vấn đề bây giờ rút gọn lại thành tìm ứng với model \(\mathbf{\theta}\) và chuỗi đầu vào \(X\) thì cần phải decode ra cụm output \(W\) có độ dài không quá \(T\) tức là tìm ra \(W\) thỏa mãn:</strong></p>

\[W=\mbox{argmax}_{W\in V^{\leq T}}p\left(W\mid \mathbf{\theta};X\right)=\mbox{argmax}_{W\in V^{\leq T}}\sum_{\pi\in f^{-1}(W)}p\left(\pi\mid \mathbf{\theta};X\right)\]

<p>Bài toán đặt ra là số lượng khả năng cần phải tính toán là vô cùng nhiều, do đó cách làm hiệu quả rơi vào 1 trong hai cách:</p>

<ul>
  <li>Dùng <strong>dynamic programming</strong> để tìm ra <strong>alignment</strong> dựa theo công thức ở trên. Phương pháp này thi thoảng gọi là <strong>Connectionist Temporal Segmentation (CTC)</strong>.</li>
  <li>Dùng <strong>transducer</strong>: tạo ra một network để học <strong>alignment</strong> thông qua backpropagation và dựa vào phoneme ở step trước.</li>
</ul>

<p>Transducer thì nhanh hơn rất nhiều so với dynamic programming, và vì xác suất của từng chuỗi \(p\left(\pi\mid \mathbf{\theta};X\right)\) được mô hình bằng neural network nên tính toán có tính thích ứng cao hơn với dữ liệu.</p>

<p>Nhìn chung, task ASR khá thuận lợi cho việc ứng dụng mô hình <code class="language-plaintext highlighter-rouge">seq2seq</code> như Transformer hay BERT.
Dữ liệu thì cũng dồi dào nhất là tiếng Anh với hàng ngàn giờ đọc cuả đủ loại speech như audio books <a class="citation" href="#panayotov2015librispeech">[15]</a>, báo chí Wall Street Journal <a class="citation" href="#paul1992design">[16]</a>, phát âm <a class="citation" href="#garofolo1993darpa">[17]</a>, .v.v…
Thì những bộ dữ liệu trên thuộc nhóm task <strong>conversational speech recognition</strong>.
Những task này đòi hỏi phải transcribe ngay khi âm thanh phát ra.
Ngoài ra cũng có nhóm task <strong>command speech recognition</strong> tức là kiểu như người dùng nói vào loa Echo và câu lệnh được ghi nhận.
Đó thường là các câu lệnh ngắn.
Với tiếng Anh, có bộ dữ liệu Speech Commands <a class="citation" href="#warden2018speech">[18]</a>.</p>

<p>Code kiếc, tool thiếc cũng nói chung sẵn có dồi dào: ESPNet <a class="citation" href="#watanabe2018espnet">[19]</a>, Microsoft ASR <a class="citation" href="#deng2013recent">[20]</a>, DeepSpeech v1 &amp; v2 <a class="citation" href="#hannun2014deepspeech">[21, 22]</a>, PaddlePaddle <a class="citation" href="#ao2021end">[23, 24]</a> hay <code class="language-plaintext highlighter-rouge">fairseq</code> <a class="citation" href="#ott2019fairseq">[25]</a>.
Mà những công việc chuẩn bị dữ liệu <code class="language-plaintext highlighter-rouge">recipes</code> các cái là có sẵn trong các framework trên rồi như là với ESPNet bạn có thể tham khảo tại:</p>
<ul>
  <li><a href="https://github.com/espnet/espnet/tree/master/egs2">https://github.com/espnet/espnet/tree/master/egs2</a></li>
</ul>

<p>Nên nói chung là cũng chả cần code mấy đâu, chủ yếu là code vài dòng thể hiện ý tưởng.
Nếu bạn thích <code class="language-plaintext highlighter-rouge">ăn sẵn</code> hơn nữa thì đấy lại có cái Amazon Lex v2<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<h2 id="sound-classification-và-speaker-identification">Sound classification và Speaker Identification</h2>

<p><strong>Sound classification.</strong> Ứng với một âm thanh bất kỳ, tag âm thanh đó bởi một <a class="citation" href="#gemmeke2017audioset">[26]</a> hoặc nhiều labels <a class="citation" href="#fonseca2019audio">[27]</a>.</p>

<p><img src="https://storage.googleapis.com/kaggle-media/competitions/freesound/task2_freesound_audio_tagging.png" alt="FAT2019" /></p>

<p>Một vấn đề xuất phát từ community này là vấn đề <em>label noise</em>.
Ví dụ như Freesound Audio Taggings 2019 (FAT 2019, <a class="citation" href="#fonseca2019audio">[27]</a>) cung cấp 1 bộ dữ liệu noise mở ra nhiều hướng đi mới như unsupervised/semi-supervised và learning from noisy data.
Nhìn chung ngoài xử lý dữ liệu mel spectrogram để lấy ra features thì hầu như phần còn lại như 1 bài toán classification bình thường.
Kiến trúc thì có thể dùng recurrent neural network và <code class="language-plaintext highlighter-rouge">seq2seq</code> để học.</p>

<p><strong>Speaker Identification.</strong> Nhìn chung thì xử lý lấy features mel spectrogram xong thì có thể sử dụng để nhận dạng người dùng, giống như nhận dạng khuôn mặt hay vân tay.
Những cách làm truyền thống của bài toán SI thì chủ yếu dùng GMM <a class="citation" href="#reynolds1995robust">[9, 10, 4, 5]</a>.
Một model background sẽ được extract từ toàn bộ tập train (không phân biệt cá thể, identity), sau đó thì ứng với mỗi identity thì mình lại xây dựng <em>supervector</em>.
Nói chung tôi cũng không thấy có gì mới trong hướng đi này.
Gần đây thì có thêm cái DNN embeddings <a class="citation" href="#snyder2018x">[6]</a> thì vẫn chỉ là thay bởi DNN và chứng tỏ DNN tốt hơn GMM supervectors.
Ngoài ra thì có những hướng đi như contrastive metric learning, deep metric learning, .v.v…
Nói chung trừ cái chỗ extract MFCC ra thì tôi thấy cũng không khác mấy bài toán identification trong các dữ liệu hình ảnh (face và fingerprint).
Mảng này chắc có cái bài thú vị là <strong>Speaker Diarization</strong> thì trong 1 đoạn hội thoại của nhiều người, thì segment ra từng đoạn xem là ai nói.
Nhìn chung cách làm truyền thống vẫn là GMM và supervector, nhưng gần đây chúng tôi có thêm mấy cái temporal segment networks.
Nói chung so với bên hình ảnh thì ngoài cái phần features âm thanh ra, phần còn lại (machine learning và pattern recognition) thì cũng không quá khác biệt (dùng chung công nghệ).</p>

<h2 id="voice-conversion-text-to-speech-và-speech-synthesis">Voice Conversion, Text-To-Speech và Speech Synthesis</h2>

<h1 id="công-nghệ-chính">Công nghệ chính</h1>

<h2 id="i-vector-và-x-vector-cho-bài-toán-speaker-identification">i-vector và x-vector cho bài toán Speaker Identification</h2>

<p><img src="/assets/images/i-vector-timeline.jpg" alt="speaker" /></p>

<h2 id="seq2seq-và-các-transformers-cho-bài-toán-asr"><code class="language-plaintext highlighter-rouge">seq2seq</code> và các Transformers cho bài toán ASR</h2>

<h1 id="lưu-ý-khi-train-model-asrsi">Lưu ý khi train model ASR/SI</h1>

<h2 id="giá-thành-khi-train-model-state-of-the-art-sota">Giá thành khi train model State-of-the-art (SOTA)</h2>

<p>Quay lại với mảng ASR, nhìn đi nhìn lại cũng lại hậu duệ của <code class="language-plaintext highlighter-rouge">seq2seq</code> nên chắc mẩm là giá sẽ không mềm rồi.
Như hình dưới tôi đưa ra ví dụ dựa trên mô tả của một nhóm làm về ASR trong Google Brain <a class="citation" href="#zhang2020pushing">[8]</a>.
Giá cả của TPU thì v3 mà trên 32 cores thì chỉ mới hỗ trợ tài Hà Lan nên các bạn chú ý nhé.
Kết luận chung là để mà train vài ngày trên bộ Librispeech thôi chả hạn là cũng tốn vài chục ngàn Mỹ kim rồi.
Tóm lại là cuộc chơi của người giàu các bạn ạ!</p>

<p><img src="/assets/images/asr-price-deep-models.jpg" alt="pricing" /></p>

<h1 id="tài-liệu-tham-khảo">Tài liệu tham khảo</h1>

<ol class="bibliography"><li><span id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. 2017. Attention is all you need. <i>Advances in neural information processing systems</i> (2017), 5998–6008.</span><a class="details" href="https://wanted2.github.io/bibliography/vaswani2017attention/">Details</a></li>
<li><span id="devlin2019bert">Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i> (2019), 4171–4186.</span><a class="details" href="https://wanted2.github.io/bibliography/devlin2019bert/">Details</a></li>
<li><span id="hsu2021hubert">Hsu, W.-N., Bolte, B., Tsai, Y.-H.H., Lakhotia, K., Salakhutdinov, R. and Mohamed, A. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>. 29, (2021), 3451–3460.</span><a class="details" href="https://wanted2.github.io/bibliography/hsu2021hubert/">Details</a></li>
<li><span id="kenny2007joint">Kenny, P., Boulianne, G., Ouellet, P. and Dumouchel, P. 2007. Joint factor analysis versus eigenchannels in speaker recognition. <i>IEEE Transactions on Audio, Speech, and Language Processing</i>. 15, 4 (2007), 1435–1447.</span><a class="details" href="https://wanted2.github.io/bibliography/kenny2007joint/">Details</a></li>
<li><span id="dehak2011frontend">Dehak, N., Kenny, P.J., Dehak, R., Dumouchel, P. and Ouellet, P. 2011. Front-End Factor Analysis for Speaker Verification. <i>IEEE Transactions on Audio, Speech, and Language Processing</i>. 19, 4 (2011), 788–798. DOI:https://doi.org/10.1109/TASL.2010.2064307.</span><a class="details" href="https://wanted2.github.io/bibliography/dehak2011frontend/">Details</a></li>
<li><span id="snyder2018x">Snyder, D., Garcia-Romero, D., Sell, G., Povey, D. and Khudanpur, S. 2018. X-vectors: Robust dnn embeddings for speaker recognition. <i>2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</i> (2018), 5329–5333.</span><a class="details" href="https://wanted2.github.io/bibliography/snyder2018x/">Details</a></li>
<li><span id="gulati2020conformer">Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y. and others 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. <i>Proc. Interspeech 2020</i>. (2020), 5036–5040.</span><a class="details" href="https://wanted2.github.io/bibliography/gulati2020conformer/">Details</a></li>
<li><span id="zhang2020pushing">Zhang, Y., Qin, J., Park, D.S., Han, W., Chiu, C.-C., Pang, R., Le, Q.V. and Wu, Y. 2020. Pushing the limits of semi-supervised learning for automatic speech recognition. <i>arXiv preprint arXiv:2010.10504</i>. (2020).</span><a class="details" href="https://wanted2.github.io/bibliography/zhang2020pushing/">Details</a></li>
<li><span id="reynolds1995robust">Reynolds, D.A. and Rose, R.C. 1995. Robust text-independent speaker identification using Gaussian mixture speaker models. <i>IEEE transactions on speech and audio processing</i>. 3, 1 (1995), 72–83.</span><a class="details" href="https://wanted2.github.io/bibliography/reynolds1995robust/">Details</a></li>
<li><span id="reynolds2000speaker">Reynolds, D.A., Quatieri, T.F. and Dunn, R.B. 2000. Speaker verification using adapted Gaussian mixture models. <i>Digital signal processing</i>. 10, 1 (2000), 19–41.</span><a class="details" href="https://wanted2.github.io/bibliography/reynolds2000speaker/">Details</a></li>
<li><span id="hinton2012deep">Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-rahman, Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N. and others 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. <i>IEEE Signal processing magazine</i>. 29, 6 (2012), 82–97.</span><a class="details" href="https://wanted2.github.io/bibliography/hinton2012deep/">Details</a></li>
<li><span id="yu2016automatic">Yu, D. and Deng, L. 2016. <i>Automatic speech recognition</i>. Springer.</span><a class="details" href="https://wanted2.github.io/bibliography/yu2016automatic/">Details</a></li>
<li><span id="graves2013speech">Graves, A., Mohamed, A.-rahman and Hinton, G. 2013. Speech recognition with deep recurrent neural networks. <i>2013 IEEE international conference on acoustics, speech and signal processing</i> (2013), 6645–6649.</span><a class="details" href="https://wanted2.github.io/bibliography/graves2013speech/">Details</a></li>
<li><span id="graves2013hybrid">Graves, A., Jaitly, N. and Mohamed, A.-rahman 2013. Hybrid speech recognition with deep bidirectional LSTM. <i>2013 IEEE workshop on automatic speech recognition and understanding</i> (2013), 273–278.</span><a class="details" href="https://wanted2.github.io/bibliography/graves2013hybrid/">Details</a></li>
<li><span id="panayotov2015librispeech">Panayotov, V., Chen, G., Povey, D. and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. <i>2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</i> (2015), 5206–5210.</span><a class="details" href="https://wanted2.github.io/bibliography/panayotov2015librispeech/">Details</a></li>
<li><span id="paul1992design">Paul, D.B. and Baker, J. 1992. The design for the Wall Street Journal-based CSR corpus. <i>Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992</i> (1992).</span><a class="details" href="https://wanted2.github.io/bibliography/paul1992design/">Details</a></li>
<li><span id="garofolo1993darpa">Garofolo, J.S., Lamel, L.F., Fisher, W.M., Fiscus, J.G. and Pallett, D.S. 1993. DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1. <i>NASA STI/Recon technical report n</i>. 93, (1993), 27403.</span><a class="details" href="https://wanted2.github.io/bibliography/garofolo1993darpa/">Details</a></li>
<li><span id="warden2018speech">Warden, P. 2018. Speech commands: A dataset for limited-vocabulary speech recognition. <i>arXiv preprint arXiv:1804.03209</i>. (2018).</span><a class="details" href="https://wanted2.github.io/bibliography/warden2018speech/">Details</a></li>
<li><span id="watanabe2018espnet">Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., Soplin, N.-E.Y., Heymann, J., Wiesner, M., Chen, N. and others 2018. ESPnet: End-to-End Speech Processing Toolkit. <i>Proc. Interspeech 2018</i>. (2018), 2207–2211.</span><a class="details" href="https://wanted2.github.io/bibliography/watanabe2018espnet/">Details</a></li>
<li><span id="deng2013recent">Deng, L., Li, J., Huang, J.-T., Yao, K., Yu, D., Seide, F., Seltzer, M., Zweig, G., He, X., Williams, J. and others 2013. Recent advances in deep learning for speech research at Microsoft. <i>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</i> (2013), 8604–8608.</span><a class="details" href="https://wanted2.github.io/bibliography/deng2013recent/">Details</a></li>
<li><span id="hannun2014deepspeech">Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., Prenger, R., Satheesh, S., Sengupta, S., Coates, A. and others 2014. DeepSpeech: Scaling up end-to-end speech recognition. <i>arXiv preprint arXiv:1412.5567</i>. (2014).</span><a class="details" href="https://wanted2.github.io/bibliography/hannun2014deepspeech/">Details</a></li>
<li><span id="amodei2016deep">Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen, G. and others 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. <i>International conference on machine learning</i> (2016), 173–182.</span><a class="details" href="https://wanted2.github.io/bibliography/amodei2016deep/">Details</a></li>
<li><span id="ao2021end">Ao, Y., Wu, Z., Yu, D., Gong, W., Kui, Z., Zhang, M., Ye, Z., Shen, L., Ma, Y., Wu, T. and others 2021. End-to-end Adaptive Distributed Training on PaddlePaddle. <i>arXiv preprint arXiv:2112.02752</i>. (2021).</span><a class="details" href="https://wanted2.github.io/bibliography/ao2021end/">Details</a></li>
<li><span id="paddlepaddle_github">Guides-Document-PaddlePaddle Deep Learning Platform.</span><a class="details" href="https://wanted2.github.io/bibliography/paddlepaddle_github/">Details</a></li>
<li><span id="ott2019fairseq">Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D. and Auli, M. 2019. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. <i>Proceedings of NAACL-HLT 2019: Demonstrations</i> (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/ott2019fairseq/">Details</a></li>
<li><span id="gemmeke2017audioset">Gemmeke, J.F., Ellis, D.P.W., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M. and Ritter, M. 2017. Audio Set: An ontology and human-labeled dataset for audio events. <i>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i> (2017), 776–780.</span><a class="details" href="https://wanted2.github.io/bibliography/gemmeke2017audioset/">Details</a></li>
<li><span id="fonseca2019audio">Fonseca, E., Plakal, M., Font, F., Ellis, D.P.W. and Serra, X. 2019. AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION. <i>Acoustic Scenes and Events 2019 Workshop (DCASE2019)</i> (2019), 69.</span><a class="details" href="https://wanted2.github.io/bibliography/fonseca2019audio/">Details</a></li></ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn <code class="language-plaintext highlighter-rouge">đồ ăn sẵn</code> thì các bạn có thể tham khảo <a href="https://docs.aws.amazon.com/lexv2/latest/dg/what-is.html">Amazon Lex</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2022-02-06">06 Feb 2022</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#BERT">#BERT</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Sequence-to-sequence">#Sequence-to-sequence</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Transformer">#Transformer</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#deep-metric-learning">#deep metric learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#metric-learning">#metric learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speaker-diarization">#speaker diarization</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speaker-identification">#speaker identification</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speaker-verification">#speaker verification</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speech-recognition">#speech recognition</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speech-synthesis">#speech synthesis</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#voice-conversion">#voice conversion</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#話者同定">#話者同定</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#音声合成">#音声合成</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#音声変換">#音声変換</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#音声認識">#音声認識</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="https://wanted2.github.io//seq2seq/"> &laquo; Seq2Seq và kiến trúc Encoder-Decoder</a>
            
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'caineng'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi" style="max-height: 48px;" /> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://caineng.us20.list-manage.com/subscribe/post?u=76342d3d74a6807aac5aec0d7&id=b5645e19be" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語 (7)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Project-Management">Project Management (2)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Software-Engineering">Software Engineering (3)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence (4)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Site-Reliable-Engineering">Site Reliable Engineering (3)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2022 AiFi 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="https://wanted2.github.io/assets/js/mediumish.js"></script>


<script src="https://wanted2.github.io/assets/js/lazyload.js"></script>


<script src="https://wanted2.github.io/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//caineng.disqus.com/count.js"></script>


</body>
</html>
