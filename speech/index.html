<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="https://wanted2.github.io/assets/images/favicon.ico">

<title>Speech and Sequence-to-sequence | AiFi</title>

 
    
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-2CDTCF0EP6" crossorigin="anonymous"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-2CDTCF0EP6');
        </script>
    


<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Speech and Sequence-to-sequence | AiFi</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Speech and Sequence-to-sequence" />
<meta name="author" content="tuan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Như trong bài viết trước thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì seq2seq đều đang làm chủ. Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà seq2seq và các hậu duệ (thuộc dòng dõi Transformer [44] và BERT [44, 44]) cũng đang nắm thế chủ động. Khởi đầu bài viết, tôi định viết lại về i-vector [44, 44] và x-vector [44] là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị seq2seq chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer [44, 44] 12. Với bài toán SI thì cách làm cổ điển sẽ là dùng Gaussian Mixture Models (GMM, [44, 44, 44, 44]), còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì Deep Neural Networks (DNN) với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, [44]) lẫn ASR (DNN-HMM, [44]). Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy. Gần đây thì đến lượt những tiến bộ bắt nguồn từ seq2seq DNN-RNN, rồi đến những model end-to-end, transformers. Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. &#8617; Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn đồ ăn sẵn thì các bạn có thể tham khảo Amazon Lex. &#8617;" />
<meta property="og:description" content="Như trong bài viết trước thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì seq2seq đều đang làm chủ. Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà seq2seq và các hậu duệ (thuộc dòng dõi Transformer [44] và BERT [44, 44]) cũng đang nắm thế chủ động. Khởi đầu bài viết, tôi định viết lại về i-vector [44, 44] và x-vector [44] là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị seq2seq chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer [44, 44] 12. Với bài toán SI thì cách làm cổ điển sẽ là dùng Gaussian Mixture Models (GMM, [44, 44, 44, 44]), còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì Deep Neural Networks (DNN) với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, [44]) lẫn ASR (DNN-HMM, [44]). Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy. Gần đây thì đến lượt những tiến bộ bắt nguồn từ seq2seq DNN-RNN, rồi đến những model end-to-end, transformers. Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. &#8617; Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn đồ ăn sẵn thì các bạn có thể tham khảo Amazon Lex. &#8617;" />
<meta property="og:site_name" content="AiFi" />
<meta property="og:image" content="https://fosspost.org/wp-content/uploads/2019/02/rendered.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-06T00:00:00+09:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://fosspost.org/wp-content/uploads/2019/02/rendered.jpg" />
<meta property="twitter:title" content="Speech and Sequence-to-sequence" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"tuan"},"dateModified":"2022-02-06T00:00:00+09:00","datePublished":"2022-02-06T00:00:00+09:00","description":"Như trong bài viết trước thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì seq2seq đều đang làm chủ. Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà seq2seq và các hậu duệ (thuộc dòng dõi Transformer [44] và BERT [44, 44]) cũng đang nắm thế chủ động. Khởi đầu bài viết, tôi định viết lại về i-vector [44, 44] và x-vector [44] là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị seq2seq chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer [44, 44] 12. Với bài toán SI thì cách làm cổ điển sẽ là dùng Gaussian Mixture Models (GMM, [44, 44, 44, 44]), còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì Deep Neural Networks (DNN) với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, [44]) lẫn ASR (DNN-HMM, [44]). Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy. Gần đây thì đến lượt những tiến bộ bắt nguồn từ seq2seq DNN-RNN, rồi đến những model end-to-end, transformers. Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. &#8617; Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn đồ ăn sẵn thì các bạn có thể tham khảo Amazon Lex. &#8617;","headline":"Speech and Sequence-to-sequence","image":"https://fosspost.org/wp-content/uploads/2019/02/rendered.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"/https://wanted2.github.io/speech/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/https://wanted2.github.io/assets/images/favicon.ico"},"name":"tuan"},"url":"/https://wanted2.github.io/speech/"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    
<link href="https://wanted2.github.io/assets/css/screen.css" rel="stylesheet">

<link href="https://wanted2.github.io/assets/css/main.css" rel="stylesheet">

<script src="https://wanted2.github.io/assets/js/jquery.min.js"></script>
<script src="https://kit.fontawesome.com/d0b91d895e.js" crossorigin="anonymous"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
    }
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript" crossorigin="anonymous"></script>
<script src="https://d3js.org/d3.v4.js" crossorigin="anonymous"></script>
<!-- <script src="https://bl.ocks.org/mbostock/raw/4061502/0a200ddf998aa75dfdb1ff32e16b680a15e5cb01/box.js" crossorigin="anonymous"></script> -->
</head>


<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="https://wanted2.github.io/">
    <img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi">
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">

        <!-- Begin Menu -->

            <ul class="navbar-nav ml-auto">

                
                <li class="nav-item">
                
                <a class="nav-link" href="https://wanted2.github.io/">Blog</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/about">About</a>
                </li>

                <li class="nav-item">
                <a class="nav-link" href="https://wanted2.github.io/projects">Projects</a>
                </li>

                <!-- <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/"> Docs</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-wordpress/"><i class="fab fa-wordpress-simple"></i> WP Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://www.wowthemes.net/themes/mediumish-ghost/"><i class="fab fa-snapchat-ghost"></i> Ghost Version</a>
                </li>

                <li class="nav-item">
                <a target="_blank" class="nav-link" href="https://github.com/wowthemesnet/mediumish-theme-jekyll"><i class="fab fa-github"></i> Fork on Github</a>
                </li> -->

                <!-- <script src="https://wanted2.github.io/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="https://wanted2.github.io/assets/js/lunrsearchengine.js"></script> -->

            </ul>

        <!-- End Menu -->

    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->

<div class="site-content">

<div class="container">

<!-- Site Title
================================================== -->
<div class="mainheading">
    <h1 class="sitetitle">AiFi</h1>
    <p class="lead">
        An AI Engineer's blog
    </p>
</div>

<!-- Content
================================================== -->
<div class="main-content">
    <!-- Begin Article
================================================== -->
<div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-2 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Speech and Sequence-to-sequence&url=https://wanted2.github.io/speech/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=https://wanted2.github.io/speech/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=https://wanted2.github.io/speech/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="mailto:?subject=Speech and Sequence-to-sequence&body=https://wanted2.github.io/speech/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fas fa-envelope"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
    
    <div class="sep">

    </div>
    <ul>
        <li class="small">
        4402
     words</li>
        <li class="small">24 minutes</li>
    </ul>
</div>

        </div>

        <!-- Post -->
        

        <div class="col-md-9 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Author Box -->
                
                <div class="row post-top-meta">
                    <div class="col-xs-12 col-md-2 col-lg-2 text-left mb-4 mb-md-0">
                        
                        <img class="author-thumb" src="https://wanted2.github.io/assets/images/favicon.png" alt="AiFi">
                        
                    </div>
                    <div class="col-xs-12 col-md-10 col-lg-10 text-right">
                        <a target="_blank" class="link-dark" href="">AiFi</a>
                        <!-- <a target="_blank" href="" class="btn follow">Follow</a> -->
                        <!-- LikeBtn.com BEGIN -->
                        <span class="likebtn-wrapper" 
                            data-site_id="61cfccd36fd08b2d68c1929e"
                            data-theme="custom" 
                            data-icon_l_url="/assets/images/OK_EM.png" 
                            data-icon_l_url_v="/assets/images/OK_EM_clicked.png" 
                            data-identifier="/speech/" 
                            data-show_like_label="false" 
                            data-like_enabled="false" 
                            data-dislike_enabled="false" 
                            data-icon_dislike_show="false" 
                            data-voting_cancelable="false" 
                            data-counter_show="true"
                            data-counter_frmt="comma"></span>
                        <script>(function(d,e,s){if(d.getElementById("likebtn_wjs"))return;a=d.createElement(e);m=d.getElementsByTagName(e)[0];a.async=1;a.id="likebtn_wjs";a.src=s;m.parentNode.insertBefore(a, m)})(document,"script","//w.likebtn.com/js/w/widget.js");</script>
                        <!-- LikeBtn.com END -->
                        <span class="author-description"></span>
                    </div>
                </div>
                

                <!-- Post Title -->
                <h1 class="posttitle">Speech and Sequence-to-sequence</h1>

            </div>

            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            <!-- Post Featured Image -->
            

            
            <img class="featured-image img-fluid lazyimg" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAMAAAACCAQAAAA3fa6RAAAADklEQVR42mNkAANGCAUAACMAA2w/AMgAAAAASUVORK5CYII=" data-src="https://fosspost.org/wp-content/uploads/2019/02/rendered.jpg" alt="Speech and Sequence-to-sequence">
            

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                    
                    <div class="toc mt-4 mb-4 lead">
                        <h3 class="font-weight-bold">Summary</h3>
                        <ul>
  <li><a href="#giới-thiệu-chung-về-asrsi-và-các-bài-toán-liên-quan">Giới thiệu chung về ASR/SI và các bài toán liên quan</a>
    <ul>
      <li><a href="#automatic-speech-recognition">Automatic Speech Recognition</a></li>
      <li><a href="#sound-classification-và-speaker-identification">Sound classification và Speaker Identification</a></li>
      <li><a href="#voice-conversion-text-to-speech-và-speech-synthesis">Voice Conversion, Text-To-Speech và Speech Synthesis</a></li>
    </ul>
  </li>
  <li><a href="#công-nghệ-chính">Công nghệ chính</a>
    <ul>
      <li><a href="#i-vector-và-x-vector-cho-bài-toán-speaker-identification">i-vector và x-vector cho bài toán Speaker Identification</a></li>
      <li><a href="#seq2seq-và-các-transformers-cho-bài-toán-asr"><code class="language-plaintext highlighter-rouge">seq2seq</code> và các Transformers cho bài toán ASR</a></li>
    </ul>
  </li>
  <li><a href="#lưu-ý-khi-train-model-asrsi">Lưu ý khi train model ASR/SI</a>
    <ul>
      <li><a href="#giá-thành-khi-train-model-state-of-the-art-sota">Giá thành khi train model State-of-the-art (SOTA)</a></li>
    </ul>
  </li>
  <li><a href="#tài-liệu-tham-khảo">Tài liệu tham khảo</a></li>
</ul>
                    </div>
                
                <!-- End Toc -->
                <p>Như trong <a href="/seq2seq/">bài viết trước</a> thì chúng ta đã tìm hiểu và biết trong mảng NLP cũng như Vision-Language (VL) thì <code class="language-plaintext highlighter-rouge">seq2seq</code> đều đang làm chủ.
Trong bài viết này chúng ta sẽ tìm hiểu 1 mảng khác mà <code class="language-plaintext highlighter-rouge">seq2seq</code> và các hậu duệ (thuộc dòng dõi Transformer <a class="citation" href="#vaswani2017attention">[1]</a> và BERT <a class="citation" href="#devlin2019bert">[2, 3]</a>) cũng đang nắm thế chủ động.
Khởi đầu bài viết, tôi định viết lại về i-vector <a class="citation" href="#kenny2007joint">[4, 5]</a> và x-vector <a class="citation" href="#snyder2018x">[6]</a> là các biểu diễn nổi bật trong speaker identification (SI)/ automatic speech recognition (ASR), nhưng xem ra mảng speech recognition cũng bị <code class="language-plaintext highlighter-rouge">seq2seq</code> chiếm hết rồi nên chúng ta sẽ nói thêm về một số state-of-the-art thuộc dạng này như Conformer <a class="citation" href="#gulati2020conformer">[7, 8]</a> <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.
Với bài toán SI thì cách làm cổ điển sẽ là dùng <strong>Gaussian Mixture Models (GMM, <a class="citation" href="#reynolds1995robust">[9, 10, 4, 5]</a>)</strong>, còn với ASR thì dùng GMM để classify HMM states (GMM-HMM), tuy nhiên là như kết quả của nhiều nhóm nghiên cứu về SI/ASR thì <strong>Deep Neural Networks (DNN)</strong> với đạt kết quả tốt hơn hẳn cho cả SI (DNN embeddings, <a class="citation" href="#snyder2018x">[6]</a>) lẫn ASR (DNN-HMM, <a class="citation" href="#hinton2012deep">[11]</a>).
Do vậy về mặt lịch sử thì GMM/GMM-HMM là cách làm truyền thống, từ khoảng 2012 thì DNN/DNN-HMM chứng tỏ DNN tốt hơn hẳn HMM về mặt representations lẫn accuracy.
Gần đây thì đến lượt những tiến bộ bắt nguồn từ <code class="language-plaintext highlighter-rouge">seq2seq</code> DNN-RNN, rồi đến những model end-to-end, transformers.</p>

<!--more-->

<h1 id="giới-thiệu-chung-về-asrsi-và-các-bài-toán-liên-quan">Giới thiệu chung về ASR/SI và các bài toán liên quan</h1>

<h2 id="automatic-speech-recognition">Automatic Speech Recognition</h2>
<p>Automatic Speech Recognition (ASR, <a class="citation" href="#yu2016automatic">[12]</a>) là nhiệm vụ chuyển một chuỗi âm thanh giọng nói (waveform) sang một chuỗi văn bản được chứa trong âm thanh đó.
Cách làm truyền thống nhất là sử dụng <strong>Hidden Markov Model (HMM)</strong> để mô hình thông tin chuỗi, và dùng GMM để fit lượng thông tin của các states trong HMM <a class="citation" href="#young2008hmms">[13]</a> vào 1 phân bố có sẵn và đưa ra nhận dạng cho âm thanh trong time window tương ứng.
Tuy nhiên, GMM có nhược điểm là khó có thể mô hình được dữ liệu phi tuyến tính (non-linear manifold) dẫn đến nếu lượng dữ liệu ít có thể xảy ra tình trạng overfit hoặc model HMM không mô tả hết manifold dữ liệu (ếch ngồi đáy giếng).
Do đó, thông qua kết quả thực nghiệm <a class="citation" href="#hinton2012deep">[11]</a>, thì nếu thay GMM bởi DNN để xuất ra xác suất của từng âm một từ states của HMM thì kết quả outperform GMM-HMM trong nhiều bộ dữ liệu lớn.
Và quan trọng là càng train nhiều dữ liệu thì model DNN-HMM càng stable.</p>

<p>Nhìn chung DNN-HMM thì cũng là một kiểu model <code class="language-plaintext highlighter-rouge">seq2seq</code> tức là chuyển chuỗi (audio) thành chuỗi (text hoặc phoneme <a class="citation" href="#graves2013speech">[14, 15]</a>).
Vì vậy khá đơn giản để ứng dụng model <code class="language-plaintext highlighter-rouge">seq2seq</code> vào ASR.
Chuỗi âm thanh (waveform) được chuyển thành chuỗi đặc trưng MFCC và rồi input vào <code class="language-plaintext highlighter-rouge">seq2seq</code>.
Và việc này cũng giúp chúng ta loại bỏ luôn HMM để thực hiện training <code class="language-plaintext highlighter-rouge">end-to-end</code>.</p>

<p>Khi decode output của end-to-end (E2E) network, với giả thiết chúng ta có input là chuỗi \(X=\left\{\mathbf{x}_t\right\}_{t=1}^T\subset\mathbb{R}^d\) và output của E2E network là vector xác suất \(\mathbf{p}_t=\left[p_{t,0},p_{t,2},\ldots,p_{t,N}\right]^\top\in [0,1]^N\) với \(p_{t,i}\) là xác suất của từ thứ \(i\) trong vocabulary ở bước thứ \(t\).
Và hiển nhiên \(\sum_{i=1}^Np_{t,i}=1\).
Ở đây ta có vocabulary \(V=\left\{\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_N\right\}\) gồm \(N\) từ.
Ta sẽ phải tính thêm những quãng <code class="language-plaintext highlighter-rouge">ngừng</code> trong output, nên ta thêm từ rỗng vào vocabulary \(\overline{V}=V\cup\phi\).
<strong>Vậy xác suất của một chuỗi phoneme như \(W=\left[\mathbf{w}_{i_1},\mathbf{w}_{i_2},\ldots,\mathbf{w}_{i_M}\right], M\leq T, 1\leq i_j\leq N\) là bao nhiêu?</strong>
Chúng ta cần hiểu là từ chuỗi \(P=\left[\mathbf{p}_1,\ldots,\mathbf{p}_T\right]\) để chuyển sang chuỗi \(W\) thì đã thêm từ rỗng \(\phi\) xen kẽ vào \(W\) để có độ dài \(T\).
Ta định nghĩa một ánh xạ <code class="language-plaintext highlighter-rouge">many-to-one</code> \(f: V^T\rightarrow V^{\leq T}\) là một phép toán xóa bỏ tất cả các từ rỗng khỏi từ có độ dài \(T\).
Vậy phép toán ngược <code class="language-plaintext highlighter-rouge">one-to-many</code> \(f^{-1}: V^{\leq T}\rightarrow 2^{V^T}\).
Và \(f^{-1}(W)\) là tập các từ có độ dài \(T\) mà loại bỏ hết từ rỗng đi thì còn lại \(W\).
Xác xuất của 1 từ \(\pi = \left[\mathbf{w}_{l_1},\mathbf{w}_{l_2},\ldots,\mathbf{w}_{l_T}\right]\in f^{-1}(W) \subset V^T\) là</p>

\[p\left(\pi \mid \mathbf{\theta};X\right) = \sum_{t=1}^T p\left(\mathbf{w}_{l_t}\mid \mathbf{\theta};\mathbf{x}_t\right)=\sum_{t=1}^Tp_{t,l_t}\]

<p>trong đó \(\mathbf{\theta}\) là parameters của model E2E.
Vậy xác xuất để model output ra cụm từ \(W\) là</p>

\[p\left(W\mid \mathbf{\theta};X\right)=\sum_{\pi\in f^{-1}(W)}p\left(\pi\mid \mathbf{\theta};X\right)\]

<p><strong>Vấn đề bây giờ rút gọn lại thành tìm ứng với model \(\mathbf{\theta}\) và chuỗi đầu vào \(X\) thì cần phải decode ra cụm output \(W\) có độ dài không quá \(T\) tức là tìm ra \(W\) thỏa mãn:</strong></p>

\[W=\mbox{argmax}_{W\in V^{\leq T}}p\left(W\mid \mathbf{\theta};X\right)=\mbox{argmax}_{W\in V^{\leq T}}\sum_{\pi\in f^{-1}(W)}p\left(\pi\mid \mathbf{\theta};X\right)\]

<p>Bài toán đặt ra là số lượng khả năng cần phải tính toán là vô cùng nhiều, do đó cách làm hiệu quả rơi vào 1 trong hai cách:</p>

<ul>
  <li>Dùng <strong>dynamic programming</strong> để tìm ra <strong>alignment</strong> dựa theo công thức ở trên. Phương pháp này thi thoảng gọi là <strong>Connectionist Temporal Segmentation (CTC)</strong>.</li>
  <li>Dùng <strong>transducer</strong>: tạo ra một network để học <strong>alignment</strong> thông qua backpropagation và dựa vào phoneme ở step trước.</li>
</ul>

<p>Transducer thì nhanh hơn rất nhiều so với dynamic programming, và vì xác suất của từng chuỗi \(p\left(\pi\mid \mathbf{\theta};X\right)\) được mô hình bằng neural network nên tính toán có tính thích ứng cao hơn với dữ liệu.</p>

<p>Nhìn chung, task ASR khá thuận lợi cho việc ứng dụng mô hình <code class="language-plaintext highlighter-rouge">seq2seq</code> như Transformer hay BERT.
Dữ liệu thì cũng dồi dào nhất là tiếng Anh với hàng ngàn giờ đọc cuả đủ loại speech như audio books <a class="citation" href="#panayotov2015librispeech">[16]</a>, báo chí Wall Street Journal <a class="citation" href="#paul1992design">[17]</a>, phát âm <a class="citation" href="#garofolo1993darpa">[18]</a>, .v.v…
Thì những bộ dữ liệu trên thuộc nhóm task <strong>conversational speech recognition</strong>.
Những task này đòi hỏi phải transcribe ngay khi âm thanh phát ra.
Ngoài ra cũng có nhóm task <strong>command speech recognition</strong> tức là kiểu như người dùng nói vào loa Echo và câu lệnh được ghi nhận.
Đó thường là các câu lệnh ngắn.
Với tiếng Anh, có bộ dữ liệu Speech Commands <a class="citation" href="#warden2018speech">[19]</a>.</p>

<p>Code kiếc, tool thiếc cũng nói chung sẵn có dồi dào: ESPNet <a class="citation" href="#watanabe2018espnet">[20]</a>, Microsoft ASR <a class="citation" href="#deng2013recent">[21]</a>, DeepSpeech v1 &amp; v2 <a class="citation" href="#hannun2014deepspeech">[22, 23]</a>, PaddlePaddle <a class="citation" href="#ao2021end">[24, 25]</a> hay <code class="language-plaintext highlighter-rouge">fairseq</code> <a class="citation" href="#ott2019fairseq">[26]</a>.
Mà những công việc chuẩn bị dữ liệu <code class="language-plaintext highlighter-rouge">recipes</code> các cái là có sẵn trong các framework trên rồi như là với ESPNet bạn có thể tham khảo tại:</p>
<ul>
  <li><a href="https://github.com/espnet/espnet/tree/master/egs2">https://github.com/espnet/espnet/tree/master/egs2</a></li>
</ul>

<p>Nên nói chung là cũng chả cần code mấy đâu, chủ yếu là code vài dòng thể hiện ý tưởng.
Nếu bạn thích <code class="language-plaintext highlighter-rouge">ăn sẵn</code> hơn nữa thì đấy lại có cái Amazon Lex v2<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<h2 id="sound-classification-và-speaker-identification">Sound classification và Speaker Identification</h2>

<p><strong>Sound classification.</strong> Ứng với một âm thanh bất kỳ, tag âm thanh đó bởi một <a class="citation" href="#gemmeke2017audioset">[27]</a> hoặc nhiều labels <a class="citation" href="#fonseca2019audio">[28]</a>.</p>

<p><img src="https://storage.googleapis.com/kaggle-media/competitions/freesound/task2_freesound_audio_tagging.png" alt="FAT2019" /></p>

<p>Một vấn đề xuất phát từ community này là vấn đề <em>label noise</em>.
Ví dụ như Freesound Audio Taggings 2019 (FAT 2019, <a class="citation" href="#fonseca2019audio">[28]</a>) cung cấp 1 bộ dữ liệu noise mở ra nhiều hướng đi mới như unsupervised/semi-supervised và learning from noisy data.
Nhìn chung ngoài xử lý dữ liệu mel spectrogram để lấy ra features thì hầu như phần còn lại như 1 bài toán classification bình thường.
Kiến trúc thì có thể dùng recurrent neural network và <code class="language-plaintext highlighter-rouge">seq2seq</code> để học.</p>

<p><strong>Speaker Identification.</strong> Nhìn chung thì xử lý lấy features mel spectrogram xong thì có thể sử dụng để nhận dạng người dùng, giống như nhận dạng khuôn mặt hay vân tay.
Những cách làm truyền thống của bài toán SI thì chủ yếu dùng GMM <a class="citation" href="#reynolds1995robust">[9, 10, 4, 5]</a>.
Một model background sẽ được extract từ toàn bộ tập train (không phân biệt cá thể, identity), sau đó thì ứng với mỗi identity thì mình lại xây dựng <em>supervector</em>.
Nói chung tôi cũng không thấy có gì mới trong hướng đi này.
Gần đây thì có thêm cái DNN embeddings <a class="citation" href="#snyder2018x">[6]</a> thì vẫn chỉ là thay bởi DNN và chứng tỏ DNN tốt hơn GMM supervectors.
Ngoài ra thì có những hướng đi như contrastive metric learning, deep metric learning, .v.v…
Nói chung trừ cái chỗ extract MFCC ra thì tôi thấy cũng không khác mấy bài toán identification trong các dữ liệu hình ảnh (face và fingerprint).
Mảng này chắc có cái bài thú vị là <strong>Speaker Diarization</strong> thì trong 1 đoạn hội thoại của nhiều người, thì segment ra từng đoạn xem là ai nói.
Nhìn chung cách làm truyền thống vẫn là GMM và supervector, nhưng gần đây chúng tôi có thêm mấy cái temporal segment networks.
Nói chung so với bên hình ảnh thì ngoài cái phần features âm thanh ra, phần còn lại (machine learning và pattern recognition) thì cũng không quá khác biệt (dùng chung công nghệ).</p>

<h2 id="voice-conversion-text-to-speech-và-speech-synthesis">Voice Conversion, Text-To-Speech và Speech Synthesis</h2>
<p>Cái mảng Voice Transformation (VT, <a class="citation" href="#stylianou2008voice">[29]</a>) thì phần cổ điển ở trong SGK <a class="citation" href="#benesty2008springer">[30, 31]</a> rồi, nên mình chỉ tập hợp xem mảng này gần đây họ làm cái gì thôi nhé.
Đứng về phía ứng dụng bảo mật mà nói thì nói chung mấy cái speech synthesis với voice conversion này là xếp vào hạng mục <strong>spoofing attacks</strong> vào hệ thống thông tin.
Tức là mình tìm cách bắt chước giọng nói với tạo ra giọng nói giống thật thì mình là kẻ tấn công rồi!</p>

<p>Nhìn chung cũng có ứng dụng khác như lồng tiếng nhân vật hoạt hình, game, … nhưng nói chung mấy cái tạo dữ liệu giả mạo kiểu synthesis với voice clone này thì có vẻ là nhiều về mảng bảo vệ hệ thống thông tin.
Thì về hướng này có thể kể đến các chuỗi challenges như ASVspoof <a class="citation" href="#wu2015asvspoof">[32, 33, 34, 35, 36]</a> hoặc Voice Conversion Challenge <a class="citation" href="#toda2016voice">[37, 38, 39]</a>.</p>

<p><img src="/assets/images/spoof-voice.png" alt="" />
<em>Bức tranh lớn về mảng anti-spoofing</em></p>

<p>Nhìn vào bức tranh lớn trên thì trong chuỗi challenge ASVspoof từ năm 2015, mọi người đã tập trung giải quyết các vấn đề như các đòn tấn công <code class="language-plaintext highlighter-rouge">speech synthesis</code> hay <code class="language-plaintext highlighter-rouge">voice conversion</code>, đến 2017 thì chúng ta làm thêm đòn tấn công <code class="language-plaintext highlighter-rouge">replay</code>, năm 2019 thì cũng không có đòn nào mới, nhưng mình mở rộng bộ dữ liệu của các đòn tấn công lên.
Bản thân tôi thấy cái mảng này có vẻ cũng thích tấn công hơn là phòng thủ, nên là có vẻ lại bài toán phân biệt thật giả nhãn hiệu Goodfellow.
Thế là lại hai bên tấn công phòng thủ đuổi bắt lẫn nhau, bên tấn công càng tinh vi thì bên phòng thủ cũng lại nâng cấp vũ trang.
Kết cục là đuổi bắt không có hồi kết mà ông Goodfellow ông ấy viết mấy cái mô hình neuralnetwork với mấy cái chuyên gia làm hàng giả.
Thì bây giờ cái này là bên tấn công thì cần phải xem đòn tấn công phải tạo ra sản phẩm “nhái” giống hệt, càng giống, càng tự nhiên càng tốt.
Rồi nảy sinh cái cycle consitency loss để check xem nhái giống chưa?
Tuy nhiên vấn đề speech thì có một cái là người phải nghe giống thì mới OK, nên lại phải có <strong>subjective evaluation</strong> tức là bỏ tiền ra thuê mấy đối tượng thí nghiệm nghe và đánh giá chất lượng âm thanh.
Thì họ lại nghĩ cái metric là <strong>Mean Opinion Score (MOS)</strong> tức là lấy ý kiến những người đánh giá, rồi cộng điểm lấy trung bình, xem đoạn âm thanh đã được nhái ổn thỏa chưa.</p>

<p>Đấy nên tôi thấy mấy cái ứng dụng TTS với synthesis, voice conversion nhìn thì hay hay, xem mấy cái demo cũng này kia phết, nhưng chắc chủ yếu là quay lại chỗ detect <strong>hàng nhái</strong> là chủ yếu.</p>

<p><img src="/assets/images/spoof-det.jpg" alt="" /></p>

<p>Trên đây là một hệ thống phát hiện hàng nhái âm thanh, mà tức là classifier mang tính phòng thủ.
Thì bây giờ có vẻ có nhiều hệ thống tấn công rồi, chắc sắp tới cũng phải nâng cấp hệ thống phòng thủ cho nó tương xứng.</p>

<p>Nếu theo dõi kết quả của VCC2020 <a class="citation" href="#Yi2020vcc">[39]</a> thì có vẻ là những team sử dụng mô hình encoder-decoder (tức là <code class="language-plaintext highlighter-rouge">seq2seq</code>) đạt kết quả tốt hơn các team dùng GAN.
Ngoài đánh giá bằng MOS thì để đánh giá chất lượng giọng “nhái” cũng sử dụng similarity scores.
Với task VCC sẽ có 2 kiểu bài toán là parallel (target audio cũng nói cùng 1 câu với source audio, chỉ khác giọng) hoặc non-parallel (target audio và source khác nhau cả giọng lẫn nội dung).
ASVspoof thì cũng là challenge sản xuất hàng nhái nhưng có nhiều attacks hơn mỗi voice cloning.
Nhìn chung, method dẫn đầu cũng là 1 hệ thống end-to-end TTS nốt.</p>

<h1 id="công-nghệ-chính">Công nghệ chính</h1>

<p>Để extract được feature MFCC thì các bạn có thể dùng thư viện <a href="https://librosa.org/doc/main/generated/librosa.feature.mfcc.html#librosa.feature.mfcc"><code class="language-plaintext highlighter-rouge">librosa</code></a> hoặc <a href="https://projets-lium.univ-lemans.fr/sidekit/api/featuresextractor.html"><code class="language-plaintext highlighter-rouge">sidekit</code></a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># Dùng librosa
</span><span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="n">mfcc</span><span class="p">(</span><span class="n">signal</span><span class="p">,</span> 
    <span class="n">sr</span><span class="o">=</span><span class="mi">44100</span><span class="p">,</span> 
    <span class="n">n_mfcc</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
    <span class="n">lifter</span><span class="o">=</span><span class="mi">22</span><span class="p">,</span> 
    <span class="n">hop_length</span><span class="o">=</span><span class="mi">441</span><span class="p">,</span> 
    <span class="n">win_length</span><span class="o">=</span><span class="mi">1102</span><span class="p">,</span> 
    <span class="n">n_fft</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> 
    <span class="n">window</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="p">,),</span> 
    <span class="n">n_mels</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span>
    <span class="n">norm</span><span class="o">=</span><span class="s">'ortho'</span>
<span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Trong ví dụ trên tôi extract MFCC từ audio với độ rộng window là 25ms và bước rộng là 10ms (ví dụ, bước thứ nhất sẽ là 25ms + 10ms = 35ms).
Audio được sample ở 44kHz.
Xử lý extract MFCC thì quá quen thuộc rồi, các bạn cần chia audio thành các frames theo từng bước 10ms, và độ rộng 25ms nói trên.
Sau đó trong từng frame, các bạn tính biểu diễn Fourier rời rạc cũng như spectrogram của window.
Vì tai người khó phân biệt được thay đổi ở tần số thấp nên các bạn cần lọc tín trong không gian tần số theo filter bank để có được feature tốt.
Sau đó các bạn tính hệ số MFCC, và chỉ cần giữ lại 13-20 hệ số đầu (thường chỉ dùng 13 hệ số đâù là đủ).
Như vậy mỗi frame window sẽ được biểu diễn bởi 1 vector 13-20 chiều, và audio có 489 frames thì sẽ được biểu diễn bởi ma trận \(489\times d\). 
Sau khi sử dụng filter bank, bạn cũng cần dùng Discrete Cosine Transform để lấy được feature robust.</p>

<h2 id="i-vector-và-x-vector-cho-bài-toán-speaker-identification">i-vector và x-vector cho bài toán Speaker Identification</h2>
<p>Thư viện <code class="language-plaintext highlighter-rouge">sidekit</code> cung cấp implementations tối ưu (song song hóa đa nhiệm) cho cả <a href="https://projets-lium.univ-lemans.fr/sidekit/api/factoranalyser.html">i-vector</a> và <a href="https://projets-lium.univ-lemans.fr/sidekit/api/nnet/xvector.html">x-vector</a>.</p>

<p>Mô hình i-vector như các bạn đã biết:</p>

\[\begin{eqnarray}
\underbrace{\mathbf{S}}_{\mbox{super-vector}}&amp;=&amp;\underbrace{\mathbf{u}}_{\mbox{speaker-independent UBM vector}}\\\newline
&amp;+&amp;\underbrace{\mathbf{T}}_{\mbox{total variability matrix}}\underbrace{\mathbf{m}}_{\mbox{i-vector}}\sim \mathcal{N}\left(\mathbf{0};\mathbf{T}\mathbf{T}^\top\right)
\end{eqnarray}\]

<p>i-vector được giả định là \(\mathbf{m}\sim\mathcal{N}\left(\mathbf{0};\mathbf{I}\right)\) tức là tuân theo phân bố chuẩn. Như vậy, để có thể extract i-vector, các bạn cần train 2 models:</p>
<ul>
  <li>Universal Background Model (GMM)</li>
  <li>Total Variability (TV)</li>
</ul>

<p>Cả 2 đều train bằng thuật toán EM cả nên rất là <code class="language-plaintext highlighter-rouge">OK EM</code>.
Ví dụ, các bạn có thể train model UBM với 32 components (thường nên chọn từ 32 components trở lên):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GMM</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">GMM</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">gmm</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Thì ta cứ lấy hết đặc trưng MFCC của tất cả audio files trong bộ train là có ma trận \(X\) thôi.
Training Total Variability đòi hỏi phải tính toán zero-order và first-order statistics như trong <a href="https://projets-lium.univ-lemans.fr/sidekit/tutorial/tv_estimation.html">tutorial</a> của sidekit.</p>

<p><img src="/assets/images/i-vector-timeline.jpg" alt="speaker" /></p>

<p>Implementation của <a href="https://projets-lium.univ-lemans.fr/sidekit/_modules/nnet/xvector.html">x-vector</a>.</p>

<h2 id="seq2seq-và-các-transformers-cho-bài-toán-asr"><code class="language-plaintext highlighter-rouge">seq2seq</code> và các Transformers cho bài toán ASR</h2>

<p><strong>Listen, Attend and Spell (LAS, <a class="citation" href="#chan2016listen">[40]</a>).</strong></p>

<p><strong>SpecAugment <a class="citation" href="#park2019specaugment">[41]</a>.</strong></p>

<p><strong><code class="language-plaintext highlighter-rouge">wav2vec</code> <a class="citation" href="#schneider2019wav2vec">[42, 43]</a></strong></p>

<p><strong>TERA <a class="citation" href="#liu2021tera">[44]</a>.</strong></p>

<p><strong>HuBERT <a class="citation" href="#hsu2021hubert">[3]</a>.</strong></p>

<p><strong>Conformer <a class="citation" href="#gulati2020conformer">[7, 8]</a>.</strong></p>

<h1 id="lưu-ý-khi-train-model-asrsi">Lưu ý khi train model ASR/SI</h1>

<h2 id="giá-thành-khi-train-model-state-of-the-art-sota">Giá thành khi train model State-of-the-art (SOTA)</h2>

<p>Quay lại với mảng ASR, nhìn đi nhìn lại cũng lại hậu duệ của <code class="language-plaintext highlighter-rouge">seq2seq</code> nên chắc mẩm là giá sẽ không mềm rồi.
Như hình dưới tôi đưa ra ví dụ dựa trên mô tả của một nhóm làm về ASR trong Google Brain <a class="citation" href="#zhang2020pushing">[8]</a>.
Giá cả của TPU thì v3 mà trên 32 cores thì chỉ mới hỗ trợ tài Hà Lan nên các bạn chú ý nhé.
Kết luận chung là để mà train vài ngày trên bộ Librispeech thôi chả hạn là cũng tốn vài chục ngàn Mỹ kim rồi.
Tóm lại là cuộc chơi của người giàu các bạn ạ!</p>

<p><img src="/assets/images/asr-price-deep-models.jpg" alt="pricing" /></p>

<h1 id="tài-liệu-tham-khảo">Tài liệu tham khảo</h1>

<ol class="bibliography"><li><span id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I. 2017. Attention is all you need. <i>Advances in neural information processing systems</i> (2017), 5998–6008.</span><a class="details" href="https://wanted2.github.io/bibliography/vaswani2017attention/">Details</a></li>
<li><span id="devlin2019bert">Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i> (2019), 4171–4186.</span><a class="details" href="https://wanted2.github.io/bibliography/devlin2019bert/">Details</a></li>
<li><span id="hsu2021hubert">Hsu, W.-N., Bolte, B., Tsai, Y.-H.H., Lakhotia, K., Salakhutdinov, R. and Mohamed, A. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>. 29, (2021), 3451–3460.</span><a class="details" href="https://wanted2.github.io/bibliography/hsu2021hubert/">Details</a></li>
<li><span id="kenny2007joint">Kenny, P., Boulianne, G., Ouellet, P. and Dumouchel, P. 2007. Joint factor analysis versus eigenchannels in speaker recognition. <i>IEEE Transactions on Audio, Speech, and Language Processing</i>. 15, 4 (2007), 1435–1447.</span><a class="details" href="https://wanted2.github.io/bibliography/kenny2007joint/">Details</a></li>
<li><span id="dehak2011frontend">Dehak, N., Kenny, P.J., Dehak, R., Dumouchel, P. and Ouellet, P. 2011. Front-End Factor Analysis for Speaker Verification. <i>IEEE Transactions on Audio, Speech, and Language Processing</i>. 19, 4 (2011), 788–798. DOI:https://doi.org/10.1109/TASL.2010.2064307.</span><a class="details" href="https://wanted2.github.io/bibliography/dehak2011frontend/">Details</a></li>
<li><span id="snyder2018x">Snyder, D., Garcia-Romero, D., Sell, G., Povey, D. and Khudanpur, S. 2018. X-vectors: Robust dnn embeddings for speaker recognition. <i>2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</i> (2018), 5329–5333.</span><a class="details" href="https://wanted2.github.io/bibliography/snyder2018x/">Details</a></li>
<li><span id="gulati2020conformer">Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y. and others 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. <i>Proc. Interspeech 2020</i> (2020), 5036–5040.</span><a class="details" href="https://wanted2.github.io/bibliography/gulati2020conformer/">Details</a></li>
<li><span id="zhang2020pushing">Zhang, Y., Qin, J., Park, D.S., Han, W., Chiu, C.-C., Pang, R., Le, Q.V. and Wu, Y. 2020. Pushing the limits of semi-supervised learning for automatic speech recognition. <i>arXiv preprint arXiv:2010.10504</i>. (2020).</span><a class="details" href="https://wanted2.github.io/bibliography/zhang2020pushing/">Details</a></li>
<li><span id="reynolds1995robust">Reynolds, D.A. and Rose, R.C. 1995. Robust text-independent speaker identification using Gaussian mixture speaker models. <i>IEEE transactions on speech and audio processing</i>. 3, 1 (1995), 72–83.</span><a class="details" href="https://wanted2.github.io/bibliography/reynolds1995robust/">Details</a></li>
<li><span id="reynolds2000speaker">Reynolds, D.A., Quatieri, T.F. and Dunn, R.B. 2000. Speaker verification using adapted Gaussian mixture models. <i>Digital signal processing</i>. 10, 1 (2000), 19–41.</span><a class="details" href="https://wanted2.github.io/bibliography/reynolds2000speaker/">Details</a></li>
<li><span id="hinton2012deep">Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-rahman, Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N. and others 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. <i>IEEE Signal processing magazine</i>. 29, 6 (2012), 82–97.</span><a class="details" href="https://wanted2.github.io/bibliography/hinton2012deep/">Details</a></li>
<li><span id="yu2016automatic">Yu, D. and Deng, L. 2016. <i>Automatic speech recognition</i>. Springer.</span><a class="details" href="https://wanted2.github.io/bibliography/yu2016automatic/">Details</a></li>
<li><span id="young2008hmms">Young, S. 2008. HMMs and related speech recognition technologies. <i>Springer Handbook of Speech Processing</i>. Springer. 539–558.</span><a class="details" href="https://wanted2.github.io/bibliography/young2008hmms/">Details</a></li>
<li><span id="graves2013speech">Graves, A., Mohamed, A.-rahman and Hinton, G. 2013. Speech recognition with deep recurrent neural networks. <i>2013 IEEE international conference on acoustics, speech and signal processing</i> (2013), 6645–6649.</span><a class="details" href="https://wanted2.github.io/bibliography/graves2013speech/">Details</a></li>
<li><span id="graves2013hybrid">Graves, A., Jaitly, N. and Mohamed, A.-rahman 2013. Hybrid speech recognition with deep bidirectional LSTM. <i>2013 IEEE workshop on automatic speech recognition and understanding</i> (2013), 273–278.</span><a class="details" href="https://wanted2.github.io/bibliography/graves2013hybrid/">Details</a></li>
<li><span id="panayotov2015librispeech">Panayotov, V., Chen, G., Povey, D. and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. <i>2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</i> (2015), 5206–5210.</span><a class="details" href="https://wanted2.github.io/bibliography/panayotov2015librispeech/">Details</a></li>
<li><span id="paul1992design">Paul, D.B. and Baker, J. 1992. The design for the Wall Street Journal-based CSR corpus. <i>Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992</i> (1992).</span><a class="details" href="https://wanted2.github.io/bibliography/paul1992design/">Details</a></li>
<li><span id="garofolo1993darpa">Garofolo, J.S., Lamel, L.F., Fisher, W.M., Fiscus, J.G. and Pallett, D.S. 1993. DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc 1-1.1. <i>NASA STI/Recon technical report n</i>. 93, (1993), 27403.</span><a class="details" href="https://wanted2.github.io/bibliography/garofolo1993darpa/">Details</a></li>
<li><span id="warden2018speech">Warden, P. 2018. Speech commands: A dataset for limited-vocabulary speech recognition. <i>arXiv preprint arXiv:1804.03209</i>. (2018).</span><a class="details" href="https://wanted2.github.io/bibliography/warden2018speech/">Details</a></li>
<li><span id="watanabe2018espnet">Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., Soplin, N.-E.Y., Heymann, J., Wiesner, M., Chen, N. and others 2018. ESPnet: End-to-End Speech Processing Toolkit. <i>Proc. Interspeech 2018</i>. (2018), 2207–2211.</span><a class="details" href="https://wanted2.github.io/bibliography/watanabe2018espnet/">Details</a></li>
<li><span id="deng2013recent">Deng, L., Li, J., Huang, J.-T., Yao, K., Yu, D., Seide, F., Seltzer, M., Zweig, G., He, X., Williams, J. and others 2013. Recent advances in deep learning for speech research at Microsoft. <i>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</i> (2013), 8604–8608.</span><a class="details" href="https://wanted2.github.io/bibliography/deng2013recent/">Details</a></li>
<li><span id="hannun2014deepspeech">Hannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G., Elsen, E., Prenger, R., Satheesh, S., Sengupta, S., Coates, A. and others 2014. DeepSpeech: Scaling up end-to-end speech recognition. <i>arXiv preprint arXiv:1412.5567</i>. (2014).</span><a class="details" href="https://wanted2.github.io/bibliography/hannun2014deepspeech/">Details</a></li>
<li><span id="amodei2016deep">Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen, G. and others 2016. Deep speech 2: End-to-end speech recognition in english and mandarin. <i>International conference on machine learning</i> (2016), 173–182.</span><a class="details" href="https://wanted2.github.io/bibliography/amodei2016deep/">Details</a></li>
<li><span id="ao2021end">Ao, Y., Wu, Z., Yu, D., Gong, W., Kui, Z., Zhang, M., Ye, Z., Shen, L., Ma, Y., Wu, T. and others 2021. End-to-end Adaptive Distributed Training on PaddlePaddle. <i>arXiv preprint arXiv:2112.02752</i>. (2021).</span><a class="details" href="https://wanted2.github.io/bibliography/ao2021end/">Details</a></li>
<li><span id="paddlepaddle_github">Guides-Document-PaddlePaddle Deep Learning Platform.</span><a class="details" href="https://wanted2.github.io/bibliography/paddlepaddle_github/">Details</a></li>
<li><span id="ott2019fairseq">Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D. and Auli, M. 2019. fairseq: A Fast, Extensible Toolkit for Sequence Modeling. <i>Proceedings of NAACL-HLT 2019: Demonstrations</i> (2019).</span><a class="details" href="https://wanted2.github.io/bibliography/ott2019fairseq/">Details</a></li>
<li><span id="gemmeke2017audioset">Gemmeke, J.F., Ellis, D.P.W., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M. and Ritter, M. 2017. Audio Set: An ontology and human-labeled dataset for audio events. <i>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i> (2017), 776–780.</span><a class="details" href="https://wanted2.github.io/bibliography/gemmeke2017audioset/">Details</a></li>
<li><span id="fonseca2019audio">Fonseca, E., Plakal, M., Font, F., Ellis, D.P.W. and Serra, X. 2019. AUDIO TAGGING WITH NOISY LABELS AND MINIMAL SUPERVISION. <i>Acoustic Scenes and Events 2019 Workshop (DCASE2019)</i> (2019), 69.</span><a class="details" href="https://wanted2.github.io/bibliography/fonseca2019audio/">Details</a></li>
<li><span id="stylianou2008voice">Stylianou, Y. 2008. Voice transformation. <i>Springer handbook of speech processing</i>. Springer. 489–504.</span><a class="details" href="https://wanted2.github.io/bibliography/stylianou2008voice/">Details</a></li>
<li><span id="benesty2008springer">Benesty, J., Sondhi, M.M., Huang, Y. and others 2008. <i>Springer handbook of speech processing</i>. Springer.</span><a class="details" href="https://wanted2.github.io/bibliography/benesty2008springer/">Details</a></li>
<li><span id="rabiner2007introduction">Rabiner, L.R. and Schafer, R.W. 2007. <i>Introduction to digital speech processing</i>. Now Publishers Inc.</span><a class="details" href="https://wanted2.github.io/bibliography/rabiner2007introduction/">Details</a></li>
<li><span id="wu2015asvspoof">Wu, Z., Kinnunen, T., Evans, N., Yamagishi, J., Hanilçi, C., Sahidullah, M. and Sizov, A. 2015. ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge. <i>Sixteenth annual conference of the international speech communication association</i> (2015).</span><a class="details" href="https://wanted2.github.io/bibliography/wu2015asvspoof/">Details</a></li>
<li><span id="kinnunen2017asvspoof">Kinnunen, T., Sahidullah, M., Delgado, H., Todisco, M., Evans, N., Yamagishi, J. and Lee, K.A. 2017. The ASVspoof 2017 challenge: Assessing the limits of replay spoofing attack detection. (2017).</span><a class="details" href="https://wanted2.github.io/bibliography/kinnunen2017asvspoof/">Details</a></li>
<li><span id="delgado2018asvspoof">Delgado, H., Todisco, M., Sahidullah, M., Evans, N., Kinnunen, T., Lee, K.A. and Yamagishi, J. 2018. ASVspoof 2017 Version 2.0: meta-data analysis and baseline enhancements. <i>Odyssey 2018-The Speaker and Language Recognition Workshop</i> (2018).</span><a class="details" href="https://wanted2.github.io/bibliography/delgado2018asvspoof/">Details</a></li>
<li><span id="wang2020asvspoof">Wang, X., Yamagishi, J., Todisco, M., Delgado, H., Nautsch, A., Evans, N., Sahidullah, M., Vestman, V., Kinnunen, T., Lee, K.A. and others 2020. ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech. <i>Computer Speech &amp; Language</i>. 64, (2020), 101114.</span><a class="details" href="https://wanted2.github.io/bibliography/wang2020asvspoof/">Details</a></li>
<li><span id="kamble2020advances">Kamble, M.R., Sailor, H.B., Patil, H.A. and Li, H. 2020. Advances in anti-spoofing: from the perspective of ASVspoof challenges. <i>APSIPA Transactions on Signal and Information Processing</i>. 9, (2020).</span><a class="details" href="https://wanted2.github.io/bibliography/kamble2020advances/">Details</a></li>
<li><span id="toda2016voice">Toda, T., Chen, L.-H., Saito, D., Villavicencio, F., Wester, M., Wu, Z. and Yamagishi, J. 2016. The Voice Conversion Challenge 2016. <i>Interspeech</i> (2016), 1632–1636.</span><a class="details" href="https://wanted2.github.io/bibliography/toda2016voice/">Details</a></li>
<li><span id="lorenzo2018voice">Lorenzo-Trueba, J., Yamagishi, J., Toda, T., Saito, D., Villavicencio, F., Kinnunen, T. and Ling, Z. 2018. The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods. <i>Proc. Odyssey 2018 The Speaker and Language Recognition Workshop</i> (2018), 195–202.</span><a class="details" href="https://wanted2.github.io/bibliography/lorenzo2018voice/">Details</a></li>
<li><span id="Yi2020vcc">Yi, Z., Huang, W.-C., Tian, X., Yamagishi, J., Das, R., Kinnunen, T., Ling, Z.-H. and Toda, T. 2020. Voice Conversion Challenge 2020 –- Intra-lingual semi-parallel and cross-lingual voice conversion –-. (Oct. 2020), 80–98.</span><a class="details" href="https://wanted2.github.io/bibliography/Yi2020vcc/">Details</a></li>
<li><span id="chan2016listen">Chan, W., Jaitly, N., Le, Q. and Vinyals, O. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. <i>2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)</i> (2016), 4960–4964.</span><a class="details" href="https://wanted2.github.io/bibliography/chan2016listen/">Details</a></li>
<li><span id="park2019specaugment">Park, D.S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E.D. and Le, Q.V. 2019. SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. <i>Proc. Interspeech 2019</i> (2019), 2613–2617.</span><a class="details" href="https://wanted2.github.io/bibliography/park2019specaugment/">Details</a></li>
<li><span id="schneider2019wav2vec">Schneider, S., Baevski, A., Collobert, R. and Auli, M. 2019. wav2vec: Unsupervised Pre-Training for Speech Recognition. <i>Proc. Interspeech 2019</i> (2019), 3465–3469.</span><a class="details" href="https://wanted2.github.io/bibliography/schneider2019wav2vec/">Details</a></li>
<li><span id="baevski2020wav2vec">Baevski, A., Zhou, Y., Mohamed, A. and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. <i>Advances in Neural Information Processing Systems</i> (2020), 12449–12460.</span><a class="details" href="https://wanted2.github.io/bibliography/baevski2020wav2vec/">Details</a></li>
<li><span id="liu2021tera">Liu, A.T., Li, S.-W. and Lee, H.-yi 2021. Tera: Self-supervised learning of transformer encoder representation for speech. <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing</i>. 29, (2021), 2351–2366.</span><a class="details" href="https://wanted2.github.io/bibliography/liu2021tera/">Details</a></li></ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Nguyên tắc vẫn như cũ, chọn những bài top nhiều citations và có độ tăng trưởng tốt thôi nhé. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Bài viết này chủ yếu là giới thiệu công nghệ thôi. Chứ còn <code class="language-plaintext highlighter-rouge">đồ ăn sẵn</code> thì các bạn có thể tham khảo <a href="https://docs.aws.amazon.com/lexv2/latest/dg/what-is.html">Amazon Lex</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>

            </div>

            <!-- Rating -->
            

            <!-- Post Date -->
            <p>
            <small>
                <span class="post-date"><time class="post-date" datetime="2022-02-06">06 Feb 2022</time></span>           
                
                </small>
            </p>

            <!-- Post Categories -->
            <div class="after-post-cats">
                <ul class="tags mb-4">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Categories -->

            <!-- Post Tags -->
            <div class="after-post-tags">
                <ul class="tags">
                    
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#BERT">#BERT</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Sequence-to-sequence">#Sequence-to-sequence</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#Transformer">#Transformer</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#deep-metric-learning">#deep metric learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#metric-learning">#metric learning</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speaker-diarization">#speaker diarization</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speaker-identification">#speaker identification</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speaker-verification">#speaker verification</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speech-recognition">#speech recognition</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#speech-synthesis">#speech synthesis</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#voice-conversion">#voice conversion</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#話者同定">#話者同定</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#音声合成">#音声合成</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#音声変換">#音声変換</a>
                    </li>
                    
                    <li>
                        <a class="smoothscroll" href="https://wanted2.github.io/tags#音声認識">#音声認識</a>
                    </li>
                    
                </ul>
            </div>
            <!-- End Tags -->

            <!-- Prev/Next -->
            <div class="row PageNavigation d-flex justify-content-between font-weight-bold">
            
            <a class="prev d-block col-md-6" href="https://wanted2.github.io//seq2seq/"> &laquo; Seq2Seq và kiến trúc Encoder-Decoder</a>
            
            
            <div class="clearfix"></div>
            </div>
            <!-- End Categories -->

        </div>
        <!-- End Post -->

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'caineng'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->

</div>


<!-- Bottom Alert Bar
================================================== -->
<div class="alertbar">
	<div class="container text-center">
		<span><img src="https://wanted2.github.io/assets/images/favicon.ico" alt="AiFi" style="max-height: 48px;" /> &nbsp; Never miss a <b>story</b> from us, subscribe to our newsletter</span>
        <form action="https://caineng.us20.list-manage.com/subscribe/post?u=76342d3d74a6807aac5aec0d7&id=b5645e19be" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group">
            <input type="email" placeholder="Email" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
	</div>
</div>

    
</div>

<!-- Categories Jumbotron
================================================== -->
<div class="jumbotron fortags">
	<div class="d-md-flex h-100">
		<div class="col-md-4 transpdark align-self-center text-center h-100">
            <div class="d-md-flex align-items-center justify-content-center h-100">
                <h2 class="d-md-block align-self-center py-1 font-weight-light">Explore <span class="d-none d-md-inline">→</span></h2>
            </div>
		</div>
		<div class="col-md-8 p-5 align-self-center text-center">
            
            
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Tiếng-Việt,-日本語">Tiếng Việt, 日本語 (7)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Project-Management">Project Management (2)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Software-Engineering">Software Engineering (3)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Artificial-Intelligence">Artificial Intelligence (4)</a>
                
                    <a class="mt-1 mb-1" href="https://wanted2.github.io/categories#Site-Reliable-Engineering">Site Reliable Engineering (3)</a>
                
            
            
		</div>
	</div>
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-6 text-center text-lg-left">
                Copyright © 2022 AiFi 
            </div>
            <div class="col-md-6 col-sm-6 text-center text-lg-right">    
                <a target="_blank" href="https://www.wowthemes.net/mediumish-free-jekyll-template/">Mediumish Jekyll Theme</a> by WowThemes.net
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="https://wanted2.github.io/assets/js/mediumish.js"></script>


<script src="https://wanted2.github.io/assets/js/lazyload.js"></script>


<script src="https://wanted2.github.io/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//caineng.disqus.com/count.js"></script>


</body>
</html>
