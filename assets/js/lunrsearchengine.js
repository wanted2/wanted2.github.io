
var documents = [{
    "id": 0,
    "url": "https://wanted2.github.io/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://wanted2.github.io/about",
    "title": "Tuan Nguyen-Anh",
    "body": "Contacts: tuan. nguyenanh. brse@gmail. com| Curriculum Vitae: English Experience: Bridge System Engineer/Project Management/Product Owner/Scrum Master/Techlead/TeamleadSystem engineering/Web development      Backend: Java SpringBoot, Scala Playframework, PHP Laravel, Python Flask/Django, . etc.     Frontend: VueJS, ReactJS, AngularJS, Svelte, jQuery, . etc.     Cloud: AWS, Google GCP, Azure    DevOps, SRE, . etc.     Artificial Intelligence/Internet of Things/Computer Vision/Machine Learning "
    }, {
    "id": 2,
    "url": "https://wanted2.github.io/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "https://wanted2.github.io/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                           ハノイではリモートワークが本当に人気でしょうか？                              :               ちらほら海外からお誘いがありました．「どうかリモートワークをしましょうか」という誘いです．なんかコロナ禍で海外では皆がよくリモートワークをしているかと思います．おー，本当にベトナムでもリモートワークが人気だろうなと思っていたが，周りを見るとハノイではそうではない気もしました．業界はそれぞれで，現場にいかないと仕事を行えない土木系の仕事なども多いという原因があります．IT業界でも実際はコロナ禍は主要な原因でリモートワークをしているが，コロナ禍がなくなると，IT業界もみんな本気でオフ. . . :                                                                                                                                                                       AiFi                                27 Apr 2022                                                                                                                                                                                                                                                                                                                  Edge-Cloud architectures &amp; TPU resources                              :               Edge-Cloud (EC) architectures is an emerging computing paradigm recently, which takes the AI computation to the edge devices and only aggregates processed important data to the cloud. Tensor Processing Unit (TPU) is an AI accelerator application-s. . . :                                                                                                                                                                       AiFi                                05 Mar 2022                                                                                                                                                                                                                                                                                                                  Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices                              :               Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự. . . :                                                                                                                                                                       AiFi                                12 Feb 2022                                                                                                                                                                                                                                                                                                                  Machine Learning for Network Intrusion Detection: From Local to Production                              :               Network Intrusion Detection System Network intrusion detection system (NIDS) is an independent platform that examines network traffic patterns to identify intrusions for an entire network. It needs to be placed at a choke point where all tra. . . :                                                                                                                                                                       AiFi                                29 Jan 2022                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              All Stories:                                                                                                     ハノイではリモートワークが本当に人気でしょうか？              :       ちらほら海外からお誘いがありました．「どうかリモートワークをしましょうか」という誘いです．なんかコロナ禍で海外では皆がよくリモートワークをしているかと思います．おー，本当にベトナムでもリモートワークが人気だろうなと思っていたが，周りを見るとハノイではそうではない気もしました．業界はそれぞれで，現場にいかないと仕事を行えない土木系の仕事なども多いという原因があります．IT業界でも実際はコロナ禍は主要な原因でリモートワークをしているが，コロナ禍がなくなると，IT業界もみんな本気でオフ. . . :                                                                               AiFi                27 Apr 2022                                                                                                                                    Edge-Cloud architectures &amp; TPU resources              :       Edge-Cloud (EC) architectures is an emerging computing paradigm recently, which takes the AI computation to the edge devices and only aggregates processed important data to the cloud. Tensor Processing Unit (TPU) is an AI accelerator application-s. . . :                                                                               AiFi                05 Mar 2022                                                                                                                                    Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices              :       Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự. . . :                                                                               AiFi                12 Feb 2022                                                                                                                                    Machine Learning for Network Intrusion Detection: From Local to Production              :       Network Intrusion Detection System Network intrusion detection system (NIDS) is an independent platform that examines network traffic patterns to identify intrusions for an entire network. It needs to be placed at a choke point where all tra. . . :                                                                               AiFi                29 Jan 2022                                                                                                                                    責任者              :       ベトナムに帰ってからベトナムの職場文化になってからはもはや2年間になっております．ベトナム職場でいうと，恥ずかしいけど，楽しい経験もあるし，悲しい経験もありました．仕事の責任者として働いた経験もあり，悲しい時で，部下に怒られて，そろそろ殴られる経験もありました．なぜなら，背景から考えると，文化の違いかなと思います．ベトナム職場では上下関係は社会的に存在するけど，きちんと働かないとね，上下関係なく殴られるそうです．「殴られる」は厳しい言葉ですが，主にいうと，ベトナム職場で下記の三大. . . :                                                                               AiFi                16 Jan 2022                                                                                                                                    RapidAPI and RapidAPI Hub              :       Image Credit: FinanceFeedsRakuten launched RapidAPI Marketplace in 2018 as a result of the collaboration between Japan’s Rakuten Inc and San Francisco-based startup RapidAPI. The API marketplace aims to provide software developers in Japan and A. . . :                                                                               AiFi                09 Jan 2022                                               &laquo; Prev       1        2        3        4        5        6        7        8        9        10        11      Next &raquo; "
    }, {
    "id": 4,
    "url": "https://wanted2.github.io/projects",
    "title": "Projects and Demos",
    "body": "Projects: Demos: AI demos "
    }, {
    "id": 5,
    "url": "https://wanted2.github.io/service",
    "title": "Services",
    "body": "I also provide consultancy service for clients who need supports in their development projects. Consulting servicesI can advise on the design and development of outsourcing projects which carry out formal development from high-level design, development to testing, deployment and maintenance. I also help to plot out the research ideas and proposals. Just reach me for talking ;-) My domain includes:  Information Systems incl. Intelligent Systems Software Development methodologies Aritificial Intelligence, Machine Learning, Computer Vision and Natural Language Processing. BookingTo book a session, you need to follow the instructions below. Please book at least 1 day prior to the dating, otherwise, a reschedule may happen. Buy Me A CoffeeYou can also buy me a coffee if you want: "
    }, {
    "id": 6,
    "url": "https://wanted2.github.io/bibliography/abstraction1988hierarchy/",
    "title": "Data Abstraction and Hierarchy",
    "body": "  Liskov, B. 1988. Data Abstraction and Hierarchy. SIGPLAN Notices. 23, 5 (1988), 17–34.                                                                                             @article{abstraction1988hierarchy, author = {Liskov, Barbara}, journal = {SIGPLAN Notices}, number = {5}, pages = {17--34}, title = {Data Abstraction and Hierarchy}, volume = {23}, year = {1988}}                                                        "
    }, {
    "id": 7,
    "url": "https://wanted2.github.io/bibliography/balaji2018benchmarking/",
    "title": "Benchmarking automatic machine learning frameworks",
    "body": "  Balaji, A. and Allen, A. 2018. Benchmarking automatic machine learning frameworks. arXiv preprint arXiv:1808. 06492. (2018).                                                                                             @article{balaji2018benchmarking, author = {Balaji, Adithya and Allen, Alexander}, journal = {arXiv preprint arXiv:1808. 06492}, title = {Benchmarking automatic machine learning frameworks}, year = {2018}}                                                        "
    }, {
    "id": 8,
    "url": "https://wanted2.github.io/bibliography/behutiye2017analyzing/",
    "title": "Analyzing the concept of technical debt in the context of agile software development: A systematic literature review",
    "body": "  Behutiye, W. N. , Rodrı́guez Pilar, Oivo, M. and Tosun, A. 2017. Analyzing the concept of technical debt in the context of agile software development: A systematic literature review. Information and Software Technology. 82, (2017), 139–158.                                                                                             @article{behutiye2017analyzing, author = {Behutiye, Woubshet Nema and Rodr{\'\i}guez, Pilar and Oivo, Markku and Tosun, Ay{\c{s}}e}, journal = {Information and Software Technology}, pages = {139--158}, publisher = {Elsevier}, title = {Analyzing the concept of technical debt in the context of agile software development: A systematic literature review}, volume = {82}, year = {2017}}                                                        "
    }, {
    "id": 9,
    "url": "https://wanted2.github.io/bibliography/bourque2004swebok/",
    "title": "SWEBOK",
    "body": "  Bourque, P. and Fairley, R. 2004. SWEBOK. Nd: IEEE Computer society. (2004).                                                                                             @article{bourque2004swebok, author = {Bourque, Pierre and Fairley, R}, journal = {Nd: IEEE Computer society}, title = {SWEBOK}, year = {2004}}                                                        "
    }, {
    "id": 10,
    "url": "https://wanted2.github.io/bibliography/brown2013software/",
    "title": "Software architecture for developers",
    "body": "  Brown, S. 2013. Software architecture for developers. Coding the Architecture. (2013).                                                                                             @article{brown2013software, author = {Brown, Simon}, journal = {Coding the Architecture}, title = {Software architecture for developers}, year = {2013}}                                                        "
    }, {
    "id": 11,
    "url": "https://wanted2.github.io/bibliography/cleanarch/",
    "title": "Clean Architecture - A Craftman’s Guide to Software Structure and Design",
    "body": "  Martin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall.                                                                                             @book{cleanarch, author = {Martin, Robert C. }, publisher = {Prentice Hall}, title = {Clean Architecture - A Craftman's Guide to Software Structure and Design}, year = {2017}}                                                        "
    }, {
    "id": 12,
    "url": "https://wanted2.github.io/bibliography/cunningham1992wycash/",
    "title": "The WyCash portfolio management system",
    "body": "  Cunningham, W. 1992. The WyCash portfolio management system. ACM SIGPLAN OOPS Messenger. 4, 2 (1992), 29–30.                                                                                             @article{cunningham1992wycash, author = {Cunningham, Ward}, journal = {ACM SIGPLAN OOPS Messenger}, number = {2}, pages = {29--30}, publisher = {ACM New York, NY, USA}, title = {The WyCash portfolio management system}, volume = {4}, year = {1992}}                                                        "
    }, {
    "id": 13,
    "url": "https://wanted2.github.io/bibliography/DBLP_journals/corr/BodlaSCD17/",
    "title": "Improving Object Detection With One Line of Code",
    "body": "  Bodla, N. , Singh, B. , Chellappa, R. and Davis, L. S. 2017. Improving Object Detection With One Line of Code. CoRR. abs/1704. 04503, (2017).                                                                                             @article{DBLP:journals/corr/BodlaSCD17, archiveprefix = {arXiv}, author = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S. }, eprint = {1704. 04503}, journal = {CoRR}, title = {Improving Object Detection With One Line of Code}, url = {http://arxiv. org/abs/1704. 04503}, volume = {abs/1704. 04503}, year = {2017}}                                                        "
    }, {
    "id": 14,
    "url": "https://wanted2.github.io/bibliography/feige1979casual/",
    "title": "The casual causal relationship between money and income: Some caveats for time series analysis",
    "body": "  Feige, E. L. and Pearce, D. K. 1979. The casual causal relationship between money and income: Some caveats for time series analysis. The Review of Economics and Statistics. (1979), 521–533.                                                                                             @article{feige1979casual, author = {Feige, Edgar L and Pearce, Douglas K}, journal = {The Review of Economics and Statistics}, pages = {521--533}, publisher = {JSTOR}, title = {The casual causal relationship between money and income: Some caveats for time series analysis}, year = {1979}}                                                        "
    }, {
    "id": 15,
    "url": "https://wanted2.github.io/bibliography/fisher1983getting/",
    "title": "Getting to Yes: Negotiating Agreement Without Giving In",
    "body": "  Fisher, R. and Ury, W. 1983. Getting to Yes: Negotiating Agreement Without Giving In. New York: Penguin Books.                                                                                             @book{fisher1983getting, author = {Fisher, Roger and Ury, William}, publisher = {New York: Penguin Books}, title = {Getting to Yes: Negotiating Agreement Without Giving In}, year = {1983}}                                                        "
    }, {
    "id": 16,
    "url": "https://wanted2.github.io/bibliography/granger1969investigating/",
    "title": "Investigating causal relations by econometric models and cross-spectral methods",
    "body": "  Granger, C. W. J. 1969. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: journal of the Econometric Society. (1969), 424–438.                                                                                             @article{granger1969investigating, author = {Granger, Clive WJ}, journal = {Econometrica: journal of the Econometric Society}, pages = {424--438}, publisher = {JSTOR}, title = {Investigating causal relations by econometric models and cross-spectral methods}, year = {1969}}                                                        "
    }, {
    "id": 17,
    "url": "https://wanted2.github.io/bibliography/hamburger1986three/",
    "title": "Three perceptions of project cost",
    "body": "  Hamburger, D. H. 1986. Three perceptions of project cost. Project Management journal. (1986), 372–378.                                                                                             @article{hamburger1986three, author = {Hamburger, DH}, journal = {Project Management journal}, pages = {372--378}, title = {Three perceptions of project cost}, year = {1986}}                                                        "
    }, {
    "id": 18,
    "url": "https://wanted2.github.io/bibliography/Kirillov_2019_CVPR/",
    "title": "Panoptic Segmentation",
    "body": "  Kirillov, A. , He, K. , Girshick, R. , Rother, C. and Dollar, P. 2019. Panoptic Segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Jun. 2019).                                                                                             @inproceedings{Kirillov_2019_CVPR, author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = jun, title = {Panoptic Segmentation}, year = {2019}}                                                        "
    }, {
    "id": 19,
    "url": "https://wanted2.github.io/bibliography/kuznetsova2020open/",
    "title": "The open images dataset v4",
    "body": "  Kuznetsova, A. , Rom, H. , Alldrin, N. , Uijlings, J. , Krasin, I. , Pont-Tuset, J. , Kamali, S. , Popov, S. , Malloci, M. , Kolesnikov, A. , Duerig, T. and Ferrari, V. 2020. The open images dataset v4. International Journal of Computer Vision. 128, 7 (2020), 1956–1981.                                                                                             @article{kuznetsova2020open, author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and Duerig, Tom and Ferrari, Vittorio}, journal = {International Journal of Computer Vision}, number = {7}, pages = {1956--1981}, publisher = {Springer}, title = {The open images dataset v4}, volume = {128}, year = {2020}}                                                        "
    }, {
    "id": 20,
    "url": "https://wanted2.github.io/bibliography/lin2014microsoft/",
    "title": "Microsoft coco: Common objects in context",
    "body": "  Lin, T. -Y. , Maire, M. , Belongie, S. , Hays, J. , Perona, P. , Ramanan, D. , Dollár, P. and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. European conference on computer vision (2014), 740–755.                                                                                             @inproceedings{lin2014microsoft, author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence}, booktitle = {European conference on computer vision}, organization = {Springer}, pages = {740--755}, title = {Microsoft coco: Common objects in context}, year = {2014}}                                                        "
    }, {
    "id": 21,
    "url": "https://wanted2.github.io/bibliography/machinery1950computing/",
    "title": "Computing machinery and intelligence",
    "body": "  Turing, A. 1950. Computing machinery and intelligence. Mind. 49, 236 (1950), 433.                                                                                             @article{machinery1950computing, author = {Turing, Alan}, journal = {Mind}, number = {236}, pages = {433}, title = {Computing machinery and intelligence}, volume = {49}, year = {1950}}                                                        "
    }, {
    "id": 22,
    "url": "https://wanted2.github.io/bibliography/meredith2017project/",
    "title": "Project management: a strategic managerial approach",
    "body": "  Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons.                                                                                             @book{meredith2017project, author = {Meredith, Jack R and Shafer, Scott M and Mantel Jr, Samuel J}, publisher = {John Wiley \&amp; Sons}, title = {Project management: a strategic managerial approach}, year = {2017}}                                                        "
    }, {
    "id": 23,
    "url": "https://wanted2.github.io/bibliography/qureshi2019reduce/",
    "title": "Reduce Cost of Batch Processing Microsoft Azure Cloud",
    "body": "  Qureshi, A. N. 2019. Reduce Cost of Batch Processing Microsoft Azure Cloud. JETIR-International Journal of Emerging Technologies and Innovative Research. (2019), 2349–5162.                                                                                             @article{qureshi2019reduce, author = {Qureshi, Asfak N}, journal = {JETIR-International Journal of Emerging Technologies and Innovative Research}, pages = {2349--5162}, title = {Reduce Cost of Batch Processing Microsoft Azure Cloud}, year = {2019}}                                                        "
    }, {
    "id": 24,
    "url": "https://wanted2.github.io/bibliography/risco2021gpu/",
    "title": "GPU-Enabled Serverless Workflows for Efficient Multimedia Processing",
    "body": "  Risco, S. and Moltó, G. 2021. GPU-Enabled Serverless Workflows for Efficient Multimedia Processing. Applied Sciences. 11, 4 (2021), 1438.                                                                                             @article{risco2021gpu, author = {Risco, Sebasti{\'a}n and Molt{\'o}, Germ{\'a}n}, journal = {Applied Sciences}, number = {4}, pages = {1438}, publisher = {Multidisciplinary Digital Publishing Institute}, title = {GPU-Enabled Serverless Workflows for Efficient Multimedia Processing}, volume = {11}, year = {2021}}                                                        "
    }, {
    "id": 25,
    "url": "https://wanted2.github.io/bibliography/soh2020overview/",
    "title": "Overview of Azure Platform as a Service",
    "body": "  Soh, J. , Copeland, M. , Puca, A. and Harris, M. 2020. Overview of Azure Platform as a Service. Microsoft Azure. Springer. 43–55.                                                                                             @incollection{soh2020overview, author = {Soh, Julian and Copeland, Marshall and Puca, Anthony and Harris, Micheleen}, booktitle = {Microsoft Azure}, pages = {43--55}, publisher = {Springer}, title = {Overview of Azure Platform as a Service}, year = {2020}}                                                        "
    }, {
    "id": 26,
    "url": "https://wanted2.github.io/bibliography/thurman1988chickens/",
    "title": "Chickens, eggs, and causality, or which came first",
    "body": "  Thurman, W. N. , Fisher, M. E. and others 1988. Chickens, eggs, and causality, or which came first. American journal of agricultural economics. 70, 2 (1988), 237–238.                                                                                             @article{thurman1988chickens, author = {Thurman, Walter N and Fisher, Mark E and others}, journal = {American journal of agricultural economics}, number = {2}, pages = {237--238}, publisher = {Agricultural and Applied Economics Association}, title = {Chickens, eggs, and causality, or which came first}, volume = {70}, year = {1988}}                                                        "
    }, {
    "id": 27,
    "url": "https://wanted2.github.io/bibliography/TIAN20201/",
    "title": "Computer vision technology in agricultural automation –A review",
    "body": "  Tian, H. , Wang, T. , Liu, Y. , Qiao, X. and Li, Y. 2020. Computer vision technology in agricultural automation –A review. Information Processing in Agriculture. 7, 1 (2020), 1–19. DOI:https://doi. org/10. 1016/j. inpa. 2019. 09. 006.     DOI                                                                                            @article{TIAN20201, author = {Tian, Hongkun and Wang, Tianhai and Liu, Yadong and Qiao, Xi and Li, Yanzhou}, doi = {10. 1016/j. inpa. 2019. 09. 006}, issn = {2214-3173}, journal = {Information Processing in Agriculture}, keywords = {Computer vision, Image processing, Agricultural automation, Intelligent detection}, number = {1}, pages = {1-19}, title = {Computer vision technology in agricultural automation --A review}, url = {https://www. sciencedirect. com/science/article/pii/S2214317319301751}, volume = {7}, year = {2020}}                                                        "
    }, {
    "id": 28,
    "url": "https://wanted2.github.io/bibliography/witte2019event/",
    "title": "Event-driven workflows for large-scale seismic imaging in the cloud",
    "body": "  Witte, P. A. , Louboutin, M. , Modzelewski, H. , Jones, C. , Selvage, J. and Herrmann, F. J. 2019. Event-driven workflows for large-scale seismic imaging in the cloud. SEG Technical Program Expanded Abstracts 2019. Society of Exploration Geophysicists. 3984–3988.                                                                                             @incollection{witte2019event, author = {Witte, Philipp A and Louboutin, Mathias and Modzelewski, Henryk and Jones, Charles and Selvage, James and Herrmann, Felix J}, booktitle = {SEG Technical Program Expanded Abstracts 2019}, pages = {3984--3988}, publisher = {Society of Exploration Geophysicists}, title = {Event-driven workflows for large-scale seismic imaging in the cloud}, year = {2019}}                                                        "
    }, {
    "id": 29,
    "url": "https://wanted2.github.io/bibliography/dosovitskiy2020image/",
    "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
    "body": "  Dosovitskiy, A. , Beyer, L. , Kolesnikov, A. , Weissenborn, D. , Zhai, X. , Unterthiner, T. , Dehghani, M. , Minderer, M. , Heigold, G. , Gelly, S. , Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010. 11929. (2020).                                                                                             @article{dosovitskiy2020image, title = {An image is worth 16x16 words: Transformers for image recognition at scale}, author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uskoreit, Jakob and Houlsby, Neil}, journal = {arXiv preprint arXiv:2010. 11929}, year = {2020}}                                                        "
    }, {
    "id": 30,
    "url": "https://wanted2.github.io/bibliography/carion2020end/",
    "title": "End-to-end object detection with transformers",
    "body": "  Carion, N. , Massa, F. , Synnaeve, G. , Usunier, N. , Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. European conference on computer vision (2020), 213–229.                                                                                             @inproceedings{carion2020end, title = {End-to-end object detection with transformers}, author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey}, booktitle = {European conference on computer vision}, pages = {213--229}, year = {2020}, organization = {Springer}}                                                        "
    }, {
    "id": 31,
    "url": "https://wanted2.github.io/bibliography/gao2021fast/",
    "title": "Fast convergence of detr with spatially modulated co-attention",
    "body": "  Gao, P. , Zheng, M. , Wang, X. , Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. Proceedings of the IEEE/CVF International Conference on Computer Vision (2021), 3621–3630.                                                                                             @inproceedings{gao2021fast, title = {Fast convergence of detr with spatially modulated co-attention}, author = {Gao, Peng and Zheng, Minghang and Wang, Xiaogang and Dai, Jifeng and Li, Hongsheng}, booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages = {3621--3630}, year = {2021}}                                                        "
    }, {
    "id": 32,
    "url": "https://wanted2.github.io/bibliography/ge2021yolox/",
    "title": "Yolox: Exceeding yolo series in 2021",
    "body": "  Ge, Z. , Liu, S. , Wang, F. , Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107. 08430. (2021).                                                                                             @article{ge2021yolox, title = {Yolox: Exceeding yolo series in 2021}, author = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian}, journal = {arXiv preprint arXiv:2107. 08430}, year = {2021}}                                                        "
    }, {
    "id": 33,
    "url": "https://wanted2.github.io/bibliography/redmon2018yolov3/",
    "title": "Yolov3: An incremental improvement",
    "body": "  Redmon, J. and Farhadi, A. 2018. Yolov3: An incremental improvement. arXiv preprint arXiv:1804. 02767. (2018).                                                                                             @article{redmon2018yolov3, title = {Yolov3: An incremental improvement}, author = {Redmon, Joseph and Farhadi, Ali}, journal = {arXiv preprint arXiv:1804. 02767}, year = {2018}}                                                        "
    }, {
    "id": 34,
    "url": "https://wanted2.github.io/bibliography/ilyas2018plastic/",
    "title": "Plastic waste as a significant threat to environment–a systematic literature review",
    "body": "  Ilyas, M. , Ahmad, W. , Khan, H. , Yousaf, S. , Khan, K. and Nazir, S. 2018. Plastic waste as a significant threat to environment–a systematic literature review. Reviews on environmental health. 33, 4 (2018), 383–406.                                                                                             @article{ilyas2018plastic, title = {Plastic waste as a significant threat to environment--a systematic literature review}, author = {Ilyas, Muhammad and Ahmad, Waqas and Khan, Hizbullah and Yousaf, Saeeda and Khan, Kifayatullah and Nazir, Shah}, journal = {Reviews on environmental health}, volume = {33}, number = {4}, pages = {383--406}, year = {2018}, publisher = {De Gruyter}}                                                        "
    }, {
    "id": 35,
    "url": "https://wanted2.github.io/bibliography/soh2020overviex/",
    "title": "Overview of Azure Platform as a Service",
    "body": "  Soh, J. , Copeland, M. , Puca, A. and Harris, M. 2020. Overview of Azure Platform as a Service. Microsoft Azure. Springer. 43–55.                                                                                             @incollection{soh2020overviex, title = {Overview of Azure Platform as a Service}, author = {Soh, Julian and Copeland, Marshall and Puca, Anthony and Harris, Micheleen}, booktitle = {Microsoft Azure}, pages = {43--55}, year = {2020}, publisher = {Springer}}                                                        "
    }, {
    "id": 36,
    "url": "https://wanted2.github.io/bibliography/witte2019evenu/",
    "title": "Event-driven workflows for large-scale seismic imaging in the cloud",
    "body": "  Witte, P. A. , Louboutin, M. , Modzelewski, H. , Jones, C. , Selvage, J. and Herrmann, F. J. 2019. Event-driven workflows for large-scale seismic imaging in the cloud. SEG Technical Program Expanded Abstracts 2019. Society of Exploration Geophysicists. 3984–3988.                                                                                             @incollection{witte2019evenu, title = {Event-driven workflows for large-scale seismic imaging in the cloud}, author = {Witte, Philipp A and Louboutin, Mathias and Modzelewski, Henryk and Jones, Charles and Selvage, James and Herrmann, Felix J}, booktitle = {SEG Technical Program Expanded Abstracts 2019}, pages = {3984--3988}, year = {2019}, publisher = {Society of Exploration Geophysicists}}                                                        "
    }, {
    "id": 37,
    "url": "https://wanted2.github.io/bibliography/balaji2018benchmarkinh/",
    "title": "Benchmarking automatic machine learning frameworks",
    "body": "  Balaji, A. and Allen, A. 2018. Benchmarking automatic machine learning frameworks. arXiv preprint arXiv:1808. 06492. (2018).                                                                                             @article{balaji2018benchmarkinh, title = {Benchmarking automatic machine learning frameworks}, author = {Balaji, Adithya and Allen, Alexander}, journal = {arXiv preprint arXiv:1808. 06492}, year = {2018}}                                                        "
    }, {
    "id": 38,
    "url": "https://wanted2.github.io/bibliography/risco2021gpv/",
    "title": "GPU-Enabled Serverless Workflows for Efficient Multimedia Processing",
    "body": "  Risco, S. and Moltó, G. 2021. GPU-Enabled Serverless Workflows for Efficient Multimedia Processing. Applied Sciences. 11, 4 (2021), 1438.                                                                                             @article{risco2021gpv, title = {GPU-Enabled Serverless Workflows for Efficient Multimedia Processing}, author = {Risco, Sebasti{\'a}n and Molt{\'o}, Germ{\'a}n}, journal = {Applied Sciences}, volume = {11}, number = {4}, pages = {1438}, year = {2021}, publisher = {Multidisciplinary Digital Publishing Institute}}                                                        "
    }, {
    "id": 39,
    "url": "https://wanted2.github.io/bibliography/qureshi2019reducf/",
    "title": "Reduce Cost of Batch Processing Microsoft Azure Cloud",
    "body": "  Qureshi, A. N. 2019. Reduce Cost of Batch Processing Microsoft Azure Cloud. JETIR-International Journal of Emerging Technologies and Innovative Research. (2019), 2349–5162.                                                                                             @article{qureshi2019reducf, title = {Reduce Cost of Batch Processing Microsoft Azure Cloud}, author = {Qureshi, Asfak N}, journal = {JETIR-International Journal of Emerging Technologies and Innovative Research}, pages = {2349--5162}, year = {2019}}                                                        "
    }, {
    "id": 40,
    "url": "https://wanted2.github.io/bibliography/martin2009clean/",
    "title": "Clean code: a handbook of agile software craftsmanship",
    "body": "  Martin, R. C. 2009. Clean code: a handbook of agile software craftsmanship. Pearson Education.                                                                                             @book{martin2009clean, title = {Clean code: a handbook of agile software craftsmanship}, author = {Martin, Robert C}, year = {2009}, publisher = {Pearson Education}}                                                        "
    }, {
    "id": 41,
    "url": "https://wanted2.github.io/bibliography/martin2007professionalism/",
    "title": "Professionalism and test-driven development",
    "body": "  Martin, R. C. 2007. Professionalism and test-driven development. IEEE Software. 24, 3 (2007), 32–36.                                                                                             @article{martin2007professionalism, title = {Professionalism and test-driven development}, author = {Martin, Robert C}, journal = {IEEE Software}, volume = {24}, number = {3}, pages = {32--36}, year = {2007}, publisher = {IEEE}}                                                        "
    }, {
    "id": 42,
    "url": "https://wanted2.github.io/bibliography/binder2000testing/",
    "title": "Testing object-oriented systems: models, patterns, and tools",
    "body": "  Binder, R. 2000. Testing object-oriented systems: models, patterns, and tools. Addison-Wesley Professional.                                                                                             @book{binder2000testing, title = {Testing object-oriented systems: models, patterns, and tools}, author = {Binder, Robert}, year = {2000}, publisher = {Addison-Wesley Professional}}                                                        "
    }, {
    "id": 43,
    "url": "https://wanted2.github.io/bibliography/ipa01/",
    "title": "情報システムの障害状況 - 2019 年後半データ",
    "body": "  松田 晃一, 村岡 恭昭 and 齋藤 毅 2019. 情報システムの障害状況 - 2019 年後半データ. IPA社会基盤センター.                                                                                             @techreport{ipa01, author = {松田, 晃一 and 村岡, 恭昭 and 齋藤, 毅}, title = {情報システムの障害状況 - 2019 年後半データ}, institution = {IPA社会基盤センター}, year = {2019}}                                                        "
    }, {
    "id": 44,
    "url": "https://wanted2.github.io/bibliography/kent2006guide/",
    "title": "Guide to Computer Security Log Management: Recommendations of the National Institute of Standards and Technology",
    "body": "  Kent, K. A. and Souppaya, M. 2006. Guide to Computer Security Log Management: Recommendations of the National Institute of Standards and Technology. US Department of Commerce, Technology Administration, National Institute of Standards and Technology.                                                                                             @book{kent2006guide, title = {Guide to Computer Security Log Management:~Recommendations of the National Institute of Standards and Technology}, author = {Kent, Karen Ann and Souppaya, Murugiah}, year = {2006}, publisher = {US Department of Commerce, Technology Administration, National Institute of Standards and Technology}}                                                        "
    }, {
    "id": 45,
    "url": "https://wanted2.github.io/bibliography/cass2019taking/",
    "title": "Taking AI to the edge: Google’s TPU now comes in a maker-friendly package",
    "body": "  Cass, S. 2019. Taking AI to the edge: Google’s TPU now comes in a maker-friendly package. IEEE Spectrum. 56, 5 (2019), 16–17.                                                                                             @article{cass2019taking, title = {Taking AI to the edge: Google's TPU now comes in a maker-friendly package}, author = {Cass, Stephen}, journal = {IEEE Spectrum}, volume = {56}, number = {5}, pages = {16--17}, year = {2019}, publisher = {IEEE}}                                                        "
    }, {
    "id": 46,
    "url": "https://wanted2.github.io/bibliography/murshed2021machine/",
    "title": "Machine learning at the network edge: A survey",
    "body": "  Murshed, M. G. S. , Murphy, C. , Hou, D. , Khan, N. , Ananthanarayanan, G. and Hussain, F. 2021. Machine learning at the network edge: A survey. ACM Computing Surveys (CSUR). 54, 8 (2021), 1–37.                                                                                             @article{murshed2021machine, title = {Machine learning at the network edge: A survey}, author = {Murshed, MG Sarwar and Murphy, Christopher and Hou, Daqing and Khan, Nazar and Ananthanarayanan, Ganesh and Hussain, Faraz}, journal = {ACM Computing Surveys (CSUR)}, volume = {54}, number = {8}, pages = {1--37}, year = {2021}, publisher = {ACM New York, NY}}                                                        "
    }, {
    "id": 47,
    "url": "https://wanted2.github.io/bibliography/shi2020communication/",
    "title": "Communication-efficient edge AI: Algorithms and systems",
    "body": "  Shi, Y. , Yang, K. , Jiang, T. , Zhang, J. and Letaief, K. B. 2020. Communication-efficient edge AI: Algorithms and systems. IEEE Communications Surveys &amp; Tutorials. 22, 4 (2020), 2167–2191.                                                                                             @article{shi2020communication, title = {Communication-efficient edge AI: Algorithms and systems}, author = {Shi, Yuanming and Yang, Kai and Jiang, Tao and Zhang, Jun and Letaief, Khaled B}, journal = {IEEE Communications Surveys \&amp; Tutorials}, volume = {22}, number = {4}, pages = {2167--2191}, year = {2020}, publisher = {IEEE}}                                                        "
    }, {
    "id": 48,
    "url": "https://wanted2.github.io/bibliography/wang2019benchmarking/",
    "title": "Benchmarking tpu, gpu, and cpu platforms for deep learning",
    "body": "  Wang, Y. E. , Wei, G. -Y. and Brooks, D. 2019. Benchmarking tpu, gpu, and cpu platforms for deep learning. arXiv preprint arXiv:1907. 10701. (2019).                                                                                             @article{wang2019benchmarking, title = {Benchmarking tpu, gpu, and cpu platforms for deep learning}, author = {Wang, Yu Emma and Wei, Gu-Yeon and Brooks, David}, journal = {arXiv preprint arXiv:1907. 10701}, year = {2019}}                                                        "
    }, {
    "id": 49,
    "url": "https://wanted2.github.io/bibliography/nguyen2021factors/",
    "title": "Factors influencing home-based telework in Hanoi (Vietnam) during and after the COVID-19 era",
    "body": "  Nguyen, M. H. 2021. Factors influencing home-based telework in Hanoi (Vietnam) during and after the COVID-19 era. Transportation. 48, 6 (2021), 3207–3238.                                                                                             @article{nguyen2021factors, title = {Factors influencing home-based telework in Hanoi (Vietnam) during and after the COVID-19 era}, author = {Nguyen, Minh Hieu}, journal = {Transportation}, volume = {48}, number = {6}, pages = {3207--3238}, year = {2021}, publisher = {Springer}}                                                        "
    }, {
    "id": 50,
    "url": "https://wanted2.github.io/bibliography/nguyen2021perception/",
    "title": "Perception and preference for home-based telework in the covid-19 era: A gender-based analysis in Hanoi, Vietnam",
    "body": "  Nguyen, M. H. and Armoogum, J. 2021. Perception and preference for home-based telework in the covid-19 era: A gender-based analysis in Hanoi, Vietnam. Sustainability. 13, 6 (2021), 3179.                                                                                             @article{nguyen2021perception, title = {Perception and preference for home-based telework in the covid-19 era: A gender-based analysis in Hanoi, Vietnam}, author = {Nguyen, Minh Hieu and Armoogum, Jimmy}, journal = {Sustainability}, volume = {13}, number = {6}, pages = {3179}, year = {2021}, publisher = {Multidisciplinary Digital Publishing Institute}}                                                        "
    }, {
    "id": 51,
    "url": "https://wanted2.github.io/bibliography/nguyen2021factorsaffecting/",
    "title": "Factors affecting the growth of e-shopping over the covid-19 era in hanoi, vietnam",
    "body": "  Nguyen, M. H. , Armoogum, J. and Nguyen Thi, B. 2021. Factors affecting the growth of e-shopping over the covid-19 era in hanoi, vietnam. Sustainability. 13, 16 (2021), 9205.                                                                                             @article{nguyen2021factorsaffecting, title = {Factors affecting the growth of e-shopping over the covid-19 era in hanoi, vietnam}, author = {Nguyen, Minh Hieu and Armoogum, Jimmy and Nguyen Thi, Binh}, journal = {Sustainability}, volume = {13}, number = {16}, pages = {9205}, year = {2021}, publisher = {Multidisciplinary Digital Publishing Institute}}                                                        "
    }, {
    "id": 52,
    "url": "https://wanted2.github.io/bibliography/nguyen2021impact/",
    "title": "The impact of Covid-19 on children’s active travel to school in Vietnam",
    "body": "  Nguyen, M. H. , Pojani, D. , Nguyen, T. C. and Ha, T. T. 2021. The impact of Covid-19 on children’s active travel to school in Vietnam. Journal of Transport Geography. 96, (2021), 103191.                                                                                             @article{nguyen2021impact, title = {The impact of Covid-19 on children's active travel to school in Vietnam}, author = {Nguyen, Minh Hieu and Pojani, Dorina and Nguyen, Thanh Chuong and Ha, Thanh Tung}, journal = {Journal of Transport Geography}, volume = {96}, pages = {103191}, year = {2021}, publisher = {Elsevier}}                                                        "
    }, {
    "id": 53,
    "url": "https://wanted2.github.io/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 54,
    "url": "https://wanted2.github.io/page2/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 55,
    "url": "https://wanted2.github.io/page3/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 56,
    "url": "https://wanted2.github.io/page4/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 57,
    "url": "https://wanted2.github.io/page5/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 58,
    "url": "https://wanted2.github.io/page6/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 59,
    "url": "https://wanted2.github.io/page7/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 60,
    "url": "https://wanted2.github.io/page8/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 61,
    "url": "https://wanted2.github.io/page9/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 62,
    "url": "https://wanted2.github.io/page10/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 63,
    "url": "https://wanted2.github.io/page11/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 64,
    "url": "https://wanted2.github.io/hanoi-remote-work/",
    "title": "ハノイではリモートワークが本当に人気でしょうか？",
    "body": "2022/04/27 - ちらほら海外からお誘いがありました．「どうかリモートワークをしましょうか」という誘いです．なんかコロナ禍で海外では皆がよくリモートワークをしているかと思います．おー，本当にベトナムでもリモートワークが人気だろうなと思っていたが，周りを見るとハノイではそうではない気もしました．業界はそれぞれで，現場にいかないと仕事を行えない土木系の仕事なども多いという原因があります．IT業界でも実際はコロナ禍は主要な原因でリモートワークをしているが，コロナ禍がなくなると，IT業界もみんな本気でオフィスに戻っている気もしています．そういう疑問を持ちながら，解答を探しております．  コロナ禍の後にハノイではリモートワークが生きるか？ リモートワークを促進する要因は何か？支障する要因は何か？結論としては，ハノイではコロナ禍後リモートワークが基本的になくなるだろう． コロナ禍後のリモートワーク本日2022年4月27日，ベトナム医療省が海外から入国者に対してコロナ検査など水際対策が停止する方針を発表しました．また，2022年3月中には，3月から全面で観光や貿易などの活動を再開するよう方針を定められました．今は，もうハノイではコロナ禍後という時代を考えたほうが良いでしょうか？ コロナ禍を理由として，リモートワークを採用した会社は少なくないと思います．特にIT業界では，去年10月ごろでほぼリモートワークだったそうです．しかし，コロナ禍後なので，もうリモートワークをする背景はないでしょう．もうこれから，ハノイでは，さすがに毎日リモートワークをするのがあまりないだろう．全市には完全に否定できないが，どこかにリモートワークをしている方がいるとしても，まわりから見ると，ちょっと変な人だと思われるかもしれません． ハノイでのリモートワーク The first concept of working far from a workplace (i. e. , telecommuting) was introduced in the USA in the 1970s to handle transport-related issues such as traffic congestion and air pollution by reducing commuting between home and the workplace. [1] テレワークはなんとアメリカで1970年代から人気になったそうです．その背景は，通勤時間を短縮したくて，空気汚染の影響も削減したいという動機がありました．コロナ禍がなかった時代で，この2つはリモートワークの主な動機でした． ハノイではね，空気もちょっとあまり良くないし，渋滞も多いです．しかし，それでも，コロナ禍前に，皆がきちんと出社していました．ほぼの会社は勤怠管理システムを導入しています．指紋認証や顔認証などで勤怠登録する場合もありました．  Regarding the perception of HBT, while the fear of COVID-19 was a strong positive factor, difficulties in focusing on work and accessing data were negative factors. [2] そうですね．ハノイでは，なぜコロナ禍でリモートワークをしているかというと，コロナ禍は怖いからだけの理由です．それ以外，リモートワークを促進する強い理由はないだろう．主な支障は，  仕事に集中できない．うん，まあ，家にいるから家族もいるし，集中しにくいですね． 仕事に必要なデータと機器を入手できない．そうですね．アクセス制限もあるので，多くの場合，出社しないといけないですね．やはり，仕事を考えると，コロナ禍がなくなると出社しましょうよ！ 他にも，コロナ禍中で子供の通学の動機についての研究とオンラインショッピング活動におけるコロナ禍の影響についての研究[3, 4]もあります． 結論もうベトナム政府が全面で活動を再開する方針も出したし，コロナ禍はなくなったため，ハノイにいる場合，出社することは避けられないと思いますね．ハノイなら，出社しましょう！ 参考文献Nguyen, M. H. and Armoogum, J. 2021. Perception and preference for home-based telework in the covid-19 era: A gender-based analysis in Hanoi, Vietnam. Sustainability. 13, 6 (2021), 3179. DetailsNguyen, M. H. 2021. Factors influencing home-based telework in Hanoi (Vietnam) during and after the COVID-19 era. Transportation. 48, 6 (2021), 3207–3238. DetailsNguyen, M. H. , Armoogum, J. and Nguyen Thi, B. 2021. Factors affecting the growth of e-shopping over the covid-19 era in hanoi, vietnam. Sustainability. 13, 16 (2021), 9205. DetailsNguyen, M. H. , Pojani, D. , Nguyen, T. C. and Ha, T. T. 2021. The impact of Covid-19 on children’s active travel to school in Vietnam. Journal of Transport Geography. 96, (2021), 103191. Details"
    }, {
    "id": 65,
    "url": "https://wanted2.github.io/edge-cloud-tpu/",
    "title": "Edge-Cloud architectures & TPU resources",
    "body": "2022/03/05 - Edge-Cloud (EC) architectures is an emerging computing paradigm recently, which takes the AI computation to the edge devices and only aggregates processed important data to the cloud. Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google’s own TensorFlow software. Google started selling the Cloud TPUv3 in 2018 for third-party users. Their edge TPU version (Coral)1 was started in the same year. Both cloud and edge TPU support Tensorflow and its Lite version but do not support other deep learning frameworks like PyTorch or MXNet. TPU is useful for EC architectures. Edge computingEdge computing is a new paradigm of distributed computing on resource-constrained IoT devices, such as sensors and actuators, which has become ubiquitous recently [1, 2].  More than 25 billion IoT devices all over the world have been used by 2020. One crucial need is to aggregate the information from these devices and make decisions from the data. Thus, a machine learning (ML) system is used to aggregate such data. But the IoT devices have only low resources, which makes almost all on-device ML systems is impossible. A typical solution for this problem is to offload them into an external data processing system like cloud computing. It worsens latency, increases communication costs, and introduces additional privacy concerns. Edge servers. (Source: Murshed et al. , 2021) The solution requires allocation of a new edge server near the IoT devices, which aggregates local information and makes quick decisions at the local edge (distributed inference). Only processed and meaningful information, which is advantageous to the global model, is sent to cloud computing servers to update global model parameters (distributed training). This decentralized architecture has many advantages:  Reducing cost: The volume of data needed to be transferred to a central computing location is reduced because some of it is processed byedge-devices.  Improving latency: The physical proximity of edge-devices to the data sources makes it possible to achieve lower latency which improves real-timedata processing performance.  Better privacy: For the cases where data must be processed remotely, edge devices can be used to discard personally identifiable information(PII) prior to data transfer, thus enhancing user privacy and security.  Better failure handling: Decentralization can make systems more robust by providing transient services during network failures or cyberattacks.  But everything comes with a cost. And edge computing is no exception!While the applications range from real-time video analytics, image recognition, automatic speech recognition, user privacy, fraud detection, creating new datasets, autonomous vehicles, smart home/cities, human safety to augmented reality, the power to run has many problems:  How to adapt existing ML algorithms to edge devices?: For this problem, we need to consider more distributed-computing specific algorithms, care about hardware constraints, and build lighter/faster model architectures (which also requires pruning techniques and compression).  How to improve latency of distributed training and inference systems while preserving privacy? What are the common hardware devices used to enable edge intelligence? What is the nature of the emerging software ecosystem that supports this new end-edge-cloud architecture for real-time intelligent systems?. For the third question, we will consider a choice in the art: the Tensor Processing Unit (TPU) along with other options like Central Processing Unit (CPU) and Graphical Processing Unit (GPU). It has both cloud and edge versions that make it possible for new end-edge-cloud architecture. CPU, GPU, and TPU Central Processing Unit (CPU) is the main processing unit in a computer. It takes inputs, referring to data in memory units, and produces outputs in every task. It takes control of the computation and does arithmetic/logic computations. How CPU works (Source: Tinhte. vn) CPU in the von-Neumann architectures stores computation results in L1 cache or register due to the fact that the next computation is unknown (thus, the current results must be stored somewhere for future computing). However, such L1 cache and registers are limited in their memory capacity, and sometimes, cache misses happen, which we call as von-Neumann bottleneck. Because there are many matrix computations (multiplication and addition) in deep learning, bottleneck problems arise more and more.  Graphical Processing Unit (GPU) can compute more than one arithmetic logic at a time, then it has higher parallelism than CPU (which is a sequential processing unit). While a CPU only has a few Arithmetic Logic Unit (ALU), a GPU can have some thousands of ALUs, making GPU parallel computing is fast, especially for arithmetic computations. GPU maybe is not good for non-arithmetic computations such as Office tasks, but in the case of deep learning, it is a dominant choice by now. However, although GPU is faster for deep learning, it does not solve the bottleneck problem!That’s why Google had to design TPU! How GPU works (Source: Tinhte. vn) Tensor Processing Unit (TPU) is the neural network hardware designed by Google. It means it is good ONLY for neural network computing, not for all. It cannot do Office tasks, control rockets, or monitor bank transactions. It can only do arithmetic computations! How TPU works (Source: Tinhte. vn) In essence, TPU only access memory once. Then for each multiplication, the result is passed to the next multiplication when performing the additions at the same time. Therefore, no more memory access is needed. This is how TPU solved the von-Neumann bottleneck! Source: Wang et al. , 2019 In a recent benchmark [3], TPU showed that it is a better choice than GPU for large CNNs, suggesting that TPU is highly-optimized for CNNs. While TPU is a better choice for Recurrent Neural Networks (RNNs), it is not as flexible as GPU for embedding computations. The smallest gain perhaps is when TPU does fully-connected computations (FC). PricingNow, we already understand that TPU is fast and has been designed for neural nets. We may want to add these resources to our project, but the question here is how does it cost? Google somewhat offers two different choices for computing: a cloud version2 and an edge version3. Cloud TPU: You have two choices: you can rent a single TPU or a cluster of TPUs (TPU Pod)2. Unfortunately, TPU is only available in three zones:  Iowa (us-central1); Netherlands (europe-west4); and Taiwan (asia-east1). And Taiwan zone does not have Pods, while the Iowa zone does not have TPUv3 Pods! For a single GPU device with 8 TPUv3 cores, you must pay 8USD/hour for on-demand options or 2. 4USD/hour for spot options. Pods with 32 TPUv3 cores require a 1-year or 3-year commitment with an Evaluation price is 32USD/hour. If you afford a 3-year plan, then the monthly pricing is 10,512USD/month. The total price for annual options in the Netherlands is as follows: Edge TPU: Google also offered an Edge TPU version with up to 4 TOPS. Edge TPU has been a new device for edge recently [1, 4]. If you already knew NVIDIA Jetson Nano4, then this is the same version for TPU. Source: Google Coral In Vietnam, I can buy a 1GB RAM DevBoard with a budget that approximates five million bucks (tax included)5. Google Coral announced the price is 129. 99USD/board. It is equipped with 4TOPS and only favors Tensorflow Lite models. ConslusionEdge-Cloud architectures will be the next generation of computing platforms. TPUs are good for neural network computations and then are good for ML as well. We need to allocate the budget for our project first, and it is good to know both cloud and edge versions of TPU. They differ about 1100x in terms of annual cost (Edge TPU 129. 99USD/lifetime vs. 132,000USD?year Cloud Pod TPUv2)! ReferencesMurshed, M. G. S. , Murphy, C. , Hou, D. , Khan, N. , Ananthanarayanan, G. and Hussain, F. 2021. Machine learning at the network edge: A survey. ACM Computing Surveys (CSUR). 54, 8 (2021), 1–37. DetailsShi, Y. , Yang, K. , Jiang, T. , Zhang, J. and Letaief, K. B. 2020. Communication-efficient edge AI: Algorithms and systems. IEEE Communications Surveys &amp; Tutorials. 22, 4 (2020), 2167–2191. DetailsWang, Y. E. , Wei, G. -Y. and Brooks, D. 2019. Benchmarking tpu, gpu, and cpu platforms for deep learning. arXiv preprint arXiv:1907. 10701. (2019). DetailsCass, S. 2019. Taking AI to the edge: Google’s TPU now comes in a maker-friendly package. IEEE Spectrum. 56, 5 (2019), 16–17. Details      https://cloud. google. com/edge-tpu/ &#8617;        https://cloud. google. com/tpu/pricing &#8617; &#8617;2        https://coral. ai/products/ &#8617;        NVIDIA Jetson Nano &#8617;        Coral DevBoard Edge TPU SoM at PiVietnam &#8617;    "
    }, {
    "id": 66,
    "url": "https://wanted2.github.io/seq2seq-cv/",
    "title": "Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices",
    "body": "2022/02/12 - Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự án cần seq2seq. Vision TransformersViT: Về nguyên lý chung của seq2seq thì chúng ta có hai bài viết trước nói khá nhiều rồi12 nên các bạn tham khảo nhé. Nhà cũng đang có cái “eo hẹp” là chỉ được tối đa 7 citations và 9 phút đọc thôi nên các bạn cần thì đọc lại hai bài viết12 để xem thêm kiến thức về attention, Transformer, …Vision Transformer (ViT, [1]) đưa khái niệm seq2seq vào vision. Nếu các bạn đã quen seq2seq thì hiểu ngay là ta cần chuyển hình ảnh thành 1 chuỗi: Source: Google Để làm được việc này, các tác giả đề xuất:  Image patches: Hình ảnh được chia ra thành patches và đánh số thự tự để input vào transformer.  Position embedding: thứ tự chỉ đơn giản là chuỗi 1D.  MLP với GELU activation: MLP sử dụng GELU để kích hoạt. Model không chứa CNN, có tối thiểu 12 tầng, với kích cỡ hidden size từ 768, ngoài ra có 12-16 heads.  Large-scale pre-training and fine-tuning: Một điểm đáng chú ý khác là pre-training dạng supervised trên một dataset 300 triệu ảnh tạo ra 1 model rất mạnh. Các tác giả báo cáo cải tiến trên khá nhiều bộ dữ liệu lớn, và ngoài ra cả con số TPUv3-core-days = số lượng TPUv3 cores x sô lượng ngày train. Cái con số TPUv3-core-days thì cứ mỗi thí nghiệm là vài ngàn tới vài chục ngàn, mà mỗi core thì cứ 10$/ngày thì các bạn cứ nhẩm tính xem budget của hội con nhà giàu này đầu tư vào nó lớn cỡ nào đấy. DETR: DETR [2, 3] tiếp tục ứng dụng transformer vào Object Detection và Segmentation. Source: [2]  Backbone: DETR dùng CNN quen thuộc như resnet-50 hoặc 101.  Encoder: DETR dùng \(1\times 1\) convolution để dimention reduction các feature maps rồi input vào. Position embedding là cố định vì transformer là không phụ thuộc vào permutation.  Decoder: thay vì decode từng object query, thì DETR decode song song cùng lúc tất cả các queries.  Sau khi decode thì dùng FFN để predict vị trí và class. Để tính hàm loss thì dùng thuật toán Hungarian để matching. Về mặt giá cả thì DETR tốn 300 epochs để hội tụ trên COCO, nên về sau gần đây có khá nhiều nghiên cứu để giảm giá thành hộ tụ (chỉ cần 50 epochs thôi chả hạn). YOLOX: So với phiên bản cũ [4] thì YOLOX [5] ứng dụng khá nhiều kỹ thuật mới như decoupled heads, strong augmentation (Moáic và Mixup), anchor-free, pulti-positives, và SimOTA. Backbone thì ngoài Darknet ra cũng dùng thêm những backbone nhỏ hơn như Tiny. Dưới đây là kết quả inference của model YOLOX-Tiny: Xây dựng tài nguyên GPUGiá thành khi train model State-of-the-art (SOTA): Nhìn chung là nếu chỉ hình ảnh với bộ dữ liệu nhỏ nhỏ như COCO [6] tầm trăm ngàn ảnh thì có bảng giá dưới đây: chúng ta lấy ví dụ từ báo cáo của 1 state-of-the-art thì họ dùng 8 cái V100, train tầm 6 ngày liên tục (\(6\times 24\) giờ) thì tổng tiền cho một lượt trên tầm ngàn Mỹ kim cho 6 ngày, 1 tháng cứ tầm 5 ngàn Mỹ kim. Mà các bạn cũng nhớ giá này là giá Spot tức là có thể bị interrupt giữa chừng nên mới rẻ thế. Chứ nếu bạn mà chọn on-demand thì có mà gấp 10 lần.  Nhưng ở trên mới chỉ là giá bộ COCO có hơn 100k ảnh nhé. Bộ Open Images [7] với 1. 7 triệu ảnh thì còn máu nữa. Search trên Kaggle mà có đồng chí chịu khó bỏ tiền ra ngồi train và báo cáo kết quả cho anh biết (xin cám ơn đồng chí): Kaggle Open Images 2019 challenge 6th place solution. Thì kết quả là đồng chí ấy báo cáo:  train 8 models trên V100 (chắc lại EC2 P3 thôi thì mình cứ dùng p3. xlarge để làm phân tích giá nhé) rồi ensemble.  mỗi model train mất 18-36 ngày (tùy model). Thì đồng chí này train 8 GPUs khác nhau.  sau khi train xong các model thì mất thêm 1 ngày nữa để inference và 1 ngày nữa để ensemble (dùng NMS).  Vậy tổng thể đã tiêu tốn \(36\times 8+1\times 8+1=297\) ngày train, tức là \(297\times 24=7128\) giờ train. p3. xlarge thì giá mềm nhất là Spot cũng tầm $0. 918/h. Tức là để train được accuracy tầm 60% đã mất \(7128\times 0. 918\) tức là tầm 6543 Mỹ kim và hơn tháng ngồi monitor màn hình train. Xây dựng hệ thống 8~16 GPU: Nhìn chung thì theo dòng lịch sử có 3 loại NVIDIA GPU dành cho cloud khá thông dụng như sau (tôi không nói tới hai dòng GTX và RTX nhé):  NVIDIA V100 hay Volta: nói đến dòng này chúng ta có những sự lựa chọn chủ yếu liên quan tới V100 Tensor Core mà đại diện cho thuê là p3. 16xlarge và p3dn. 24xlarge.  Với băng thông mạng của phiên bản P3. 16xlarge cao hơn tới 4 lần, phiên bản P3dn. 24xlarge của Amazon EC2 là sự bổ sung mới nhất cho dòng phiên bản P3, được tối ưu hóa cho machine learning phân tán và các ứng dụng HPC. Các phiên bản này cung cấp thông lượng kết nối mạng lên tới 100 Gbps, 96 vCPU Intel® Xeon® Có thể mở rộng (Skylake) tùy chỉnh, 8 GPU NVIDIA® V100 Tensor Core với 32 GB bộ nhớ mỗi GPU và 1,8 TB ổ lưu trữ SSD cục bộ chuẩn NVMe. Các phiên bản P3dn. 24xlarge cũng hỗ trợ Elastic Fabric Adapter (EFA). Giao diện này tăng tốc các ứng dụng machine learning phân tán sử dụng Thư viện giao tiếp chung NVIDIA (NCCL). EFA có thể mở rộng quy mô lên đến hàng nghìn GPU, cải thiện đáng kể thông lượng và khả năng mở rộng của các mô hình huấn luyện deep learning, từ đó cho kết quả nhanh hơn. Source: Amazon Web Service  NVIDIA T4 hay Turing: với AWS EC2 thì bạn có thể thuê g4dn. metal.  NVIDIA A100 hay Ampere: Với AWS EC2 thì có thể thuê p4d. 24xlarge, với Azure HPC thì có thể thuê Standard_ND96amsr_A100_v4. GCP thì có a2-highgpu-8g hoặc bản 16 GPU là a2-highgpu-16g. Thì về mặt spec Ampere là khỏe nhất nếu nói về TFLOPS. Dưới đây là bảng giá thành của NVIDIA 8x A100 Tensor Core. Trong bảng này có 2 cột mà các bạn nên để ý là giá thành thuê theo giờ (Hourly cost) và tỷ lệ GFLOPS/USD (đáng giá thế nào). Giả định chung là hệ thống được xây dựng tối thiểu 4x GPU và được dùng ít nhất 24 tháng, mỗi tháng dùng 22 ngày (T7/CN nghỉ ngơi).  Nói chung tự build thì các bạn có thể tham khảo cấu hình của DGX-13, DGX-24, và DGX-A1005 để mua các bộ phận về tự ráp thì sẽ tiết kiệm công lắp ráp, nhưng nhìn chung tôi nghĩ cũng phải 50 ngàn Mỹ Kim. Các cloud solutions: Trong trường hợp bạn có bài toán train dữ liệu mà mất hàng tháng trời train với GTX/RTX thì bạn sẽ nghĩ phải thuê GPUs trên data center (8x-16x GPU). Thì ngoài AWS/Azure/GCP là khá cùng rank nên bảng giá không chênh lệch nhau mấy, bạn có thể tham khảo thêm các trang cho thuê GPU bên ngoài để tìm được chỗ thuê hợp lý hơn. Như kết quả tìm kiếm của AIFI thì hiện tại có trang vast. ai cung cấp khá nhiều sự lựa chọn cho thuê ở mức giá thấp hơn 5 USD/hour. Còn lời giải nào khác?: Nhìn chung tự build thì có hai khả năng:  Mua đồ sẵn như DGX345 thì các bạn cứ chuẩn bị 100k Mỹ kim trở lên.  Mua bộ phận về tự ráp thì các bác tham khảo cấu hình của DGX rồi độ lại tùy theo nhu cầu. Tuy nhiên, chắc chỉ giảm được tiền công, và tối ưu một chút kiểu DGX dùng nhiều RAM thì mình giảm RAM xuống. Nói chung chắc cũng phải 50K Mỹ Kim. Về cá nhân, tôi thiên về thuê!Nếu tự build thì mua mấy cái RTX/GTX dòng Ti là ổn rồi. Tuy nhiên nếu bài toán lớn thì bạn bắt buộc phải dùng data center GPU thì lúc ấy phải có TIỀN!  Vấn đề của đi thuê là tiền tính theo giờ nên bạn cần phải ước lượng được số giờ sử dụng. Nếu tầm trên 200h/tháng, tôi nghĩ nên thuê theo năm hoặc 3 năm. Spot price thì cũng tàm tạm thôi, vì mất công chờ với nó ngắt điện (interupt) mình cũng phải chịu ấy, nên là rẻ nhưng lại mất thời gian chờ và bị ngắt. Mà vấn đề với Spot là nó không có luôn ấy (phải chờ đến khi cái server ấy nó open mình mới được dùng). Kết luận Hơi buồn nhưng đầu tiên vẫn là . . . tiền đâu Như vậy chúng ta đã điểm qua một số SOTAs của seq2seq cho Vision và nhìn chung các seq2seq vẫn đang nắm vị trí số 1. Tuy vậy, vấn đề lớn khi “đua đòi” vào mảng này thì vẫn là tài nguyên thôi. Nếu chuẩn bị được budget và plan nghiên cứu nghiêm chỉnh (mà đầu tiên là tiền đâu) thì về mặt nghiệp vụ PM tôi nghĩ không nên triển khai làm gì mất time anh em. Ít nhất là cần vốn 200k Mỹ Kim thì cũng phải có tầm 100k trong túi hãy nghĩ! Tài liệu tham khảoDosovitskiy, A. , Beyer, L. , Kolesnikov, A. , Weissenborn, D. , Zhai, X. , Unterthiner, T. , Dehghani, M. , Minderer, M. , Heigold, G. , Gelly, S. , Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010. 11929. (2020). DetailsCarion, N. , Massa, F. , Synnaeve, G. , Usunier, N. , Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. European conference on computer vision (2020), 213–229. DetailsGao, P. , Zheng, M. , Wang, X. , Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. Proceedings of the IEEE/CVF International Conference on Computer Vision (2021), 3621–3630. DetailsRedmon, J. and Farhadi, A. 2018. Yolov3: An incremental improvement. arXiv preprint arXiv:1804. 02767. (2018). DetailsGe, Z. , Liu, S. , Wang, F. , Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107. 08430. (2021). DetailsLin, T. -Y. , Maire, M. , Belongie, S. , Hays, J. , Perona, P. , Ramanan, D. , Dollár, P. and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. European conference on computer vision (2014), 740–755. DetailsKuznetsova, A. , Rom, H. , Alldrin, N. , Uijlings, J. , Krasin, I. , Pont-Tuset, J. , Kamali, S. , Popov, S. , Malloci, M. , Kolesnikov, A. , Duerig, T. and Ferrari, V. 2020. The open images dataset v4. International Journal of Computer Vision. 128, 7 (2020), 1956–1981. Details      https://wanted2. github. io/seq2seq/ &#8617; &#8617;2 &#8617;3        https://wanted2. github. io/speech/ &#8617; &#8617;2 &#8617;3        https://www. nvidia. com/en-us/data-center/dgx-1/ &#8617; &#8617;2        https://www. nvidia. com/en-us/data-center/dgx-2/ &#8617; &#8617;2        https://www. nvidia. com/en-us/data-center/dgx-a100/ &#8617; &#8617;2    "
    }, {
    "id": 67,
    "url": "https://wanted2.github.io/ml-ids/",
    "title": "Machine Learning for Network Intrusion Detection: From Local to Production",
    "body": "2022/01/29 - Network Intrusion Detection System  Network intrusion detection system (NIDS) is an independent platform that examines network traffic patterns to identify intrusions for an entire network. It needs to be placed at a choke point where all traffic traverses. A good location for this is in the DMZ. An IDS is reactive in nature: it only monitors and sends alerts to a group of specific people like administrators. The above figure shows a common NIDS architecture, where a DMZ is placed in between external firewall and internal firewall (to an internal network). Here, in the DMZ, a NIDS can be set up to monitor the traffic of the whole corporation and identify the anomalies. A NIDS can have the following architecture:  A streaming engine which ingests packet stream into the NIDS A package decoder which turns packet content into visibility A detection engine which identify intrusions A policy engine which decide and suggest what to do with an intrusion Finally, the intrusion details are collected and logged. Alerts will be sent to admins and optionally, scripted actions can be performed in according to policies. Two typical examples of NIDS are Snort IDS and Bro IDS. A nicer example that can be integrated into network monitoring can be Zabbix’s Problem Detection Engine. Machine Learning at scale: some solutions in AWSOverview: The theory about NIDS perhaps is a huge bundle of knowledge!Corporations have set up their own NIDS (in most cases in DMZ) for years. We will not talk about such solutions anymore. The most interesting part is in the cloud platforms like AWS.  While companies are moving their resources to the cloud, where is the NIDS? In the Shared Responsibility Model (SRM). The responsibility of protecting cloud resources like computing instances (EC2) and networking is of AWS. Users have the responsibility to tune the best configurations of firewall, instances, load balancers, and other resources in AWS. So a platform IDS is already managed at AWS, and the at the users (application developers), the remaining task is to implement a best Network/HIDS (endpoint protection). There are several choices for a HIDS in the AWS Marketplace like TrendMicro’s Deep Security. For a custom NIDS, users can implement a Transit DMZ Gateway. An examplar implementation can be as follows: Challenges in NIDS operations: A standard DMZ with NIDS can be implemented at the cloud or data center. However, the hurdle only comes when we operate our solution: this is the hard part!  When it comes to operations (運用保守), it often require an enormous amount of manual work! We need a large number of low-paid workers who will sit in front of the monitoring screen and then manually mark each access as legal or not. It is the real-world hurdle!  However, the technical issues start when we want automation: how to reduce false alarms and misses? Good automated solutions will reduce manual work a lot. But it is not straightforward!Rule-based systems can have many misses or false alarms.  A weak rule misses many, but a strong rule alerts too much! (So both strong and weak ones are useless!) Machine Learning at localYou have a dataset and perform some analytics at your local or edge PC. You don’t use any server or cloud solution at all.  You should ask us why do you need to care about these works while cloud platforms already prepare so many ready-to-use solutions for you? Agree! Machine learning engineers behind the platforms already do such works (model engineering). Then when you do these works, that means you are:  A student who is learning ML at an educational place (like university); A researcher (or engineer) who perform a project for NGO, government, or a big company (who is building a platform solution); and The worst luck: you’re only an ML enthusiast who is looking into ML when you have free time. Not so many people do model engineering on their own laptop (or desktop), so in my experience, they fall into one of these three categories. For the second category, people in that category is ML engineer/scientist who will make the model for thousands to million application developers over the world (who uses the platforms). Such category is quite a few, and to be in, you need qualification!The most common cases are in the first. The third category is possible but is rare and complex: while the first and second category have their own goals with ML (for studying and for works), the third category has no particular purpose. They only do it in their free time and for fun (like doing a hobby)!The first and second ones will have outcomes (successes and non-successes), but the third one is only for fun!  No one can ban the hobby of a man, and we only do the hobby: we start when we want and stop when we don’t like it anymore!That’s why I call it (the third category) the worst luck! Actually, people in the third category already have another job (but that job does not relate to ML or even AI). They do ML as hobbies but don’t complete anything (because ML is not their business, even more, ML does not give them bread and butter)!  It is said, but in practice, if a thing doesn’t give any benefit, it won’t be done properly! Nevertheless, whatever your category is, when you do these things in your local computing environment, ML matters with you in some ways!We will see what a local IDS model would look like. And, in a synthetic way! A Kaggle synthetic dataset: We start with a synthetic dataset from Kaggle.  The dataset to be audited was provided that consists of a wide variety of intrusions simulated in a military network environment. It created an environment to acquire raw TCP/IP dump data for a network by simulating a typical US Air Force LAN. The LAN was focused like a real environment and blasted with multiple attacks. A connection is a sequence of TCP packets starting and ending at some time duration between which data flows to and from a source IP address to a target IP address under some well-defined protocol. Also, each connection is labeled as either normal or as an attack with exactly one specific attack type. Each connection record consists of about 100 bytes.  For each TCP/IP connection, 41 quantitative and qualitative features are obtained from normal and attack data (3 qualitative and 38 quantitative features). The class variable has two categories:    Normal  Anomalous Explorative analysis: We notice that not all features are useful. Some features like is_Host_login have only a constant value. Our dataset is big enough (25K entries) and contains both categorical and numerical features: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748RangeIndex: 25192 entries, 0 to 25191Data columns (total 42 columns): #  Column            Non-Null Count Dtype --- ------            -------------- -----  0  duration           25192 non-null int64  1  protocol_type        25192 non-null object 2  service           25192 non-null object 3  flag             25192 non-null object 4  src_bytes          25192 non-null int64  5  dst_bytes          25192 non-null int64  6  land             25192 non-null int64  7  wrong_fragment        25192 non-null int64  8  urgent            25192 non-null int64  9  hot             25192 non-null int64  10 num_failed_logins      25192 non-null int64  11 logged_in          25192 non-null int64  12 num_compromised       25192 non-null int64  13 root_shell          25192 non-null int64  14 su_attempted         25192 non-null int64  15 num_root           25192 non-null int64  16 num_file_creations      25192 non-null int64  17 num_shells          25192 non-null int64  18 num_access_files       25192 non-null int64  19 num_outbound_cmds      25192 non-null int64  20 is_Host_login        25192 non-null int64  21 is_guest_login        25192 non-null int64  22 count            25192 non-null int64  23 srv_count          25192 non-null int64  24 serror_rate         25192 non-null float64 25 srv_serror_rate       25192 non-null float64 26 rerror_rate         25192 non-null float64 27 srv_rerror_rate       25192 non-null float64 28 same_srv_rate        25192 non-null float64 29 diff_srv_rate        25192 non-null float64 30 srv_diff_Host_rate      25192 non-null float64 31 dst_Host_count        25192 non-null int64  32 dst_Host_srv_count      25192 non-null int64  33 dst_Host_same_srv_rate    25192 non-null float64 34 dst_Host_diff_srv_rate    25192 non-null float64 35 dst_Host_same_src_port_rate 25192 non-null float64 36 dst_Host_srv_diff_Host_rate 25192 non-null float64 37 dst_Host_serror_rate     25192 non-null float64 38 dst_Host_srv_serror_rate   25192 non-null float64 39 dst_Host_rerror_rate     25192 non-null float64 40 dst_Host_srv_rerror_rate   25192 non-null float64 41 class            25192 non-null object dtypes: float64(15), int64(23), object(4)memory usage: 8. 1+ MBAnyway, this is a synthetic dataset with some characteristics based on simulation. However, it is close to real-world examples enough. Next, we will try some machine learning models for predicting anomalies. With a visualization technique, like T-SNE, we have a diagram of data distribution. A green point is a normal data point, and a red one is an anomaly. We can see that it would be hard to draw a linear boundary between normal points and anomalies. Play with some machine learners: In this section, we will try two different ML models with two different training/inference strategies:  Generative model with a reconstruction strategy: the Auto-Encoder12345678910# creating the autoencoder modeli = Input(shape=(X_train_normal. shape[1],))encoder = Dense(64, activation='relu')(i)encoder = Dense(32, activation='relu')(encoder)code = Dense(16, activation='relu')(encoder)decoder = Dense(32, activation='relu')(code)decoder = Dense(64, activation='relu')(decoder)decoder = Dense(X_train_normal. shape[1], activation='sigmoid')(decoder)model = Model(i,decoder) Discriminative model with a classification strategy: the quite classical Random Forest!12from sklearn. ensemble import RandomForestClassifierrf = RandomForestClassifier(n_estimators=100, criterion='gini')Training and validation: We need to transform the data a little bit. It is usual to use MinMaxScaler to perform data transformation. 1234from sklearn. preprocessing import MinMaxScalerscaler = MinMaxScaler()X_train_normal[to_norm] = scaler. fit_transform(X_train_normal[to_norm])X_val_normal[to_norm] = scaler. transform(X_val_normal[to_norm])The Auto-Encoder is trained in a reconstruction manner, i. e. , it learns to reconstruct the input: 1r = model. fit(X_train_normal, X_train_normal, validation_data=(X_val_normal, X_val_normal), epochs=5)But the Random Forest learns to classify data as usual: 1rf. fit(X=X_train, y=y_train_num)Results of Auto-Encoder: Since the prediction of AE relies on how good it can reconstruct the input:  If the reconstruction error is over a threshold, then the reconstruction fails, and the input is anomaly.  Otherwise, the input is normal. Then we must define a threshold to separate anomalies and normal inputs. To find such threshold, we can compute from the training dataset: 1234# reconstructing the train setpred = model. predict(X_train_normal)# finding the mean reconstruction errornp. mean(np. mean(np. power(pred - X_train_normal, 2), axis=1))Since the mean of construction error is 0. 0018520295581492838, we can choose the threshold thres = 0. 0018. Validation in validation set: 1234567X_val[to_norm] = scaler. transform(X_val[to_norm])pred = model. predict(X_val)mse = np. mean(np. power(pred - X_val, 2), axis=1)# classifying the samples based on thresholdpred_class = ['anomaly' if val &gt; thres else 'normal' for val in mse]print(confusion_matrix(y_val, pred_class))print(f Acc. = {accuracy_score(y_val, pred_class)*100}[%], F1 = {f1_score(y_val, pred_class, pos_label='anomaly')*100}[% )Results: 123[[2901  69] [ 563 2765]]Acc. = 89. 96506827564306[%], F1 = 90. 17718371153248[%]~90% of F1 score. Hmm, not so bad! Results of Random Forest: Let’s see the results with Random Forest: 123[[3321  7] [ 14 2956]]Acc. = 99. 66656081295649[%], F1 = 99. 64604753076016[%]99. 6%, it is quite good! Some methods for Machine Learning based IDS: There are many ways to identify anomalous access (i. e. , intrusion) in the network. Rule-based methods tend to find a “good” heuristic that can be generalized to a global policy for all feature space. The same policy can be applied to every input. For example, every connection which lasted for more than 7 minutes are anomalies is a policy to identify intrusions. The problem with this approach is that many false negatives (misses) can be raised. Because the rule is clear, counterfactuals try to make themselves legal (try to connect faster) and overcome the 7-minute rule. So a fixed rule is not enough.  The attacks become more and more advanced, but the rules cannot be changed so fast, making an Achilles' heel in the defense system. Machine Learning-based methods try to define a soft boundary that can be learned and improved over time. The main advantage of the Machine Learning approach is that since the boundary is soft and there is no clear rule, the intruders cannot know the rule exactly (how many minutes should they make for a successful intrusion?). Another important aspect is that since the soft rule is not fixed, it can evolve with the intrusions: the more advanced the intrusion is, the more advanced the defender is. When this sounds ambiguous, but for the security systems, it becomes exactly a common strategy to overcome incidents. The Machine Learning methods can be generative or discriminative, but they must be somewhat non-linear and ambiguous enough to hide details of the system to hackers. ConclusionWe have reviewed several views of NIDS: at a corporation network view, cloud solutions, and machine learning models. On the synthetic examples, we observed that a quite classical model like Random Forest can outperform neural nets. It is not a new thing: we already empirically knew that Random Forest is good at this problem, especially when it is in synthetic environments. Somebody can argue that there is a chance for overfitting with Random Forest: hmm, I don’t think so.  And even if it overfits, this is quite good, because that’s what we want: We want our model to overfit to this dataset.  Anonymous Source code for this article can be found at:  AIFI-INC’s ML-based IDS"
    }, {
    "id": 68,
    "url": "https://wanted2.github.io/pm-sekininsha/",
    "title": "責任者",
    "body": "2022/01/16 - ベトナムに帰ってからベトナムの職場文化になってからはもはや2年間になっております．ベトナム職場でいうと，恥ずかしいけど，楽しい経験もあるし，悲しい経験もありました．仕事の責任者として働いた経験もあり，悲しい時で，部下に怒られて，そろそろ殴られる経験もありました．なぜなら，背景から考えると，文化の違いかなと思います．ベトナム職場では上下関係は社会的に存在するけど，きちんと働かないとね，上下関係なく殴られるそうです．「殴られる」は厳しい言葉ですが，主にいうと，ベトナム職場で下記の三大原則はあります．  原則1：Có làm thì mới có ăn. You must work to be fed. 職あり食ある．  原則2：Có lỗi thì sửa là được. Don’t be panic, just fix bugs, then it’s OK. 問題にダラダラしないで，修正すれば問題なし．（ただ何もしないなら，↑の原則１をみてください）．  原則3：Làm thì làm cho nghiêm chỉnh. You should do the work thoroughly. 徹底的にやりましょう． ベトナム職場の約2年間の経験をまとめて，一度整理したいと思っております． 第一原則：「職」と「食」我々は職場で「仕事」を大事にしております．仕事が順調であれば，食感もよくなるという発想です．仕事をしなければ，食えるもんがなく，大変ですね．ですが，仕事（職）があって，就いたけど何もしないことで組織には不満が引き起こされたことが多いようです． ですので，組織全体で皆は統一して，職務をしてから食べるということを理解したらよかったです．もちろん，海外から来る要員だと，現地の文化がわからず，仕事とは何か現地の人の考え方は最初でわからないこともあります．しかし，時間が経過すると，その意義を理解すれば，原則を守る姿勢を整い，結局「労働」は第一だという考え方に帰着しました．組織全体といえば，作業人だけではなく，管理職でもきちんと働き姿勢を見せればよいと思います． ここで，一番気になることは，この原則にはもう一つの意味の層があります．仕事をやるといっても，何をやっても認められるのではないです．「すごいことをやる」のも間違いです．ある「職」に就く時，自分の役割と権限は決まっております．ある場合，期待もあります．しかし，開発の現場では，ほとんどの場合，役割と権限と責任範囲・仕事内容はすでに決まっております．（※申し訳ないが，それぐらい曖昧に契約してしまえば，ちょっとそれがまずい職になるかもしれません）．ですので，「自分の役割と権限・責任範囲の中に行動し，仕事を仕上げる」ことで職を行っているねといいます．まあ，「職あり食ある」という句です． ※曖昧に定めてしまった職に入った場合，もちろん仕事をしたから食べることがあるけど，ちょっと食べ物は美味しくないかもしれないです．おいしい食をどうやって作るか山ほど研究がありますので，それらをGoogleして参考にすればと思います． 第二原則：チームは障害にだらだらしないで，乗り越えることは大事！チーム運営の中に，一緒に働くので，楽しい時も，悲しい時も一緒に乗り越えています．楽しい時はいいんだけど，悲しい時はどんな時かな？自分の経験では，パニック状態になるときです．多くの場合，急に障害が起きるときとかですね． 確かに，パニック状態のハンドルをうまく取り込めているチームはすごいねとおもいます．パニック状態になった場合，よいチームは取り込むけど，悪いチームは責任追及ゲームを遊びます．なぜ責任追及ゲームは悪いのかというと，緊急対応なのに，問題対応をせずに，ゲームをしているからです．よいチームは問題を見極めて，対処法を第一優先し対応を取り込むのです． ※ちなみに，チームで開発する場合には，運営中で時間がかかってしまう状態があります．それが，パニック状態と，会議状態です．なぜなら，この2つだけは，一人の時間を取るだけではなく，チーム皆の時間を取っているからです．だらだらして，仕事が進まない罠に落ちやすいのです．チームのパフォーマンスを上げたい場合に，責任者として，回避と対応策を計画しなければなりません． ベトナム職場では，なぜこの原則が取り込まれているかというと，チームがどこかにだらだらすると，めっちゃ時間がかかっているから，まずは乗り越えることを第一優先したいからです．人がミスをすることは人間の根性ですので，それよりももっと悪いことは，「なにもしないこと」です．直さないことや修正しないことなどはまた，原則1で処分されると思います．いつもチームを前向きに進行させることができればと思います． 第三原則：自分の仕事へのこだわりも重視！完成したら，もう一度見直そう！2年間ベトナム職場で感じたもう一つの原則です．第一原則と第二原則を相互に働かせているため，「作成」と「修正」を交互に行っています．しかし，これらを交互に働かせると，永遠に修正のループに入る可能性はまだ残っています．ですので，途中でもっと修正してもあまり報われないと感じるときとか，早く止めた方がよいと感じるときとか，もう一つのコントローラーが必要でしょうか．それは第三原則です．必ず，（再）作成と（再）修正が完成したら，もう一度客観的にレビューしましょう．狭い視野で詳細をレビューするだけではなく，広い視野で，この作成と修正は長期的によいか悪いか一回考えるべきというステップがあれば，なおよいです．ここで，責任者として，技術部分だけではなく，スコープ管理とか要望管理とかスケジュール管理とかうまく併せてやる必要があります． 結論これが，おそらく私が2年間で観測したベトナム職場で支配されている三大原則かと思います．これらは，なぜか職場をコントロールして，各PJを進行させることが多いそうです． "
    }, {
    "id": 69,
    "url": "https://wanted2.github.io/rapidapi/",
    "title": "RapidAPI and RapidAPI Hub",
    "body": "2022/01/09 - Image Credit: FinanceFeeds Rakuten launched RapidAPI Marketplace in 2018 as a result of the collaboration between Japan’s Rakuten Inc and San Francisco-based startup RapidAPI. The API marketplace aims to provide software developers in Japan and Asia unified access to more than 8,000 APIs with localized documentation and resources in Japan’s language and English. The API marketplace platform will connect API providers and developers. Developers in Japan and across Asia will be able to find, test, and connect to thousands of APIs for their applications. The marketplace will also allow API providers to connect with the global developer community through personalized API portals.  What is RapidAPI?Let us assume that you have an API that is ready for production. You need to add authentication like API key, OAuth 2, or something else. You need to deploy your API to somewhere that is stable and reliable.  What is the shortest path to achieving your goal? You are an application developer, and you need to manage the records of some data for the app. For example, you need to maintain the list of public holidays in your app. You don’t want to hardcode those things in the code. Note that the public holidays change between countries and sometimes due to the law it will change between years. It is somewhat troublesome to maintain the records in your database as it will make you allocate some effort and human resources there.  What is the most convenient way to maintain such data? In both scenarios, Rakuten RapidAPI Marketplace gives you excellent solutions. Either maintenance of the data (public holidays) or publishing a new API, you can do all of the lifecycles in one platform. For example, when you want to check a day is a holiday or not, you can thus search for a free API like this one and make a request. Because all maintenance is up on the providers, this solution costs you nothing: you don’t need to worry about maintaining the records of holidays data (which shouldn’t be your matter in any way) and focus on your own application logic. Note that the Public Holidays API has low latency (59ms) and is completely free. Another solution is to build an endpoint in your own API like /api/v1/holidays to validate the holidays, but while such a ready-to-use solution is there, why should you waste time and money to build/manage/maintain on your own? RapidAPI helps your API to distribute and monetize. Adding your API to the RapidAPI Hub gets you instant exposure to our growing user base, a search-engine-optimized profile page for your API, as well as features like user management and billing services. RapidAPI also serves functional testings, API monitoring dashboards, and many other premiere features like API authentication. RapidAPI for API VendorsThe workflow between an app developer’s client to a vendor API can be as follows:  An API Key is generated and appended to the request’s header to RapidAPI servers.  RapidAPI authenticate the request (using API Key and optionally a configured authentication method like OAuth 2). Then it modifies the requests header to append X-RapidAPI-* headers.  The vendor API (destination API in the diagram) checks the X-RapidAPI-* headers and authenticates the modified requests.  A response is generated according to the requested information and is then returned to RapidAPI.  RapidAPI modifies the response from vendor servers. It appends Rapid API headers (for example, headers about rate limits) or generates a new response. As you can see, RapidAPI Marketplace acts as a proxy between app servers (client in the diagram) and the vendor API servers. The vendors register their APIs and fine-tune the settings in RapidAPI dashboard. All API endpoints are relative to a base URL, which is added as a “prefix” to all API endpoints. This approach avoids the need to define absolute URLs for endpoints every time and increases API portability by changing the base URL. API vendors can add basic authentication or OAuth 2 to their APIs. RapidAPI supports automatic API provisioning using OpenAPI and custom transformations. RapidAPI has basic plan options so app developers can choose among these options to pay:       API Type   Description         Free APIs   APIs that do not require a credit card or subscription to consume.        Pay Per Use   APIs that don’t have a subscription fee associated with them. A credit card is required as you pay for what you use on the API.        Freemium APIs   Paid APIs that also include a limited free tier. These require a credit card, even for the free plan.        Paid APIs   APIs that require a paid subscription plan and credit card to consume.    Some notes on security: RapidAPI supports secret headers and parameters:  RapidAPI allows you to add secret headers and/or query string parameters to API requests. The RapidAPI proxy adds these secrets to every request but is hidden from the API consumers. Note that even the consumers who make the requests do not know about these secrets. This differs from header and query authentication methods where consumers know all secrets in the requests they make to RapidAPI. Users should configure RapidAPI security features like firewalls, threat protection, schema validation, and request size limit (which returns error code 413). Vendors can set their API to private where only invited users can access. Audit and marketing tools: RapidAPI provides Provider Dashboard where vendors can monitor their API usages. Another nice thing is that as a vendor, you can make your monetization more useful using Marketing API. When you have an API, you should make sure you don’t miss a checklist when publishing your solution: This checklist helps you have a better SEO for your API. API Testing: Testing is quite tedious!RapidAPI helps vendors reduce testing costs with their API testing feature.  If you are already familiar with postman-tool you are ready to go with RapidAPI advanced testing.  RapidAPI for App DevelopersAs an app developer, you can find that RapidAPI Hub now has more than 10,000 APIs. Even you want to develop an OCR app or a Translation app, you can find your API right away. All you need is to register a RapidAPI account, choose your API and then make a payment. Finally, you can connect to your paid API using the API key.  ConclusionIt is worth noting that RapidAPI supports not only REST API but also GraphQL, SOAP, and Kafka APIs. We did not touch RapidAPI for Teams, but it might be useful at the organization level. "
    }, {
    "id": 70,
    "url": "https://wanted2.github.io/year-end/",
    "title": "Chia tay 2021!",
    "body": "2021/12/31 - Đây là bài post thứ 61 của blog AiFi trong năm 2021, cũng là bài viết chia tay 2021, trong tâm thế đón chờ 2022 tươi mới hơn. Theo quan điểm làm việc scrum, thì coi như đây là thời điểm kết thúc 1 chu kỳ, cũng là lúc làm một số việc để nhìn lại một năm đã qua (bao gồm cả GKPT hay Good, Keep, Problem, Try). 2021年中61番目の投稿です．2021年と別れて，2022年を迎える時期の投稿です．一年間を1スプリントとすると，いろいろなことができたと思いますので，スクラムの行事として，レビューとレトロ会をここで開催したいと思います．Good, Keep, Problem, Try も含めてやります． Nhìn lại năm 2021 của blog AiFiNhìn từ thống kê người dùng: Hiện tại AiFi blog sử dụng Google Analytics để track và lấy thống kê người dùng. Các sự kiện như view, scroll, referal, … được báo cáo theo phút lên server của Google. Đầu tiên là thống kê về người dùng và nguồn giới thiệu. Trong năm 2021, blog tuy mới ra mắt và còn nhiều khó khăn vất vả nhưng đã thu hút được 552 user mới từ khắp nơi trên thế giới. 552 người dùng này đã ghi lại 7309 sự kiện. Một con số đáng khích lệ với blog mới 1 năm tuổi đời.  Một điểm đáng chú ý là dù facebook. com là nơi tác giả hay chia sẻ bài viết, nhưng user lại phần lớn đến từ 2 nguồn: google và direct. Về yếu tố địa lý thì đa phần người dùng đến từ Việt Nam, Mỹ và Nhật Bản. Các nước khác vẫn chưa đóng tỷ trọng lớn trong cơ cấu người dùng của AiFi.  Tỷ lệ người dùng của AiFi gia tăng tính từ tháng 7. Trong năm 2021, số lượng sự kiện user engagement là 1852, và số page view là 2622 lượt. Ngoài ra, 3 bài viết đạt số lượng truy cập cao nhất (không tính trang chủ) là:  mOCR: A real-time application of OCR with Google MLKit and Android CameraX Adobe Creative Cloud: An All-in-One Platform for Creators Implementing a complex system in AWS Lambda: Should or shouldn’t?Sự “vùng lên” của bài viết Adobe Creative Cloud: An All-in-One Platform for Creators thật thú vị vì bài viết được xuất bản trên blog AiFi vào tháng cuối năm nhưng lại đứng thứ nhì.  Về hệ điều hành, trình duyệt và ngôn ngữ đầu vẫn là Windows, Chrome và English. Theo sau lần lượt là MacOS, Safari và tiếng Nhật.  Nhìn từ kết quả tìm kiếm: Kết quả tìm kiếm về “AiFi Caineng” trên google. com và Bing Search trong ngày 31 tháng 12 năm 2021 như sau: Kết quả tìm kiếm từ khóa “aifi” và thậm chí “aifi caineng” quả là hơi nghèo nàn và dễ bị lẫn vào các từ khóa tìm kiếm khác như “wifi” chẳng hạn. Đây cũng là 1 thiếu sót do blog mới chỉ 1 năm, và tác giả vẫn đang bận bịu công việc chính cuả tác giả. Tuy nhiên, từ năm 2022, ở mức độ nhất định việc nâng rank trong các cỗ máy tìm kiếm từ khóa sẽ được tối ưu hóa nhằm đưa tri thức của AiFi đến với đông đảo bạn đọc và nâng cao chất lượng phục vụ. Good, Keep, Problem, TryViệc chạy sprint kéo dài 1 năm quả là hơi lạ, tuy nhiên là cũng dễ hiểu vì viết blog chỉ là việc phụ làm trong thời gian rảnh rỗi của tác giả.       Good   Keep   Problem   Try         Đã tạo được và thu hút lượng người dùng nhất định.    Duy trì tần suất chia sẻ bài viết.    Thứ hạng trên search engine chưa cao.    Tối ưu hóa SEO                   Tối ưu hóa từ khóa                   Tối ưu thẻ HTML, …               Chưa tạo ra thu nhập từ blog   Xem xét đưa vào và tối ưu hóa quảng cáo.        Các nguồn Google và Facebook đã đem đến lượng người dùng nhất định.    Tiếp tục duy trì quảng bá trên Google và Facebook.    Nguồn Facebook chưa đem lại nhiều người dùng mới.    Tối ưu hóa quảng bá blog trên Facebook.                Một số nguồn cấp khác như Twitter và LinkedIn vẫn chưa đem lại nhiều người dùng.    Lên chiến lược quảng bá trên các nền tảng này.    Tổng kếtKết thúc Sprint 2021, hướng tới Sprint 2022, blog AiFi xin cám ơn đông đảo bạn đọc, đặc biệt là 552 người dùng đã có, vì sự quan tâm và thịnh tình trong năm qua. Trong năm 2022, AiFi sẽ tiếp tục cập nhật và mong muốn lan tỏa tri thức cho anh em, với phương châm, troll trước học sau. "
    }, {
    "id": 71,
    "url": "https://wanted2.github.io/subcription-optimize/",
    "title": "Nên chọn gói subscription Adobe/Games nào cho nhóm phát triển và nghiên cứu?",
    "body": "2021/12/25 - Ở các quốc gia tôn trọng và thực thi nghiêm chỉnh luật bản quyền, thì vị trí leader có một “đặc sản” khá thú vị là kỹ năng quản lý license, hay là tối ưu hóa license (サブスクリプションの最適化, optimize subscriptions). Giải thích thì cũng mất thời gian, tôi xin lấy ví dụ bài toán như sau:  Giả sử bạn là chủ nhiệm (hoặc người đứng đầu) một phòng nghiên cứu và phát triển có lượng nhân sự ổn định là 40 người. Hàng năm có 20 người tốt nghiệp và đi chỗ khác, và bạn sẽ lại đón khoảng 20 người mới vào để phân đề tài hướng dẫn. Và vì vậy nhân sự luôn ổn định tầm 40 người. Thế mạnh của phòng là xử lý đồ họa, thiết kế web, và mấy cái game ghiếc, truyền hình (TV). Những gì trong thế mạnh thì là cái bạn sẽ hướng dẫn (chứ không có chuyện là nhận vào rồi cho làm mấy cái text tiếc vớ vẩn hay mấy cái random là không có đâu).  Vậy bây giờ là phát sinh ra trong hướng nghiên cứu của phòng bạn, sẽ phải mua máy móc như màn hình cỡ lớn, màn hình đặc dụng cho ngành truyền hình, … và đăng ký mua license phần mềm như Adobe Photoshop, Illustrator, XD, … rồi lại còn games, …. Thế kế hoạch quản lý license của bạn như thế nào? Với giả định bạn là đứng đầu lab và trách nhiệm quản lý sẽ thuộc về bạn. Bản chất của bài toán quản lý license (Software License Management)Thực ra nói là tôn trọng bản quyền là 1 vấn đề nhưng nó chỉ là một cái mang tính chất bề nổi của tảng băng chìm. Ẩn đằng sau kỹ năng quản lý license chính là vấn đề cost (giá thành) và security (bảo mật). 6 bước cần phải làm với quản lý license (SLM) chính là:       Quy trình   Mô tả         Usage analysis   Phân tích nhu cầu sử dụng của nhóm phát triển, tổ chức dưới quyền, nhân viên. Nhóm đứng đầu sẽ phải phân tích nhu cầu sử dụng thực sự của nhân viên là gì. Ở bước này chưa cần tối ưu ngay, nhưng cần phải ngăn chặn những request mua đồ không cần thiết.        License optimization   Sau khi đã tối ưu hóa requests từ phía end-users, thì việc tiếp theo là với bộ request đã tối ưu ấy rồi, thì phải lựa chọn mua (hay không mua) những phần mềm, thiết bị nào. License nào sẽ bỏ tiền ra để subscribe, và sẽ subscribe theo gói nào?       Maintenance analysis   Cái này cũng nói đi nói lại nhiều lần rồi, nếu bước này không tính toán dài hạn dựa trên usage analysis thì ví dụ phiên bản phần mềm nâng cấp thì lại phải mua lại license, dẫn đến tốn tiền nâng cấp license. Ngoài ra, nếu không kịp thời nâng cấp mua mới license thì khi hết hạn nhân viên lại phải chờ sếp mua license mới, rất tốn công.        Standardized data collection   Với các tổ chức lớn như viện nghiên cứu hay trường đại học có hàng trăm thậm chí hàng ngàn nhân viên, việc quản lý tập trung, thu thập thông tin sử dụng phần mềm của nhân viên (cài phiên bản nào, license thế nào) là vô cùng quan trọng. Về mặt bảo mật, thì khi có license hết hạn, người dùng vi phạm license agreement, nhà quản lý sẽ được thông báo trực tiếp từ data, và như thế đối ứng sự cố bảo mật mới tốt.        Cyber-threat detection   Thực ra thì thu thập dữ liệu mà muốn lên cảnh báo thì phải có một feature là threat detection. Tức là phát hiện nguy cơ liên quan tới license ngay từ dữ liệu usage của nhân viên.        Audit Prevention   Phòng chống các vấn đề khi đã phát hiện ra từ dữ liệu ra là cần thiết. Auditor sẽ phải đưa ra những suggestions để tổ chức và nhà quản lý đưa thành policy để implement trong tổ chức.    Đấy, nghe thì có vẻ đơn giản là làm 1 bảng Excel để quản lý license một cách thủ công, nhưng breakdown ra thì cũng đủ thứ. Làm nhà quản lý cho 1 team nhỏ tầm 5-6 người, hay dù quản lý cả 1 department hay là hiệu trưởng một trường đại học danh tiếng có hàng vạn sinh viên thì những vấn đề như thế này không được lơ là. Theo kinh nghiệm của tôi thì cứ tốt nhất mua luôn gói nào nó có sẵn hết các chức năng trên. Ví dụ thu thập dữ liệu và detect nguy cơ là Adobe nó cũng có hết trong gói Business rồi đấy. Tuy nhiên, nếu trong nhóm chỉ có 1-2 người cần mua thì bài toán giá thành cân đối bảo mật sẽ phát sinh vì ít nhu cầu dùng quá (cost vs. security trade-off). Tuy nhiên, nếu là lãnh đạo của 1 department hàng trăm nhân khẩu dưới quyền thì cứ thoải mái thôi, vì khi số lượng đặt hàng lên đến hàng trăm, hàng ngàn thì chắc chắn Adobe sẽ có discount. Vào mấy cái viện ít người mới lo chứ còn vào những chỗ quân đông tướng lắm như trường đại học, tập đoàn, cty lớn thì chủ yếu là kỹ năng quản lý thôi. Quay lại ví dụ lab 40 ngườiĐại học lớn vs. đại học nhỏ (và viện ít người): Ví dụ nhân sự 40 người nói ở trên là một trường đại học và tập đoàn lớn, nơi cứ mỗi năm có hàng ngàn người mới, hết thế hệ này tới thế hệ khác. Vì vậy làm giáo sư hay teamlead ở một tập đoàn hay trường đại học lớn, thì vấn đề nhân sự lại không phải việc khó. Cứ lên làm lead thì thiếu người muốn nhận bao nhiêu chả được. Cái quan trọng là năng lực quản lý của anh được bao nhiêu người?Thường thì chuyện quản 40 người thậm chí hàng trăm người là chuyện bình thường ở các cơ sở lớn như vậy. Còn đối với các viện nhỏ, đại học nhỏ thì đôi khi mỗi leader một năm chỉ quản có vài ba nhân viên. Chuyện quản lý có khi dễ dàng, người đứng đầu chả thèm quan tâm xây dựng văn hóa đội nhóm, văn hóa team, …Nếu là nhà nghiên cứu thì đi vào những nơi này họ sẽ có nhiều thời gian làm chuyên môn hơn là lo quản lý hàng trăm hàng ngàn miệng ăn phía dưới. Chỉ cần lo cho bản thân mình và mấy cậu phía dưới thôi. Tuy nhiên, cái gì cũng có mặt trái của nó, nếu cái kiểu vài cơ sở nhỏ như trên thì gánh nặng quản lý không có nhưng giá thành sẽ đắt vì hầu như các gói subscription của các phần mềm như Office hay Adobe thì chỉ rẻ khi có nhiều trên 100 người dùng. Ông làm nghiên cứu mà lab ông chỉ có 2 anh postdocs thì dù là viện nghiên cứu cấp quốc gia thì khi mua license thì cũng chỉ tính được cho 3 người (ông và 2 anh postdocs). Mà thế thì hầu như muốn mua phần mềm gì cũng chỉ có vài khả năng:  là mua gói personal subscription, giá sẽ đắt trên đầu người. Ngoài ra, sẽ phải mua đứng tên từng cá nhân nên control của tổ chức sẽ không có. Tức là như đại học lớn họ mua hàng loạt cho ngàn người dùng thì họ sẽ được tặng kèm gói audit để thực hiện thu thập dữ liệu, detect nguy cơ và prevention.  là ông phải chung với các lab lớn khác. Nhiều lab nhỏ cùng có nhu cầu dùng hợp với nhau để mua. Thì nó lại có cái dở là ông admin ở lab khác cũng sẽ quản lý được dữ liệu thu thập được từ nhân viên lab ông. Thành ra là có vấn đề về riêng tư. Thế nên là vào mấy cái startups hay đội nhóm nhỏ, đại học nhỏ ấy thì thường mọi người chấp nhận không dùng bản có phí, dùng open source để tránh việc phải xài những cái đòi licenses như MATLAB, Office, Adobe, …Cái này cũng có cái hay cái dở. Thì thực ra tình huống nó bắt buộc là mình ít người mình chịu. Nhưng nó lại dở vì đa số người ta dùng Adobe, MATLAB, … mình lại không mua được thì thành ra là lại outliers. Ví dụ nghiên cứu của lĩnh vực đều dùng Adobe Photoshop, thì mình lại tái hiện trên GIMP thì ôi thôi! Mà cái quan trọng nhất lại không chỉ giá tiền mà là audit. Dù ông nhà nghiên cứu ở viện nhỏ, ông có mua personal subscription hay open source thì chắc chắn sẽ không có control của tổ chức. Nên nếu ông đề nghị mua phần mềm cho 2 cậu postdocs thì chắc chắn ông lãnh đạo viện sẽ bắt ông phải mua theo option 2. Mà như thế là dữ liệu của nhóm ông tự dưng lại phải chia sẻ ra cho các nhóm khác trong viện mà thậm chí ngoài viện. Còn nếu ông không đề nghị mua, tức là ông giấu lãnh đạo viện thì sẽ không cân đối kế toán được. Còn ông mà bảo postdocs nó dùng open source thì lại xảy ra vấn đề là không minh bạch vì không có control trong audit. Nên lúc cân nhắc làm việc ở viện lớn hay viện nhỏ, đại học danh tiếng hay ít danh tiếng, tập cty lớn hay mấy startups, theo kinh nghiệm của tôi nên cân nhắc bài toán quản lý license. Lựa chọn subscription cho phần mềm đồ họa: Adobe software: Bây giờ chúng ta sẽ bắt đầu thực chiến quản lý license. Đầu tiên, về nhu cầu sử dụng, usage analysis, thì quan điểm là người đứng đầu có quyền chỉ định đề tài nghiên cứu cho nhân viên. Đặc biệt nếu là công ty lớn, đại học lớn thì chuyện này là hiển nhiên. Bạn nghĩ (a) nhân viên tự đề xuất đề tài nghiên cứu rồi tự ý làm hay (b) lãnh đạo chỉ đạo chọn đề tài nghiên cứu thì tốt hơn?Về mặt quản lý license, chắc chắn (b) sẽ tốt hơn với trường hợp có tới 40 nhân viên như chúng ta đang giả định. Vì sao? bởi vì là công nghệ lõi của lab vẫn là xử lý đồ họa, nên chắc chắn lúc đầu năm lúc xin kinh phí, lãnh đạo phải có kế hoạch để mua license nào phục vụ hướng nghiên cứu của lab. Thế giờ tự cho nhân viên tự tìm đề tài, nó lại tìm ra cái đề tài lệch 180 độ so với hướng đi của lab, rồi đòi mua 1 cái license mà ko có tác dụng gì cho nhóm nghiên cứu thì kế hoạch kinh phí đã xin là phá sản?Thế nên những trường hợp cũng không làm gì được vì tự do nghiên cứu phát triển, chống harrassment thì không thể bắt ép được. Lại phải delay thôi vì em đánh đố thế thì thày cũng chịu nhá. Ngoài ra không chỉ vấn đề tiền mà cả về bảo mật thì nhân viên tự ý chọn đề tài ra ngoài rồi đòi mua license ngoài dự tính như vậy, thì chắc chắn là nếu đề xuất mua lên trên, thì lãnh đạo trường chắc chắn bắt mua chung với 1 lab khác, cho đủ con số người dùng để được giảm giá, thì lại xuất hiện bài toán phải chia sẻ dữ liệu với lab khác. Mà hợp tác với lab khác mà chuyện chia sẻ dữ liệu nhập nhằng cũng sẽ nhiều vấn đề. Nên cái thằng nhân viên nó chọn đề tài mang tính outliers là phải xử lý. Thì thực ra cái gì cũng có thể giải quyết được, đề tài lệch 180 độ mà hai bên thống nhất thì cũng “bẻ lái” được thôi, nên thày trò lãnh đạo nhân viên chịu khó nói chuyện rồi mà giải quyết với nhau là êm đẹp nhất. Còn nếu vướng mắc quá thì lại phải thuyên chuyển sang nơi khác mà nhiều nhân viên dùng cái license ấy thôi. Theo kinh nghiệm của tôi, thì lãnh đạo mát tay là có khi ông ấy control luôn cả usage của 40 nhân viên. Tức là không nói thẳng ra là chỉ đạo nhưng mà nhìn chung là cứ có cái gì lệch lạc tổ chức họp “bẻ lái”, làm sao cho cả 40 nhân viên đều làm đồ họa. Thằng nào thích làm NLP thì chủ động dùng open source đi vì NLP phần lớn open source chứ ít kiểu license nhiều như bên đồ họa, vision. Tôi từng gặp một trường hợp lãnh đạo rất mát tay: chọn đề tài từ trên xuống dưới cách thể hiện qua ngôn ngữ thì nhìn qua tên đề tài thấy chả có liên quan tới vision đồ họa mấy, nhưng đến lúc bóc tách ra thì toàn các tính năng của Adobe Creative Suite. Thế là 40 con người vẫn đều phải phụ thuộc vào Adobe, mà thế thì cả phòng dùng Adobe thì viết đơn xin mua Adobe cho 40 người bên trên làm sao từ chối được?Cả phòng làm về Adobe thì mua sản phẩm Adobe là đúng rồi còn gì. Thế nên, nếu mà control usage ngay từ đầu thì lúc đề xuất mua cái gì cũng dễ pass, cái bước usage analysis cũng chả phải làm vì cả phòng làm Adobe thì còn phân tích cái gì nữa. Mát tay nó là như thế đấy. Bây giờ, thày trò họp hành, rồi thảo luận trao đổi, “bẻ lái” thế nào đó để cả 40 con người đồng bộ nhu cầu rồi nhé. Bước tiếp theo là License optimization, tức là chúng ta sẽ phải lựa chọn gói tối ưu nhất của Adobe để mua đảm bảo 40 con người có đồ mà dùng. Thực ra bước này làm cùng với phân tích nhu cầu sử dụng cũng được. Thì cứ giả định là thời gian sử dụng là 24/7 luôn đi.  Tham khảo các gói sử dụng của Adobe Creative Cloud tại https://www. adobe. com/creativecloud/plans. html. Bây giờ có 40 người thì có hai cases:  Chúng ta là phòng nghiên cứu phát triển của 1 cty lớn và cả 40 nhân viên đều chính thức     Nếu mua cho 40 nhân viên 40 gói individuals thì sẽ tốn mỗi tháng là $40\times 52. 99=2199. 6$ USD. Một năm sẽ tốn tầm 25k USD.          Bất lợi: là không có quản lý license đi kèm.           Nếu mua theo gói Business thì         https://www. adobe. com/creativecloud/business/teams. html     Bạn sẽ trả 79. 99 đô/tháng cho mỗi license, thì nhu cầu thay đổi chỉ cần hủy license và lấy license mới.      Tuy nhiên là nếu license dừng lại giữa tháng thì sẽ xuất hiện lãng phí và lại phải chia sẻ account để đỡ phí. Mà về mặt bảo mật thì chia sẻ account lại không nên. Thế nên là lại phải quản lý nhân viên để không có ai bỏ việc giữa tháng.      Lợi điểm là các tính năng để collaborate và quản lý license có đi kèm.      Trong trường hợp không chia sẻ tài khoản thì tốn $79. 99\times 40=3199. 6$ đô môĩ tháng, 1 năm mất 38-39K.      Trong trường hợp chia sẻ tài khoản thì có quy định sau:             Số lượng máy có thể cùng lúc dùng cho 1 license thì chỉ tối đa 2 máy được login cùng license.        Thế nên coi như giảm đi 1 nửa so với không chia sẻ tài khoản thì mỗi tháng mất 1600 đô và mỗi năm tầm 19k đô. Vẫn rẻ hơn so với mua gói cá nhân hoặc không chia sẻ tài khoản.                       Chúng ta là ở trong đại học lớn và cả 40 nhân viên đều là students     Thì kể cả gói cá nhân cho 40 sinh viên cũng chỉ mất 19. 99 đô/tháng. vậy mỗi tháng mất $19. 99\times 40=800$ đô. Mỗi năm mất dưới 10k.          Tuy nhiên cũng sẽ không có chức năng quản lý đi kèm hoặc team nhóm.           Nếu mua theo đơn vị department hoặc institution.          Nếu mua theo department thì vì chỉ có 40 người mà yêu cầu là 100 người nên sẽ phải mua chung với nhóm khác. Trong trường hợp có mua chung thì mỗi sinh viên chỉ tốn 12 đô, 1 tháng 40 sinh viên mất có 480 đô, 1 năm mất có 5-6k.      Nếu mua theo named-user license thì cứ mỗi user mỗi tháng mất 34. 99 đô, 1 tháng 40 users mất $34. 99\times 40=1399. 6$ đô, 1 năm mất tầm 16-17k.      Nếu mua theo device thì giả sử cứ 5 người dùng chung 1 device (1 tuần luân phiên nhau dùng) thì cần 8 máy, mỗi máy 1 năm mất 330 đô, cả nhóm phải chi cho 8 máy 1 năm tầm $330\times 8=2640$ đô.           Nhìn chung là nếu cài theo máy thì sẽ rẻ nhất (cả năm mất có dưới 3k đô). Thì vì có người cần dùng khi đi công tác nên sẽ phải có cả máy bàn và máy laptop: 4 máy bàn và 4 máy laptop có cài Adobe Creative Cloud. Một máy giờ mua cũng tầm 2500 đô, dùng tầm 5 năm thì thay. Thì tổng tiền máy cần đầu tư là $2500\times 8=20000$ đô cho 5 năm. Vấn đề là sẽ phát sinh thêm công quản lý máy móc: ví dụ khi đi công tác mượn máy lap có cài Adobe thì quy chế ra sao, …Thì vì lab có 40 sinh viên, năm nào cũng có sinh viên mới nên chuyện có 4-5 cái máy bàn và mỗi sinh viên có 1 cái laptop là chuyện bình thường. Ca khó khăn là trường hợp lab trong cty thôi vì Adobe không cung cấp gói device cho bên cty nên phòng nghiên cứu bên cty mà 40 người đều dùng thì mất 20k/năm là chuyện bình thường. Thế nên lại đòi hỏi giải pháp ở đây:  Đẩy phần nghiên cứu đồ họa này về các trường đại học để giảm giá     Thì nếu đẩy được thì trong phòng bạn lãnh đạo chỉ cần giữ lại khoảng 4-5 ông làm về đồ họa nhưng cũng chỉ là các ông kiểu PM/Techlead chứ dev là để bên đại học họ lấy sinh viên làm cho (có khi các em nó còn làm không công để tốt nghiệp ấy chứ), còn lại cho đi làm dự án khác.    Phần dự án cần nhiều license của Adobe thì thôi đẩy cho mấy ông thày bên đại học lo. Mình hợp tác thôi vì bên đại học họ mua license được số lượng lớn giá rẻ.    Chuyện hợp tác đương nhiên cũng phải có trả lương nhất định cho các bên, nhưng chắc chắn bên đại học thì mình sẽ phải chi ít hơn (sinh viên bỏ vào thì có phải trả đâu mà có trả cũng ít hơn nhân viên chuyên nghiệp trong các cty tập đoàn).    Tóm lại là nên đẩy những mảng nghiên cứu này về phía đại học vì nhiều lý do và phương châm là thúc đẩy hợp tác giữa hai giới cty và học thuật. Vầ tất nhiên người được lợi cuối cùng chỉ có bên giới công nghiệp và các sếp thôi. Các bạn thử ngẫm mà xem:  Bên Adobe bán license thì dù thế nào tiền vẫn vào túi họ nhé.  Bên cty muốn hợp tác thì tiền chắc chắn họ tiết kiệm được một khoản khá nhờ đẩy việc vào đại học nhé.  Các sếp cũng được thêm tiền dự án. Thế nên là tốt nhất cứ đi làm, còn đi nghiên cứu thì phải đòi thù lao nó xứng đáng các em ạ. Về mặt maintain thì nhìn chung nên dùng gói Cloud, có gì nó cập nhật luôn mà vẫn tính trong tiền hàng tháng như vậy. Còn các mục audit quản lý khác thì nằm trong gói do Adobe cung cấp rồi. Lựa chọn gói nào cho games: Thi thoảng vẫn có một số nghiên cứu liên quan tới games. Ví dụ như nghiên cứu để tìm ra 1 framework mới giúp việc thiết kế games trở nên mô phỏng gần với hành vi người dùng hơn: This is an embedded Microsoft Office presentation, powered by Office. Thì về phân tích nhu cầu thì nó lại thế này: nhân viên của mình thì không dùng mà mua về để cho những người tham gia thí nghiệm dùng. Những người tham gia thí nghiệm sẽ được tuyển chọn dựa trên base của thí nghiệm thôi. Thì lại phải mua cho những người tham gia ấy chơi. Tức là không phải mua cho nhân viên mình chơi mà là cho đối tượng thí nghiệm chơi. Thì tùy vào setup của thí nghiệm mà có khi phải mua cả bộ Xbox hoặc Playstation. Có khi phải mua bản Gold hoặc bản Ultimate chứ bản Standard cũng ko được.  Bảng giá Assassin’s Creed Valhalla Tuy nhiên, cũng cần nhớ nếu mình chọn đối tượng thí nghiệm là students ấy thì sẽ được discount 15%: https://store. ubi. com/us/student-discount. html?lang=en_USBao giờ student cũng được discount nên chọn loại đối tượng này thì sẽ tốt hơn. Hoặc có khi mình có thể setup thí nghiệm là chỉ khảo sát những đối tượng nào đã chơi game trên thôi thì chuyện mua game là của họ, mình chỉ cần tìm ra họ thôi. Tuy nhiên là đôi khi là nhu cầu thí nghiệm đòi monitor cả quá trình chơi game của đối tượng thì lại phải mua cho họ games để họ chơi. Trong trường hợp phải mua cho họ chơi ấy, thì nên chọn student để có discount. Lựa chọn gói truyền hình gì?: Đồ họa và games coi như là hai nhánh chính thống của Computer Vision. Một phần nào đó thì cũng là nơi mà CV thực sự được ứng dụng nghiêm chỉnh. Chúng ta ít khi nghĩ đến những đề tài nghiên cứu về truyền hình nhưng thực ra là cũng liên quan kha khá đấy. Đơn cử là có những nghiên cứu về interactive television:  Truyền hình tương tác là một dạng truyền hình cho phép người xem tham gia, điều khiển các chương trình truyền hình. Với dạng truyền hình truyền thống, đường truyền truyền hình là một chiều. Các nhà đài cho phép khán giả xem gì, vào giờ nào, trên kênh nào là quyền của họ. Source: Forbes Thì cũng như đồ họa và games, chủ yếu là nghiên cứu nhận kinh phí từ các nguồn dồi dào có sẵn, được đặt hàng hẳn hoi. Và họ cũng chủ yếu nghiên cứu về trải nghiệm người dùng thôi. Không cần code kiếc gì đâu, chỉ cần làm mấy thí nghiệm để kiểm chứng giả thuyết về trải nghiệm người dùng. Sẽ phải mua sắm màn hình cảm ứng touch, có kết nối truyền hình. Sau đó là setup một căn phòng và cho người dùng trải nghiệm theo kịch bản cho sẵn. Thế là sẽ phải mua một gói truyền hình cho thí nghiệm, nhưng như thế thì sẽ không dùng thường xuyên, và phải trả theo tháng thì mua kèm Internet cáp. Tốt nhất là nên lên kế hoạch thí nghiệm và chốt thời gian tổ chức để chỉ mua gói truyền hình tại tháng ấy thôi. Kết luậnNhìn chung, kỹ năng quản lý license là một kỹ năng không thể thiếu để quản trị một nhóm tầm trong quy mô 50-100 nhân viên. Ở quy mô này trở lên, bắt đầu phải có những điểm lưu ý và thảo luận để quản lý linh hoạt chủ động mà lại tối ưu. "
    }, {
    "id": 72,
    "url": "https://wanted2.github.io/adobe-creative-cloud/",
    "title": "Adobe Creative Cloud: An All-in-One Platform for Creators",
    "body": "2021/12/19 - Adobe provides three different cloud solutions for customers: Adobe Creative Cloud for creators, Adobe Experience Cloud for marketers, and Adobe Document Cloud for business. Creative Cloud is a collection of 20+ apps for photography, video, design, web, UX, and social media — plus integrated essentials like font families and the power to collaborate with anyone, anywhere. Many things can be done with this solution. We take a look at the features. Adobe Creative CloudAdobe Creative Cloud (ACC) is an all-in-one platform for creators, marketers, visual artists, and designers. Let’s hear words about the roadmap of ACC from Scott Belsky. This was highlighted in Adobe MAX 2020, a trade show event for creative users held by Adobe from October 20 to 22, 2020. You can access the online tutorials from here.  Creativity for All 3 Goals of Adobe Creative Cloud: Enable Connected Creativity:  Creativity $\times$ Organization = Impact Putting creativity into an organization makes an impact!Three pillars of your organization are: Assets, Apps, and Team. Let’s talk about the assets first. How can you share your assets with your collaborators?Adobe Behance, Adobe Bridge, and Adobe Stock make your assets sharable and organized. You can share your . psd files with your teammates, and you can search for your interested images from million of shared assets in Adobe Stock and Bridge.  Bringing 30 years of Photoshop features and code to a new platform like the web is a journey. Photoshop has more than 30 years of history. That is so long to have an impact, but now Adobe Photoshop is not only a desktop app. Adobe Photoshop for Web is in Beta now and will be available in ACC soon.  Work better together with Adobe Creative Cloud. Finally, let’s talk about the team, the last pillar. Adobe provides Spaces and Canvas for collaboration. You can not only share the PSD and AI files with your collaborators but also work with them directly in ACC. Adobe Cloud Canvases provide an overview of the whole team’s work. Users can make comments, can get real-time feedbacks in Canvases.  And don’t forget the devices. Adobe apps like Photoshop in Apple’s M1 chip run twice faster than in older chips. With ACC, your assets and apps can be synchronized between your devices. You can start your design from your iPhone and then grab it on your iPad or other devices. And you can collaborate with your co-workers from any device as well. Unleash Creative Potential: Web designers can use Adobe XD, Illustrators, and so on to create ideas. Artists can use Fresco, Design to create a lot of drawings. Photographers and video makers can capture and use Adobe Photoshop and After Effects to create images and videos for social posts. Cartoon artists can use Character Animator to create a live anime. Other people can use Lightroom or Premiere Rush to edit photos/videos on the mobile phone. Other people can search for photos, AI files, videos, and even anime and manga in Stock and Behance. Some can scan document images and get the content indexed and managed in Adobe Acrobat. So many possibilities can be done with this suite of apps. And now, all of them are available in the Cloud. Empower Creative Careers: Adobe also launched Content Authenticity Initiate (CAI) in 2018 to ensure artists get credits for their work. Any content made in Adobe today will be attached metadata about the project they belong to, and the credits always go to the creators. Non Fungible Tokens (NFT) are generally used by artists recently to access marketplaces of collectors. However, many artists have seen their works were minted without attribution to the original artists. Adobe has worked with many NFT partners such as OpenSea, KnownOrigin, SuperRare, and so on to ensure the credits will be given to the right people. When an artist creates a project in Photoshop with Content Credentials, then that information will be displayed in any related NFT partners. ToolsetsNow, we understand the goals and roles of ACC in creative activities. We will take a closer look at each of the app in this section. Image editing: Photoshop vs. Lightroom (Lightroom classic): Photoshop has more than 30 years of history. A brief overview of released versions can be found here. The latest version is CC 2021, and we mostly use it. Adobe Photoshop is specialized to photo enthusiasts who want to make beautiful images from camera-captured inputs. Source: learningavphotoshop There are some resources in YouTube for you to learn Photoshop (especially, CC 2020):  Adobe Photoshop CC 2020 Tutorial Adobe Photoshop Tutorials Photoshop CC EssentialsBased on a rough estimation, there is a requirement of at least two years to comprehend this Photoshop CC. There are too many features to learn, but think about the 30-year history of Photoshop: you only spend two years to learn 30 years of Photoshop features and code. What a benefit!  Photoshop is a comprehensive photo editing app for photographers, web designers, visual artists, and photo enthusiasts. So, what’s about Lightroom?While Tools is essential in Photoshop, Presets is primary in Lightroom. In Photoshop, you can tune your images at a detailed level using fine-grained tools. But it is completely different in Lightroom, where the presets hold only common features which are known to sufficiently produce good quality images. Because Photoshop is specific for details, it is often used by professional photographers. But with Lightroom, the user segment can be large. Because presets were tuned by professionals, and the tuned parameters can be huge (millions to billions of parameters).  Photoshop is best for professionals who want to tune every detail of a single photo, while Lightroom presets are good for unknowledgeable users who want to tune a batch of hundred images without specialized knowledge about graphics.  While Lightroom Classic is a desktop app, Lightroom is a web app with the address: lightroom. adobe. com. Illustrator: If Photoshop CC is a graphic editing app, Illustrator CC is a creator app. It can be used to create industry-standard vector artworks like logos, illustrations, and posters. A beginner course is available on YouTube:  Learn Adobe Illustrator Adobe Illustrator Tutorials Based on a rough estimation, it may take six months to a year to comprehend Adobe Illustrator (AI). Then you can create professional artworks which can be used in web designs or anywhere they need AI. Fresco for fun drawing: Similar to Adobe Illustrator but with a friendlier interface and limited toolset, Fresco provides basic features for drawing raster artworks. Fresco can be used in mobile devices like the iPhone and iPad. Because it drops the presets notion, Fresco is close to end-users who are unknowledgeable about art. In contrast, Illustrator is closer to artists and graphics professionals. There are some free courses available on YouTube about Fresco. If you are a normal end-user, you should like Lightroom and Fresco. But if you are a professional, you may need Photoshop and Illustrator.  Adobe Fresco TutorialsDimension for 3D photography: In this era of VR/AR applications, artists and producers need a tool to create 3D graphical models. The models can be saved and viewed from different perspectives.  Adobe Dimension TutorialsNeedless to say, you take advantage if you are already familiar with object-oriented design (OOD), because everything here is only 3D objects!Then you can be quite familiar with the workflow like selecting a template object, changing the Properties of the object, then defining its Scene and Actions. Adobe Dimension integrates Adobe Stock and Behance as its cloud storage and management solutions. You can save your models to Stock or share assets to Behance. Video editing: Unlike photos, videos need care about motions, perspectives, and so on. Premiere Pro and Rush: With Premiere Pro, users can edit their video durations, change replaying orders, image processing, edit audio, and so on with videos. Rush is for mobile.  Adobe Premiere Pro CourseSome nice features can be multi-perspective videos and video-audio alignment. After Effects: Unlike Premiere Pro is for professionals, After Effects has a smaller video editing toolset but yet enough for non-professional users. Some straight modifications can be quite clunky with Premiere Pro. In these scenarios, After Effects seems to be a better fit.   Affter Effects courseAudio editing: We had a look on image/video editing softwares. But now we want to edit, mix, enhance audio, we should think about Adobe Audition. Adobe Audition can be integrated into After Effects or Premiere Pro. The output audios can be published to Behance and Stock.   Adobe Audition courseStorage and Management: Adobe Stock and Adobe Bridge:  Adobe Stock, formerly known as Adobe Stock Photos, is a stock image database that was originally integrated with Adobe Bridge in Adobe Creative Suite 2 and 3. It is presently available through certain subscription packages from Adobe Creative Cloud. Users can access Adobe Stock from https://stock. adobe. com/.  Adobe Bridge is a digital asset management tool developed by Adobe. It is freely included as part of Adobe Creative Cloud and was first made available with Creative Suite 2. Its primary purpose is to link the parts of the Creative Suite together using a format similar to the file browser found in previous versions of Adobe Photoshop. It is accessible from all other components of the Creative Suite (except for the standalone version of Adobe Acrobat 8). Bridge is also included with the stand-alone Photoshop application, and can perform certain Photoshop processing functions separately (and simultaneously) with Photoshop itself. Media Encoder: Adobe Media Encoder (AME) is a video media transcoding utility developed and marketed by Adobe through Adobe Creative Cloud. It is designed for integration into workflows with other video applications such as Adobe Premiere Pro and After Effects. Animation: Adobe Animate CC: Adobe Animate CC, formerly Adobe Flash Professional CC, is the current series of the Animate application offered by subscription through Adobe Creative Cloud.  2D Animation - Free CourseAdobe Character Animator: Adobe Character Animator is a 2D puppet animation program developed by Adobe that is included with Adobe After Effects CC through Adobe Creative Cloud. It uses a multi-track motion capture recording system to apply behaviors to puppets. Some nice thing can be done with Character Animator: Web design tools: Adobe Dreamweaver: Adobe Dreamweaver, formerly Macromedia Dreamweaver, is a web development application originally based on the codebase of Backstage, which was acquired by Macromedia in March 1996. Dreamweaver has been owned and marketed by Adobe since December 2005. Dreamweaver is available for both Mac and Windows. Recent versions have incorporated support for web technologies such as CSS, JavaScript, and various server-side scripting languages and frameworks, including ASP. NET, ColdFusion, JavaServer Pages, and PHP. As a WYSIWYG Presto-based editor, Dreamweaver can hide the HTML code details of pages from the user, making it possible for non-coders to create web pages and sites. One criticism of this approach is that it can produce HTML pages whose file size and amount of HTML code can be larger than an optimally hand-coded page would be, which can cause web browsers to perform poorly. This can be particularly true because the application makes it very straightforward to create table-based layouts. In addition, some website developers have criticized Dreamweaver in the past for producing code that often does not comply with W3C standards, though recent versions have been more compliant. Dreamweaver 8. 0 performed poorly on the Acid2 Test, developed by the Web Standards Project. However, Adobe has increased the support for CSS and other ways to layout a page without tables in later versions of the application, with the ability to convert tables to layers and vice versa. Dreamweaver allows users to preview websites in many browsers, provided that they are installed on their computers. It also has some site management tools, such as the ability to find and replace lines of text or code by whatever parameters specified across the entire site and a templatization feature for creating multiple pages with similar structures. The behaviors panel also enables the use of basic JavaScript without any coding knowledge. Dreamweaver can use “Extensions” – small programs, which any web developer can write (usually in HTML and JavaScript). Extensions provide added functionality to the software for whoever wants to download and install them. Dreamweaver is supported by a large community of extension developers who make extensions available (both commercial and free) for most web development tasks, from straight rollover effects to full-featured shopping carts. Like other HTML editors, Dreamweaver edits files locally, then uploads all edited files to the remote web server using FTP, SFTP, or WebDAV. Dreamweaver CS4 now supports the Subversion (software) (SVN) version control system.  Dreamweaver TutorialsAdobe XD: Adobe XD is a vector-based user experience design and prototyping system. It is also a presentation tool for web apps and mobile apps, which is developed and published by Adobe. It is available for free but requires an account registered through Adobe Creative Cloud.  Adobe XD CourseContent Creation: Adobe InDesign: Adobe InDesign is a desktop publishing and page layout designing software application produced by Adobe Inc. It can be used to create works such as posters, flyers, brochures, magazines, newspapers, presentations, books, and ebooks. InDesign can also publish content suitable for tablet devices in conjunction with Adobe Digital Publishing Suite. Graphic designers and production artists are the principal users, creating and laying out periodical publications, posters, and print media. It also supports export to EPUB and SWF formats to create ebooks and digital publications, including digital magazines and content suitable for consumption on tablet computers. In addition, InDesign supports XML, style sheets, and other coding markups, making it suitable for exporting tagged text content for use in other digital and online formats. The Adobe InCopy word processor uses the same formatting engine as InDesign. Adobe InCopy: Adobe InCopy is a professional word processor made by Adobe Inc. that integrates with Adobe InDesign. While InDesign is used to publish printed material, including newspapers and magazines, InCopy is used for general word processing. The software enables editors to write, edit, and design documents. The software includes standard word processing features such as spell check, track changes, and word count and has various viewing modes that allow editors to visually inspect design elements — only as it looks to the designer working in Adobe InDesign. Miscellaneous: Acrobat: Adobe Acrobat is a family of computer programs developed by Adobe Systems, designed to view, create, manipulate and manage files in Adobe’s Portable Document Format (PDF). Some software in the family is commercial, and some are free of charge. Adobe Reader (formerly Acrobat Reader) is available as a no-charge download from Adobe’s website and allows the viewing and printing of PDF files. Acrobat and Reader are widely used as a way to present information with a fixed layout similar to a paper publication. As of June 2020, the main members of the Adobe Acrobat family are:  Adobe Acrobat DC, sold only by subscription through Adobe Document Cloud with mobile app support.      Adobe Acrobat Standard DC   Adobe Acrobat Pro DC includes the ability to convert scanned documents and add media files    Adobe Acrobat 2020, the last version sold through a one-time perpetual license, but without cloud features.      Adobe Acrobat Standard 2020   Adobe Acrobat Pro 2020    Adobe Acrobat Reader DC, a free client version with the ability to print, sign, and annotate. Aero: Adobe Aero is an augmented reality authoring and publishing tool developed by Adobe and marketed through Adobe Creative Cloud. Aero was originally announced as a private beta for iOS users at Adobe MAX 2018. It was officially launched during Adobe MAX 2019. Aero is part of Adobe’s 3D &amp; AR series, which includes Adobe Dimension, Fuse, and Substance. Adobe Sensei: the graphical AIAdobe Sensei is an artificial intelligence and machine learning technology being developed by Adobe. It is being applied to Adobe Analytics, Campaign, and Target. Sensei technology is also used in subject selection and removal, as seen in recent versions of Adobe Photoshop and Photoshop on iPad. Many Adobe products like Photoshop, Premiere Pro, Illustrator, and Stock use somewhat Sensei’s insight to enhance user experience. ConclusionWe took a look at the Adobe Creative Cloud ecosystem. Like most other commercial clouds like Oracle Cloud, AWS, Google GCP, or M$ Azure, it has many products and services which make our life better. And Adobe Sensei AI is impressive as it integrates many of the latest technologies in Computer Vision, Machine Learning, and Artificial Intelligence. Hope to use it in our own products soon. "
    }, {
    "id": 73,
    "url": "https://wanted2.github.io/turn-off-electric-before-leaving-why/",
    "title": "なぜ帰宅する前や使用済みの時などに電気を消すべきでしょうか?また，なぜ残業しない方がよいか?",
    "body": "2021/12/18 - あなたは何時出社しますか？何時昼休憩を取りますか？何時に帰宅しますか？これはSEにとってなかなか回答できるが興味深い質問です．「朝8時に出社して，13時に休憩を取り，17時に帰宅する」という回答は一般的であろうが，なぜその時間にしているか説明できるでしょうか？実際は上図に述べるように，SEの業務として障害対応が枠に入っており，障害がよく起きる時間にあまり欠席せずに，席にいるべきところもあります．これがIPAの調査 [1]から読み取っているが，朝8時から午後1時まであまり離席しないほうが絶対にいいのです．ですので，朝8時に出社できる方は絶対に情報システムの場所では歓迎されます．お昼休憩をとるのは13時～で大丈夫ですが，おにぎりやパンなどファストフードを買って席で食べた方が慣れたSEの風景です．私は昼休憩は取らないですが，なぜなら，このせいです．お昼はファストフードばかりで離席せずに対応を続けています．帰宅は19時～で大丈夫でしょう． 朝寝坊はダメだぞ上で説明したが，朝8時から対応しないといけない障害も起きうるため，SEの場合，朝寝坊は禁則ですね．「ITだから，徹夜してプログラミングをしたので朝寝坊してしまった」という説明もちらほら聞かれていますが，ダメです！プログラミングは勤務時間内でやれぇ．朝で遅れなくきちんと出勤した方がいいです． 帰宅時19時以降翌日7時以前の間に障害の報告が少ないです [1]1．ですので，19時～で帰宅することは問題ないでしょうか．という常識になって，ほぼ皆はその時間で帰宅してお休み時間を取っています．じゃ，例えば，深夜に出勤して何か不審な作業をやっている作業員がいると，異常でしょうか？これは障害の種であるため，いったん深夜に作業するPCなど機器を起動しないように電気を必ず消しましょう．また，例えば，深夜に障害につながる異常が発生すると，出勤できる作業員がお休み中の場合もあるから，対応も大変です．なので，19時～翌日7時までの間に，重要インフラ以外電気をきちんと使用されないように規定したほうがいいでしょう．「お金がかかっちゃう」という理由もよく聞かれたが，SEの分野では，多分それが本当の理由ではないと思います．実際の理由は障害対応の都合であろう． 物品を使用したとき上記の「帰宅時」の説明と同じように，物品の使用が終わったらリソースをクリアしましょう．「物品」とは，物理的にある会議室やサーバー部屋や機器などの施設だけではなく，仮想的に存在するサーバーやクラウドサービスの使用も含まれています．19時～ではなく，勤務時間内に一時使用するサーバーもあります．離席している最中に作業PCはONのままにしている可能性もあり，画面ロックをしない場合，セキュリティインシデントが起きうるのです．  離席時，必ず画面ロックなどで資産をロックしましょう．再度着席したら，ロック解除で大丈夫です． じゃ，使用が終わったときに，電気を消さないで，リソースをクリアしないでそのままにすると何が問題かな？以下は千葉銀行からお客様にお願いする事項2ですが，興味深いですね．  ログアウト等の励行  お客さまのパソコンを遠隔で操作し、正当な契約者が操作したように装う事例が発生しております。パソコンの外部からの不正操作を防止するため、Web-EBをご利用にならない時や、インターネット接続が必要ない時は、ご利用のパソコンをネットワークから遮断したり、無線LANを切断するなど、パソコンをインターネット環境から隔離し、常時接続をさけてください。また、パソコンをご利用にならない場合は、パソコンの電源を落とすよう徹底をお願いいたします。 電源を落とし，インターネット接続も遮断するというお願いですね．もうちょっとわかりやすいマンガで説明した投稿もあります． マンガでわかる！インターネットバンキングのセキュリティ対策 &gt; パソコンの電源を入れたままにしていませんか？ 最近，クラウド基盤へ移すサービスも少なくないので，物理ではなく仮想的な資産も存在します．例えば，AWSの仮想サーバーやサービスなどですね．それらは電源を落としても稼働できるため，リソース管理をきちんとしないといけないです．つまり，各資産の稼働時間を把握する必要があります．それ以外の時間はなんとかの方法でクリアして，再度必要になったら再度立ち上げるように仕組みを規定する必要があります． まあ，ややこしいけど，とりあえず，物理リソースの場合，上記のマンガのように電源を落としましょう．仮想リソースの場合，使用のワークフローを自動化し，心配がないようにしましょう． 残業毎日19時～帰宅した方がよいという話をしました．ですので，19時以降の「残業」はダメですね．といっても，時々仕事のためにしないといけないこともあるでしょう．しかし，それが異常程度の確率にしないと大きな問題であろう．会社100人で皆残業すると，逆に皆で合意したので体制を決めて残業することは問題ないです．しかし，体制も決めず，上司の許可も得ず勝手に残業するということはダメです．  ですので，障害を減らしたい意志で考えると，会社の体制に従うべきです．で，一般的に，残業しないことにしましょう． 仕事を自宅まで持ち帰っても，勤務時間外，つまり責任者が常駐しない時に仕事をしないことにしましょう．     WFHの場合でも，仕事を自宅でやっても責任者の許可を得ないまま仕事はしません．    仕事が途中半端で19時までに終わらせない場合でも，事前に見通して，切りのよいタイミングでリソースをクリアして，電源を落として帰宅しましょう．     効率というよりも，障害の課題です．   効率の考え方は勤務時間外で考えないで，勤務時間内のみで効率を考えるべきであろう．   結論そうですね．大学時代，「成果を出せるために，研究室に常駐」，「徹夜して，明日の発表会に成果を説明する資料を作る」，「19時以降も帰宅しない，プログラミングをします」など，大学生・院生からもよく聞かれたが，懐かしいです．大学時代で若者はこうしていったが，社会人になって，あれは「異常」です．むしろダメです． 社会人になってから，初めて朝定時に出社し，午後定時に退出すれば一番いい人間だと感じました．障害発生が少ない働きを維持しましょう．これからその人生を送りたいですね． 参考文献松田 晃一, 村岡 恭昭 and 齋藤 毅 2019. 情報システムの障害状況 - 2019 年後半データ. IPA社会基盤センター. Details      https://www. ipa. go. jp/sec/system/system_fault. html &#8617;        https://www. chibabank. co. jp/webeb/security/internet/ &#8617;    "
    }, {
    "id": 74,
    "url": "https://wanted2.github.io/svelte-and-reset-checkboxes-10years-ago/",
    "title": "Nghịch ngợm Svelte và câu chuyện code JS thuần 10 năm trước",
    "body": "2021/12/10 - Svelte1 là một framework của ngôn ngữ Javascript, với mô hình bi-direction data binding (MVVM) khiến cho trạng thái và hiển thị luôn đồng bộ, cũng giống như những người anh em khác như Vue. js, ReactJS hay AngularJS. Tuy nhiên, một ưu điểm của Svelte là cấu trúc ngôn ngữ rất thanh thoát, khiến cho lượng code mà người lập trình cần viết rất ít. Lật lại lịch sử của các JS Framework, có thể lần lại từ khoảng 2006, khi JQuery ra đời với một bước tiến trong xử lý JS dạng embedding (nhúng) vào trong trang HTML. Với JQuery, mô hình lập trình là bạn có thể nhúng những snippet code JS vào trong HTML một cách thuận tiện và control hiển thị cũng như xử lý tại frontend. Cách làm này nhìn chung là thủ công, và rất nhiều lập trình viên JS đã hiểu rằng đây là con đường code thuần và hầu như là buggy (rất nhiều bug có thể phát sinh)!Lịch sử sang trang mới vào những năm 2010-2011, cùng với sự ra đời của hàng loạt trào lưu mới như Lean startups, Agile development, … với những JS framework theo luồng MVVM (Model-View-ViewModel). Backbone và AngularJS hầu như ra đời trong giai đoạn này đã mở ra kỷ nguyên mới, khi dữ liệu và hiển thị được binding với nhau khiến cho sự đồng bộ thống nhất trên frontend được đảm bảo. Công ty tôi thời đó khởi đầu cũng làm JQuery thuần, code vô cùng nhiều bugs, mà kỹ sư OT ngày đêm lo sửa. Nhưng đó là chuyện của hơn 10 năm trước các bạn ạ. Sau đó nhiều kỹ sư trong công ty đã cực lực phản đối việc tiếp tục dùng JQuery và đề xuất dùng những con đường bug-free hơn như AngularJS. Đó là những chuyện xảy ra vào năm 2013. Nhưng cùng lúc đó, 2013, Facebook, gã nhà giàu mới nổi, bắt đầu “o bế” khá nhiều công nghệ Web mới như ngôn ngữ lập trình Hack (hậu duệ của PHP) và Meteor một kiểu JS framework cũng thực hiện MVVM, data binding để đồng bộ hiển thị và dữ liệu tại frontend. Và cùng năm 2013, ReactJS, cũng là một JS framework được FB o bế cũng ra đời và làm mưa làm gió. Hồi đó cũng có tranh luận là lên tiếp tục chịu đựng JQuery hay chuyển sang data binding là AngularJS hoặc ReactJS mà cuối cùng có vẻ là AngularJS đã được chọn. Bẵng đi ít lâu, 2013, VueJS ra đời và nhanh chóng được chào đón. Sau đó gần đây là Svelte. Một ví dụ: hộp checkbox Check All/Uncheck All và bài toán reset hiển thị đúng cáchMột ngày đẹp trời năm 2012, một yêu cầu khách hàng tới: Hồi ấy có 1 câu chuyện là thế này. Nói thì dài dòng nhưng chủ yếu là làm chức năng Check All/Uncheck All mà bạn có thể nhìn thấy ở hình ảnh bên trên. Nó là chức năng bổ trợ cho Datatables mà chúng tôi có dùng. Cũng 10 năm rồi, tôi thú thực là cũng chả nhớ chi tiết lắm, nhưng đại để là có yêu cầu thế này:  Mỗi dòng phải có 1 checkbox, và mỗi checkbox là riêng nhau, không có chuyện click checkbox ở dòng 1 mà lại thay đổi hiển thị của dòng 2 là … giải tán.  Có nút Check All mà khi click vào thì tất cả các checkbox con chuyển thành ON hết.  Có nút Uncheck All mà khi chọn thì tất cả các checkbox con của mỗi dùng đều OFF hết.  Nút Check All và Uncheck All có thể làm một để tối ưu trải nghiệm người dùng.  Nếu tất cả các nút con đều ON thì CheckAll cũng phải ON.  Nếu có ít nhất 1 nút con là OFF thì CheckAll phải OFF.  Ngược lại với Uncheck ALL (nếu tách hai hộp check ra). Đấy, yêu cầu có thế rồi chuyển xuống cho đội hakken làm. Tức là phải reset chuẩn, không được để chọn Check All nhưng lại có dòng nào không được check (dù đã check rồi). Không được để check dòng này nhưng dòng khác lại bị ảnh hưởng. Lời giải 10 năm trước: Scope cũng khá nhẹ nhàng, nhưng lúc đó anh em lại chọn dùng JQuery và code thuần luôn. Đó quả thực là một bước đi gian khổ: thứ nhất, là vấn đề data binding, chuyện bind checkbox từng dòng vào hiển thị từng dòng mà lại làm bằng JS thuần thì đòi hỏi phải chơi trò gắn nhãn và selector. Tức là với dòng thứ n gắn nhãn id= row-n  chẳng hạn, sau đó dùng selector của JS hoặc JQuery như var el = document. querySelector( #row-n ); hoặc như JQuery thì đơn giản hơn $('#row-n'), để switch hiển thị.  Bạn nghĩ sao?  Thế tức là gắn nhãn bằng tay còn gì?  Vâng, đúng thế. Gắn nhãn bằng tay và select nhãn theo tên đã đặt.  Thế nếu miss một phát là toang, select nhầm nhãn là hiển nhiên có khả năng vì là con người làm. Rồi còn đặt tên mà không có quy tắc gây nhầm lẫn lúc select là TOANG nặng! Vâng, thú thực là lúc ấy mấy công nghệ MVVM frameworks như AngularJS/ReactJS vẫn đang trong vòng thai nghén của Google và Facebook, anh em cũng chưa có điều kiện tiếp cận, mới bắt buộc phải dùng JS thuần mà code. Chuyện 10 năm trước cơ mà!(Chứ còn năm 2021 này mà còn dùng là … không nên! Phải MVVM hết. ). Cách làm 1: Làm bằng JS thuầnĐánh nhãn bằng tay đương nhiên là không tốt. Để hiểu được lý do tại sao nên dùng framework và patterns có sẵn, chúng ta nên bắt đầu từ việc định tính hóa bài toán. Bài toán: Giả sử có $n$ dòng trong bảng. Mỗi dòng có một checkbox $c_i\in\{0,1\}$ (ở đây 0 là OFF còn 1 là ON). Trạng thái của Check All là $C\in\{0,1\}$ và tương ứng của Uncheck all là $\overline{C}=1-C$. Thì bài toán đồng bộ trạng thái ở đây là đảm bảo rằng ở mọi thời điểm:  $C=\bigcap_{i=1}^n c_i$                     (1) Điều này khá thú vị, việc đảm bảo phương trình (1) luôn xảy ra đòi hỏi phải quản lý trạng thái một cách chính xác. Để làm được điều này với JS thuần, đòi hỏi  Đánh nhãn chính xác, khó nhầm lẫn.  Không select nhầm nhãn.  Quản lý trạng thái ON/OFF phải đồng bộ.  Phải set checked ngay khi có onclick. Nhìn chung là một cách làm khá tồi tệ. Đặc biệt, vì số lượng biến trạng thái là $n+1$ nên khi $n$ càng lớn việc quản lý trạng thái càng khó khăn, dễ xảy ra bug, mistakes, … Cách làm 2: Làm bằng SvelteVới Svelte’s binding, việc quản lý trạng thái trở nên dễ dàng và ít mistakes hơn. Bạn chỉ cần định nghĩa biến trạng thái của checkboxes và bind chúng vào định nghĩa của &lt;input type= checkbox &gt; nhờ từ khóa bind:checked={}. Ví dụ như sau: &lt;input  type= checkbox   class= form-check-input   aria-checked= false   bind:checked={checkedAll}  on:click={onCheckedAll}/&gt;Như trên biến checkedAll đã được bind vào checkbox. Thậm chí cũng chả cần định nghĩa id cho textbox (tức là đánh nhãn) làm gì, bạn chỉ cần quản lý biến trạng thái checkedAll trong code của mình. Mà thao tác ấy chắc chắn sẽ tự động và ít bugs hơn đánh nhãn bằng tay kiểu JS thuần. Khi biến trạng thái thay đổi, hiển thị checkbox sẽ tự động thay đổi theo và bạn chẳng phải làm mấy cái thao tác set props hay gì mất thời gian mà lại dễ lỗi. Ví dụ bạn để set prop nhầm chỗ hoặc assign nhầm label một phát vì cách đặt tên của lập trình viên cũ quá tồi thì ôi thôi có mà … toang nặng. Đến đây, việc còn lại quá nhẹ nhàng với vài dòng code chơi: bạn định nghĩa một mảng checked cho trạng thái của $n$ biến trạng thái còn lại và bind:checked={} vào các checkbox ở từng dòng. Để thực hiện logic chuyển trạng thái, bạn chỉ cần lo viết code tương tác giữa biến $C$ và $c_i, i=1,n$ chứ chả phải lo ngồi set prop, hoặc set text làm gì mất công và dễ bug. Kết luậnNhìn chung cách làm bằng JS thuần là cách làm thiếu tính trừu tượng hóa, thủ công và đánh nhãn mệt nghỉ. Thay vào đó sự tiến bộ của công nghệ những năm 2010-2014 đã cho phép lập trình frontend web có những framework kiểu MVVM đủ mạnh như AngularJS, VueJS, ReactJS và gần đây là Svelte. Sử dụng Svelte để trừu tượng hóa trạng thái hiển thị thành biến (qua binding) và quản lý trạng thái thông qua quản lý biến là một điểm chốt khiến cho việc sử dụng Svelte (cũng như VueJS, ReactJS, AngularJS) chiếm ưu thế vượt trội so với cách làm thủ công JS thuần mà 10 năm trước chúng tôi hay làm. Tài liệu tham khảo      Docs • SvelteKit &#8617;    "
    }, {
    "id": 75,
    "url": "https://wanted2.github.io/write-logs/",
    "title": "Học cách viết log nghiêm chỉnh",
    "body": "2021/12/05 - Viết log nghe như một việc tưởng chừng vô cùng đơn giản, với kỹ sư SRE thì có khi chỉ 2-3 giờ đồng hồ là có thể cấu hình chuẩn chỉnh được ngay một hệ thống log ngon nghẻ. Thế nhưng cũng có những dự án chiến lược cấp tập đoàn mà đôi khi phải dành hẳn cả 6 tháng 1 năm để cho các kỹ sư học cách viết log cho nó OK em. Và quản lý log, bao gồm cả cấu hình một hệ thống log nghiêm chỉnh để có thể audit và đối ứng sự cố tốt không phải là 1 việc đơn giản: tức là tốn công. Nhưng các bạn biết đấy, audit hay đối ứng sự cố là những nghiệp vụ không tạo ra dịch vụ sản phẩm gì mới cả, mà tập trung hơn vào việc fix lỗi, chỉ lỗi, tức là một thứ cần thiết nhưng ít thì tốt hơn. Và lại còn bới lông tìm vết nữa thì lại còn vấn đề mất lòng đồng nghiệp các cái, tóm lại là một thứ nghiệp vụ vô cùng “nửa nạc nửa mỡ”, “bỏ thì thương mà vương thì tội (trích Tam quốc diễn nghĩa)”. Và cũng chính vì vậy, trong nghiệp vụ quản lý dự án nói chung, thay vì để PM hay quản lý phải làm hết những nghiệp vụ “mất lòng” này, thì tốt nhất vẫn là nên bắt các ông dev tự mà quản lý log, tự audit và tự đối ứng sự cố thì sẽ tránh cãi vã mất thời gian mà tóm lại, code ông nào thì tự ông ấy audit log và đối ứng đi. Và như vậy đòi hỏi phải có chuẩn (standard) cho việc quản lý log, đối ứng sự cố, cũng như tutorials và training để các em dev viết log cho nó nghiêm chỉnh. Và cũng thú thực luôn là chuyện dành ra 6 tháng đến 1 năm bắt các kỹ sư chỉ học cách viết log cho nghiêm chỉnh là chuyện có thật, mà không phải ở cấp vớ vấn mà ngay cấp doanh nghiệp tập đoàn! Vai trò và nhiệm vụ của log: auditting and incident response Một nhân vật mà không ai yêu quý vì chẳng tạo ra giá trị gì mới, nhưng lại tiêu tốn tài nguyên và khiến những người khác phải chạy theo làm vừa lòng. Trước khi thảo luận xem nên viết log ra sao, thì có lẽ chúng ta cũng cần phải hiểu xem tại sao lại phải viết log? Ai sẽ đọc log? và các hình thái quản lý log. Tại sao phải viết log?: Mà đúng hơn là tại sao phải viết, lưu trữ và quản lý log?Log đã trở thành 1 phần không thể thiếu của mọi hệ thống phần mềm.  Lịch sử và đối ứng sự cố: Log lưu trữ các thông tin về các sự kiện đã từng xảy ra trong dự án và là căn cứ để điều tra cũng như phản ứng khi có sự cố xảy ra.  Phân tích (analytics): Ngoài ra, định kỳ kiểm tra và phân tích log có thể giúp người quản lý dự án nắm rõ bức tranh lớn của dự án. Những ứng dụng Log analytics trên các nền tảng Big Data như Azure App Insights dần thể hiện vai trò quan trọng trong nhiều tổ chức. Trên đây là 2 nhánh quan trọng nhất trong các hệ thống ứng dụng có liên quan tới Log Management. Một dự án của Cơ quan Tiêu chuẩn Kỹ thuật Hoa Kỳ (NIST) có liên quan tới Log Management1 cũng đang cố gắng cải thiện nền tảng phân tích log tập trung trong các cơ quan thuộc khối chính phủ Hoa Kỳ. Nhìn như trên chúng ta có thể thấy ở cấp quân sự và chính phủ, hoặc các tập đoàn kinh tế lớn việc lưu trữ và quản lý log quan trọng đến chừng nào. Dựa trên đó mà các khối này đang cải tiến và xây dựng nền tảng quản lý dữ liệu của chính phủ Hoa Kỳ ngày càng hoàn thiện và tiên tiến hơn. Hàng loạt sự cố như rò rỉ dữ liệu người dùng với khối lượng lớn, đột nhập tấn công nền tảng, … đã và đang xảy ra trên quy mô toàn cầu, đòi hỏi các cơ quan và tổ chức càng ngày phải nâng cao ý thức và trách nhiệm với các sự cố cybersecurity và một trong số những biện pháp đầu tiên chính là làm cho nghiêm chỉnh việc quản lý log (log management). Vậy với các tổ chức dân sự cấp vừa và nhỏ (中小企業), hoặc người dùng cá nhân thì sao?Họ có nên áp dụng tiêu chuẩn quản lý log cấp chính phủ và quân sự vào việc kinh doanh của mình?Câu trả lời là điều đó tùy thuộc vào nguồn lực. Nếu các tập đoàn lớn, cơ quan chính phủ hoặc quân sự có nguồn lực dồi dào, có thể đòi hỏi kỹ sư hàng tháng trời chỉ ngồi học viết mấy dòng log cho nghiêm chỉnh (nói vui vậy thôi chứ phía sau thì còn nhiều thứ, sẽ giải thích ở các section tiếp theo). Thì các tổ chức dân sự vừa và nhỏ lại không hẳn như vậy, họ ít nguồn lực hơn và nếu không có lợi ích gì lớn thì có lẽ họ sẽ không làm. Hoặc các tập đoàn đặt hàng họ làm (khách hàng) sẽ phải làm những bước log management này. Ai sẽ đọc log?: Một sự thật khá đau lòng ở đây là, mặc dù log rất quan trọng cho quá trình sửa chữa bảo trì hệ thống, khắc phục sự cố, nhưng end-user hầu không bao giờ đọc log. Bạn là người dùng FaceBook nhưng đã bao giờ được đọc 1 dòng log do kỹ sư FaceBook viết còn “vương vãi” trên giao diện Facebook (viết chủ yếu bằng JS) chưa?Nếu kỹ sư FB mà để lọt dòng log nào vào mắt bạn thì đó đúng là 1 sự cố to lớn. Có nhiều lý do cho việc này: thứ nhất, người dùng cuối không cần phải biết về chi tiết trong hệ thống. Họ có trả tiền để phải ngồi đọc log khi có sự cố không?Thường là không. Vậy developer có cần phải đọc log không?Câu trả lời là nên, nhưng developer thì còn phải viết code, làm các nghiệp vụ dự án, và nhiều việc ưu tiên hơn. Họ có thể sẽ ngồi cấu hình để sinh ra log, để viết ra log, nhưng có lẽ họ sẽ ít khi đọc. Vì sao? vì trong hàng đợi thì đọc log chính là nhiệm vụ ít giá trị nhất:  theo nghĩa nó không tạo ra giá trị mới đọc để điều tra fix lỗi thì bản thân cái lỗi nó đã là minus rồi, thì điều tra cũng chỉ là để “cứu lại” thôi, chứ đâu có tạo ra giá trị mới. Nhưng chúng ta cũng cần hiểu: “ít giá trị nhất” hiểu theo nghĩa lợi ích trước mắt. Còn về lâu dài, nếu chịu khó đọc và lưu thành các bài học kinh nghiệm để ngăn chặn sự cố tương tự xảy ra trong tương lai thì nó cũng có 1 chút giá trị. Tuy nhiên, bạn thấy đấy: với developer khi hàng đợi đang còn nhiều việc implementation chức năng mới thì mấy cái việc monitor, audit logs, đọc logs, phân tích logs chắc chắn sẽ bị biến thành nhiệm vụ thứ yếu. Mà nhìn chung theo kinh nghiệm của tôi thì thường sẽ là như vậy. Tester hay QA thì cũng chả bao giờ phải đọc log. Vì sao? Vì họ chủ yếu là dựa trên spec có sẵn để chỉ ra lỗi, chứ không phải điều tra, phân tích và đối ứng sự cố. Role này chỉ cần chỉ lỗi so với spec và thế là xong. Vậy cuối cùng ai sẽ là người hay phải đọc log nhất?    Quản lý dự án thực hiện nghiệp vụ audit.   Điều tra viên, kiểm toán cấp cao do bên trên cử xuống theo dõi và điều tra tình hình dự án.   Các chuyên viên phân tích, lập chiến lược ngắn hạn và dài hạn, …  Các điều tra viên phải ứng phó với sự cố.  Đấy có khoảng chục ông như vậy thôi. Và nếu bây giờ mà tự dưng phát sinh ra dự án thu thập dữ liệu log, xây dựng infrastructure và ứng dụng để phân tích theo dõi log, lên chiến lược kế hoạch thì chắc chắn số lượng end-users thực sự của ứng dụng ấy chỉ tầm $\leq 10$ ông thôi.  Ứng dụng log management chính là TẤT CẢ VÌ MỘT NGƯỜI: chỉ vì 10 ông mà phải huy động hết tinh anh ra trận. Các cấp độ quản lý log: Theo Bản ghi nhớ M-21-31 Improving the Federal Government’s Investigative and Remediation Capabilities Related to Cybersecurity Incidents2 giữa các lãnh đạo cấp cao các cơ quan chính phủ Hoa Kỳ liên quan tới điều tra và nâng cấp khả năng ứng phó liên quan tới các sự cố an ninh mạng, các cấp độ quản lý log được chia làm 4 cấp độ từ không hiệu quả tới cấp advanced như sau:       Cấp độ   Hạng   Mô tả         EL0   Không hiệu quả   Ở cấp này, các thông tin cần thiết ứng với các sự cố nguy hiểm (highest criticality) không được quản lý log hoặc chỉ được quản lý một phần không đầy đủ.        EL1   Basic   Các sự kiện ở cấp highest criticality được log đầy đủ và quản lý hoàn chỉnh.        EL2   Intermediate   Không chỉ các sự kiện cấp highest mà kể cả cấp intermediate cũng được quản lý log đầy đủ.        EL3   Advanced   Mọi sự kiện criticality đều được quản lý log.    Trong bản ghi nhớ nói trên, cũng đã thể hiện sự đồng thuận giữa các cơ quan chủ quản rằng ở cấp chính phủ (bao gồm cả quân sự) của Hoa Kỳ thì phải đạt đến cấp EL3. Ngoài ra, lộ trình tính từ ngày ký vào bản ghi nhớ được thống nhất như sau:  Trong vòng 60 ngày kể từ ngày ký phải bắt đầu thực hiện các biện pháp cải thiện hệ thống quản lý log trong cơ quan chủ quản. Việc này thể hiện qua việc chuẩn bị resources, lên kế hoạch và phải nộp cả bản kế hoạch thực hiện cho RMO và OFCIO.  Trong vòng 1 năm kể từ ngày ký, các cơ quan có liên quan phải đạt đến level EL1.  Trong vòng 18 tháng kể từ ngày ký, các cơ quan có liên quan phải đạt chuẩn EL2.  Trong vòng 2 năm kể từ ngày ký, các cơ quan liên quan phải đạt đến cấp EL3, cấp cao nhất thể hiện 1 hệ thống log management hoàn chỉnh.  Log được quản lý phải được cung cấp chia sẻ cho các cơ quan điều tra như CISA hay FBI để phục vụ ứng phó với các nguy cơ đến từ không gian mạng. Yêu cầu của một hệ thống log hoàn chỉnhPhân loại nguy cơ: Trong tài liệu tiêu chuẩn của NIST [1] và thậm chí trong bản ghi nhớ2 thì ít nhất ở cấp EL1 là phải quản lý log đầy đủ với mọi nguy cơ ở cấp (highest criticality) hay Criticality 0. Những thông tin ở các cấp như sau:       Criticality   Log data   Tần suất lưu trữ         0   Identity and Credential Management, Privileged Identity and Credential Management, IP and DOmain Reputation (As Indicated by Mail Server Connection), DNS - Source IP and Port, Destination IP/Port, Date and Time, Passive DNS Log, DNS/DHCP/Wifi, Routers and Switches, Load Balancer/Reverse Proxy, Proxies and Web Content Filters, General Information, OS events, OS audit records, App Account Information, Application Operations, Access to OS components and apps, System performance and operation characteristics, system configs, file access, host network communications, powershell execution history, WMI events, Registry Access, CLI, BIOS, UEFI, and other firmware   6 tháng đến 12 tháng với Active STorage và 18 tháng với Cold Storage   Cấp 0 này có 1 điểm thú vị là những log tập trung trên cloud cũng phải được lưu trữ tối thiểu 12 tháng trên Active Storage. Có 2 ngoại lệ nhỏ:  Dữ liệu IP package data thì chỉ cần lưu 72h.  Dữ liệu log của Google Cloud Platform thì chỉ cần lưu 6 tháng nếu dùng Active Storage. Nhìn chung bây giờ các bạn cũng sử dụng cloud là chính nên những recommendations từ bên chính phủ Hoa Kỳ tôi cho là rất phù hợp và nên được xem xét. Với ứng dụng thì nên log lại mọi events như API logs, authentication, authorization, . etc. Ngoài cấp Criticality 0 thì các cấp thấp hơn là Criticality 1, 2, và 3. Ở cấp EL1 đòi hỏi phải chấn chỉnh toàn bộ việc quản lý log ở cấp 0 (highest). Đến cấp EL2 doanh nghiệp tổ chức buộc phải thực hiện quản lý mọi log ở cấp 0 và 1. Còn đến cấp EL3 mặc định là mọi criticality phải được log. Và Log ở đây không chỉ là log generation, mà còn là lưu trữ, phân tích, tìm kiếm ở mức đủ để đối ứng sự cố hoặc lên chiến lược. Nhìn chung khuyến khích các bạn nên tìm đọc thêm tài liệu hướng dẫn tiêu chuẩn quân sự hoặc chính phủ của NIST hoặc FBI. Yêu cầu thực thi ở các cấp EL: Nói chung ở mọi cấp thì yêu cầu cơ bản nhất là phải thực thi các quy tắc log và thu thập log về 1 điểm tập trung duy nhất (Centralized logging system). EL1: Ở cấp EL1 những yêu cầu ở mức chỉ cần đối ứng với các nguy cơ cấp Criticality 0. Tuy nhiên, ngoài ra vẫn còn 1 đống các tiêu chí cần phải thỏa mãn. Đầu tiên là log cái gì? thì tối thiểu phải log những thông tin sau:  Log phải được định dạng sẵn và timestamps phải được đồng bộ về cùng dạng biểu diễn.  Mã trạng thái của sự kiện (mã lỗi chẳng hạn).  ID của thiết bị như địa chỉ MAC.  Session / Transaction ID Autonomous System Number Source IP (IPv4) Source IP (IPv6) Destination IP (IPv4) Destination IP (IPv6) Status Code Response Time Các headers bổ sung (i. e. , HTTP headers) Khi thích hợp, có thể log username/userID nhưng cần chú ý về tính an toàn dữ liệu người dùng.  Khi thích hợp, nên log cả command đã thực hiện ra. Như SQL statement cũng nên log ra.  Nên log ở dạng key-value cho anh em dễ xử lý nhá :-) Nếu được nhớ bỏ event ID để anh em correlation về sau!Đấy các trường cần có trong logs ít ra cũng phải được như trên. Timestamps cũng là một vấn đề đau đầu của log. Tôi từng gặp khá nhiều hệ thống log sử dụng Date/DateTime nhưng nhìn chung không hợp lý. Theo khuyến cáo của NIST chính phủ Hoa Kỳ thì tất cả Internet đều là Timestamps. Bạn có thể tham khảo thêm RFC3339. Có 2 dạng sau hay được dùng:  YYYY-MM-DDThh:mm:ss. mmmZ (Zulu time, UTC+0) và YYYY-MM-DDThh:mm:ss. mmm+04:00 (UTC+4)Nếu bạn nào chưa biết UTC hay chữ Z trên có ý nghĩa gì thì nó đơn giản là Zulu time thôi. Trong trường hợp các bạn tự thuê server mà build service thì NIST cũng cẩn thận khuyến cáo trong việc đồng bộ timestamps:  Agencies shall use a GPS master station clock as a baseline reference for timestamps used for logs and systems producing logs. If GPS reference is not possible, agencies shall use NIST’s authenticated time service. Public, unauthenticated, and unencrypted NTP pools shall only be used as an option of last resort, and only for as long as needed to begin leveraging other options. Nhưng nhìn chung là cứ deploy hết lên Cloud rồi cấu hình timezone cho đỡ đau đầu. Nhớ là nếu tự build thì đừng dùng mấy cái NTP server khả nghi là ok! Event forwarding: một tình huống hay gặp nữa là một agent được cài vào máy đối tượng thu thập logs, và sẽ forward toàn bộ log về server xử lý trung tâm. Event forwarding cũng là 1 yêu cầu trong EL1. Bảo vệ và xác thực logs: ở cấp này yêu cầu hạn chế truy cập vào logs với đối tượng nhất định (phân quyền), phải mã hóa logs để tránh rò rỉ thông tin người dùng. Việc bảo vệ logs là 1 thử thách lớn trong log management vì đôi khi có lưu thông tin người dùng vào logs. Phải định kỳ check logs xem có bị mất, hỏng gì không. Phải monitor liên tục và khi tự nhiên log dừng là phải có 1 đội phản ứng nhanh nhảy vào đối ứng kiểm tra ngay. Các tổ chức chính phủ hay quân sự thì nên log cả DNS requests. Điều này là cần thiết cho FBI và CISA tiện trong việc điều tra. Các tổ chức điều tra cảnh sát cấp liên bang như CISA hay FBI nên có quyền truy cập gần như lập tức vào dữ liệu log để tiện việc điều tra truy tìm tội phạm. Playbooks là công cụ tuyệt vời để thực hiện tự động hóa đối phó sự cố, như với SOAR. Ở cấp EL1 tổ chức nên có kế hoạch theo dõi hành vi người dùng. Và đương nhiên, logs nên được quản lý tập trung:  Logs should be centrally aggregated by an agency component-level Enterprise Log Manager (ELM). Traps for detecting data-stream disruption should be monitored by the component-level SOC. The DNS logging system and accompanying analytics shall be monitored and triaged by the component-level SOC. EL2: Về cơ bản, tổ chức được yêu cầu thực hiện đầy đủ các yêu cầu với EL1 cho cả Criticality 0, 1 và 2. Ngoài ra, tổ chức nên xuất bản 1 hướng dẫn về cách thức tổ chức cũng như định dạng của logs.  Federal agencies shall provide a document detailing the structure (schema) for those logs to CISA. Thêm nữa là tổ chức nên có những biện pháp để kiểm tra những encrypted package.  Federal agencies shall retain and store in cleartext form the data or metadata from Appendix C that is collected in their environment. If agencies perform full traffic inspection through active proxies, they should log additional available fields as described in Appendix C and can work with CISA to implement these capabilities. If agencies do not perform full traffic inspection, they should log the metadata available to them. In general, agencies are expected to follow zero-trust principles concerning least privilege and reduced attack surface, and relevant guidance from OMB and CISA relating to zero-trust architecture. Cuối cùng là top manager của SOCs nên có quyền truy cập vào mọi thành phần của quản lý logs. EL3: Ở cấp này thì khỏi phải nói, mọi criticality phải được ứng phó!Việc hunting threats và sử dụng incident response playbooks là phải hoàn chỉnh và thành thục. Việc monitor user behavior để phát hiện sớm nguy cơ là bắt buộc. Một số sự kiện nên bắt với hành vi người dùng:  Compromised user credentials Privileged-user compromise Improper asset access Compromised system/host/device Lateral movement of threat actorCuối cùng là ưu tiên sử dụng thành thục SIEM, và hệ thống phân tích xử lý logs tập trung. Thiết kế một hệ thống quản lý log nghiêm chỉnh: Kiến trúc tổng quan: Kiến trúc tổng quan của 1 hệ thống quản lý logs thường bao gồm:    Log Generation: những cái hosts sinh ra logs. Chúng có thể bao gồm backend apps, frontend apps hoặc bất kỳ thể loại apps nào mà có hoạt động trong hệ thống như send emails, … Các hosts này có thể gửi log cho các components khác của hệ thống (collectors và aggregators) để xử lý, hoặc chúng cho phép các components khác truy cập vào và copy logs từ host generation ra.     Log Analysis and Storage: Nếu dùng AWS bạn có thể xài Watchlogs, còn Azure thì Logs Analytics hoặc App Insights, … Nhìn chung là bộ phận nhận dữ liệu từ Log Generation và phân tích lưu trữ.     Log Monitoring: Có 1 cái dashboard để monitor (near-)real-time logs và review log events.  12345678910------------------    ----------------    ------------------| Log Generation | ====&gt; | Log Analysis | ====&gt; | Log Monitoring ||        |    ----------------    ------------------|        |        /\             /\|        |        ||             |||        |        \/             |||        |    ----------------         |||        | ====&gt; | Log Storage | =================|        |    ----------------------------------Các chức năng chính: Nhìn chung một hệ thống log management hoàn chỉnh nên có tối thiểu các chức năng sau:       Nhóm chức năng   Chức năng   Mô tả         General   Log parsing   Extract data from a log so that the parsed values can be used as input for another logging process.            Event filtering   Suppression of log entries from analysis, reporting, or long-term storage because their characteristics indicate that they are unlikely to contain information of interest.            Event aggregation   Similar entries are consolidated into a single entry containing a count of the number of occurrences of the event.        Storage   Log rotation   Close a log file and opening a new log file when the first file is considered to be complete. Log rotation is typically performed according to a schedule (e. g. , hourly, daily, weekly) or when a log file reaches a certain size. The primary benefits of log rotation are preserving log entries and keeping the size of log files manageable. When a log file is rotated, the preserved log file can be compressed to save space.            Log archival   Retain logs for an extended period of time, typically on removable media, a storage area network (SAN), or a specialized log archival appliance or server. Logs often need to be preserved to meet legal or regulatory requirements.            Log compression   Store a log file in a way that reduces the amount of storage space needed for the file without altering the meaning of its contents. Log compression is often performed when logs are rotated or archived.            Log reduction   Remove unneeded entries from a log to create a new log that is smaller. A similar process is event reduction, which removes unneeded data fields from all log entries.            Log conversion   Parsing a log in one format and storing its entries in a second format.            Log normalization   Each log data field is converted to a particular data representation and categorized consistently. One of the most common uses of normalization is storing dates and times in a single format.            Log file integrity checking   Calculate a message digest for each file and storing the message digest securely to ensure that changes to archived logs are detected.        Analysis   Event correlation   Find relationships between two or more log entries. The most common form of event correlation is rule-based correlation, which matches multiple log entries from a single source or multiple sources based on logged values, such as timestamps, IP addresses, and event types. Event correlation can also be performed in other ways, such as using statistical methods or visualization tools.            Log viewing   Display log entries in a human-readable format.            Log reporting   Display the results of log analysis. Log reporting is often performed to summarize significant activity over a particular period of time or to record detailed information related to a particular event or series of events.        Disposal   Log clearing   Remove all entries from a log that precede a certain date and time. Log clearing is often performed to remove old log data that is no longer needed on a system because it is not of importance or it has been archived.    Usecase: Cấu hình hệ thống log với một website viết bằng PythonLên kế hoạch thực thi: Bước thiết kế như đã nói ở trên là theo tiêu chuẩn rồi. Giờ tiến hành làm ta cần lên kế hoạch, làm rõ các tham số như:  Thời gian lưu trữ logs là bao lâu? Tần suất rotate là bao lâu một lần? Tần suất gửi log về Log Management System (LMS) Tần suất xử lý log ở local Có cần validate logs trước khi rotate? Có cần mã hóa log ở local hay không? Khi gửi log về LMS có cần mã hóa đường truyền hay dùng đường truyền riêng biệt để gửi hay không?Bạn cần phải đánh giá impact của hệ thống từ low, medium và high impact để quyết định tham số. Tất nhiên, implement thì vẫn là dùng công cụ có sẵn thôi. Như Python thì dùng logging của STL cho yên tâm về nguồn gốc xuất xứ (dùng third-party thì lại lo xuất xứ). Còn quản lý log tập trung thì anh em lại CLoudWatch hoặc Logs Analytics mà tương thôi! Lên kế hoạch vận hành: Có 4 việc nên làm:  Cấu hình các log sources, bao gồm cả log generation, storage, và security     Cái này thì anh em dùng logging của Python, để rotate thì nhớ dùng cái RotatingFileHnadler hoặc TimeRotatingFileHandler.    Để quản lý tập trung thì nên gửi vào Azure App Insights, hoặc deploy thẳng lên Azure WebApps thì nó lên thẳng luôn, nhớ enable chức năng App Insights là ổn.    Storage thì đã dùng nền tảng Azure Monitoring thì nó bill bạn theo GB/tháng.    Về security thì chính là security cho Azure account của bạn. Đọc thêm hướng dẫn của M$ để biết.     Phân tích log data     Cái này thì khuyên nên dùng những cái board kiểu như App Insights, bắn log là nó lên, ngồi tương tác chỉ trỏ rồi xem dữ liệu thôi.    Bạn có thể dùng query logs.    Bạn có thể làm thêm dashboard nếu cấu hình vào Power BI.     Initiate appropriate responses to identified events     Playbooks không lằng nhằng.    Nên quyết định rule ứng phó và chia ca để các em nó thay nhau trực.    Nhớ để các em nó testing và validation.     Manage the long-term storage of log data     Nhớ archive log đều đặn.    Nhớ tạo format chung cho toàn bộ log được lưu trữ.    Nhớ tạo message digest cho log records để đảm bảo integrity.    Nhớ mã hóa ổ đĩa nhé.    Kết luậnChúng ta đã điểm qua cách xây dựng và vận hành một hệ thống quản lý logs theo tiêu chuẩn liên bang Hoa Kỳ. Tài liệu giới thiệu ở đây có sẵn file PDF tầm trăm trang, các bạn nên dành thời gian đọc kỹ và thực hành để hiểu thêm nếu cần. Nói chung cũng tùy tình hình mà có những trường hợp phải dành ra một dự án vài năm cũng chỉ để học cách viết log cho nghiêm chỉnh. Thế nên có cơ hội các bạn nên trau dồi, vì thực sự cái tổng thời gian cấu hình chắc chỉ vài tiếng! Tài liệu tham khảoKent, K. A. and Souppaya, M. 2006. Guide to Computer Security Log Management: Recommendations of the National Institute of Standards and Technology. US Department of Commerce, Technology Administration, National Institute of Standards and Technology. Details      https://csrc. nist. gov/Projects/log-management &#8617;        M-21-31 &#8617; &#8617;2    "
    }, {
    "id": 76,
    "url": "https://wanted2.github.io/fullstask-engineer-and-lean-startup/",
    "title": "Why lean startups love fullstack engineers?",
    "body": "2021/11/27 -  Lean1 is the most widespread management philosophy of our time and is currently present in every industry, yet the concept is still vaguely defined and widely misunderstood.  Efficiency paradox … the idea of the efficiency paradox, which claims that organisations’ understanding of “true efficiency” is incorrect. It suggests that when organisations focus too much on utilising resources efficiently – the traditional and most common form of efficiency – it tends to lead to an increase in the amount of work there is to do. Consequently, the more organisations try to be efficient (being busy), the more inefficient they will actually become (taking care of non-value adding but necessary work). This is Lean addresses +how to resolve this paradox. Bắt đầu từ gần 10 năm trước, năm 2011, bởi Eric Ries, Lean đã trở thành một phương pháp luận để cải tiến hiệu suất công việc và số lượng startups là Lean startups ngày càng gia tăng cho thấy sự yêu chuộng dành cho Lean từ cộng đồng khởi nghiệp. Nếu bạn đã từng có một giai đoạn khủng hoảng: ngày nào cũng bận bịu và phải làm tăng ca (OT) đến tận khuya, hoặc thậm chí phải mang việc về nhà làm, bạn đã rơi vào efficiency paradox: bạn luôn bận rộn, nhưng chả hoàn thành (release) cái gì cả. Thực ra vế sau chính là dấu hiệu (triệu chứng bệnh) của efficiency paradox, và khi có triệu chứng bận bịu nhưng chả done cái gì, đó chính là lúc Lean phải thể hiện value. Nếu cải tiến hiệu suất là mục tiêu của Lean, thì cách thức thực hiện lại vô vùng đòi hỏi sự tỷ mỷ chi tiết. Một trong số những tiêu chí quan trọng để build 1 team khởi nghiệp Lean, chính là sự tồn tại của cross-functional team hay tức là fullstack engineers. Lý do tại sao là bởi vì như bạn có cái kìm và cái búa, khi cần đóng đinh thì búa sẽ dùng nhiều hơn và kìm hầu như không có việc, nhưng khi cần vặn ốc thì kìm bận bịu nhưng búa chả có việc gì: vậy tại sao không kiếm một cái vừa là kìm vừa là búa?  Trong khởi nghiệp Lean, khi nói đến từ FE mà bạn hiểu là Frontend Engineer thì không có value mấy, phải hiểu là Fullstack Engineer thì mới có nhiều value. Efficiency Paradox: Triệu chứng bệnhTriệu chứng bệnh của efficiency paradox thì không chỉ có mỗi triệu chứng bận mà chả làm ra cái gì như đã nói ở trên.  Triệu chứng 1: Bận mà chả release được cái gì? Tức là hiệu suất resource cao gần 100% nhưng lead time (thời gian từ ideas tới release) lại dài mà đôi khi là infinite! Tức là có thể có release nhưng nhìn chung là lead time rất lớn.  Triệu chứng 2: Release nhanh nhưng có nhiều người không có việc. Tức là team chia việc và hoàn thiện nhanh, nhưng có nhiều ムダ (tiếng Nhật nghĩa là lãng phí) tức là nhiều bạn chả có việc gì làm, mà nếu cho các bạn ấy làm việc khác thì lại phải ngồi điều tra lại từ đầu, nó lưng chừng dở dang. Dù là triệu chứng 1 hay 2 thì chắc chắn khởi nghiệp đang ở chỗ loay hoay giải quyết bài toán cân bằng hiệu suất sử dụng (tăng hiệu suất sử dụng nhân lực) và tỷ lệ hoàn thiện (rút ngắn thời gian release). Lean startupsNhận xét về Lean: Và để giải quyết bài toán hiệu suất, phương pháp cải tiến của Eric Ries bắt đầu từ những năm 2011 mang tên Lean startups là một phương pháp được ưa chuộng gần đây.  Ưu điểm lớn nhất của Lean chính là vòng tuần hoàn liên tục Build-Measure-Learn khiến cho team dần trưởng thành và hiệu suất gia tăng. Teams có thể chưa hoàn chỉnh trong vòng thứ nhất nhưng sẽ learn và lặp lại quá trình build, measure đển khi hoàn thiện.  Nhược điểm lớn nhất là việc phụ thuộc quá lớn vào skill của mỗi members: thợ học việc cỡ sinh viên, những nhân viên chỉ biết làm đúng 1 việc frontend hoặc backend, hoặc chỉ biết code mà không làm những quy trình báo cáo, tuyển dụng, … sẽ là chướng ngại mà sớm muộn cũng bị đào thải.  Thiếu tập trung là tử địa của Lean. Nếu 1 members trong Lean startups không toàn tâm toàn ý 100% cho startup mà lại phân tâm sang 1 dự án khác 50-50 hoặc 1 cty khác 50-50, Lean chắc chắn fail! Tại sao? Bởi vì vòng quay của Lean đòi hỏi mọi members trong teams phải toàn tâm toàn ý cho việc cải tiến, theo dõi, và đối ứng. Chính việc phải toàn tâm toàn ý khiến cho việc cùng lúc chạy 2 dự án là vô vùng khó khăn nếu 1 trong 2 cái là Lean.  Nuôi Lean như nuôi Thánh Gióng: cứ phải nuôi nó từ từ, và trong quá trình nuôi là phải theo dõi chăm chút từng ly từng tý. Nhưng khi hữu sự, thì phải cho nó ăn hàng gánh cơm, hàng nong đậu phụ thì nó lại lớn nhanh như thổi, một phát đánh đuổi quân ngoại xâm. Nuôi Lean chính là nuôi một đứa trẻ như vậy! Mà bà mẹ trong chuyện cổ tích Thánh Gióng là cứ phải nuôi nó đến lúc thành công, mới gọi là gặt hái. Với Lean, đó chính là lúc mà vòng quay vừa nhanh, teams luôn có việc mà tốc độ release cũng flash. Tất nhiên, thực ra trong đời thực, có những người vẫn có 2-3 đứa con, mà đứa nào cũng là “Lean”, tức là kiểu trẻ phải được kèm cặp từ nhỏ. Điều đó dẫn đến chuyện ôm cùng lúc 3 ông con, mà ông nào cũng là Lean cũng là chuyện bình thường. Nhưng sẽ rất mệt, mà nhìn chung là sẽ phải đến lúc nào đó bắt buộc phải kệ chúng nó tự lớn. Lợi ích của Lean: Nếu bạn nuôi được startup Lean đến lúc trưởng thành thì quả ngọt vô số. Lean startup khi trưởng thành thì vô vàn lợi ích mà người chăn nuôi có thể nhận được:    Improving the quality of work processes hay Cải tiến chất lượng quy trình  Reducing errors or defects in work processes hay Giảm thiểu lỗi quy trình  Reducing costs hay Giảm thiểu giá thành  Improving flow of the process hay Cải tiến flow làm việc  Simplifying complex processes hay Đơn giản hóa quy trình  Reducing lead time hay Đẩy nhanh thời điểm release  Improving employee morale hay Tăng động lực làm việc cho nhân viên Ứng dụng Lean: Để áp dụng Lean vào quy trình phần mềm (software development), việc triển khai các hạng mục trong Build-Measure-Learn là cần thiết, nhưng cũng phải tùy vào tình hình của team.  Đầu tiên, phải xác định vấn đề: Muda phát sinh từ đâu? Nguyên nhân là do vấn đề gì? Xây dựng quy trình tiêu chuẩn: mọi hoạt động của team cần được chuẩn hóa và tốt nhất nên dùng tự động hóa.  CI/CD là sống còn, chất lượng phải được cải tiến và trực quan hóa cho mọi cấp quản lý cũng như members.  Nên có quy trình “kéo” tức là trigger của quy trình này phải đến từ nhu cầu của quy trình tiếp theo. Nếu như nhìn vào biểu đồ trên, bạn thấy ngay luồng chạy của Lean startups là liên tục không gián đoạn, nhưng dữ liệu thu thập được dùng để trực quan hóa sẽ chuyển biến thành các bài học để cải tiến team. Rất tiếc, trong thực tế mặc dù Lean startups được bảo đảm sẽ hội tụ, nhưng quy trình sẽ có thể rất lâu tùy vào xuất phát điểm của teams:  Chất lượng members: các bạn đã nhiều năm kinh nghiệm dev chưa hay chỉ là sinh viên? Đã quen thực chiến, đa dzi năng chưa hay chỉ biết gán cho các mác frontend là ông suốt ngày chỉ làm frontend? Sự đa năng: Gán cho cái mác coder thì suốt ngày chỉ biết code mà chả biết làm báo cáo, chả biết quy trình team, chả biết gì đến các nghiệp vụ quản lý, tuyển dụng khác, … Tốc độ học của teams: nếu teams chậm tiếp thu, tốc độ xử lý point thấp (chỉ vài điểm một tuần) thì sẽ càng lâu đến trưởng thành.  Chất lượng chia việc: tôi gặp khá nhiều chứ không phải 1-2 trường hợp, là cứ đẩy việc cho member chất lượng thấp, nhưng lại bắt một cái worker năng lực thấp phải tính toán một khối công việc rất lớn mà lúc nó tính lâu mất thời gian thì lại sốt ruột, giục, …Muốn nhanh thì phải chia nhỏ, đơn giản hóa (đúng theo tinh thần của Lean nhé), chứ phức tạp to tát mà giao cho workers cấp thấp là … hỏng việc đấy!Đôi khi cũng là kinh nghiệm làm việc bên giới outsource thôi, là các bạn outsource chưa làm quen nhiều với mô hình Lean startups. Do đó khi ốp các bạn vào khung Lean, các bạn thường sẽ hiệu suất rất kém, thậm chí cảm thấy tự ti vì mô hình Lean đòi hỏi các bạn phải chủ động, quyết chiến, năng động, học hỏi nhanh, chịu áp lực tốt, …Nhưng sinh viên mình kể từ khi ngồi trên ghế nhà trường, tự học thì không có khả năng, năng động chủ động là không có nên hầu như thụ động tiếp thu theo kiểu Anh phải lead bọn em mới đi. Thế thì còn Lean cái nỗi gì hả em? Đây là vấn đề vô cùng đau đầu với sinh viên người Việt mà nhìn chung, mắng chửi thì bảo anh ác, nhưng mà không nện cho thì thôi còn Lean làm gì nữa? Chất lượng member nó chỉ được như thế thôi, mà rồi còn bao nhiêu vấn đề khác như đãi ngộ mặt bằng chung xã hội thấp, chế độ không tốt, …Nên nói chung đem Lean về VN là cũng có thể nhưng phải chấp nhận mất thời gian và phải hiểu là mất rất nhiều thời gian. Do đó khi làm việc với các KH mà kiểu họ hướng vào Lean, là theo kinh nghiệm của tôi là phải đánh chặn luôn: “Đây tình hình outsource VN nó chỉ được thế này thôi? Có đòi hỏi thì cũng rất mất thời gian. Các ông hiểu rồi thì nhảy vào chiến với anh em, còn không thì tôi nghĩ cũng không có giải pháp nào đâu với cái tình hình như thế này!. Chứ giờ có bảo thay đổi chế độ, thay đổi cả quốc tính dân tộc, … những chuyện to tát phức tạp như thế thì không làm được đâu! Fullstack engineers và cross-functional teamsMột đặc điểm quan trọng của các chiến binh trong dream team của Lean startup đó chính là cross-functional team, tức là các member phải đa năng: việc gì cũng biết, kể từ code backend/frontend, đến cấu hình infra, thiết kế hệ thống các cái, đến thậm chí những công việc ngoài luồng kỹ thuật như kế toán, sổ sách, làm thiết kế hệ thống basic/detail designs, quản lý, tuyển dụng phỏng vấn, … Điểm này khác với các tập đoàn công ty lớn, đòi hỏi các bộ phận phải có sự chuyên môn hóa: ông nào frontend thì cả đời cứ chạy frontend cho tôi. Anh cứ kệ em, em chỉ code thôi còn những việc khác em không biết. Thì những cái kiểu như thế chắc chắn không phù hợp với Lean startups. Các team Lean thường rất nhỏ, nhưng các member thì như biệt đội Avengers vậy, mỗi người đều có thể làm mọi khâu trong quy trình thậm chí thay thế khi đồng đội đang đảm nhiệm khâu đó nghỉ. Vì vậy, nếu có một member kiểu fullstack mà lại chơi được mọi “súng” thì không việc gì phải bàn. Ngược lại những member chỉ kiểu tròn vai thì về nguyên tắc của Lean startups là … không ổn. Kết luậnNói thì dài dòng, nhưng kết luận thì ngắn gọn thôi:  Lean startups là một phương pháp cải tiến hiệu suất cấp team nhỏ trên thế giới, nhưng đem về Vn thì phải chấp nhận rằng “chỉ được đến thế thôi”. Còn muốn hơn thì tốt nhất tuyển người qua nước tư bản kiểu chọn nhặt tài năng chứ còn cái kiểu đại trà theo outsource chỉ được đến thế thôi.  Muốn xây dựng dream teams cho Lean startups thì bắt buộc phải có vị trí Fullstack. Tài liệu tham khảo      https://thisislean. com/ &#8617;    "
    }, {
    "id": 77,
    "url": "https://wanted2.github.io/chicken-and-egg-problem/",
    "title": "Quả trứng và con gà: cái nào có trước? - Bất bình đẳng về lương và sản lượng lao động",
    "body": "2021/11/20 - Con gà và quả trứng là một bài toán kinh điển trong thống kê học cổ điển liên quan tới nhân quả (causality) và luân hồi (feedback) của các chuỗi thời gian (time series). Năm 1969, Granger (Nobel kinh tế 2003) xuất bản một seminar paper [1] định tính hóa nhân quả và luân hồi giữa các chuỗi sự kiện thời gian (temporal time series). Việc kiểm tra nhân quả và luân hồi giữa các chuỗi thời gian được định tính và định lượng thông qua Granger verification. Năm 1979, Feige và Pearce [2] nghiên cứu về mối quan hệ luân hồi giữa tiền tệ và thu nhập, có sử dụng Granger verification. Năm 1988, Thurman và Fisher [3] nghiên cứu chuỗi thời gian về sản lượng trứng cũng như chuỗi dữ liệu về số lượng gà trên toàn nước Mỹ để tìm ra quan hệ nhân quả giữa trứng và gà. Họ sử dụng công thức của Granger và lần đầu tiên kết luận mang tính thống kê rằng trứng có trước và là nguyên nhân sinh ra gà. Phân tích các chuỗi sự kiện thời gian (time series analysis) và nhân quả/luân hồi giữa các chuỗi là một chủ đề truyền thống của thống kê và kinh tế học. Gần đây, những nghiên cứu cũng cho thấy sự tồn tại nhân quả giữa sản lượng lao động và bất bình đăng thu nhập hay bài toán Productivity-Pay Gap1. Thật thú vị rằng trong 40 năm qua, mặc dù sản lượng lao động bình quân năm tăng 61. 8%, nhưng lương của người lao động không hề tăng cao, dẫn tới bất bình đẳng thu nhập gia tăng (giá trị tạo ra thay vì đi vào túi người lao động lại tập trung vào túi của tầng lớp chóp bu trong xã hội Mỹ). Granger verificationCon gà và quả trứng [3], cũng như Productivity-Pay gap1 là hai ứng dụng cơ bản của Granger method [1]. Giả sử ta có chuỗi thời gian $\mathbf{X}=\{X_t\}_{t=-\infty}^{+\infty}$ với thời điểm $t=0$ là thời điểm bắt đầu quan sát. Ta kí hiệu chuỗi tín hiệu quá khứ của thời điểm $t$ là $\overline{\mathbf{X_t}}=\{X_{t-i}\}_{i=1}^{+\infty}$. Ngoài ra, chuỗi tín hiệu quá khứ và hiện tại của thời điểm $t$ là $\overline{\overline{\mathbf{X_t}}}=\{X_{t-i}\}_{i=0}^{+\infty}$. Ta cũng ký hiệu giá trị ước đoán điều kiện của $A_t$ trong một chuỗi $\mathbf{A}$ theo least-square errors khi có quan sát là chuỗi $\mathbf{B}$ là $P_t(\mathbf{A}\mid\mathbf{B})$. Chuỗi giá trị lỗi của một dự đoán là $\epsilon_t(\mathbf{A}\mid\mathbf{B})=A_t-P_t(\mathbf{A}\mid\mathbf{B})$. Ta hãy gọi $\sigma^2_t(\mathbf{A}\mid\mathbf{B})$ là phương sai của chuỗi giá trị lỗi $\epsilon_t(\mathbf{A}\mid\mathbf{B})$. Trong trường hợp này ta hãy giả sử tất cả các chuỗi đều là sóng dừng. Bây giờ, ta có thể thêm ký hiệu $\mathbf{U}$ là chuỗi thông tin vũ trụ (universe time series) mô tả mọi trạng thái trong quá khứ, hiện tại và tương lai của thế giới. Thì ứng với một chuỗi sự kiện cụ thể $\mathbf{Y}$ thì $\mathbf{U}-\mathbf{Y}$ là chuỗi thông tin toàn cầu mà bỏ đi thông tin của $\mathbf{Y}$.    Định nghĩa nhân quả: Nếu $\sigma^2(\mathbf{X}\mid\overline{\mathbf{U}})\leq\sigma^2(\mathbf{X}\mid\overline{\mathbf{U}-\mathbf{Y}})$ thì $\mathbf{Y}$ là nguyên nhân gây ra $\mathbf{X}$. Định nghĩa này của Granger khá là trực quan, nói nôm na là nếu xóa thông tin của $\mathbf{Y}$ khỏi hệ thống thông tin toàn cầu thì dự đoán sẽ có sai lệch lớn hơn, thì rõ ràng $\mathbf{Y}$ là 1 trong các nguyên nhân gây ra $\mathbf{X}$     Định nghĩa luân hồi: Nếu đồng thời $\sigma^2(\mathbf{X}\mid\overline{\mathbf{U}})\leq\sigma^2(\mathbf{X}\mid\overline{\mathbf{U}-\mathbf{Y}})$ và $\sigma^2(\mathbf{Y}\mid\overline{\mathbf{U}})\leq\sigma^2(\mathbf{Y}\mid\overline{\mathbf{U}-\mathbf{X}})$ thì $\mathbf{Y}$ là luân hồi $\mathbf{X}$. Định nghĩa này của Granger cũng khá là trực quan, nói nôm na là nếu $\mathbf{Y}$ là nguyên nhân của $\mathbf{X}$ mà $\mathbf{X}$ cũng là nguyên nhân của $\mathbf{Y}$, thì hai chuỗi có luân hồi nghiệp quả.     Nghiệp quả xa: ta nói $\mathbf{Y}$ là nghiệp quả xa của $\mathbf{X}$ nếu có thêm thông tin quá khứ và hiện tại của $\mathbf{Y}$ thì kết quả dự đoán trở nên chính xác hơn: $P(\mathbf{X}\mid\overline{\mathbf{U}},\overline{\overline{\mathbf{Y}}})\leq P(\mathbf{X}\mid\overline{\mathbf{U}})$.     Độ lệch của nghiệp quả: là giá trị số nguyên dương nhỏ nhất thỏa mãn  $m={\arg\min}_k\{\sigma^2(\mathbf{X}\mid\mathbf{U}-\mathbf{Y}(k))\leq\sigma^2(\mathbf{X}\mid\mathbf{U}-\mathbf{Y}(k+1))\}$, tức là chỉ cần biết tối đa $m$ giá trị gần nhất trong chuỗi quá khứ và hiện tại là đủ để dự đoán. Xem xét hai chuỗi theo mô hình white-noise như sau: $X_t=\sum_{j=1}^ma_jX_{t-j}+\sum_{j=1}^mb_jY_{t-j}+\epsilon_t,$ $Y_t=\sum_{j=1}^mc_jY_{t-j}+\sum_{j=1}^md_jX_{t-j}+\eta_t,$ Theo định nghĩa của nhân quả, nếu $\exists j~\mbox{s. t. }~b_j\neq 0$ thì $\mathbf{Y}$ là nguyên nhân của $\mathbf{X}$. Và ngược lại $\exists j~\mbox{s. t. }~d_j\neq 0$ thì $\mathbf{X}$ là nguyên nhân của $\mathbf{Y}$. Nếu đồng thời hai điều kiện trên xảy ra thì chúng là luân hồi của nhau. Kiểm tra giả thuyết $H_0: b_j=0\forall j$ chính là Granger verification. Trứng và GàNăm 1988, Thurman và Fisher [3] đã thực hiện Granger verification trên bộ dữ liệu chứa hai chuỗi dữ liệu: sản lượng trứng hàng năm $\mathbf{X}$và số lượng gà hàng năm $\mathbf{Y}$ trên nước Mỹ từ 1930–1983. Vì cả 2 chuỗi trên đều có thể giả định là iid, nên ta có thể viết lại mô hình nhân quả như sau: $X_t=\sum_{j=1}^mb_jY_{t-j}+\epsilon_t,$ $Y_t=\sum_{j=1}^md_jX_{t-j}+\eta_t,$ Chúng ta kiểm định hai giả thiết null như sau: $H_{0x}: b_j=0\forall j$ $H_{0y}: d_j=0\forall j$ Giả thiết $H_{0x}$ chính là trứng không là nguyên nhân của gà. Giả thiết $H_{0y}$ chính là gà không là nguyên nhân của trứng. Trên bộ dữ liệu trứng Mỹ và gà Mỹ, có thể bác bỏ giả thuyết đầu, nhưng không thể bác bỏ giả thuyết sau. Tựu chung lại là trứng Mỹ có trước và sinh ra gà Mỹ. Productivity-Pay GapHầu hết người Mỹ đều tin rằng kinh tế phát triển thì người lao động sẽ nhận được reward. Nhưng theo 1 nghiên cứu gần đây1 thì có vẻ tình hình thu nhập của người lao động Mỹ không phải như vậy. Từ năm 1979, nước Mỹ đã có những thay đổi lớn về chính sách kinh tế, trong đó có nhiều chính sách phục vụ cho người giàu:  Starting in the late 1970s, policymakers began dismantling all the policy bulwarks helping to ensure that typical workers’ wages grew with productivity. Excess unemployment was tolerated to keep any chance of inflation in check. Raises in the federal minimum wage became smaller and rarer. Labor law failed to keep pace with growing employer hostility toward unions. Tax rates on top incomes were lowered. And anti-worker deregulatory pushes—from the deregulation of the trucking and airline industries to the retreat of anti-trust policy to the dismantling of financial regulations and more—succeeded again and again. Những chính sách như giảm thuế cho những người thu nhập cao, … đã khoét sâu “hố ngăn” thu nhập giữa tầng lớp giàu và nghèo. Lấy mốc 1979 là 100% cho cả tổng sản lượng quốc nội $\mathbf{X}$ lẫn thu nhập bình quân của tầng lớp lao động $\mathbf{Y}$. Tức là $X_{1979}=Y_{1979}=1$. Thì rõ ràng là từ năm 1948 tới 1979, lương và sản lượng quốc nội đều cùng chiều tăng. Nhưng từ mốc 1979 với sự thay đổi về chính sách, tổng sản lượng quốc nội vẫn tăng tuyến tính nhưng lương của tầng lớp lao động đã chững lại. Nên nhớ tầng lớp lao động nói đến trong nghiên cứu này chiếm tới 80% dân số lao động của Mỹ (production and unsupervisory workers). Cũng nhưu ví dụ trứng và gà, cả 2 chuỗi trên đều có thể giả định là iid, nên ta có thể viết lại mô hình nhân quả như sau: $X_t=\sum_{j=1}^mb_jY_{t-j}+\epsilon_t,$ $Y_t=\sum_{j=1}^md_jX_{t-j}+\eta_t,$ Tuy nhiên, điểm mốc 1979 gần như là mốc thay đổi tất cả. Trước 1979, hai chuỗi song song tuyến tính phát triển và quan hệ nhân quả theo phép thử Granger sẽ cho kết quả chấp nhận (không bác bỏ). Từ 1979, chuỗi thu nhập của người lao động chững lại, do đó quan hệ nhân quả luân hồi giảm đi. Đây là một ví dụ cho thấy chính sách đủ mạnh có thể thay đổi quan hệ giữa hai chuỗi, khiến cho nhân quả luân hồi biến mất. Hai bên đi theo hai hướng độc lập riêng biệt, một bên tiếp tục phát triển lên cao, còn một bên chững lại và phát triển chậm. Kết luậnGranger verification là một câu chuyện thú vị. Hai ứng dụng cơ bản là trứng và gà đã được các nhà khoa học Mỹ phát kiến ra trong thế kỷ 20: trứng Mỹ sinh ra gà Mỹ (chứ không phải ngược lại). Và hầu như không có luân hồi trong quan hệ giữa trứng và gà trên đất Mỹ. Câu chuyện thứ hai là về sự thay đổi của chính sách đủ mạnh để làm mất nhân quả: năm 1979 là năm cột mốc với những người lao động Mỹ, khi bắt đầu từ đó, sản lượng quốc nội vẫn tăng tuyến tính nhưng tiền lương của người lao động tăng chậm. Một điểm lưu ý là các chuỗi thời gian trong giả thiết của Granger đều là sóng dừng nhưng gần đây có những nghiên cứu tổng quát cho mọi loại sóng. Tài liệu tham khảoGranger, C. W. J. 1969. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: journal of the Econometric Society. (1969), 424–438. DetailsFeige, E. L. and Pearce, D. K. 1979. The casual causal relationship between money and income: Some caveats for time series analysis. The Review of Economics and Statistics. (1979), 521–533. DetailsThurman, W. N. , Fisher, M. E. and others 1988. Chickens, eggs, and causality, or which came first. American journal of agricultural economics. 70, 2 (1988), 237–238. Details      The Productivity–Pay Gap - Economic Policy Institute &#8617; &#8617;2 &#8617;3    "
    }, {
    "id": 78,
    "url": "https://wanted2.github.io/opensearch-kibana-tuts-1/",
    "title": "A Tutorial on Amazon OpenSearch Service and Kibana",
    "body": "2021/11/13 - Our problem at hand is quite predictable!A client has their staff submitting reports by CSV files every single day. One hundred files per day, but one file may have more than $10,000$ rows. The managers want to have an analytical solution for their administrators and technical specialists to view and explore the submitted reports. Data’s growing day by day, and they want a scalable and automated solution. In other words, when CPUUtilization rate goes above 80%, it must automatically add more nodes to the cluster. IntroductionAuto scale-out operations are easy because we can use some sorts of Python code like the following to update a cluster (OpenSearch service): 123456789import boto3client = boto3. client('opensearch')client. update_domain_config(  DomainName=domainName,  ClusterConfig={   'InstanceCount': 10  })And the number of instances in the cluster will be 10. However, to know when to perform this operation, the executors must be notified by a trigger. The triggers are continuously watching the cluster by metrics such as CPU Utilization rate. About scaling up, a material here is useful! The analytical solution in this time does not only enable indexing and searching of massive CSV files but also allows interactive exploring of data by dashboards and visualizations. Many features such as Anomaly Detection and Trace Analytics are desirable. To this end, we choose the ELK stack for this problem. We chose the Amazon OpenSearch that bases on Elasticsearch with Kibana dashboards. Another notable point in the specifications is the privilege problem. Here there are different user groups: administrators, specialists, and normal users. Admins and users are different poles of the permission spectrum, but the specialists are not. There are specific contents that specialists can view and not others. Finally, note the size of the dataset: $200$ files (each with more than 10,000 rows) or $2,000,000$ headlines are coming to our database every single day. For such a size, we may experience about $600–800$ million news in our database for one year. Note, the news is from different sources, so there will be no in-source duplicates (news in the same CSV are mostly unique). There might be some cross-sources duplicates (different CSV have the same news) because different newspapers are copying news from others. However, although different headlines share the same topic, they should include different personal opinions of writers. Therefore, they are different, and the portion of duplicates is quite small in this assumption. And the order of this problem is no smaller than $600$ million news per year. However, although we have many records in the database, the client will have only $10–50$ administrators and specialists. For such a scale, we will have at max $50\times 1000=50,000$ SELECT queries per day. Their office hour is from 8 AM to 5 PM in Pacific Time. Indexing operations can be done over the night (from 6 PM to 6 AM of the next day) or during the weekends. But it should not be done during the day. These end-users are non-tech. Ideally, they should experience less than a few seconds of waiting time for the execution of a query. They won’t care about the news that was over one year normally. However, sometimes, they will search for news in the previous ten years from now. So you should not rotate the news from 10 years ago, which keeps the size of the database at a regular rate of $6–8$ billion news (after ten years of deployment). Solution and design   What we are going to build is a search engine which should handle a scale of billion records in a database. Data start from 0 but grow at a speed of million news per day. Although the client only requires us to handle 8 billion news at max, we should handle 125% of the requirements to avoid overhead problems. Therefore, our solution should be able to handle at least $10$ billion news when it is used in production. As data is growing, we can set the solution to grow with data, but finally, for the first year, it should not be less than 1 billion.   First draft of the system should be posted after a month, then we can think about CICD and more interesting stories with the service.  First draft should be operational, so it is able to be deployed and managed with less effort from end-users.  If the system can handle the rate limit of $50,000$ queries per day, the customer may think to make it to a B2C service for millions of end-users.  Scratch or pre-defined solutions?: With such a short span to release of the first draft (one month), then we shouldn’t think about implementing from scratch with this client. We should rely on pre-defined solutions like the ELK stack or AWS OpenSearch service.  Since we want to start as data growing, we should move to a solution that is not so tedious to scale up and scale out.  We should implement a dashboard feature with notebooks like in Kibana.  Privilege problem should be able to be implemented easily, but it is not important at first draft. Then we move it to our backlog and will implement it in a later story. In fact, the configuration is quite simple: https://www. eksworkshop. com/intermediate/230_logging/config_es/#mapping-roles-to-users Testing is required if you implement from scratch (a less confident solution). But if you test a pre-defined solution, then you are wasting your time! Official benchmark results are already somewhere on the Internet! So another reason to use a pre-defined solution. Just google, and you will have the test results. You can find some of such benchmarks below.      https://aws. amazon. com/blogs/containers/introducing-cis-amazon-eks-benchmark/   https://logz. io/blog/benchmark-elasticsearch/   ImplementationSo we don’t want to waste our time on implementing from scratch or testing a lack-of-confident solution, then we choose AWS for sure. The source code can be found at my repo. TestingPerformance Testing what Amazon engineers have spent from hundreds to thousands of hours for verification is waste of time. What we might do is experiments to find the best start configuration (number of nodes). But it is not important with an auto-scaling solution because when there is some need to scale up or out, it should be done automatically. What we should test is simply checking whether if we can access the dashboard, or users can use the UI normally, or the cluster is running normally or not. Let’s explore the manual for end-users: Login and create index patterns:  After you create an OpenSearch cluster, you can explore the data in the OpenSearch Dashboards, which was previously known as Kibana.  Go to the provided URL, for example, https://search-x0053-a2ej7mr6xtirqhk36utyra65c4. us-east-1. es. amazonaws. com/_dashboards/app/home to login into the boards.      Remember to create a new user in your Cognito User Pool and use the username and password to log in.  For the first time, users will need to change the password.     For the first time, you will be prompted to a confirmation:  To explore the dashboards and visualizations, you need to index data first and then go to Stack Management &gt; Index Patterns to select the index. To put a new document into the indices from your local, you can use pybuilder: 12345678910111213141516171819202122$ pyb index_document -P indexName=x0053PyBuilder version 0. 13. 3Build started at 2021-11-14 16:44:26------------------------------------------------------------Amazon OpenSearch Service has finished processing changes for your domain. Domain description:[INFO] Found hostname: search-x0053-a2ej7mr6xtirqhk36utyra65c4. us-east-1. es. amazonaws. com[INFO] Building x0053 version 1. 0. 1a[INFO] Executing build in c:\users\tuan\source\repos\x0053[INFO] Going to execute task index_document[INFO] {'_index': 'x0053', '_type': '_doc', '_id': '1', '_version': 1, 'result': 'created', 'forced_refresh': True, '_shards': {'total': 2, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}------------------------------------------------------------BUILD SUCCESSFUL------------------------------------------------------------Build Summary       Project: x0053       Version: 1. 0. 1a   Base directory: c:\users\tuan\source\repos\x0053    Environments:        Tasks: index_document [2056 ms]Build finished at 2021-11-14 16:44:37Build took 10 seconds (10963 ms)Check if the document is already: 12345678910111213141516171819202122$ pyb search -P indexName=x0053     PyBuilder version 0. 13. 3Build started at 2021-11-14 16:45:30------------------------------------------------------------Amazon OpenSearch Service has finished processing changes for your domain. Domain description:[INFO] Found hostname: search-x0053-a2ej7mr6xtirqhk36utyra65c4. us-east-1. es. amazonaws. com[INFO] Building x0053 version 1. 0. 1a[INFO] Executing build in c:\users\tuan\source\repos\x0053[INFO] Going to execute task search[INFO] {'took': 180, 'timed_out': False, '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': 0. 56026673, 'hits': [{'_index': 'x0053', '_type': '_doc', '_id': '1', '_score': 0. 56026673, '_source': {'title': '9 F1 ở Hà Nội được cách ly tại nhà', 'description': 'UBND phường Trung Văn, quận Nam Từ Liêm, cho 9 F1 đủ điều kiện cách ly tại nhà theo hướng dẫn của Bộ Y tế. ', 'body': 'UBND phường Trung Văn, quận Nam Từ Liêm, cho 9 F1 đủ điều kiện cách ly tại nhà theo hướng dẫn của Bộ Y tế. \nÔng Nguyễn Đắc Long, Chủ tịch UBND phường Trung Văn, quận Nam Từ Liêm, tối 13/11 cho biết 9 F1, là người già có bệnh nền, trẻ em, thuộc hai gia đình, tiếp xúc với một F0 ngày 6/11 và một F0 khác vào ngày 8/11. Họ được cách ly ngay sau khi xác định F0, theo Hướng dẫn tạm thời giám sát và phòng, chống Covid-19, Bộ Y tế ban hành vào tháng 7. \n Quy định chung của Bộ Y tế là trường hợp người già bệnh nền, trẻ em, gia đình có đủ điều kiện phòng ốc. . . thì sẽ cách ly y tế tại nhà. Vì vậy, chúng tôi cách ly 9 F1 này theo đúng quy định , ông Long giải thích và nói rõ rằng  đây không phải là trường hợp thí điểm cách ly tại nhà của Sở Y tế Hà Nội . Tuy nhiên, 9 F1 là những người đầu tiên tại Hà Nội được áp dụng hình thức cách ly này. \nHiện, Hà Nội chưa có kế hoạch cách ly F0, F1 tại nhà. Trả lời VnExpress vài ngày trước, lãnh đạo Trung tâm Kiểm soát Bệnh tật (CDC) Hà Nội và đại diện Sở Y tế Hà Nội cho biết thành phố có đủ nguồn lực để cách ly, điều trị tập trung F1 và F0. Chỉ khi nào số lượng F0, F1 tăng vượt quá khả năng, thành phố mới tính đến phương án cách ly người không triệu chứng tại nhà. \nNhiều chuyên gia đề nghị thành phố nên tính tới phương án này sớm. Phó giáo sư, tiến sĩ Trần Đắc Phu, Cố vấn cao cấp Trung tâm đáp ứng khẩn cấp sự kiện y tế công cộng Việt Nam, Bộ Y tế, cho rằng Hà Nội không nên kiên trì cách ly tập trung, rút kinh nghiệm các tỉnh có dịch bùng phát mạnh thời gian vừa qua. Nguyên nhân là thành phố liên tiếp xuất hiện các ổ dịch mới, số ca nhiễm gia tăng trong một tuần trở lại đây khi thực hiện thích ứng an toàn, linh hoạt, kiểm soát hiệu quả dịch Covid-19. F1 tăng nhiều khi số F0 cao, dẫn tới cơ sở cách ly tập trung hết chỗ. Thêm vào đó, duy trì cách ly tập trung sẽ khiến các cơ sở bị quá tải, có nguy cơ lây nhiễm chéo. \n Cách ly F1 tại nhà giúp người bị cách ly đỡ tốn kém, người cách ly không bị ảnh hưởng nặng nề về tâm lý , ông Phu nói. Nhiều nhà dân Hà Nội đủ điều kiện, đủ cơ sở vật chất để cách ly ở nhà. Hệ thống y tế cơ sở và chính quyền từ thôn, xóm, tổ dân phố đủ năng lực, có thể giám sát, theo dõi người cách ly. \nHiện, phường Trung Văn, quận Nam Từ Liêm, đang ở cấp độ 2 (nguy cơ trung bình, màu vàng). Cả thành phố chỉ có phường Phú Đô, quận Nam Từ Liêm, dịch ở cấp độ 4 (nguy cơ rất cao, màu đỏ - cấp độ dịch cao nhất). Lần gần đây nhất chỉ có tổ dân phố Ngô Sài, thị trấn Quốc Oai, huyện Quốc Oai, được đánh giá cấp 4. \nTổng số ca nhiễm tại Hà Nội từ ngày 27/4 đến tối 13/11 là 5. 924, trong đó số cộng đồng 2. 229, số nhiễm sau khi được cách ly 3. 695 ca. \nLực lượng y tế căng mình xét nghiệm cho người dân phường Thanh Xuân Trung, quận Thanh Xuân. Ảnh: Giang Huy'}}]}}------------------------------------------------------------BUILD SUCCESSFUL------------------------------------------------------------Build Summary       Project: x0053       Version: 1. 0. 1a   Base directory: c:\users\tuan\source\repos\x0053    Environments:        Tasks: search [1355 ms]Build finished at 2021-11-14 16:45:38Build took 7 seconds (7631 ms) Now you have indexed 1 document into OpenSearch, we can be back to Kibana to create an index pattern.  Click into Next step then Create index pattern to go to the field lists screen. SQL Query Workbench: We can use SQL syntax in Query Workbench.  Open the left sidebar, select OpenSearch Plugins &gt; Query Workbench. Document for SQL in OpenSearch can be found here. Notebooks: This is also a valuable feature in OpenSearch dashboard. You can add visualization blocks or code blocks (SQL/PPL).  In this example, we used the following SQL query for full-text search: 12%sqlselect title, description from x0053 where match_query(title,  f1 );Anomaly Detection: From this view, we can create an Anomaly Detector with a specific rule like 1title contains f1Other features like Dashboards, Visualizations, Trace Analytics are also worth exploring. Operation and maintenanceInstalling OpenSearch maybe easy but managing a cluster requires more work. Configurations: Multi-region service: To prevent data loss and minimize Amazon OpenSearch Service cluster downtime in the event of a service disruption, you can distribute nodes across two or three Availability Zones in the same Region, a configuration known as Multi-AZ. Availability Zones are isolated locations within each AWS Region. For domains that run production workloads, we recommend the following configuration:  Choose a Region that supports three Availability Zones with OpenSearch Service.  Deploy the domain across three zones.  Choose current-generation instance types for dedicated master nodes and data nodes.  Use three dedicated master nodes and at least three data nodes.  Create at least one replica for each index in your cluster. Cross-cluster replication in Amazon OpenSearch Service lets you replicate indices, mappings, and metadata from one OpenSearch Service domain to another. It follows an active-passive replication model where the follower index (where the data is replicated) pulls data from the leader index. Cross-cluster replication ensures high availability in the event of an outage, and allows you to replicate data across geographically distant data centers to reduce latency. Indices: If you need to migrate indices from a cluster to another, you can use remote reindex. Whereas index rollup jobs let you reduce data granularity by rolling up old data into condensed indices, transform jobs let you create a different, summarized view of your data centered around certain fields, so you can visualize or analyze the data in different ways. One thing that is amazing about indices management is Index State Management (ISM) in Amazon OpenSearch Service. It lets you define custom management policies to automate routine tasks and apply them to indices and index patterns. You no longer need to set up and manage external processes to run your index operations. For example, you can setup a policy that moves an index from hot storage to UltraWarm, and eventually to cold storage, then deletes the index. Auditing: Monitoring: Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon OpenSearch Service and your other AWS solutions. AWS provides the following tools to monitor your OpenSearch Service resources, report issues, and take automatic actions when appropriate: Amazon CloudWatch: Amazon CloudWatch monitors your OpenSearch Service resources in real time. You can collect and track metrics, create customized dashboards, and set alarms that notify you or take actions when a metric reaches a certain threshold. Amazon CloudWatch Logs: Amazon CloudWatch Logs lets you monitor, store, and access your OpenSearch log files. CloudWatch Logs monitors the information in log files and can notify you when certain thresholds are met. Amazon EventBridge: Amazon EventBridge delivers a near real-time stream of system events that describe changes in your OpenSearch Service domains. You can create rules that watch for certain events, and trigger automated actions in other AWS services when these events occur. AWS CloudTrail: AWS CloudTrail captures configuration API calls made to OpenSearch Service as events. It can deliver these events to an Amazon S3 bucket that you specify. Using this information, you can identify which users and accounts made requests, the source IP address from which the requests were made, and when the requests occurred. Notifications: Notifications in Amazon OpenSearch Service currently contain information about available software updates and Auto-Tune events for your domains. In the future, they might also include performance optimization recommendations such as moving to the correct instance type for a domain or rebalancing shards to reduce performance bottlenecks. Security: Security is a shared responsibility between AWS and you. The shared responsibility model describes this as security of the cloud and security in the cloud:    Security of the cloud – AWS is responsible for protecting the infrastructure that runs AWS services in the AWS Cloud. AWS also provides you with services that you can use securely. Third-party auditors regularly test and verify the effectiveness of our security as part of the AWS compliance programs.     Security in the cloud – Your responsibility is determined by the AWS service that you use. You are also responsible for other factors including the sensitivity of your data, your company’s requirements, and applicable laws and regulations.  Fine-grained access control helps you to define roles, access level at indices, documents and domain levels. You can restrict the access to your dashboard by using multi-tenancy. Data protection is enhanced with encryption at rest and node-to-node encryption. Data integrity comply with many standards: Third-party auditors assess the security and compliance of Amazon OpenSearch Service as part of multiple AWS compliance programs. These programs include SOC, PCI, and HIPAA. SAML based authentication and Amazon Cognito are all supported. Management: Auto-Tune in Amazon OpenSearch Service uses performance and usage metrics from your OpenSearch cluster to suggest memory-related configuration changes, including queue and cache sizes and Java virtual machine (JVM) settings on your nodes. These optional changes improve cluster speed and stability. To enable auto-tune, 1234567891011121314POST https://es. us-east-1. amazonaws. com/2021-01-01/opensearch/domain/domain-name/config{  AutoTuneOptions : {   DesiredState :  ENABLED ,   MaintenanceSchedules : [{    StartAt : 4104152288000,    Duration : {     Value : 2,     Unit :  HOURS    },   CronExpressionForRecurrence :  cron(0 12 * * ? *)   }] }}ConclusionWe have walked through a tutorial on configurations of an OpenSearch cluster with logging options, encryption at rest, . etc. In the next steps, we will implement the following two stories: Story 1: CSV upload to S3 for streaming:  Users need to upload CSV files with three fields: title, body and description.       title   description   body         9 F1 ở Hà Nội được cách ly tại nhà   UBND phường Trung Văn, quận Nam Từ Liêm, cho 9 F1 đủ điều kiện cách ly tại nhà theo hướng dẫn của Bộ Y tế.    UBND phường Trung Văn, quận Nam Từ Liêm, cho 9 F1 đủ điều kiện cách ly tại nhà theo hướng dẫn của Bộ Y tế. \nÔng Nguyễn Đắc Long, Chủ tịch UBND phường Trung Văn, quận Nam Từ Liêm, tối 13/11 cho biết 9 F1, là người già có bệnh nền, trẻ em, thuộc hai gia đình, tiếp xúc với một F0 ngày 6/11 và một F0 khác vào ngày 8/11. Họ được cách ly ngay sau khi xác định F0, theo Hướng dẫn tạm thời giám sát và phòng, chống Covid-19, Bộ Y tế ban hành vào tháng 7. \n Quy định chung của Bộ Y tế là trường hợp người già bệnh nền, trẻ em, gia đình có đủ điều kiện phòng ốc… thì sẽ cách ly y tế tại nhà. Vì vậy, chúng tôi cách ly 9 F1 này theo đúng quy định , ông Long giải thích và nói rõ rằng  đây không phải là trường hợp thí điểm cách ly tại nhà của Sở Y tế Hà Nội . Tuy nhiên, 9 F1 là những người đầu tiên tại Hà Nội được áp dụng hình thức cách ly này. \nHiện, Hà Nội chưa có kế hoạch cách ly F0, F1 tại nhà. Trả lời VnExpress vài ngày trước, lãnh đạo Trung tâm Kiểm soát Bệnh tật (CDC) Hà Nội và đại diện Sở Y tế Hà Nội cho biết thành phố có đủ nguồn lực để cách ly, điều trị tập trung F1 và F0. Chỉ khi nào số lượng F0, F1 tăng vượt quá khả năng, thành phố mới tính đến phương án cách ly người không triệu chứng tại nhà. \nNhiều chuyên gia đề nghị thành phố nên tính tới phương án này sớm. Phó giáo sư, tiến sĩ Trần Đắc Phu, Cố vấn cao cấp Trung tâm đáp ứng khẩn cấp sự kiện y tế công cộng Việt Nam, Bộ Y tế, cho rằng Hà Nội không nên kiên trì cách ly tập trung, rút kinh nghiệm các tỉnh có dịch bùng phát mạnh thời gian vừa qua. Nguyên nhân là thành phố liên tiếp xuất hiện các ổ dịch mới, số ca nhiễm gia tăng trong một tuần trở lại đây khi thực hiện thích ứng an toàn, linh hoạt, kiểm soát hiệu quả dịch Covid-19. F1 tăng nhiều khi số F0 cao, dẫn tới cơ sở cách ly tập trung hết chỗ. Thêm vào đó, duy trì cách ly tập trung sẽ khiến các cơ sở bị quá tải, có nguy cơ lây nhiễm chéo. \n Cách ly F1 tại nhà giúp người bị cách ly đỡ tốn kém, người cách ly không bị ảnh hưởng nặng nề về tâm lý , ông Phu nói. Nhiều nhà dân Hà Nội đủ điều kiện, đủ cơ sở vật chất để cách ly ở nhà. Hệ thống y tế cơ sở và chính quyền từ thôn, xóm, tổ dân phố đủ năng lực, có thể giám sát, theo dõi người cách ly. \nHiện, phường Trung Văn, quận Nam Từ Liêm, đang ở cấp độ 2 (nguy cơ trung bình, màu vàng). Cả thành phố chỉ có phường Phú Đô, quận Nam Từ Liêm, dịch ở cấp độ 4 (nguy cơ rất cao, màu đỏ - cấp độ dịch cao nhất). Lần gần đây nhất chỉ có tổ dân phố Ngô Sài, thị trấn Quốc Oai, huyện Quốc Oai, được đánh giá cấp 4. \nTổng số ca nhiễm tại Hà Nội từ ngày 27/4 đến tối 13/11 là 5. 924, trong đó số cộng đồng 2. 229, số nhiễm sau khi được cách ly 3. 695 ca. \nLực lượng y tế căng mình xét nghiệm cho người dân phường Thanh Xuân Trung, quận Thanh Xuân. Ảnh: Giang Huy   Source: https://vnexpress. net/nhieu-f1-o-ha-noi-duoc-cach-ly-tai-nha-4385406. html  CSV is then streamed into OpenSearch for indexing.  End-user will query through Kibana (OpenSearch Dashboards) for investigations. The following tasks will be done:  CSV upload     Setup S3 bucket with triggering.    Setup indexing Lambda to handle streaming CSV.     Setup Kibana for end-usersStory 2: Fine-grained privilege control with Admins, Specialists, and Normal Users:  There are three user groups in our application:     Administrators group who have the highest privilege, members in this group can view/edit/delete anything in the dashboard.    Specialists group who need to view the dashboard but not everything. They cannot edit/delete anything in the dashboard.    Normal users group who are forbidden from viewing the dashboard.     We need to filter out before Cognito log users in to find the type of users:     if users are admins, let them in.    if users are specialists, let them in but restrict their view. Please check whether if this is doable or not?   if users are normal, don’t log in.    Tutorial about lambda triggers in Cognito.  Triggers     CreateAuthChallenge   CustomMessage   DefineAuthChallenge   PostAuthentication   PostConfirmation   PreAuthentication   PreSignUp   PreTokenGeneration   UserMigration   VerifyAuthChallengeResponse   Not need to configure all above Lambdas!    Admins/Specialists restriction problem     Create two groups for admins and specialists, no need of a group for normal users.          In PreSignup logic, if the email is from a normal user then deny sign up.      it is important here that there must be a Lambda connecting to user table to check the type of users!          In PreLogin function, log the groups of their login behaviors for later diagnosis.    Configure the role/policies to ensure that specialists can only view.          If there are specific resources specialists cannot view, then can Cognito handle?          Then we need two Lambda functions:  One PreSignup Lambda that connects and verifies user types (admins/specialists or normal) before signing up a user.  Another PreLogin Lambda that logs user behaviors. Pricing estimation: The pricing estimation of these two stories with the original story can be found here (152. 20$ per month). "
    }, {
    "id": 79,
    "url": "https://wanted2.github.io/integromat/",
    "title": "Integromat: An integration platform for workflow automation",
    "body": "2021/11/01 - Cách đây gần chục năm, có 1 câu chuyện tôi từng chứng kiến như sau: có 1 anh sếp bạn tôi giao nhiệm vụ cho nhân viên mới vào làm. Cậu nhân viên cũng mới vào ngành, nên nhiệm vụ cũng kiểu như các bài tập nhỏ để dần dần nắm bắt. Hôm ấy, có giao cho 1 nhiệm vụ như thế này:  Sếp (S): Anh giao cho em một nhiệm vụ là thu thập dữ liệu từ nguồn cấp RSS (hiện tại có vài nghìn nguồn mở, bọn anh sẽ lên danh sách sau). Nhưng giờ anh có 1 nguồn là https://vnexpress. net/rss/so-hoa. rss mà bọn anh muốn thực hiện 1 workflow như sau:    Đầu tiên lấy dữ liệu từ https://vnexpress. net/rss/so-hoa. rss và parse thành 1 list các bài báo.        yêu cầu ở đây là filter chỉ lấy ra những bài báo nào mà phần giới thiệu có nói về AI.     Sau đó convert list của em lấy về thành CSV với tất cả các trường có thể lấy được.     Upload file CSV lên OneDrive của team.        Song song với luồng CSV là luồng post bài đăng lên room chat của team trên Gitter. im  S: Em lên stack công nghệ và estimate làm trong bao lâu hộ anh trước được không? Đề xuất của nhân viên: Em xin làm 2 tuần ạ!Bài toán này thực ra không có gì mới mẻ cả, mà là một dạng task mà trong đó có các thành phần của xây dựng một data warehouse cơ bản: luồng ETL cơ bản, lưu trữ dữ liệu dạng bảng (mà thực ra bảng cũng như SQL, về sau có thể thay thế bằng CSDL dạng quan hệ), rồi luồng thông báo cảnh báo lên chat hoặc SNS. Vì hai luồng CSV và hook vào Gitter. im là song song nên có thể dính thêm tối ưu hóa đa nhiệm đa luồng. Nói chung là 1 bài tập vừa sức và không quá hóc búa, một bài tập cơ bản cho kỹ sư data mới vào nghề. Trên đây là bản chất của bài toán, còn mục tiêu của đưa bài tập này với kỹ sư data lâu năm như anh bạn tôi thì là muốn xem mindset làm việc của nhân viên để còn bồi dưỡng dần dần. Dưới đây là bản đề xuất của cậu nhân viên:  Nhân viên K: Em xin đề xuất làm việc như sau ạ:    Với luồng CSV như anh giao, em sẽ dùng stack là Python để lấy nguồn cấp RSS, dùng các thư viện như sau: xml để parse RSS tree, và pandas để tạo nội dung CSV, sau đó em dùng API của M$ để upload file lên OneDrive. Luồng này em estimate mất 7 ngày (4 ngày tạo file CSV và 3 ngày để upload lên OneDrive).   Với luồng post bài lên chat thì em phải đọc hiểu tài liệu của API của Gitter để post bài lên room, mất tầm 3 ngày nữa ạ.   Về kiến trúc để song song hóa hai luồng, em dùng multiprocessing và threading ạ.  Nhìn chung làm thế nào thì cũng được thôi. Kết quả mà ok thì cũng không vấn đề gì nếu quá trình có sai sót. Có vấn đề sai sót ở quá trình thì cứ fix rồi commit lên mà đảm bảo kết quả cuối cùng ok thì không sao. Tuy vậy kết quả kỳ vọng ở đây là gì?CÓ 3 mặt:  tốc độ làm việc: phải nhanh, 2 tuần là đề xuất của K.  sản phẩm có tính khả dụng cao:     ít bugs ở sản phẩm đầu ra, còn các sản phẩm trung gian, các quá trình thì chả ai quan tâm. Code cũng được mà no-code, low-code cũng được.    triển khai và vận hành được luôn và ít sự cố.    Danh sách chi tiết liệt kê ra thì còn dài nhưng mà tạm thời với bài tập về nhà kiểu này thì tôi nghĩ anh bạn tôi có tiêu chí đánh giá thế là ổn thỏa rồi. Nhìn chung, tôi và anh bạn tôi cũng không phải kiểu dò xét từng chi tiết khiến nhân viên cảm thấy bị soi khi làm việc, nên những chi tiết thì cứ giao các em làm, nhưng quản lý output và chất lượng. Tuy nhiên, là bản đề xuất này coi như 1 output trung gian thì quả thực là hơi khiến anh em lăn tăn.  Thứ nhất, là estimate 10 man-days tức là 2 man-weeks, 2 tuần làm việc.  Thứ hai là cách làm có vẻ hơi đòi hỏi anh em quản lý phải monitor và theo dõi từng khâu trong quy trình để đảm bảo ít bugs. Như vậy là tăng tải cho anh em quản lý.  Thứ ba là khâu triển khai và vận hành hầu như ko có ý kiến gì trong đề xuất.  Với người mới thì cũng cứ tạm cho làm rồi anh em mình uốn nắn dần Tuấn ạ. Đây là lời tâm huyết của anh bạn tôi lúc bắt đầu cho cậu nhân viên K làm việc này. Tôi lúc đó cũng không biết nói thế nào nên cứ để nó chạy thử xem. Đánh giá: Thực ra bọn anh làm tầm 5-10 phút là xong em ạ …Nhưng bây giờ nghĩ lại, tôi thấy đáng ra nên đánh chặn ngay từ đầu!Làm quản lý chả ai muốn soi nhân viên cả, nhưng đề xuất làm như K là anh em phải chuẩn hóa quy trình, lên kế hoạch theo dõi, lên cảnh báo, …trăm thứ bà rằn mà nói chung là tăng tải cho quản lý.  Kết cục là không chỉ 2 tuần mà quá trình “uốn nắn” kéo dài 6 tháng luôn mà lúc triển khai và vận hành cũng vẫn không yên tâm về sự cố bất thình lình. Tại sao không nên đánh chặn ngay thì vì lý do giáo dục để hướng dẫn nhân viên cho lên dần dần. Mình mà đánh chặn đôi khi lại gặp câu nói bất hủ:  Thế các anh làm đi? Ừ thì các anh làm thì chỉ mất tầm 5-10 phút thôi em ạ!Và bọn anh lấy ví dụ mẫu là nền tảng tích hợp dịch vụ integromat. com cho nó nhanh nhé. Cấu hình integromatĐăng ký tài khoản: Đầu tiên là cũng nên có điều tra về nền tảng integration xem có gói nào phù hợp nhất. Tuy nhiên, theo tôi tìm hiểu thì mảng tích hợp này cũng khá bão hòa và nhiều nền tảng rồi. Tính năng cũng same same nhau. Thôi thì tạm chọn integromat nhé. Thì integromat cũng offer nhiều gói: https://www. integromat. com/en/pricing. Gói Free để demo thử thì bạn sẽ chỉ tạo được tối đa 2 scenarios. Scenario timeout chỉ được 5 phút, minimum interval giữa hai lần chạy của scenario chỉ tối thiểu 15 phút. Webhook queue size (số lượng webhook được lên queue thực hiện) cũng nhỏ hơn, storage ngoài cũng chỉ được 1MB. Thì quay lại đầu bài thì thực ra cũng chỉ cần update ở tầm 60 phút một lần. Các chỉ số khác là đủ cho bản demo, và quan trọng hơn là triển khai vận hành cũng đủ để dùng trong thực tế. Ngoài ra gói có phí cơ bản cũng chỉ 9$/tháng. Đầu tiên vào trang Sign up dịch vụ, điền thông tin và đăng ký free thôi. Integormat sẽ gửi link verify vào email đăng ký, và bạn phải click vào link trong email đăng ký ấy để kích hoạt tài khoản. Nhìn chung quá trình này mất 1-2 phút. Khi bắt đầu thì sẽ có một nút bấm trong trang MyPage để guide bạn onboarding nhanh chóng: Thiết kế hệ thống: Bạn sẽ cần 1 trigger để theo dõi nguồn cấp RSS, một module để chuyển XML sang CSV, và một module để upload lên OneDrive. Song song với đó, bạn cần 1 module để tích hợp với Gitter. im. Bạn không nên quên xử lý lỗi vì mọi module đều có thể có lỗi khi fetch data. Cấu hình các modules: Triggers: Đầu tiên bạn cần ấn nút Creat a scenario để tạo ra một kịch bản thực thi cho nhiệm vụ. Mọi xử lý sẽ bắt đầu từ 1 trigger mà bạn sẽ dùng RSS để bắt đầu.  Hãy chọn Watch RSS feed items. Trong cửa sổ tiếp theo hãy chọn Show advanced settings, nhập nguồn RSS và click OK.  Trong cửa sổ Choose where to start thì cứ chọn From now on cũng ok. Thế là bạn đã có 1 trigger mà cứ mỗi khi có thay đổi trên nguồn cấp đã cho (tần suất kiểm tra sẽ cấu hình sau), thì lập tức trigger sẽ chạy luồng mà bạn sẽ cấu hình tiếp theo. XML to CSV: Nhấn vào nút Add a module như hình dưới: Sau đó chọn module CSV từ danh sách: Sau đó bạn có thể nhập chọn những trường sẽ lấy từ nguồn RSS để tạo ra file CSV: Bạn có thể chọn GROUP BY bằng trường tùy thích. Trên đây tôi chọn theo updated_at. Upload to OneDrive: Để upload CSV lên OneDrive, bạn cần add thêm module OneDrive: Hãy chọn Upload file ở cửa sổ chọn chức năng: Sau đó chọn Add để authenticate vào OneDrive: Sau đó một pop-up sẽ mở ra để bạn xác nhận quyền truy cập của ứng dụng integromat vào OneDrive. Khi xong bước này bạn sẽ quay lại thiết lập Folder thành /, tên file thành . xml và dữ liệu là text.  Để tránh lỗi bạn nên chọn Rename existing files. Routing: Vậy là bạn đã xong luồng thứ nhất mà chưa mất đến 5 phút đâu. Trước khi add luồng thứ 2, chúng ta cần có 1 module để song song hóa, kiểu như Router.  Sau khi thêm router thì thiết kế của bạn sẽ như sau: Post news to Chat room: Từ Router hãy tạo thêm 1 luồng mới và tìm module Gitter: Hãy chọn Send a mesage to a room: Bạn sẽ phải đăng nhập và cấp quyền truy cập cho integromat vào Gitter. im. Một cửa sổ popup sẽ mửo ra sau khi bạn nhấn Add. Chú ý là Gitter chỉ chấp nhận đăng nhập qua Gitlab, Github và Twitter. Đây là kênh chat dành riêng cho developer. Tôi chọn twitter thôi. Sau khi cấp quyền thành công, bạn sẽ quay lại integromat, và cấu hình tin nhắn sẽ lên Gitter. Bạn có thể dùng định dạng mẫu markdown để post bài.  Vậy đấy luồng số 2 đã xong trong chưa đầy 1 phút. Error handling: Xử lý lỗi luôn là một phần quan trọng của lập trình phần mềm. Lỗi luôn xảy ra và cách handle lỗi rất quan trọng. Bạn click chuột phải vào module cần xử lý lỗi và chọn Add error handlers. Đó là tất cả việc bạn sẽ cần làm để bắt đầu xử lý lỗi trên integromat. Tiện quá đúng không? Vậy nội dung hàm xử lý lỗi sẽ như thế nào?Bạn có thể dùng directives (điều hướng) hoặc chạy một module nào đó. Có vài điều hướng cơ bản:  Rollback: revert lại mọi thay đổi.  Break: dừng tại đó.  Resume một xử lý nào đó đang bị gián đoạn.  Commit một thay đổi hiện có.  Ignore luôn!Trong ví dụ hiện tại, tôi tạm thời cho next luôn, tức là dùng Ignore cho upload file. Làm tương tự với các module khác, bạn có thể đảm bảo có xử lý lỗi trong từng trường hợp cụ thể. Với module CSV convert thì tôi chọn Break. Mô hình thực thi cuối cùng như sau: Filters: Để thực hiện điều kiện filter những tin có liên quan tới AI, bạn có thể dùng chức năng filter của integromat như sau. Click vào link giữa trigger và router: Test: Testing là phần cần thiết trong phát triển phần mềm. Các module đã được phát triển có SLA khá cao và unit testing cẩn thận nên ta sẽ skip UT lần này. API testing cũng không cần vì các module đã được test cẩn thận và có SLA rồi. Vậy ta cần test tích hợp (IT) thì sẽ có nút bấm Run once. Tuy nhiên, bạn sẽ phải cần cấu hình Scenario như bên dưới: Sau đó thì bạn sẽ thực hiện testing nhiều lần, cứ có bugs thì lại fix cho đến khi thấy mỹ mãn thì chuyển tới mục tiếp theo để vận hành thôi. Vận hành: Trước khi triển khai vận hành, bạn phải cấu hình một chút về tần suất fetch news. Như đã nói tần suất tầm 60 phút cũng ổn. Sau đó bạn ấn vào cái Switch button để chuyển sang ON/OFF là xong. Kết luậnChúng ta đã điểm lại cách implement một dịch vụ fetch news và parse XML-to-CSV và post bài lên OneDrive cũng như Gitter. Có áp dụng cả filter, router, …Có đủ cả testing, vận hành, xử lý lỗi, … nên nhìn chung là một luồng ổn định để chiến lâu dài. Nhìn chung cách làm nào cũng được, nhưng dùng nền tảng có SLA cao thì anh em yên tâm hơn về xử lý sự cố, xử lý lỗi, …Khi có vấn đề gì thì cứ theo luồng mà chạy thôi nên độ tự tin nó cao. "
    }, {
    "id": 80,
    "url": "https://wanted2.github.io/rd-pj-lifecycle/",
    "title": "構想フェーズから終了までの研究開発サイクル",
    "body": "2021/10/23 -  研究開発は大別すると基本的な原理や性質を知るための純粋基礎研究、特定の実際的目的のために行われる目的基礎研究、特定の要請に答えるための応用研究、新製品を導入するための研究に分かれる。Source: wikipedia. com Hoạt động nghiên cứu và phát triển (R&amp;D) là hoạt động do các công ty tổ chức thực hiện nhằm đổi mới sáng tạo dẫn đến tri thức, sản phẩm và dịch vụ mới. Phân loại dự án R&amp;D thì tùy vào mục đích có thể phân thành: R&amp;D cơ bản “thuần” (phát hiện định luật, định lý, tính chất mới mang tính tổng quát), R&amp;D cơ bản “có mục đích cụ thể” (phát hiện định luật, định lý và tính chất mới trong 1 ngành hẹp, không cần tổng quát), và R&amp;D ứng dụng (sử dụng kết quả của R&amp;D cơ bản để tạo ra sản phẩm mới, dịch vụ mới). Dù là cơ bản thuần, cơ bản hẹp hay ứng dụng thì nhìn chung dự án R&amp;D đều là dự án và là đối tượng của ngành Project Management. Tuy nhiên, vì là đi tìm cái mới nên dù cái mới là mang tính lý thuyết (cơ bản thuần và cơ bản hẹp) hay ứng dụng, thì dự án R&amp;D có những đặc điểm riêng khác với dự án thông thường: outcome của dự án nghiên cứu thường khó định hình ngay từ đầu và đôi khi hơi mơ hồ với những người làm kỹ thuật (bởi vì nếu cái mới mà mình đã biết từ đầu thì nó đâu còn là cái mới nữa đúng không các bạn?)Thi thoảng dự án nghiên cứu sẽ không có outcome, nhưng đột nhiên một vài tuần sau lại output ra tới tấp! Vòng đời dự án và ví dụ về dự án nghiên cứu thiết kế hệ thống quản lý sân bayVòng đời dự án R&amp;D: Dự án nghiên cứu thường hình thành từ một quá trình là một nhà đầu tư (cá nhân hoặc tổ chức cung cấp kinh phí như NSF hay DARPA) công bố một lĩnh vực hay chủ đề nghiên cứu và kêu gọi các nhóm nghiên cứu trong toàn quốc hoặc thậm chí trên thế giới gửi proposal lên phê duyệt kinh phí (grant). Các nhóm nghiên cứu có thể ở trường đại học hay trong công nghiệp đều được. Các nhóm có thể proposal độc lập hoặc hợp tác nghiên cứu. Trong trường hợp hợp tác nghiên cứu, yếu tố như việc không để xảy ra conflicts, phân chia công việc, communication để đảm bảo việc collaborate diễn ra trơn chu sẽ được xem xét cẩn thận. Khi kêu gọi này được công bố, các viện hoặc các trường, tổ chức sẽ nhận thông báo và gửi đến từng nhóm nghiên cứu dưới quyền. Các nhóm sẽ bắt đầu thực hiện quá trình proposal để nhận grant. Các Project Investigators (PI) sẽ được bầu lên để thực hiện quá trình này. Trong quản lý dự án, nó sẽ ứng với quá trình Định nghĩa (要件定義). Sau khi có bản proposal nêu đầy đủ mục đích và giá trị nghiên cứu, những stakeholders sẽ tham gia và quan trọng nhất là tầm nhìn về cái mới mà dự án sẽ output ra, thì nhóm PIs sẽ trình bày kết quả điều tra lên hội đồng cấp duyệt kinh phí. Sau khi được phê duyệt (承認済み) thì các nhóm nghiên cứu được cấp grant sẽ tiến hành thiết kế tỷ mỷ, lên action plan mà sẽ ở dạng WBS, lên kế hoạch tỷ mỷ về cả schedule lần resource (con người và thiết bị), …  Q: Các bạn sẽ thấy lạ là tại sao có thể phê duyệt cấp grant khi mà chỉ có 1 bản proposal sơ khai chứ chưa triển khai gì hết?  A: Thì thực ra tính bất định của dự án nghiên cứu như đã nói ở trên: nếu cái mới mà đã biết từ đầu thì lại không phải cái mới! Do đó, theo tôi hiểu sẽ chủ yếu dựa vào cái gọi là “uy tín và kình nghiệm” của PIs dự án là chính. PI có nhiều kinh nghiệm sẽ viết ra 1 bản proposal chi tiết mà qua đó thể hiện hết khả năng thực thi của nhóm. Thì đúng rồi, có khả năng thực thi thì là điều kiện tiên quyết, chứ không có khả năng thực thi thì out ngay!Thế nên vị trí PI ở những tổ chức cấp grant danh giá, thường là các giáo sư có bề dày thành tích cả về nghiên cứu lần hướng dẫn sinh viên. Các thành phần trẻ hơn thì cũng có nhưng hiếm và thường là như các cụ gọi là “con ông cháu cha” (không có ý xấu nhé!). Tức là nhìn chung là sẽ ít rủi ro và không biết ra được cái gì nhưng có độ chắc chắn sẽ ra được một cái gì đó cao! Sau khi lên kế hoạch tỷ mỷ thì việc thực thi là bắt buộc. Trong quá trình thực thi việc audit dự án sẽ thường xuyên xảy ra để đảm bảo các giáo sư và nhóm nghiên cứu sử dụng kinh phí và resource hợp lý, đồng thời tiến độ diễn ra chuẩn.  Q: Vậy nếu không biết cái mới là gì từ đầu thì dự án có deadline không?  A: Nhìn chung là không, mà sẽ chạy cho đến khi hết tiền hoặc hết hạn (tổ chức có thể cấp dạng hợp đồng 1 triệu đô trong 5 năm) thì thôi. Tuy nhiên tùy vào tổ chức cấp grant, việc audit và đòi báo cáo hàng năm hoặc hàng quý là có. Đến quý đến năm mà không có gì thì việc cấp tiếp là có thể sẽ bị xem xét lại. Cuối cùng, là sau một thời gian dài vô cùng áp lực (bởi ngay cả người xin tiền cũng không biết là mình sẽ output ra cái gì, lúc nào cũng nơm nớp bị cắt grant) thì nếu có được thành quả đáng kể đúng hạn báo cáo năm thì dự án sẽ đóng lại với tổng kết và báo cáo.  Q: Tôi thấy có nhiều nhóm nghiên cứu đơn giản là publish xong paper lên hội nghị lớn hoặc tạp chí danh tiếng là kết thúc. Vậy việc thực thi lên sản phẩm thì thế nào?  A: Nhìn chung thì những nghiên cứu ứng dụng như vậy thì phần publish paper sẽ không output ra sản phẩm mà để có thể lên sản phẩm thì nhóm sẽ phải kết hợp với bên các cty để thực hiện nghiên cứu cộng tác. Ở dạng này thì sự phân công sẽ là nhóm nghiên cứu của đại học làm những phần điều tra kỹ thuật, pilot study là cùng. Sau đó thì khi có output ở dạng paper thì chuyển lại cho bên nghiên cứu công nghiệp triển khai sản phẩm. Tuy nhiên, có nhiều nghiên cứu công nghiệp mang tính tuyệt mật, chuyện publish paper bị chặn luôn mà bên cty công nghiệp có thể yêu cầu chỉ nộp patent thôi chả hạn. Nhưng nếu grant là của NSF hoặc các tổ chức phi lợi nhuận thì nhóm bạn có thể yên tâm ngồi viết paper. Ví dụ về dự án hệ thống quản lý sân bay: Để cho gần với thực tiễn ngành trí tuệ nhân tạo, chúng ta cùng nhìn vào 1 dự án thực tế giấu tên. Hệ thống quản lý sân bay (Airport Management System, hay viết tắt là AMS) là một phần không thể thiếu trong tối ưu hóa hoạt động của sân bay, đặc biệt là các sân bay lớn như JFK airport. Năm nay tổ chức Foundation X lại tự dưng xuất fund nghiên cứu cho lĩnh vực tự động hóa sân bay, chủ đề tuyệt vời cho các nhóm nghiên cứu trong hầu hết các lĩnh vực của trí tuệ nhân tạo như tối ưu hóa nguồn lực, bảo mật hệ thống, logistic, tính toán song song, …Grant tạm tính sơ là 1 triệu đô trong 5 năm đi chẳng hạn hoặc tùy vào estimate của các proposals. 10 nhóm nghiên cứu từ các trường đại học khác nhau trong toàn quốc nhận thấy năng lực thực thi phù hợp và muốn cùng nhau viết grant để giành fund này. 10 giáo sư đứng đầu các nhóm đứng tên làm PI và phân chia mỗi người mỗi mảng ứng với 10 mảng được công bố trong thư mời nộp proposal. Trong đó có 2 nhóm nghiên cứu của giáo sư A và giáo sư B muốn hợp tác cùng nhau. Nhóm A làm về tối ưu hóa nhân lực và lên kế hoạch di chuyển trong hệ thống. Nhóm B làm về hệ thống theo dõi và bảo mật. Tại sao 2 nhóm này muốn cộng tác bởi vì tiền đề để thực hiện các nghiệp vụ tương ứng trong mảng của họ thì đều dựa trên thu thập observations từ sensors. Sau khi tracking, classifying, detecting, analysing flows của các observations thì họ aggregate kết quả và output ra những cảnh báo real-time (nghiệp vụ của giáo sư B) hoặc tích lũy vào data warehouse và rồi xử lý analytics dạng batch (nghiệp vụ của giáo sư A). Với xuất phát điểm chung từ điểm phân tích dữ liệu nên 2 giáo sư và cũng là 2 PIs quyết định hợp tác nghiên cứu. Đầu tiên là họ sẽ dành ra 6 tháng để làm proposal và họ sẽ thuê 1 postdoc làm trợ tá để làm hộ thí nghiệm pilot study (deadline của proposal là tháng 8, nhưng các giáo sư đã phong phanh nghe tin và rục rịch chuẩn bị từ tháng 2 lúc call for proposals còn đang ở dạng draft). Giai đoạn 1: Định nghĩaGiai đoạn này, có thể từ vài tháng, nhưng thường khoảng 6 tháng và output là 1 bản proposal có thể giành grant để anh em cùng nhau chiến tiếp. Thì có khi sản phẩm là cả 1 hệ thống warehouse lớn, nhưng trong giai đoạn này chỉ làm những demo, PoC hoặc pilot study nhỏ. Hoặc cùng lắm tạo ra bộ khung. Và người hiểu rõ nhất chỉ có PI. Còn kể postdoc hay Phd student hay associate thì nhìn chung cũng chỉ nhận task và làm báo cáo cho giáo sư đọc. Big picture vẫn nằm trong tay PI. Nay có 1 postdoc C có background đúng về mảng tracking, analytics như vậy thì đương nhiên 2 giáo sư sẽ phải xâu xé để giành lấy. Nhưng sẽ tốt hơn về mặt phân chia nguồn lực nếu 2 giáo sư cùng dùng chung postdoc. Thì giáo sư A chủ yếu về mảng quản lý dự án dạng big data (xử lý cũng ko cần real-time) mà có thể tích lũy và xử lý theo batch trên server trung tâm. Giáo sư là PI mảng Resource Allocation and Schedule Planning. Ví dụ như sân bay JFK thì bên trong phải có một số lượng lớn lao công hoạt động trong ngày. Nếu phân bổ nguồn tài nguyên này hợp lý có thể tiết kiệm việc sử dụng ngân sách cho việc lau dọn. Ví dụ như nếu track được 1 nhân viên lao công Y đang gần khu vực hành khách Z thì có thể tính điểm assign Y vào lau dọn Z ngay nếu trạng thái của Z đang là bẩn. Tuy nhiên khi scale bài toán lên vài trăm khu vực và vài trăm lao công chia thành nhiều ca thì bài toán không còn nhỏ nhưu vậy. Giáo sư B thì lại chuyên về Security. Ví dụ như từ camera mà phát hiện ra người khả nghi và track được người đó trong nhiều frame tiếp theo đang thực hiện hành vi phạm tội thì có thể lên cảnh báo theo ngưỡng. Ngoài cảnh báo realtime thì giáo sư còn làm early detection và prediction để dự báo những nguy cơ và ngăn chặn chúng khi chưa xảy ra trong thì tương lai gần. Các bạn thấy đấy, phần chung thì vẫn là những xử lý như vậy, sau đó thì phần riêng tuy khác nhau nhưng nếu phải thuê 2 postdocs thì hơi lãng phí nên có thể thuê 1 postdoc rồi các giáo sư thay phiên nhau dùng. Nhiệm vụ của postdoc C được thuê sẽ là phải thực hiện phân tích pilot trên dữ liệu sân bay đã. Khi được cả 2 giáo sư thông qua thì move sang phần analytics của giáo sư A trước rồi làm phần real-time của giáo sư B. Hoặc song song luôn cũng OK. Tuy nhiên, hàng tuần nên book lịch họp weekly với cả 2 giáo sư và tốt nhất lời khuyên của tôi là sớm nắm bắt cái big picture và cái phần Resource Management and Schedule Management của giáo sư A đã. Mình có 6 tháng thôi nên chủ động là không thừa!  Q: Chủ động book lịch với GS nhưng xong em cũng chả biết nói gì anh ạ.  A: (“Đụt” thế thì anh cũng chịu!) Nói chung là communicate với giáo sư cũng như là bắt sóng ăng ten ấy mà. Sóng của PI luôn là sóng tần cao (high frequency signal) mà ăng ten của em hơi kém thì cứ nói thẳng vấn đề ra để giáo sư nâng cấp cho. Chủ đề nói chuyện em nên quyết định trước khi book. Nhưng trường hợp của dự án này thì chắc chắn là liên quan tới việc xin grant. Em có thể nói thẳng cảm nghĩ ra vì giáo sư rất là open. Ví dụ em điều tra thấy độ khả thi thấp em cứ thằng thắn “Tôi thấy cái dự án này như sh. Tôi nghĩ không xin được grant đâu. ” thì giáo sư là người open ông cũng sẽ lắng nghe em thôi. Không nên để muộn quá vì bản thân PI cũng đang muốn dùng em để kiểm tra xem cái đề tài này có phải sh thật không?nên tốt nhất ngồi điều tra 1-2 hôm thấy bế tắc là phải báo ngay để PI dừng lại hoặc phân bổ em vào dự án khác cho đỡ lãng phí resource.  Q: Giai đoạn này có nên nộp paper không?  A: ôi tuyệt quá. Em cần hiểu PI cũng đang muốn dùng 6 tháng này để kiểm tra độ khả thi của proposal. Nếu nộp paper bị reject thì cũng là tín hiệu để họ biết nên dừng lại, đổi đề tài. Còn accept thì cứ thế thẳng tiến. Vì paper cũng sẽ là luận cứ để tiếp tục nên em nhớ nếu định nộp paper thì nên nộp ngay 1-2 tháng đầu. Vì chẳng may độ khả thi thấp (dự án đề tài kém) thì người ta sớm điều chỉnh lại resource hoặc revise lại proposal của grant. Chứ đừng để đến cuối giai đoạn mới nộp là không trở tay kịp. Nhìn chung thì các bạn cần hiểu là paper thì với các giáo sư nó chỉ mang tính chất xác nhận một cái grant nào đó và cái grant đó mới là cái đem lại tiền chứ paper thì chả đem lại đồng nào dù là hội nghị danh tiếng như CVPR/ICCV/ECCV/… hay tạp chí Q1 (đã ko đem lại tiền lại còn tiêu tiền đi dự hội nghị, phí đăng bài, …). Nên cố gắng nộp theo chủ để grant của giáo là tốt nhất, thì giáo còn thấy có lợi ích. Giai đoạn 2: Thiết kế và lên kế hoạchSau khi proposal được phê duyệt lần đầu, có thể sẽ phải dành thêm 1 thời gian ngắn để lên kế hoạch tỷ mỷ những việc cần làm. PI sẽ phải tìm và chỉ định các chức danh dự án như PM, PO, developers, research associates, …Sau khi có danh sách members, thì việc tiếp theo là mua sắm trang bị, rồi lên action plan. Lúc này có thể tạo ra Work Breakdown Structure (WBS), nhưng nhìn chung do đi tìm cái mới nên sẽ dừng ở mức assign mỗi member một chủ đề và coi mỗi chủ đề như 1 PJ nhỏ (sub-PJ). Và giáo sư sẽ như là supervisor trong việc lên kế hoạch cũng như thực thi. Tuy nhiên khi đã trở thành chủ nhân của 1 sub-PJ thì mỗi bạn sẽ đóng vai trò như 1 leader hoặc sub-PM. Đây sẽ là các bạn PhD hoặc postdoc có đề tài cụ thể, và tạo sub-WBS cho từng sub-PJ và assign tasks cho các bạn master BS bên dưới. Các nghiệp vụ dự án như quản lý nhân lực tài nguyên, schedule, tiến độ, communication, setup họp hành sự kiện cũng phải có xác nhận của PI và báo cáo nên đều đặn. Giai đoạn 3: Thực hiện và kiểm chứngGiai đoạn này là giai đoạn nóng nhất vì nó sẽ tạo ra output cho các sub-PJ nói riêng và PJ lớn nói chung. Tại sao phải chia nhỏ ra nhiều mảng thành nhiều sub-PJ bởi vì nếu để cả cục lớn thì nếu cục ấy ko có output là dự án chết luôn. Nhưng khi chia nhỏ thì sub-PJ này chưa có output “mới” thì sub-PJ khác có output “mới” mà mỗi lần báo cáo chỉ cần có 1 cái mới là đủ OK!Việc tiến hành vẫn phải đảm bảo tính minh bạch thông qua nghiệp vụ audit từ bên cấp fund. Ngoài ra các hoạt động báo cáo tiến độ, tố chức xử lý sự cố, vấn đề, họp hành, chia sẻ tri thức trong dự án cũng không nên coi nhẹ. Dự án nghiên cứu nên cứ chạy cho tới khi hết tiền hoặc hết hạn. Do vậy sau mỗi kỳ báo cáo năm, có thể các PI sẽ tổ chức hoạch định lại để nâng cao chất lượng output. Cũng vì thế việc thay đổi WBS có thể diễn ra theo năm hoặc nửa năm. Tốt nhất mỗi sub-PJ nên nhận một mảng và thiết lập WBS của công việc đủ chạy trong nửa năm/1 năm hoặc đến kỳ báo cáo kết quả dự án của PI với tổ chức cấp grant. Tùy đặc điểm dự án mà quá trình thực hiện và kiểm chứng có những điểm chết riêng. Ví dụ như dự án sân bay thì phải hợp tác với bên sân bay mới có thể lấy dữ liệu và chạy PoC hiện trường được. Thế nên lại xuất hiện bài toán phải collaboration mà PI phải nhìn thấu từ giai đoạn định nghĩa. Nhưng trong giai đoạn này thì việc collaboration cần nên được thực hiện rõ ràng trách nhiệm và phân công. Tránh việc nhóm nghiên cứu thực hiện task lấn sân của bên khác. Ví dụ bên sân bay phải thực hiện thu thập dữ liệu nhưng nhóm nghiên cứu sẽ chỉ đóng vai trò tư vấn giám sát trong nghiệp vụ đó. Hoặc việc lắp đặt thiết bị có phần mềm thì bên nhóm nghiên cứu sẽ làm trên nền tảng sân bay nhưng sẽ có giám sát và hỗ trợ thế nào đó từ bên sân bay. Một vấn đề hay xảy ra ở cấp quản lý resource ở đây là các bạn phải resolve conflicts giữa các thành phần công việc trong PJ lẫn sub-PJ. Các bạn ở cấp postdoc và PhD sẽ phải quản lý các bạn ở cấp dưới. Các bạn master student và BS thì sẽ tham gia với vai trò dev là chính. Nhưng đôi khi có conflict ko có tiến độ báo cáo thì cấp trên phải ra tay xử lý. Giai đoạn 4: Tổng kết và báo cáoCuối cùng thì sau một thời gian vài năm các sub-PJ chạy và mỗi năm có báo cáo output mới, thì cái mới tích lũy lại đủ to thì cũng có thể close. Nhưng nhìn chung sẽ close khi gần hết hạn hoặc hết tiền. Chứ còn tiền còn hạn thì nghĩ thêm bài toán mà chạy chứ close làm gì vội cho mất công đi xin lại. Lúc kết thúc thì cũng lại có những buổi họp đúc kết kinh nghiệm, báo cáo đề tài, chia tay và allocate/assign lại tài nguyên sang đâu đó. Nói chuyện tương lai công việc của các postdoc, PhD ai giữ lại ai cho đi, … Kết luậnTrên đây là khái lược big picture của dự án nghiên cứu theo mô hình lab mà chắc là nhắm vào đối tượng từ PhD student trở lên. Với các bạn master và BS thì có thể các vẫn tham gia ở mức DEV, tức là nhận task cụ thể rồi các bạn làm, chứ chưa nhìn ở tầm người làm quản lý. Từ PhD Student (chưa phải PhD nhé) thì đã bắt đầu nắm trọng trách trong 1 sub-PJ nhỏ của giáo mà sẽ phải làm từ định nghĩa. Giáo sẵn sàng bỏ sức ra giúp các bạn làm cho xong đề tài, nhưng về phía các bạn làm PhD là cũng phải chủ động giữ tư thế owner thường xuyên tạo ra cái mới cho giáo. Còn postdoc thì bắt đầu thành thục hơn ở cấp trợ tá cho giáo trong quản lý dự án rồi. Nói chung một lợi thế khi làm ở các lab đẳng cấp thế giới là các bạn dù làm nghiên cứu cơ bản thuần, cơ bản hẹp hay ứng dụng thì cũng sẽ được hướng dẫn để làm chỉn chu từ bước định nghĩa. Đó là một điều đáng quý và nếu có cơ hội nên chú ý học hỏi! "
    }, {
    "id": 81,
    "url": "https://wanted2.github.io/kanji/",
    "title": "日本流【幹事】について",
    "body": "2021/10/23 - 昔数回「幹事をしろう」と言われたことがあるけど、よく幹事のお仕事について少し知識不足でした。ホッとするともう2012年2013年ごろだったので、今だと8年か9年前の話でした。  プロジェクト・マネージャーには幹事力があった方がいい。 月日が経ってPM職にはなぜ【幹事力】が必要か少し理解できるようになりました。日本語の中に【カンジ】というと【漢字】と書けるし、【幹事】と【監事】とも書けます。もちろん、PMには字を読めないといけないので、【漢字力】が必要ですし、日々でプロジェクトの品質や正常な状態を保つためには、監査作業を行うのも重要で、その監査を行うには【監事力】も必要になりますが、総合的にPJをちゃんと稼働させるためには、【幹事力】は大事だなと思っています。 幹事とは日本語にはよくある現象ですが、読み方が同じものでも、書き方が違って意味が違うものがちらほらあります。【カンジ】もそのシャドウ現象の一例です。日本語学校に通った時で、2千文字以上の漢字を学んだことがあるけど、日本語能力試験には【漢字力】が必要なためです。プロジェクトマネジメントの側面に近いけれども、PJはちゃんとしているか監査作業が必要になり、監査を行った後には、監査報告書を書くことになり、【監事力】が必要な場合も珍しくありません。しかし、この二つのカンジと重なって、【幹事】というものもあります。主に飲み会の幹事などよく見られています。飲み会だろうから、PMには関係ないじゃないか。業務外の話じゃないかと思うかもしれませんが、実際にはよくPMの話と重なっています。  「自分がプロジェクトリーダーになる際不安なことはなんですか？」という質問への回答として、以下のようなお悩みをよく伺います。    目的やゴールの設定が苦手  スケジュールや計画の立て方がわからない  関係部署と綿密な連携（コミュニケーション）が取れるか不安  重視するリスクがわからない これも幹事さんの悩みと良く重なっています。幹事力を持っている方はPM力も鍛えます。 幹事をやるとPMの仕組み、【定義】、【計画・事前準備】、【実施】、【終結】などのステップを鍛えることができるメリットがあります。しかし、それだけではなく、メンバー間の関係を密にさせたり、チームを盛り上げて生産力を向上したりするために、行事を催すことも大事です。その行事をうまく開催するためには、もちろん幹事さんは必要で監事力もある場面で不可欠な要素です。 幹事力を身につくワークフローを稼働する前に: 幹事さんのお仕事、特にチームワークの時のイベントの幹事さんのお仕事は、仕事ですので、決まったフローがあります。 飲み会を催すことなんだけれども、プロジェクト管理に近いですね。 定義: まず、PMの【要件定義】と同様に、幹事職に就く時には、  目的：なんのためにこの会を行うか↓ 主役決め: 誰が何を担当するか。諸段階では、数人のPIを読んで構成を作ることもPMではよくあるシナリオで、幹事の場合、幹事さんが連絡担当や注文担当などを決めて進める。↓ 工夫: 主役メンバーの会議でどの工夫を行って主役も参加メンバーも会を楽しめるか一緒に決めといた方が良い。目的や開催テーマを決めない場合、話しづらいという会はよくありますので、事前に主役メンバーで決めて、参加メンバーの間の会話を促進すると良いでしょう。テーマを決めないでそのまま進めてしまうというありがちなミスは若手幹事にはよくあります。 事前準備: 【要件定義】はある程度出来たら、次にようやく計画段階に入れます。スケジュール管理やリソース管理は先に行い、リスク管理を考えて、上長の承認とメンバーへの通知をするという流れはPM職にはよくあります。偶然にも幹事のお仕事にも同じワークフローがあります。 まず、決めたテーマはチームのニーズに合うかどうかニーズ調査を行ってテーマを調整した方がいいです。チームのニーズにあったテーマが出来上がったら、参加者を絞って、参加者名簿を作ればいいです。後で連絡やリマインドを送るためにはメーリングリスト（ML）を作ったらいいでしょう。その次に、平行に行えると思うが、日程調整と場所調整をメンバー間で話せます。日程調整の際に、メンバーの出欠確認も同時に行えることがあります。開催時間も事前にチームで合意をとった方がよいでしょう。長すぎて拘束時間は大幅になると若者が飽きてしまうため、2時間程度で良いと思われます。場所、つまり、お店を選択する際に、メンバーのこだわりを聞いた方が良いが、事前に可能なお店のリストを作って、皆の投票で決める手段も良いです。アレルギーがあるメンバーもいるかもしれないため、場所を選択する時、考慮した方が無事です。また、すべての決断をチームで行うことも第一原則で、チームの中には透明性があげられます。 メンバーと日程と場所が決まったら、計画はほぼ完了したが、ここで一旦リスクを見直した方がいいです。リスク管理を行うときに、プロジェクトがうまくいかない原因に、「事前の下調べが足りておらず、プロジェクト進行中や後日トラブルが発生することになった」というものを気づけた方が無難です。幹事の場合、お店の選定にリスクが潜んでいます。最近は、検索サイトで便利にお店を探すことができますが、リスクマネジメントの観点から下見をおすすめします。上級者を目指すならお店の責任者と名刺交換し、会の趣旨を伝えておくと万全です。お店のホスピタリティもわかって安心ですね。あとは、資金調達もやったらいいです。支払方法を皆の都合に合わせばいいんで、終了後でも決められます。あとは、二次会が出て、遅くなるとどうするか、気配ったら良い計画になるであろう。 リスクをちゃんと管理できれば、次に、予約する前に、一回上長と相談し、経験のある方に意見をもらって承認をいただくと無難でしょう。予約が出来たら、案内状をチームに告知する必要があります。これも進捗報告という義務です。最後に、出欠確認も忘れずに行きましょう。 実施: ようやく実施に入ります。 多忙で忘れてしまうメンバーもいるので、開催当日の朝礼や前日には一回リマインドをした方がよいです。メールで連絡するとわからない人もいるので、チャットや口頭で確認したらいいです。会の前にお店へ移動します。予約時間にするときにアクセスも重要なポイントで、できるだけ皆はすぐにアクセスできるお店を選びます。予約時間に遅れそうな場合、お店に連絡します。会計の方法と時間も確認します。開始の際には、席へメンバーを誘導し、注文を始めます。進行中には、乾杯や挨拶、遅れたメンバーの追加注文などのイベントも忘れずにやっていきましょう。会話を盛り上げるためには、司会役も工夫を行いましょう。会の後半になると、スピーチなどで会を盛り上げます。終了時に忘れ物がないように、注意の声をかけ、会計もちゃんとしましょう。 事後報告: 実施終了はPJの終了ではない。同様に会の実施終了後にも幹事さんがやる仕事があります。  安否確認でメンバーがちゃんと帰宅できたかやった方が無難です。人というリソースを大事にするためです。 報告書を書いた方がいいです。上長が実施状況がわかるように、メンバーも今後の実施の改善になる要素です。 振り返りは主役メンバーで行い、今後改善する可能性を開けます。参加メンバーを誘っても良いです。ツール: 私は昔幹事さんをサポートするツールの開発にもかかわった経験があったが、それが10年前の話でした。現在、世の中に幹事さんの業務フロー全体をサポートするツールはたくさんあります。  定義や計画を書きコミュニケーションを補足するためには、GoogleのMeetやOfficeやSlackなど充実しています。 日程調整は、調整さんやPollyなどのツールがあります。PollyはチャットツールのSlackやTeamsにも連携できます。 場所調整のために、食べログなどを使えばいいでしょう。一つの場所にまとまっていないが、コンボで利用できるので、幹事さんもPMさんもコンボ利用を使っているのであろう。今後、すべて一つの場所に集中する基盤があるといいかもしれません。 なぜそれが重要か：幹事に学ぶプロジェクトマネジメントチームワークを良くする: 行事は飲み会だけではなく、PJ内で情報を共有し、透明性をあげ効率を向上する勉強会などの開催もしばしば必要になります。これらの会を行うおかげで、チームワークをスムーズに活性化できます。信頼関係を構築できるし、楽しく働けます。 チームワークの効率の改善: 長時間でコードを書いて、飽きてしまって、効率も悪くなるし、モチベーションも下がってしまうため、さぼることは避けられません。これは開発現場の実践でした。もう10年前の話だったが、幹事さんの立場を大事にしなければなりません。PMも幹事力を身に付きましょう。 結論出典： 調整さん 偶然にも、幹事さんもPMさんも似ているワークフローでチームワークを向上しています。行事は飲み会だけではなく、勉強会などでも幹事力を身についた方が良いです。例えば、↑の図で調整さんの対象行事の一覧を引用しますが、飲み会以外にも同窓会や忘年会なども対象です。  最後に確かに、PMさんもその幹事力を身につきましょう。 "
    }, {
    "id": 82,
    "url": "https://wanted2.github.io/parallelism-clients-developer-business/",
    "title": "Lập trình song song với đối ứng khách hàng: khách hàng business và khách hàng developer",
    "body": "2021/10/10 - Khách hàng business (non-dev) là những người không có chức năng tham gia trực tiếp vào quy trình phát triển. Bản thân họ có thể thuê thêm tư vấn/QA bên ngoài hoặc bên trong công ty họ để đảm bảo khâu chất lượng sản phẩm. Tuy nhiên, nhìn chung cái họ quan tâm là kết quả sản phẩm (bao gồm cả deadline, resources) hơn là quy trình. Cũng có một số khách hàng business cũng sẽ đòi hỏi về mặt quy trình, nhưng đó thường là để đảm bảo chất lượng. Vì mối quan tâm lớn nhất của họ là sản phẩm nên việc QA đảm bảo chất lượng cao là việc không thể tránh khỏi (bởi bản thân có biết dev đâu thì đương nhiên phải quan tâm QA). Bên cạnh đó, khách hàng developer là những khách hàng mà bản thân họ cũng sẽ là dev trong sản phẩm. Khác với khách hàng business là những người sẽ không trực tiếp tham gia vào quy trình tạo ra sản phẩm, khách hàng dev lại là những người tham gia cùng đội dev vào quá trình đó. Họ sẽ quan tâm việc có cùng code được với nhau (quy trình) không hơn là sản phẩm cuối cùng. Vì sao? Vì họ cũng chịu trách nhiệm làm ra sản phẩm, và họ hiểu là kết quả là do cả team cùng làm ra nên kết quả do team quyết định. Vì vậy để ra một kết quả tốt thì yếu tố quan trọng nhất chính là có làm việc được với nhau không và làm việc $=$ code đấy nhé!Chỉ cần code thôi với những khách hàng này! Nên phần lớn những vị trí không tham gia trực tiếp vào code là đa phần biến thành “người thừa” trong mắt họ. Khách hàng business (không thể dev) thì trong mắt họ quan trọng nhất là quản lý để đảm bảo tiến độ cho họ và các vị trí bảo đảm chất lượng cho họ như PM, tester, QA. Còn với khách hàng developer (có thể dev và là dev xịn) thì trong mắt họ lại chỉ quan trọng những vị trí tạo ra code (và phải là code xịn), QA, tester, quản lý có khi họ đảm nhiệm hết vì họ cũng là dev lâu năm trong nghề (có lần tôi bị phân vào dự án mà khách phủ đầu ngay là mọi việc liên quan tới những mảng kia họ sẽ làm hết, chúng tôi chỉ được phép code thôi). Cách đối đãi thì có nhiều góc nhìn, tôi thì cũng chủ yếu làm kỹ thuật nên sẽ cung cấp cách nhìn về định hướng quy trình từ đầu với từng loại khách theo hệ quy chiếu của lập trình đa luồng. Khách hàng developer: Quan trọng là anh em có cùng code được với nhau không?: Code là tất cả?: Nhưng đầu tiên cần hiểu code không chỉ là đâm đầu vào viết code như trâu bò. Code một cách chuyên nghiệp, chân chính thì có hàng tá sách vở như Clean Code, Clean Architecture mà nội dung nằm ngoài scope bài viết này. Anh em chủ động tìm đọc và contribute nhé!Code một cách có não, để được gọi là programmer (LTV) là phải đảm đương mọi khâu trong quá trình sản sinh ra code: hiểu requirements (chủ động tìm hiểu), tự thiết kế solution để giải quyết requirements, tự tay mà code lấy, tự tay test lấy ở cấp unit, và chủ động nhờ anh em code cùng test chéo ở các cấp cao hơn, nộp hàng/delivery, viết documents, tối ưu, fix bugs, …Ở cấp lãnh đạo thì sẽ phải thêm những nghiệp vụ khác như phân nhỏ nhiệm vụ, phân công, quản lý tiến trình/tài nguyên và truyền đạt cho anh em, định hướng cho anh em cách làm tốt nhất, kiểm tra và đôn đốc công việc của anh em thông qua feedback, … Nhưng còn song song hóa?: Code tốt không chỉ có việc làm đúng quy trình và vai trò (roles) mà còn phải đảm bảo hiệu suất. Bây giờ tôi đưa ra một tình huống: Tình huống: Một nhóm khách hàng dev A gồm $M$ dev, và giờ đặt hàng cùng làm việc với $N$ dev bên bạn. Họ không quan tâm kết quả ngay từ đầu vì làm agile trong quá trình làm kế hoạch sẽ thay đổi thường xuyên. Họ chỉ quan tâm anh em 2 nhóm có làm việc trôi chảy với nhau không? Họ sẽ coi quy trình làm việc là một hệ thống đa luồng (multi-threading) mà về nguyên tắc nếu thêm $N$ dev vào mà tốc độ không tăng lên $\frac{M+N}{M}$ lần thì có vẻ là một mối hợp tác kém!Còn về tài nguyên thì là không được có “người thừa, việc thừa”. Yêu cầu là gì? Thì đấy, yêu cầu chính là  Nhanh lên tối thiểu $\frac{M+N}{M}$ lần.  Tài nguyên không phát sinh “người thừa, việc thừa”. Đầu tiên là không có “người thừa, việc thừa”: Một việc mà lặp lại 2 lần 3 lần là không được trong hệ thống song song hóa tối ưu. Một nhân viên mà phải cấp phát nhiều tài khoản khác nhau là cũng tiêu tốn tài nguyên hệ thống. Có thể dùng cách nghĩ về các hệ thống federated authentication, single sign-on để suy nghĩ ra cách làm tối ưu. Và tất nhiên phải có bàn bạc đồng thuận từ phía khách để tiến hành. Hiện tại mọi người dùng nhiều những nền tảng như Atlassian hay Backlog trong quản lý rồi, nên tự làm thêm nhiều tài liệu trùng lặp với cái có sẵn là dưới góc nhìn của khách là dev chuyên nghiệp, họ sẽ chỉ có đánh giá là … kém! Còn chuyện máy móc để làm việc thì không vấn đề gì, vì thêm người thêm máy là chuyện bình thường nên thêm máy không phải việc thừa, mà thậm chí còn là việc cần thiết vì không máy thì không làm việc được! Cuối cùng vẫn là phải nhanh lên: Khi $M+N$ dev cùng chạy song song thì vấn đề là gì? Chính là conflict khi tiến hành merge các branches. Nên merge theo order nào để tối ưu hóa thời gian resolve conflict? Tình huống: Tôi thuê 1 nhóm dev làm chức năng X. Sáng nay, vì tôi cũng là dev nên tôi nhận làm feature/1, nhóm dev làm chức năng feature/2 và cần merge vào develop trong tuần này nhánh feature/1. Nhưng vì sao đó, đội kia phát triển feature/2 nhưng cần lock develop lại đến khi họ làm xong mới cho các nhánh khác merge vào. Họ lock lại develop đến hết tuần này mới unlock, dẫn đến tôi phải OT cuối tuần để kịp release. Và tôi là khách hàng, vậy ở vị trí của tôi, tôi có thể nghĩ là anh em có làm việc được với nhau không? (làm việc là code nhé) Vậy đây là lỗi của ai? Đầu tiên là người nhận lead hai feature 1 và 2 đã ôm một cục quá lớn không chịu chia nhỏ ra, kết cục là xuất hiện một nhiệm vụ mất đến 5 ngày mới xong. Các bạn cần hiểu phát triển agile đề cao tính song song, thì nếu bạn mà dev một feature nào mất nhiều công số thì có hai khả năng:  Các bạn không lock target branch thì vì thời gian estimate quá dài thì trong khi đó sẽ có rất nhiều branch khác phát triển song song và nếu họ merge trước và có conflict thì bạn phải resolve.  Các bạn lock cho đến khi xong thì người khác lại không merge được phải chờ. Kết cục là chờ lâu quá là có người phải OT mà hiệu suất song song hóa lại … kém!Lock/unlock để mutual exclusive thì là khái niệm quá cơ bản trong lập trình song song hóa. Vấn đề ở đây là đưa ra một cục estimate những 5 ngày ($=$ 1 tuần làm việc) thì là chưa tốt. Phài chi tiết hóa và chia nhỏ thành những task ở đơn vị vài tiếng/task: Phải nhanh nếu không là conflict hoặc bắt người khác chờ!Và tốt nhất là nên commit/merge sớm ngay từ những step nhỏ, chứ đừng có tích cả cục 5 ngày rồi merge một thể là thời gian quá dài. Schedule merge là phải rõ ràng, và tránh conflict sau khi review kỹ codebase. Tại sao phải tránh conflict? Là vì nếu để conflict là lại phải ngồi resolve dẫn đến tốn thêm công cho khách. Về tổng thể nhanh lên $\frac{M+N}{M}$ lần, nhưng về chi tiết là nên có thêm chỉ số về thời gian idle của từng member trong từng task và tổng thể. Anh em: Cuối cùng thì với kiểu khách hàng developer thì chúng ta đều hiểu: vì họ cũng commit code như dev bình thường nên họ sẽ nhìn $N$ dev bên bạn như những “chiến hữu, anh em”. Và chiến hữu anh em là phải hỗ trợ nhau, cùng nhau nếm ngọt trải bùi, ngậm đắng nuốt cay, và đặc biệt là không cản trở lẫn nhau. Vì vậy, trong quá trình làm ngoài việc chuyên môn code, thì có thể tập hợp những anh em cùng là code để nâng cao motivation cho anh em. Thế nên nếu có gì mà khó làm việc là sẽ dẫn đến sự đánh giá không cao. Họ không cần kết quả hay nouhinbutu gì đâu, họ chỉ cần chiến binh tốt làm việc trong vòng quay của họ thôi.  Thành tích không quan trọng, chỉ quan trọng chiến hữu! Và đương nhiên chiến hữu giỏi thì xác xuất có sản phẩm xịn sẽ cao. Khách hàng business: Khoán trọn gói thì các anh/chị làm sao thì làm, đến lúc nộp hàng xịn là ok. : Khách hàng business thì thường là quan hệ đối tác làm ăn, chứ không có chuyện chia ngọt sẻ bùi cùng anh em như khách developer. Họ không có trách nhiệm dev và cũng thường là không thể dev được, vì vậy họ thường chọn vị thế người dùng và khoán thẳng cho các bên vendor: ví dụ khoán cho bên A làm dev sản phẩm, bên B test và QA, bên C quản lý dự án. Trung tâm của cung cách làm việc này chính là sản phẩm. Các bên vendor muốn thắng thầu thường phải có thực tích và quy trình nghiệp vụ ổn. Khác với bên khách developer (vì là cùng nhau làm và cùng nhau chịu trách nhiệm) nên thường có sự thông cảm và thậm chí bên vendor nghiệp vụ kém, khách có thể sẵn sàng hỗ trợ training. Thì bên business, đã khoán thì chỉ có chiếu theo hợp đồng và quan hệ làm ăn để giải quyết là chính khi xảy ra vấn đề. Vì vậy, quản lý và bảo đảm chất lượng là những quy trình không thể không chuẩn chỉnh khi làm việc với kiểu khách này. Tất nhiên, khách developer cũng có đòi hỏi nhưng dẫu sao đã là anh em cùng nhau dev dự án thì vẫn có sự chia sẻ nhất định. Còn với khách business, thì liệu bạn có thể bắt người dùng cuối tự mổ xẻ phần mềm để tự chữa không?Trường hợp ấy thì với họ thà mua cái mới còn hơn và sản phẩm lỗi thì bị đánh giá là kém. Vậy còn song song hóa?: Với kiểu khách hàng này, thường là khoán thì đội dev thường sẽ tự mình đảm trách hết và chỉ communication với khách qua kênh sales. Sự can thiệp của khách là nhỏ, gánh nặng communication cũng trở nên định kỳ hơn. Vì vậy đội dev sẽ khá tập trung và có khi có thời gian chỉn chu testing/QA cẩn thận trước khi nộp hàng. Do đó, thường sẽ là quy trình tuần tự xong cái này mới đi đến bước tiếp theo trong quy trình. Chứ không có chuyện nhảy cóc hay song song và resolve conflict với khách. Kể cả có resolve thì đó là nghiệp vụ của bên comtor/BrSE/PM/PO/Sales, … Với khách developer thì nhìn chung khi chạy trong môi trường song song hóa tối ưu thì dù có làm chuẩn quy trình kiểu tuần tự thì cũng vẫn xảy ra conflict là chuyện có thể. Kết chung: Nhìn chung, nắm bắt tâm lý khách hàng, biết rõ loại khách hàng nào và nên làm gì với họ cho đúng và khéo léo là kỹ năng mà mọi thành viên trong team đều nên có. Biết khách của mình là ai (biz hay dev), biết rõ môi trường đang đứng là gì (song song hay tuần tự), biết rõ mình đóng vai trò gì (dev, quản lý, QA, …) luôn là những điểm quan trọng trong thực chiến dự án. "
    }, {
    "id": 83,
    "url": "https://wanted2.github.io/git-merge-rebase/",
    "title": "So sánh git merge và git rebase",
    "body": "2021/10/08 - git merge và git rebase là 2 câu lệnh quen thuộc với lập trình viên chỉ để giải quyết cùng 1 bài toán: với 2 branches được phát triển song song, nay cần migrate các tính năng của branch feature vào branch chính (main). Vấn đề là làm thế nào? Một cách chi tiết thì chỉ có 2 cách tương ứng với 2 câu lệnh ở trên tiêu đề bài viết mà chúng ta sẽ đi sâu trong bài này. Đôi lời mởCó khá nhiều hướng dẫn về cách tích hợp 2 nhánh công việc khác nhau, với nhiều cái tên “mỹ miều” như migration, integration và những tài liệu dài hàng trang chỉ để giải thích các cách làm. Nhưng nếu là LTV lâu năm đều nhìn ra “tư duy” đằng sau và biết câu lệnh đơn thuần nhất để giải quyết: người mới có khi phải làm “bằng tay” cả khối công việc đó trong một vài tháng để chỉ hiểu được “tư duy” ẩn giấu, nhưng người đã biết thì chỉ cần 1 câu lệnh như git merge là giải quyết xong công việc. Bạn thử nghĩ xem, nếu chỉ cần 1 vài giây là chạy xong git merge nhưng nếu bạn không biết và phải làm theo nguyên tắc: tức là chi tiết hóa và làm toàn bộ chi tiết của git merge bằng tay thì liệu 1 tháng có đủ không? Vì vậy cách suy nghĩ đúng là chi tiết hóa rồi trừu tượng hóa. Chi tiết hóa giúp nhận ra vấn đề là gì, sau khi nắm rõ vấn đề, trừu tượng hóa giúp nhận ra “à với công việc này, thì sử dụng công cụ này sẽ chỉ cần 1 vài dòng lệnh là giải quyết vấn đề”. Chỉ chi tiết hóa thôi thì sẽ bị sa đà vào tiểu tiết, lãng phí công số. Chỉ trừu tượng hóa thôi thì sẽ có rủi ro là bị sai hướng vì không nắm rõ chi tiết. Trừu tượng hóa trước thì sẽ bị sai hướng dẫn đến chi tiết hóa hoàn toàn sai lệch. Vì vậy chỉ có cách suy nghĩ đúng là chi tiết hóa rồi trừu tượng hóa. The devil is in the detailsTại sao trừu tượng hóa trước thì không tốt? Đơn giản thôi: bởi để migrate tính năng, ngoài merge thì còn có rebase, nếu không chi tiết bài toán cũng như sự khác biệt giữa 2 câu lệnh thì có thể đi sai hướng và dùng sai công cụ. git merge: Đầu tiên chúng ta sẽ tìm hiểu cách thông dụng nhất và hay được khuyến khích dùng nhất để giải bài toán của chúng ta. Bản chất của git merge thì cũng như hình vẽ bên: tạo 1 commit mới hẳn và chuyển tất cả tính năng của feature/1 vào đó. Đây là lựa chọn mặc định của git merge, bạn có thể thay đổi bằng thêm --squash. Chi tiết câu lệnh merge thì như bên dưới có 2 options mặc định là --ff cho fast-forward và --commit (tức là tạo commit mới hay là merge commit). 1234567891011121314151617181920212223242526272829303132333435363738$ git merge -husage: git merge [&lt;options&gt;] [&lt;commit&gt;. . . ]  or: git merge --abort  or: git merge --continue  -n          do not show a diffstat at the end of the merge  --stat        show a diffstat at the end of the merge      --summary       (synonym to --stat)  --log[=&lt;n&gt;]      add (at most &lt;n&gt;) entries from shortlog to merge commit message  --squash       create a single commit instead of doing a merge  --commit       perform a commit if the merge succeeds (default)  -e, --edit      edit message before committing  --cleanup &lt;mode&gt;   how to strip spaces and #comments from message  --ff         allow fast-forward (default)  --ff-only       abort if fast-forward is not possible  --rerere-autoupdate  update the index with reused conflict resolution if possible  --verify-signatures  verify that the named commit has a valid GPG signature  -s, --strategy &lt;strategy&gt;             merge strategy to use  -X, --strategy-option &lt;option=value&gt;             option for selected merge strategy  -m, --message &lt;message&gt;             merge commit message (for a non-fast-forward merge)  -F, --file &lt;path&gt;   read message from file  -v, --verbose     be more verbose  -q, --quiet      be more quiet  --abort        abort the current in-progress merge  --quit        --abort but leave index and working tree alone  --continue      continue the current in-progress merge  --allow-unrelated-histories             allow merging unrelated histories  --progress      force progress reporting  -S, --gpg-sign[=&lt;key-id&gt;]             GPG sign commit  --autostash      automatically stash/stash pop before and after  --overwrite-ignore  update ignored files (default)  --signoff       add a Signed-off-by trailer  --no-verify      bypass pre-merge-commit and commit-msg hooksVậy quá trình sẽ xảy ra suôn sẻ? Đương nhiên conflict có thể xảy ra và dev sẽ phải resolve bằng tay, sau đó thì dùng git merge --continue để tiếp tục merge. Trong trường hợp đang merge mà gặp phải tình huống conflict không thể giải quyết được thì git merge --abort sẽ đưa bạn về thời điểm trước khi merge. --abort đương nhiên là điều không mong muốn, mà điều mong muốn là --continue đến khi xong. git merge nhìn chung là 1 quy trình an toàn nếu bạn chịu khó làm đến cùng. Nếu mới gặp conflict đầu tiên bạn đã sợ hãi --abort ngay thì sẽ không bao giờ đi đến cùng được. Đồng thời quy trình sẽ rất tự động nếu 2 branches không conflict nhiều. Vì vậy, trước khi quyết định có merge hay không thì tốt nhất nên điều tra sơ xem có nhiều file trùng nhau giữa lịch sử 2 branches với base không?Nếu nhìn git diff sơ mà thấy khác nhau nhiều quá là có lẽ … không nên cho phép merge. Bởi như vậy không khác gì làm bằng tay (cứ tí lại gặp conflict phải resolve)! Tuy nhiên, so với giải pháp tiếp theo mà chúng ta giới thiệu thì git merge có ít rủi ro gặp conflict hơn và nhìn chung số lần resolve chỉ là $\leq 1$, nên git merge luôn luôn là giải pháp ưu tiên cho bài toán của chúng ta. git rebase: Một lựa chọn khác cho bài toán của chúng ta là git rebase. git rebase sẽ không tạo ra commit mới nào cả mà sẽ sửa lại lịch sử git của main. Như hình vẽ bên trái, bạn thấy rõ là các commit của feature/1 sẽ được bố trí xen kẽ theo thứ tự thời gian vào main. Bắt đầu từ commit đầu tiên của feature branch thì thực hiện merge và nếu gặp conflict thì dev phải resolve bằng tay rồi --continue. Cũng như git merge bạn có thể --abort để quay lại trạng thái ban đầu nếu … sợ!Bạn cũng có thể --quit, nhưng nhớ là nếu --abort dọn dẹp để đưa branch main về trạng thái ban đầu thì --quit sẽ để lại mớ hỗn độn nguyên xi đó. Trong git merge thì dù 2 branches conflict nhiều thế nào thì cũng chỉ cần 1 merge commit để chứa tất cả. Nhưng bạn thấy đấy với git rebase số lần conflict có thể tỷ lệ thuận với số commit của hai branches. Và công việc sẽ không khác gì bằng tay nếu hai branches conflict nhiều!Nhìn chung đây là cách làm nguy hiểm hơn git merge rất nhiều. Bạn có thể dùng git rebase -i để chọn lựa những commit muốn bỏ vào main nếu không phải tất cả mọi commit trên feature/1 đều có giá trị với bạn. Sau khi chỉnh sửa lịch sử thì bạn có thể cập nhật branch bằng git push -f. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ git rebase -husage: git rebase [-i] [options] [--exec &lt;cmd&gt;] [--onto &lt;newbase&gt; | --keep-base] [&lt;upstream&gt; [&lt;branch&gt;]]  or: git rebase [-i] [options] [--exec &lt;cmd&gt;] [--onto &lt;newbase&gt;] --root [&lt;branch&gt;]  or: git rebase --continue | --abort | --skip | --edit-todo  --onto &lt;revision&gt;   rebase onto given branch instead of upstream  --keep-base      use the merge-base of upstream and branch as the current base  --no-verify      allow pre-rebase hook to run  -q, --quiet      be quiet. implies --no-stat  -v, --verbose     display a diffstat of what changed upstream  -n, --no-stat     do not show diffstat of what changed upstream  --signoff       add a Signed-off-by trailer to each commit  --committer-date-is-author-date             make committer date match author date  --reset-author-date  ignore author date and use current date  -C &lt;n&gt;        passed to 'git apply'  --ignore-whitespace  ignore changes in whitespace  --whitespace &lt;action&gt;             passed to 'git apply'  -f, --force-rebase  cherry-pick all commits, even if unchanged  --no-ff        cherry-pick all commits, even if unchanged  --continue      continue  --skip        skip current patch and continue  --abort        abort and check out the original branch  --quit        abort but keep HEAD where it is  --edit-todo      edit the todo list during an interactive rebase  --show-current-patch show the patch file being applied or merged  --apply        use apply strategies to rebase  -m, --merge      use merging strategies to rebase  -i, --interactive   let the user edit the list of commits to rebase  --rerere-autoupdate  update the index with reused conflict resolution if possible  --empty &lt;{drop,keep,ask}&gt;             how to handle commits that become empty  --autosquash     move commits that begin with squash!/fixup! under -i  -S, --gpg-sign[=&lt;key-id&gt;]             GPG-sign commits  --autostash      automatically stash/stash pop before and after  -x, --exec &lt;exec&gt;   add exec lines after each commit of the editable list  -r, --rebase-merges[=&lt;mode&gt;]             try to rebase merges instead of skipping them  --fork-point     use 'merge-base --fork-point' to refine upstream  -s, --strategy &lt;strategy&gt;             use the given merge strategy  -X, --strategy-option &lt;option&gt;             pass the argument through to the merge strategy  --root        rebase all reachable commits up to the root(s)  --reschedule-failed-exec             automatically re-schedule any `exec` that fails  --reapply-cherry-picks             apply all changes, even those already present upstreamNói chung chỉnh sửa lịch sử là quá trình rất nguy hiểm!Vì vậy cần chi tiết hóa trước để tránh đi theo những cách bất thường, sau đó trừu tượng hóa để tìm ra công cụ tối ưu. Lời kếtBài viết lấy ví dụ về bài toán migrate tính năng từ feature/1 vào main. Bạn nên nhớ những Gitflow thực tế sẽ còn phức tạp hơn với những quy tắc như chỉ develop mới được merge vào main, tuy nhiên đó là chủ đề khác. Ngoài ra, nếu bài toán của bạn là bất thường, đòi hỏi phải edit history thì có thể bạn phải dùng rebase. Nhưng nhìn chung trong các trường hợp bình thường, tôi thấy hầu như đều dùng git merge tức là tạo ra merge commit mới hẳn và chuyển hết tính năng cần thiết vào. Thêm nữa, đây là migrate tính năng chứ không chỉ là files hay tài nguyên, tức là các tính năng được implement trên feature/1 phải tái hiện như thế trên main.  Nói chung tạo cái mới thì tốt hơn! "
    }, {
    "id": 84,
    "url": "https://wanted2.github.io/agile-feature/",
    "title": "アジャイル開発でタスクにアサインされる場合、直ぐにやるかどうかどうやって判定するか？",
    "body": "2021/09/27 - アジャイル開発現場では、看板に乗っているタスクをメンバーに割り当てる際に、そのメンバーは直ぐにそのタスクをやるかどうか見てみます。時々、どうやってやるかどうかを決める方法について教えてくださいとよく言われています。まずは、優先順位を決めて、一番優先の高いタスクを取ってやるから、依頼したタスクはその優先順位トップに入らない場合、断ります。トップに入るとやります。しかし、ここで微妙にどうやってその優先順位を早く決めるか方法論を考えたいですが、やはりPriorityQueueを使いたいです。第一優先のタスクを$O(1)$時間量で計算できます。次に、決定木を使うべきです。優先順位がつかない場合、いくつかのYes/No質疑でやるかどうかを決めることもできます。 優先順位でアジャイルタスクの受託を決定する日々でストーリーは新たに定められ、サブタスクに分けられます。バグがある場合、バグ報告でチケットを新規起票し、バックログに登録されます。同時に複数の複数のサブタスクとバグが発生する場合、先に何をやるべきか決定しないとまずいです。必要な作業をさぼって、不要なことをやってしまうと無駄（顧客価値に繋がらない作業）をやっちゃってダメです。 ストーリー、サブタスクとバグチケットにはポイント数が決まっており、主に  緊急性 重要性 コストなどでポイントが定められ、優先順位は総合的に評価されます。そのポイント関数は非常に重要なものです。しかし、例えば、$N=1000$タスクが看板に登録された時に、新規タスクの依頼が来ると$O(log(N))$時間量ぐらいで先にやるものを決めれないと重要なタスクに遅延が発生します。  どんなデータ構造で優先順位を列挙したり、けんさくしたりするのでしょう。 PriorityQueueを使いたいですね。キューでFirst-In-First-Out (FIFO)が実装されるが、優先付きキューだと優先度で並んでいます。冒頭には必ず一番優先度が高いタスクがありますので、素早く検索できます。 配列か循環リストを使って実装したいですが、C++でポインターは8バイトのオーバーヘッドが発生するため、配列で実装しましょう。親は子供より優先されるということなら、2分木でやります。つまり、ヒープが必要です。ヒープの挿入は$O(log(N))$時間量で削除は$O(1)$. 実装例C++の実装: C++11以降では、標準ライブラリSTLにはstd::priority_queueというテンプレートクラスが存在します。それを使ってタスク管理で優先順位を管轄することも可能ですが、なかなかSTLに勝負したいという気持ちで一回実装してみます。 子会のターゲットのタスクはこのように定義します。 12345678910struct task{  std::string name;  int p = -1;  void operator=(const task &amp;other) {    this-&gt;name = other. name;    this-&gt;p = other. p;  }};ヒープはこのように実装します。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546struct heap{  task *t;  int N;  int n = 0;};void init(heap *&amp;h, const size_t &amp;n=100){  if (h == nullptr)  {    h = new heap();    h-&gt;N = n;    h-&gt;t = (task *)::operator new(h-&gt;N * sizeof(task));    for (int i = 0; i &lt; h-&gt;N; i++) {      h-&gt;t[i]. p = -1;    }  }}void reinit(heap *&amp;h){  if (h != nullptr)  {    h-&gt;N += 100;    task *t = (task *)::operator new(h-&gt;N * sizeof(task));    memcpy(t, h-&gt;t, h-&gt;n * sizeof(task));    task *q = h-&gt;t;    h-&gt;t = t;    // ::delete q;    q = nullptr;  }  else  {    init(h);  }}void print(const heap *h){  for (int i = 0; i &lt; h-&gt;n; i++)  {    std::cout &lt;&lt;  (  &lt;&lt; h-&gt;t[i]. name &lt;&lt;  ,   &lt;&lt; h-&gt;t[i]. p &lt;&lt;  )  ;  }  std::cout &lt;&lt; std::endl;}配列は基本的に固定長で長さを変更したい場合、新しい配列を設定し、そこにmemcpyでデータ移行を行う必要です。その移行の操作はreinitの中身になります。因みに、STLでは動的に長さを変更するためには、std::priority_queueの実装でstd::vectorを使っています。後で話しますが、STLでも自分の実装でもreinitの回数で遅くなることもあります。 ここから、ヒープの実装に入ります。make_heapには、こどもは親より優先度が高いなら、swapするという話で上下関係を正しくしています。これはシフト・アップとシフト・ダウンの話です。pushはヒープにデータ挿入を行います。popはヒープから一番優先度の高いタスクを取って実装します。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162void swap(heap *h, int i, int j) {  if (i &lt; h-&gt;n &amp;&amp; j &lt; h-&gt;n) {    task t = h-&gt;t[i];    h-&gt;t[i] = h-&gt;t[j];    h-&gt;t[j] = t;  }}void make_heap(heap *h, int i) {  if (i &lt; h-&gt;n) {    int parent = (i - 1) / 2;    int left_child = i * 2 + 1;    int right_child = i * 2 + 2;    if (h-&gt;t[parent]. p &lt; h-&gt;t[i]. p) {      // SHIFT UP      swap(h, i, parent);      make_heap(h, parent);    } else {      // SHIFT DOWN      if (left_child &lt; h-&gt;n &amp;&amp; h-&gt;t[left_child]. p &gt; h-&gt;t[i]. p) {        swap(h, i, left_child);        make_heap(h, left_child);      } else if (right_child &lt; h-&gt;n &amp;&amp; h-&gt;t[right_child]. p &gt; h-&gt;t[i]. p) {        swap(h, i, right_child);        make_heap(h, right_child);      }    }  }}void push(heap *h, const task &amp;t){  if (h-&gt;n &gt;= h-&gt;N - 50) {    reinit(h);  }  auto name = t. name;  auto p = t. p;  memcpy(h-&gt;t+h-&gt;n, &amp;t, sizeof(task));  h-&gt;n++;  make_heap(h, h-&gt;n-1);}task *pop(heap *&amp;h){  if (h-&gt;n &lt; 1)  {    return nullptr;  }  task *t = new task();  t-&gt;name = h-&gt;t[0]. name;  t-&gt;p = h-&gt;t[0]. p;  h-&gt;N--;  h-&gt;n--;  h-&gt;t = &amp;h-&gt;t[1];  make_heap(h, 0);  return t;}int max(heap *&amp;h){  return h-&gt;t[0]. p;}使う利便性で考えるとクラスを作ります。 123456789101112131415161718192021222324252627282930313233343536373839#ifndef _PQ_HPP_#define _PQ_HPP_#include  x0050. hpp class PriorityQueue {  public:  PriorityQueue(const size_t &amp;n=100) {    init(h, n);  }  ~PriorityQueue() {    ::delete h;  }  void push_heap(task t) {    if (t. p &lt;= 0) {      std::cerr &lt;&lt;  Priority point must be positive!  &lt;&lt; std::endl;      return;    }    push(h, t);  }  task find_max() const {    return h-&gt;t[0];  }  task * pop_heap() {    return pop(h);  }  void print_heap() {    print(h);  }  private:  heap * h = nullptr;};#endif実験: 計画でやりますが、主に考えたい確認項目は  正確性 速度・リソース正確性: 正確性の確認で下記の実験を行います。 1234567891011121314151617181920212223242526272829#include  pq. hpp int main() {  task tasks[10] = {    { 1 , 10},    { 2 , 5},    { 3 , -7},    { 4 , 20},    { 5 , 17},    { 6 , 1},    { 7 , 6},    { 8 , 2},    { 9 , 15},    { 10 , 25}  };  PriorityQueue * pq = new PriorityQueue();  for (auto &amp;t : tasks) {    pq-&gt;push_heap(t);  }  std::cout &lt;&lt;  Max priority:   &lt;&lt; pq-&gt;find_max(). name &lt;&lt;  ,   &lt;&lt; pq-&gt;find_max(). p &lt;&lt; std::endl;  pq-&gt;print_heap();  auto t = pq-&gt;pop_heap();  std::cout &lt;&lt;  Max priority:   &lt;&lt; pq-&gt;find_max(). name &lt;&lt;  ,   &lt;&lt; pq-&gt;find_max(). p &lt;&lt; std::endl;  std::cout &lt;&lt;  Previous max priority:   &lt;&lt; t-&gt;name &lt;&lt;  ,   &lt;&lt; t-&gt;p &lt;&lt; std::endl;  task task10 = { 11 , 13};  pq-&gt;push_heap(task10);  pq-&gt;print_heap();  return 0;}10タスクは少ないけど、PriorityQueue::print_heapでヒープの内容を確認し、アルゴリズムの正確性が確認できます。最初のメンバーは優先度が一番高いものであれば、PASSです。 実験結果 1234567&gt; . \Debug\x0050. exe  Priority point must be positive!Max priority: 10, 25(10, 25) (4, 20) (1, 10) (5, 17) (6, 1) (7, 6) (8, 2) (2, 5) (9, 15)Max priority: 4, 20Previous max priority: 10, 25(4, 20) (11, 13) (5, 17) (1, 10) (7, 6) (8, 2) (2, 5) (9, 15) (6, 1) ヒープの性質としてはPASSです。 速度・メモリ: std::priority_queueに勝ちたいため、$N=1000, 10000, 100000, 1000000$で比較します。 実験コードは下記になります。 123456789101112131415161718192021222324252627282930313233343536373839#include  pq. hpp #include &lt;string&gt;#include &lt;chrono&gt;#include &lt;queue&gt;struct comp {bool operator()(const task&amp; a, const task&amp; b) {  return a. p &lt; b. p;}};int main() {  PriorityQueue * pq = new PriorityQueue(1000000);  auto t1 = std::chrono::high_resolution_clock::now();  for (int i = 0; i &lt; 1000000; i++) {    task t = {std::to_string(i), std::rand() % 1000 + 1};    pq-&gt;push_heap(t);  }  auto t2 = std::chrono::high_resolution_clock::now();  std::chrono::duration&lt;double&gt; elapsed = t2 - t1;  std::cout &lt;&lt;  Exec time =   &lt;&lt; elapsed. count() &lt;&lt;   [secs]  &lt;&lt; std::endl;  std::priority_queue&lt;task, std::vector&lt;task&gt;, comp&gt; pq_stl;  t1 = std::chrono::high_resolution_clock::now();  for (int i = 0; i &lt; 1000000; i++) {    task t = {std::to_string(i), std::rand() % 1000 + 1};    pq_stl. push(t);  }  t2 = std::chrono::high_resolution_clock::now();  elapsed = t2 - t1;  std::cout &lt;&lt;  Exec time =   &lt;&lt; elapsed. count() &lt;&lt;   [secs]  &lt;&lt; std::endl;  return 0;}実行結果はなんと 123&gt; . \Debug\x0050_big. exeExec time = 2. 31148 [secs]→自分の実装Exec time = 3. 00065 [secs]→STLの実装おー、勝ちました！ なぜなら、よく考えると、std::priority_queueは固定長の配列の欠点を克服するために、std::vectorを使っているため、フルになるとreinitと同じく長さを延長することが必要ですが、自分の実装でPriorityQueue * pq = new PriorityQueue(1000000);で最初から十分なメモリを与えて、延長の作業に時間を使っていないため高速です。 決定木：もう一つの考え方 2分木の形を取っています。これは、「やるかどうかを決定する」というよりも、「次に何をやるか」を決定するデータ構造です。いくつかの切口とする質問をおいて、Yes/Noで２派に分けて、実装パスを定める方法です。やはり、優先度で決めた方がいいですね。 "
    }, {
    "id": 85,
    "url": "https://wanted2.github.io/secure-browser/",
    "title": "セキュアブラウザについて",
    "body": "2021/09/09 - 中小企業向けテレワークセキュリティガイドライン（第5版）が令和3年5月には公表されました。無料で直ぐに点検できますので、結構貴重な資料であろう。その中で、セキュアブラウザを用いたテレワークのセキュリティガイドラインも一章ぐらいあります。普通にウエブサイトを閲覧するためのIEやChromeなどもブラウザですが、セキュアブラウザって何でしょうか？さらに、一般のブラウザと比べると付加価値とは何か？ひょっとしたら調べていきます。専門家レベルに対し豆知識かもしれないが、市民講座に参加するエンドユーザのレベルで理解できなければ、普及を妨げるし、セキュリティ向上につながらないため、頑張って調べます。 皆はセキュアブラウザだなかなか、Firefox、Chrome、IEのホームページで見ると皆はセキュアだと主張しています。「ウエブサイトのコンテンツを視聴できるブラウザに、セキュリティとプライバシー保護の機能組を十分に実装されている」ものはセキュアブラウザであるみたいです。コンテンツ視聴には、見る、聴く、話す、情報共有だけではなく、検索、フィルターなどの操作もできるが、一応、コンテンツの知識で、ここではネット上でユーザがデータをやり取りする操作のすべてを指すとしましょう。これらの機能は「ブラウザ」の基礎機能となり、最低限で実装すべきの機能でしょう。 それに加えて、セキュリティとプライバシー保護の機能組はなぜ必要になるか？    セキュリティ: 悪意のあるユーザが誠実ユーザや組織から情報を盗んだり、攻撃に利用したりする恐れもあります。ブラウザの領域だと、組織よりもエンドユーザの情報の盗聴の可能性があります。もちろん、そのエンドユーザが会社の従業員である場合、組織の機密情報レベルに関係します。ですので、システムレベルでポリシーを設定し、ブラウザの挙動を制限する政策も企業のガバナンスの立場になります。しかし、それだけでは足りず、ブラウザ自体にも対策を備えないとインシデントが多発する可能性もあります。要するに、ブラウザ自体にはどれぐらいエンドユーザをネット上の機器から守れるかセキュア性を評価する指標になります。ちなみに、システムレベルで設定する方法は受動的であり、接続先のIPなどのコネクションの情報を見てブロックするというルール化するプロセスも有効な方法です。ここで、セキュアブラウザには受動ではなく、エンドユーザ自身の能動的に活動させる方法もあります。戦争で、相手軍の動きを待たずに、先に主導権をとって、有利地を確保する手です。     プライバシー保護: 先に説明した盗聴の話と少し違います。業務での機密情報ではなく個人情報の話かと思います。エンドユーザはネット上で活動し、活動履歴が出てきます。名前、メールアドレス、住所といった真の個人情報だけではなく、活動履歴も個人情報です。また、金融機関が発行するクレジットカードの番号や明細なども個人情報として扱われるべきであろう。福祉サービスや行政サービスと連携する個人番号、保険情報、住民票、住民移動履歴なども個人情報として扱います。機密情報がバレルと会社の利益に損害が発生する恐れがわかりやすいが、個人情報が盗聴すると何が起きるか？例えば、不注意でクレジットカードの番号が怪しい人に見せて、そのあとに講座から大金額が引き落としたなどエンドユーザ側で情報漏洩が発生するインシデントもあります。サービス経営側でユーザの個人情報を漏洩してしまうインシデントもあります。   ベルシステム24事件: クレジットカード会社の派遣社員による悪用が原因の情報漏えい  クレジットカード会社のコールセンター「ベルシステム」に勤める契約社員が、顧客の情報を不正に使用する犯罪もありました。コールセンターの業務で知り得た顧客のクレジットカード番号とセキュリティコードなどを、悪用目的で盗み出したというものです。顧客のクレジットカードにより購入した商品の額は900万円近くにのぼりました。情報セキュリティ部門に関連する部署の職員犯行により、社会的にも大きく騒がれました。 セキュアブラウザはもちろんサービス運営側よりもエンドユーザ側に主導的なアプローチを適用し、セキュリティとプライバシー保護の両方を充実します。金融機関には不正送金やクレジット窃盗などの詐欺（Fraud）を検出する受動的なアプローチも多く実施されるが、今回のセキュアブラウザはエンドユーザ側の対策になります。 主要機能：アバストセキュアブラウザの例前節で説明したように、コンテンツ視聴機能組には、十分にセキュリティとプライバシーを守る機能組を足してセキュアブラウザになりますが、情報漏洩をエンドユーザ側で対策を備えることになります。どんな機能が実装されるか見てみましょう。今回はアバストセキュアブラウザを例にして解析します。    アンチウィルスと連携: マルウエアやランサムウェアによる攻撃が増え続けています。ハッカーが対象のデバイスに情報がなくてもそのデバイスをコントロールし、攻撃に利用する恐れもあります。セキュアブラウザはアバストアンチウィルスに連携し、ウエブページ閲覧の時にマルウエアとランサムウェアをブロックすることでより安全なブラウジング体験を提供します。     バンクモード1: オンラインでショッピングやバンキングをするときに、金融情報を求めたり、フォームに入力したりすることがあります。バンクモードは、実際のPCの内部で安全かつクリーンなPCとしての役割を果たす仮想デスクトップを提供します。バンクモード仮想デスクトップは、悪意のあるスクリプトの挿入、キーストロークロギング、サードパーティアプリによるスクリーンショットの試行からユーザーを保護します。オンラインバンキングのサイトにアクセスするときや、オンライン決済を行うときは毎回、バンクモードを使用することをおすすめします。   VPN: 仮想プライベートネットワークの略語です。下記に述べるプライベートモードとは違い、エンドユーザの活動をすべての第三者の侵入者から守れます。第三者の侵入者はハッカーというだけではなく、インターネットプロバイダーや政府からも保護します。アバストセキュアブラウザにはVPN機能が実装されていますので、そのまま使うとインターネット上でより安全な体験ができます。Source: Restore Privacy    アドブロック: 余計な広告をブロックする話です。3つのレベルで設定できます。「最小限」レベルでは、バナーやポップアップ、音声付き自動再生動画をブロックするなどできます。「バランス」レベルでは、ウエブ閲覧を低速化するとされた広告をすべてブロックします。これはデフォルトレベルと設定されます。「厳格」レベルでは、ほぼすべての広告をブロックします。一部のウエブサイトへのアクセスができなくなり、読み込めないなどもあります。     指紋採取対策: ネット上での活動経歴も個人情報です。サービス運営側と広告ネットワークがユーザの活動経歴を求めたいです。なぜなら、その履歴で最適な体験、広告をユーザに提供します。CookieやIPアドレスでユーザを特定する手法も古くて、ユーザ固有のブラウザ設定でユーザを監視できます。しかし、活動経歴をウエブサイトと広告ネットワークから守りたい場合、ユーザ固有のブラウザ指紋（ブラウザバージョンやインストールした拡張機能など）を隠す必要があります。できるだけ、VPNをオンにしてからこの機能もオンにすることで追加プライバシー保護レイヤーを設定できます。     モバイル保護: 近年、モバイルデバイスの普及により、携帯モバイルでのインシデントも多発しています。よって、PC版ではなく、モバイルでコンテンツ視聴を守る必要あります。アバストセキュアブラウザはAndroid版とiOS版も提供しています。PCとともにモバイルにも導入しましょう。     フィッシング対策: 訪問先のウエブサイトを危険なリストに照合してチェックすることでフィッシングのような詐欺サイトへのアクセスを防止できます。     トラッキング対策: 訪問先でトラッキングスクリプトをスキャンし、それらユーザーの活動経歴をトラッキングする行為を発見します。この機能により、ユーザの訪問経歴を広告会社と分析会社から守れます。     プライベートモード: プライバシー保護のために、活動経歴を保存せずに、サクション終了時にCookieやウエブキャッシュ、セクションデータをすべて破棄します。     パスワードマネージャー: オンラインで安全にウエブサイトを閲覧できるために、かならず異なるサイトで異なるパスワードを設定することが必要です。それに伴い、設定パスワードが多発しているので、パスワードを忘れずに保管して管理する手法も必要となります。エンドユーザは自らパスワードを記憶することが不要で、付箋や紙媒体にパスワードを記録することも不要です。さらに、入力時に、パスワードを打たずに、ワンクリックで素早く入力できることで、再入力で漏れることもないでしょう。アバストセキュアブラウザにはパスワードマネージャーが常にオンにしましょう。ブラウザ（デフォルト）のものを使っても良くて、外部のパスワードマネージャーへの連携も可能となります。     拡張機能ガード: 拡張機能が役に立ちますが、同時にエンドユーザを悪意のあるハッカーとエクスプロイトにさらされるため、拡張機能をチェックする必要があります。アバストセキュアブラウザの拡張機能ガードをオンにすると、拡張機能の身元を確認し、信頼できない拡張機能のインストールを防止できます。自分で信頼できると判断したが、拡張機能ガードによりブロックされた場合、設定で例外を追加できますが、例外を追加する前に信頼性を必ず確保してください。     プライバシークリーナー: この機能で閲覧セクションで貯蓄された不要なデータを一括削除できます。閲覧履歴やキャッシュされた画像やCookieやダウンロード履歴などを削除できます。     ハックチェック: メールアドレスを入力することで、そのメールアドレスにリンクされたすべてのオンラインアカウントを監視し、個人情報が漏洩された場合、アラートを上げることになります。  まとめセキュアブラウザの機能についてアバストセキュアブラウザの例でわかりました。コンテンツ視聴だけではなく、安全に閲覧できるために、セキュリティとプライバシー保護の機能組を十分に実装する必要があります。セキュアブラウザはあくまでもエンドユーザ側を保護するための手法であり、テレワークでは不十分である可能性もありますが、チェックリストをキチンとさらされてポリシー化し、受動的手法も能動的手法もいろいろと活用しましょう。 上記の機能組では、VPNはアバストセキュアブラウザで有料機能になるが、Pro版にアップグレードするか、外部の無償VPNを利用する手法が必要です。 参考文献      (https://support. avast. com/ja-jp/article/Use-Bank-Mode/)(https://support. avast. com/ja-jp/article/Use-Bank-Mode/) &#8617;    "
    }, {
    "id": 86,
    "url": "https://wanted2.github.io/c4-diagrams-software-design/",
    "title": "C4 diagrams for software architecture visualization - Context, Containers, Components, and Code -",
    "body": "2021/09/03 - Using MS Word and Excel, an engineer can describe the system in language and visuals. Such conventional methodologies can be sufficient for small architecture with few components. When the software system scales and changes frequently, maintaining records of the architecture by documents can be tedious. Hence, the lack of interaction in conventional document methods may present a stiff learning curve for a new member to the project team. To cope with these challenges, the C4 diagram model12, [1] was created to give interactive views of the software architectures. Systems are modeled at four different levels: system context, containers, components, and code. These levels are represented by diagrams. Intuitively, we can see the C4 diagrams as large visualization systems in which engineers can zoom in and out to see the details and the big picture. The C4 modelThe cure of representation: Knowledge sharing is important in software projects. Using representations like documents and diagrams, teams can keep the understanding of the product identically among team members. Also, having such powerful representations can keep stakeholders motivated. A good representation of the software projects, especially the architectures, helps audiences to capture the mechanism of the project at a glance.  A good representation should be lightweight but doesn’t omit any aspects of the software product. It should contain as many details as possible but also must be compact and fast. I had experiences in several projects where knowledge sharing is important. People realized the need, and they tried to share by different means: verbal conversations (i. e. , meetings or discussions), visual communications (presentations and diagrams), and textual things (documents by MS Office, notes, …). However, things will be like “a ton of documents” have been produced, and new members need to read all when joining the project. Then when a business staff wants to have a short and compact description to put in presentations (and will give the presentation to the clients or investors), someone will need to do the summarization after reading the whole bundle. Such textual summary was a good means for knowledge sharing, but the compression ratio was not quite good: several sentences can capture only some aspects of the projects, and then QA sections always come for clients and members to understand more. 1913 Piqua Ohio Advertisement - One Look Is Worth a Thousand Words. Source: Wikipedia Another direction to find a compact representation is using visual information.  A picture is worth a thousand words. Diagrams and interactions are always good things. Some studies have shown that children will learn faster by visual representations like pictures and interesting pictures. Finding such a compact and meaningful visual representation also needs to compress the whole architecture into small diagrams which are organized hierarchically. In other words, architects who find such representations also need to refine, prune and search for compact architectures to put into the compact diagrams. Source: c4model. com Therefore, the search for good representations faces the tradeoff between the learning cost and the compactness of representations. What C4 model provides is a compact, interactive and hierarchical representation of any software architecture. The model divides the architectural diagrams into four levels: context, containers, components, and code. Low-level representations such as code and components represent details for implementations and maintenance. Thus, they should be used by developers. High-level representations such as context and containers represent an overview of the architecture such as system diagrams and container architecture (container here is not Docker!). Another aspect of a good representation is handling of changes. Projects always change: requirements change, designs change, people change, code change, architectures change, . etc. Then the representations will change. Having a general model which is valid for many software architectures is a challenge. Neither searching for such an architecture can be done soon, but for the short-term thinking, we need a representation that needs to change only a little even when the architectures change a lot.  A good representation must be not only robust to changes in micro-level details (code and components designs) but also persistent to changes at macro-level architectures and management. Metamodel and notations: The author of C4 model Simon Brown once talked about the invention as follows.  The C4 model was created as a way to help software development teams describe and communicate software architecture, both during up-front design sessions and when retrospectively documenting an existing codebase. It’s a way to create maps of your code, at various levels of detail, in the same ways you would use something like Google Maps to zoom in and out of an area you are interested in.  Although primarily aimed at software architects and developers, the C4 model provides a way for software development teams to efficiently and effectively communicate their software architecture at different levels of detail, telling different stories to different types of audiences when doing up-front design or retrospectively documenting an existing codebase.  The C4 model consists of a hierarchical set of software architecture diagrams for context, containers, components, and code. Elements and relationships: The following elements and relationships form the diagrams in C4. Notations and metamodel are described.       Terms   Description   Notation   Parent   Properties         Person   A person represents one of the human users of your software system (e. g. , roles, personas, etc. ).       -   Name, Description, Location (Internal or External)       Software system   A software system is the highest level of abstraction and describes something that delivers value to its users, whether they are human or not. It should be something large, contains all smaller levels of abstractions such as software containers.       -   Name, Description, Location (Internal or External), The set of containers that make up the software system       Container   An application or data store. A container is essentially a context or boundary inside which some code or some data is stored.       A software system   Name, Description, Technology, The set of components within the container       Component   A component is a grouping of related functionality encapsulated behind a well-defined interface.       A container   Name, Description, Technology, The set of code elements (e. g. classes, interfaces, etc. ) that the component is implemented by       Code   This is the lowest level in C4. The diagrams here show the details of code elements (e. g. , classes, interfaces, objects, functions, etc. ).    -   A component   Name, Description, Fully qualified type       Relationship   Relationships are permitted between any elements in the model in either direction.       -   Description and Technology   Views: The C4 model consists of 4 basic views with respect to 4 levels of diagrams.       View type   Scope   Permitted elements   Examples         1. System Context   A software system.    Software systems, People          2. Container   A software system   Software systems, People, Containers within the software system in scope          3. Component   A container   Software systems, People, Other containers within the parent software system of the container in scope, Components within the container in scope          4. Code   A component   Code elements (e. g. , classes, interfaces, etc. ) that are used to implement the component in scope       System Context diagrams describe business usecases in which the interaction between users (people) and the software system is visualized. By seeing system context, stakeholders catch the key use-cases of the system, how end-users will use the system, and so on.  Container diagrams visualize the architecture of each container. Formally, it is a diagram of users and containers.  Component diagrams visualize the internal architecture of a container with components are atomic elements.  Code diagrams show classes, interfaces, objects, and relationships. One can use UML diagrams to visualize. The class diagram or the ER diagram can be examples. Supplementary diagrams: Besides four basic views, there are several supplementary views for the C4 model to capture the dynamic and the big picture of software systems.  The System Landscape diagram shows the target software systems in a landscape with other related systems. For example, the target system is an Internet Banking System. It needs to interact with other systems in the banks like Email Systems, Mainframe Banking Systems, and other roles like Customer Service Staff and Back Office Staff. To do this, add another diagram that sits “on top” of the C4 diagrams to show the system landscape from an IT perspective. Source: c4model. com    The Dynamic diagram considers how elements in a static model collaborate at runtime to implement a user story, use case, feature, etc. One can reuse the UML communication diagram to show it.     The Deployment diagram is based on the UML deployment diagram. It illustrates how software systems and/or containers in the static model are mapped to infrastructure.  ExamplesTools: Tools for diagramming can be a lot. For only diagramming, I would like to recommend the Diagrams. net, which is very convenient for drawing system architecture. It has many toolboxes for drawing deployment diagrams, AWS/GCP/Azure Cloud architectures, etc. For modeling the software architectures with diagramming, one can refer to Archi2. A tutorial: One way to learn to draw is through the tutorial video. The following tutorial introduces all about C4 model by the author with a part of PlantUML use-case with C4. ConclusionThe C4 model solved the problem of a compact representation for software architectures without disregarding any aspects of the product. The model was designed for Agile projects and to keep the team communicating better with team members as well as stakeholders. ReferencesBrown, S. 2013. Software architecture for developers. Coding the Architecture. (2013). Details      The C4 model for visualising software architecture &#8617;        Archi – Open Source ArchiMate Modelling &#8617; &#8617;2    "
    }, {
    "id": 87,
    "url": "https://wanted2.github.io/liskov-substisution-principle/",
    "title": "The Last Pillar: The Liskov Substitution Principle",
    "body": "2021/08/29 - In 1988, Barbara Liskov [1] wrote about the substitutions of software modules:  What is wanted here is something like the following substitution property: If for each object $o_1$ of type $S$ there is an object $o_2$ of type $T$ such that for all programs $P$ defined in terms of $T$, the behavior of $P$ is unchanged when $o_1$ is substituted for $o_2$ then $S$ is a subtype of $T$. The Liskov Substitution Principle (LSP, [2], [1]) states that a software module should be built up from interchangeable parts. Any violations of the principle lead to confusion and horrible mistakes in production. In object-oriented designs, if different classes and modules behave differently, then one should not be the abstraction of the others. The principle is helpful for designs of inheritance and REST API. Liskov Substitution PrincipleThe following diagram shows a design that conforms LSP. The interface License has two implementations: PersonalLicense and BusinessLicense. Although BusinessLicense has a custom property named users, both implementations do not have custom behaviors compared to behaviors of License (which has only one method: calculateFee()). Therefore, these implementations are interchangeable as License. They are subtypes of License.  The Billing class does not depend on the concrete implementations of the interface License, and this is one of the benefits of LSP. Both are substitutable to License. Examples of violations Violations are bad! We should learn to avoid these negativities. In the previous section, we learn a positive example of the Liskov Substitution Principle (LSP). In this section, we will learn negative examples where LSP is violated. Ducks and Toys: The following design violates the LSP. In this design, a DuckToy can quack() like a Duck. However, when the remainBatteryAmount=0, it can raise an exception!Neither the Duck can raise such an exception, so the behaviors are different. The DuckToy is not a subtype of Duck. So consider the following code: 1234567891011121314class Duck { public:  Duck() {}  virtual ~Duck() = default;  void quack() {   std::cout &lt;&lt;  Just quacking . . .   &lt;&lt; std::endl;  }};Duck duck;EXPECT_NO_THROW({ duck. quack();});This code will work fine with Duck but let us substitute it with DuckToy: 1234567891011121314151617181920class DuckToy : public Duck { public:  DuckToy(int battery) : remainBatteryAmount(battery) {}  DuckToy() : remainBatteryAmount(0) {}  virtual ~DuckToy() = default;  void quack() {   if (this-&gt;remainBatteryAmount &lt; 1) {    // throw an exception   }   std::cout &lt;&lt;  Just quacking . . .   &lt;&lt; std::endl;  } private:  int remainBatteryAmount = 0;};DuckToy duckToy; // remainBatteryAmount = 0EXPECT_NO_THROW({ duckToy. quack(); // this will throw an exception});In class design, conforming LSP is more than drawing a diagram but also enforces the code: programmers must write a clean inheritance. An API design: Assume that we have a taxi driver management system. Each driver identity has a dispatch URI in the driver database. For example, we have a driver Bob, and the dispatch URI is: 1aificorp. in/driver/BobWhen there is a new request from a customer assigned to Bob, the system dispatches all information needed for a pickup like 12aificorp. in/driver/Bob/ pickupAddress/%s/pickupTime/%s/destination/%sNote that aificorp. in is the domain of a partner company, and it is different among partners. Assume that developers in aificorp. in are doing their job good. The problem arises when a new developer joins the team in a partner acme. com, and the new member dispatches the destination by the abbreviation dest! 12acme. com/driver/Alice/ pickupAddress/%s/pickupTime/%s/dest/%sWhat is the problem?That is, now we need to add an exception in our system for this partner only: 123if (partnerDomain. rfind( acme. com , 0) == 0) {  // handle the exception}Now our dispatches table is as follows.       URI   Dispatch format         Acme. com   /pickupAddress/%s/pickupTime/%s/dest/%s       *. *   /pickupAddress/%s/pickupTime/%s/destination/%s   And so, our architect has had to add a significant and complex mechanism to deal with the fact that the interfaces of the restful services are not all substitutable. ReferencesLiskov, B. 1988. Data Abstraction and Hierarchy. SIGPLAN Notices. 23, 5 (1988), 17–34. DetailsMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details"
    }, {
    "id": 88,
    "url": "https://wanted2.github.io/aggregation-segregation/",
    "title": "Aggregation and Segregation",
    "body": "2021/08/28 - Interface Segregation Principle (ISP) [1] is one of the five pillars in SOLID design principles. The main spirit of ISP is that user interfaces shouldn’t rely on features or operations they don’t need. Therefore, instead of designing an aggregated interface that contains functions for various member classes, we should segregate the functions into several sub-interfaces, for which each type of user only needs to a specific feature. When other features change, the current feature is not affected, and the user service will not be interrupted. AggregationLet’s consider the following interface. There is a HumanInst class which has three operations calculate to calculate money, walk for walking, and ride for riding. A Police class only needs calculate operation to calculate the money they collected from criminals. A Pedestrian class only needs walk, and a Rider only needs ride. However, in this aggregated design HumanInst, all actors have access to functions they don’t need. The implementation of this aggregation can be found below: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#ifndef _AGGREGATION_H_#define _AGGREGATION_H_#include &lt;iostream&gt;#include &lt;string&gt;class Human {public:  Human() {}  virtual ~Human() = default;  virtual void calculate() = 0;  virtual void walk() = 0;  virtual void ride() = 0;};class HumanInst : public Human {public:  HumanInst() {}  virtual ~HumanInst() = default;  void calculate() {    std::cout &lt;&lt;  Calculating . . .   &lt;&lt; std::endl;  }  void walk() {    std::cout &lt;&lt;  Walking . . .   &lt;&lt; std::endl;  }  void ride() {    std::cout &lt;&lt;  Riding . . .   &lt;&lt; std::endl;  }};class PoliceInst : public HumanInst {public:  PoliceInst() {}  virtual ~PoliceInst() = default;};class PedestrianInst : public HumanInst {public:  PedestrianInst() {}  virtual ~PedestrianInst() = default;};class RiderInst : public HumanInst {public:  RiderInst() {}  virtual ~RiderInst() = default;};#endifAnd the main function: 1234567891011#include  aggregation. hpp int main() {  PoliceInst police;  police. calculate();  PedestrianInst pedestrian;  pedestrian. walk();  RiderInst rider;  rider. ride();  return 0;}We can find the result: 123456$ mkdir build &amp;&amp; cd buld &amp;&amp; cmake . . $ cmake --build . $ . /Debug/aggregation. exeCalculating . . . Walking . . . Riding . . . SegregationThe Interface Segregation Principle (ISP, [1]) states that the aggregated HumanInst class in the previous section was not good enough:whenever changes happen in one function, it will force the whole to be re-compiled, and then affect all other functions and actors. The ISP guides us that the following design will be better. A Police interface (actually, in C++ we don’t have interfaces, but we can use abstract class instead), or a PoliceInst class can have only one calculate function and doesn’t rely on walk and ride. The aggregation can be persisted by making HumanInst class inherited from all Police, Pedestrian and Rider. Let’s see the implementation: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#ifndef _SEGREGRATION_HPP_#define _SEGREGRATION_HPP_#include &lt;iostream&gt;#include &lt;string&gt;class Police {public:  Police() {}  virtual ~Police() = default;  void calculate() {    std::cout &lt;&lt;  Calculating . . .   &lt;&lt; std::endl;  }};class PoliceInst : public Police {public:  PoliceInst() {}  virtual ~PoliceInst() = default;};class Pedestrian {public:  Pedestrian() {}  virtual ~Pedestrian() = default;  void walk() {    std::cout &lt;&lt;  Walking . . .   &lt;&lt; std::endl;  }};class PedestrianInst : public Pedestrian {public:  PedestrianInst() {}  virtual ~PedestrianInst() = default;};class Rider {public:  Rider() {}  virtual ~Rider() = default;  void ride() {    std::cout &lt;&lt;  Riding . . .   &lt;&lt; std::endl;  }};class RiderInst : public Rider {public:  RiderInst() {}  virtual ~RiderInst() = default;};class HumanInst : public Police, public Pedestrian, public Rider {public:  HumanInst() {}  virtual ~HumanInst() = default;};#endifAnd the main function: 1234567891011121314151617#include  segregation. hpp int main() {  PoliceInst police;  police. calculate();  PedestrianInst pedestrian;  pedestrian. walk();  RiderInst rider;  rider. ride();  std::cout &lt;&lt;  Human also can calculate, walk and ride:  &lt;&lt; std::endl;  HumanInst human;  human. calculate();  human. walk();  human. ride();  return 0;}we see the result again: 12345678910$ mkdir build &amp;&amp; cd buld &amp;&amp; cmake . . $ cmake --build . $ . /Debug/segregation. exeCalculating . . . Walking . . . Riding . . . Human also can calculate, walk and ride:Calculating . . . Walking . . . Riding . . . This design is perfect as it keeps the aggregation and resolves the segregation issues at the same time. A sample CMakeLists. txt: 1234567cmake_minimum_required(VERSION 3. 0)project(isp CXX)include_directories(${PROJECT_SOURCE_DIR})add_executable(aggregation main. cpp)add_executable(segregation main_seg. cpp)ReferencesMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details"
    }, {
    "id": 89,
    "url": "https://wanted2.github.io/mobile-and-web/",
    "title": "Phân tích xu hướng: Tại sao các hiện tượng thành công gần đây lại ít có phiên bản web hơn ứng dụng mobile?",
    "body": "2021/08/26 - Cũng phải 9-10 năm trước đây, khi mới bắt đầu làm các nền tảng với mong muốn xây dựng “một phút huy hoàng rồi vụt tắt”, tôi hay thấy các nhà phát triển lựa chọn cùng lúc phát triển 3 phiên bản: Web, mobile (Android/iOS) và một bản có thể desktop. Đó là những năm 2012 và việc đó chủ yếu để tăng độ phủ của dịch vụ. Nhưng năm nay là năm 2021, và chúng ta không thể hướng dẫn các em trẻ bằng bộ não của những “ông già” mà thời huy hoàng từ những năm 2012. Năm 2021, chúng ta cũng cần cập nhật và nhìn lại những ứng dụng thành công trong 2 năm qua của kỷ nguyên COVID, tôi chợt nhận ra:ứng dụng ClubHouse thành công gọi vốn vòng C tương đương 100 triệu đô lẻ nhưng không hề cung cấp giao diện web, chỉ toàn ứng dụng mobile. Ứng dụng na ná thế của Việt Nam là onMic cũng không thèm chơi với web luôn!Ồ, chúng ta đã già rồi với giới trẻ có lẽ đã không còn mặn mà với việc xây một phiên bản cho desktop browser nữa. Năm 2021 phải là thời đại của mobile. Chúng ta có lẽ đã sai, và cũng nên nhìn nhận lại khả năng của mobile apps (native chứ không phải nhúng mấy cái Javascript vớ vẩn)! Responsive web apps và mobile appsTổng quan: By 2020, 90% dân số thế giới trên 6 tuổi (6,1 tỷ người) có sử dụng điện thoại di động. Nguồn: TheNextWeb Chúng ta luôn tự hỏi tại sao ngày nay lại phải học làm những thứ có thể hiển thị và hoạt động được trên mobile?Câu trả lời là vì chỉ trong vài năm, số lượng thuê bao di động trên thế giới đã phủ khắp đến 90% dân số thế giới, tính tới 2020. Điện thoại di động không phải quá khứ hay tương lai mà là hiện tại ngay trước mắt. Giải pháp để khiến dịch vụ của bạn phủ rộng tới tất cả người dùng di động hiện tại chỉ có hai nhánh chính: responsive web apps và mobile apps. Những năm 2012, chúng ta, những “ông già” phải phát triển song song 3 phiên bản vì lúc đó tỷ lệ mobile chỉ tầm 1. 3 tỷ/6 tỷ dân, tức là khoảng 23% hơn. Responsive web apps thì chủ yếu là HTML5/CSS3 có những tinh chỉnh để vừa vặn vào màn hình mobile. Lợi thế cũng khả rõ ràng:  Một phiên bản cho tất cả người dùng. cách làm web apps thì chỉ cần browser và quan trọng nhất là kết nối Internet. Ngắt mạng là … giải tán! Vận hành và bảo trì đơn giản hơn vì trang web dễ nâng cấp, bảo trì và fix lỗi. Người dùng thậm chí còn không nhận được thông báo khi xảy ra nâng cấp.  Phiên bản web rẻ hơn. SEO cũng dễ dàng hơn, và bảo trì cũng sẽ rẻ hơn. Mobile apps là những ứng dụng có thể cài đặt lên điện thoại di động và máy tính bảng. Ứng dụng di động ưu thế nổi trội sẽ là trải nghiệm người dùng tốt hơn. Nếu như trang web nhắm tới tìm kiếm người dùng mới (vì vậy không tạo barrier trong cài đặt nâng cấp), thì ứng dụng mobile tập trung vào người dùng trung thành. Thường không nên nghĩ ứng dụng mobile chỉ là bản clone của web app, nó sẽ là một suy nghĩ sai lầm!Và cuối cùng cái quan trọng là push notification, đó là ưu thế không thể từ chối của mobile apps. Ưu thế của mobile apps tập trung vào:    Tập trung vào tính năng đặc biệt của nền tảng mobile. Ví dụ người dùng Instagram có thể xem ảnh trên web vô tư nhưng để upload họ phải dùng mobile apps.   Mobile apps hoạt động trực tiếp với nền tảng, đòi hỏi cài đặt và nhận được những hỗ trợ tốt hơn về security cũng như tình năng nền tảng OS Mobile apps có thể hoạt động offline. Còn nói thực, tôi cũng làm nhiều với đủ kiểu web apps, web socket, mà nói chung cứ ngắt net là phiền! Không có Internet, đám webapps chỉ để nhìn.  Tối ưu hóa trải nghiệm người dùng Cung cấp các component native của mobile phone như network, voice, …. Lập trình web hiện đại với Vue. JS hay reactJS có khá nhiều web component, nhưng nhìn chung cũng có giới hạn và hoạt động cũng không tốt ví dụ khi ngắt net!Summary: Tạo tạm một bảng so sánh tính năng giữ hai sự lựa chọn để bạn đọc tham khảo.       Perspective   Responsive website   Mobile app         Compatibility   Mobile version of the site is equally displayed in all browsers, despite the device model.    Requires development of several applications for various platforms.        Audience   All devices that have access to the Internet.    Only smartphones and tablets.        Cost of entering the market   Payments for domain and hosting.    Developer licenses in the app stores.        Ease of use   Doesn’t require download and installation.    Requires downloading and installation.        Working offline   Not all devices support.    Possible.        Support, updates and bug fixing   Easy to update, maintain and fix bugs.    Difficult to update and monitor the application after it’s downloaded. Bugs will be fixed only in the next version.        Convenience of regular using   Average.    Good for regular using.        Personalization   Average. Mobile site is more focused on the service.    Good. App is more aimed at the individual user.    Giới thiệu ứng dụng voice chat room ClubHouseThế giới đang chuyển dần từ service-centric sang user-centric, từ web-based sang mobile-based. Hiện tượng ClubHouse nổi lên từ thàng 4/2020, tức là mới hơn 1 năm. Chúng ta sẽ nhìn nhận lại giá trị của ClubHouse trong ngành dọc: tức là các mạng xã hội âm thanh. Mạng xã hội âm thanh: Mạng xã hội âm thanh là mạng xã hội dựa trên việc chia sẻ âm thanh chứ không phải hình ảnh hay video. Các mạng xã hội chia sẻ âm nhạc như Zing MP3, hay Spotify đã tồn tại từ lâu và với hình thức chia sẻ file. Một đặc điểm của ClubHouse chính là việc chia sẻ thông qua nói chuyện tức là âm thanh cuộc nói chuyện được chia sẻ. Người tham gia không nhìn thấy mặt nhau là đặc điểm khiến cho người dùng có thể tập trung vào nội dung nói chuyện hơn là nhìn sắc mặt nhau. Hiện tượng ClubHouse: Tại sao ClubHouse lại thành hiện tượng như vậy?Bạn nên nhớ khi đạt giá trị 100 triệu đô, Facebook đã sở hữu 5,5 triệu người dùng vào năm 2005. Nhưng cùng mức giá ấy, ClubHouse chỉ có vỏn vẹn 5000 người dùng vào năm 2020. Thời cổ đại nếu một đội quân 5000 người dù dũng mãnh thế nào mà được xếp ngang hàng với đội quân 5,5 triệu người thì đó cũng là chuyện khá hiếm. Twitter khi đạt giá trị 100 triệu đô cũng có 1,2 triệu người dùng, và Instagram cũng có 1,7 triệu MAU.  Vậy cái gì làm nên giá trị của ClubHouse?ClubHouse đã thiết kế UX tức là trải nghiệm người dùng theo nguyên lý FOMO (Fear of missing out, tức là cảm giác bị lãng quên), khiến cho engagement của người dùng được nâng cao. Thiết kế này đòi hỏi 3 tính năng nhất quyết bị loại bỏ (không được implement) là: chức năng mời, chức năng lưu trữ archive, và chức năng nhận xét comment. Việc thiếu 3 chức năng, cùng với việc thiết kế room chat để chỉ có thể nghe người mình muốn nói chuyện vào lúc nửa đêm khiến người dùng càng gắn bó với app. Bởi vì họ luôn ở trạng thái cảm thấy bị miss mất cái gì đó nên sẽ không dám đóng app. Vậy tiềm năng của ClubHouse có thực sự đáng giá?Cái này có lẽ phải để thời gian trả lời. Cá nhân tôi khi dùng thử phát hiện ra một điểm tuyệt vời là thiết kế UX rất đơn giả và ưu việt. Họ không thiết kế kiểu ôm đồm mọi tính năng, mà tất cả đều tuân theo một thiết kế UX có trọng tâm. FOMO là một ví dụ về việc: không phải cứ implement hết mọi tính năng thì sẽ tốt mà đôi khi bỏ đi không thèm implement một số tính năng lại đem lại trải nghiệm tuyệt vời. Thế nên cũng không cần làm hết đâu!Ví dụ họ cũng không cần web vì đúng là với chiến thuật của họ thì mấy cái Vue với React cũng chả để làm gì cả (90% dân số thế giới đã có mobile trên tay và native app thì còn lo gì ngắt mạng) :yum: Tại thời điểm ClubHouse gọi được 100 triệu đô vốn đầu tư thì cũng có nhiều ý kiến phản hồi là tại sao lại giá cao thể trong khi chỉ có 5000 users?Hãy để thời gian minh chứng cho kết cục của startup này. Tuy nhiên, cái chúng ta học được ở đây đó là tối ưu trải nghiệm người dùng trên mobile đang là xu hướng, và web sẽ không làm được việc đó. "
    }, {
    "id": 90,
    "url": "https://wanted2.github.io/open-close-principle/",
    "title": "Nguyên lý đóng mở: Để thay đổi hãy thêm mới chứ đừng sửa cái cũ",
    "body": "2021/08/25 - Một ngày đẹp trời, khách hàng gửi yêu cầu thay đổi phần mềm để chữa lỗi hiển thị trên chức năng báo cáo tài chính. Một lập trình viên lão luyện đều sẽ hiểu ngay nguyên lý căn bản mà họ phải động đến sẽ là nguyên lý đóng mở (Open/Closed Principle hay OCP, [1]). Dù là chữa lỗi thì sửa một đoạn code có sẵn trong hệ thống cũng là hành vi sai trái so với bộ nguyên tắc SOLID của thiết kế phần mềm. Nguyên lý đóng mở (OCP) trong thiết kế phần mềm nêu rõ: “Mọi thiết kế phần mềm nên mở với thay đổi, nhưng đóng với sửa cái vốn có”. Một kỹ sư phần mềm lão luyện sẽ hiểu để chữa lỗi thì hành vi sửa cái đang code đang có do người trước viết ra (dù người đó là chính mình) đều là hành vi thiếu suy nghĩ: đúng hơn là phải thêm code mới để sửa lại tính năng bị lỗi. Nguyên lý đóng mở trong thiết kế phần mềmNguyên lý: Bertrand Meyer giới thiệu nguyên lý OCP vào năm 1988 với một dạng đơn giản như sau:  A software artifact should be open for extension but closed for modification.  Một module phần mềm nên mở với nâng cấp mở rộng, nhưng đóng với chỉnh sửa (thô bạo). Nguyên lý này phát biểu bởi hai phần: Mở với nâng cấp mở rộng và đóng với chỉnh sửa thô bạo. Chúng ta sẽ bắt đầu từ một ví dụ trước. Ví dụ: Ví dụ trên đây thể hiện thiết kế một module phần mềm đọc dữ liệu từ database, phân tích tính toán và hiển thị trên web cũng như in report. Bạn có thể thấy hai thiết kế quan trọng trong basic design ở đây là thiết kế màn (画面設計) và thiết kế bản xuất (帳票設計). Tuy nhiên, scope bài viết sẽ không đề cập sâu vào những vấn đề không cần thiết đó vội. Thiết kế này cần đảm bảo tuân thủ OCP: cần phân chia class và component sao cho phần mềm dễ dàng nâng cấp mở rộng (thêm tính năng mới) mà không cần những chỉnh sửa thô bạo vào hệ thống đã có. Trong hệ thống trên đây, mũi tên mở là chỉ quan hệ using (ví dụ Financial Report Controller thì sử dụng interface Financial Report Presenter). Mũi tên đóng chỉ quan hệ implement hoặc inheritance (thừa kế). Thiết kế component đảm bảo những component nào dễ thay đổi thì sẽ không phụ thuộc vào những component khó thay đổi hơn. Ví dụ Interactor là bộ phận chứa business rules nên cần tránh để các thay đổi của Controller hay Database làm ảnh hưởng tới Interactor. Vì vậy, Interactor là thành phần tự do nhất trong hệ thống. Những thay đổi trên Controller hay Presenters sẽ không làm Interactor thay đổi. Nhưng thay đổi trên Interactor sẽ khiến tất cả thay đổi theo. Thiết kế này mở với nâng cấp mở rộng: bạn có thể thêm các classes mới cho Presenters hay Controller tùy ý. Bạn thậm chí có thể tạo một Interactor mới và bỏ vào thay thế Interactor cũ. Thiết kế này đóng với chỉnh sửa thô bạo: thô bạo nhất có lẽ là chỉnh sửa Interactor vì nó khiến tất cả thay đổi theo. Mà đôi khi còn bất khả thi theo nghĩa: nếu đã tốn công thay đổi hết thì làm luôn cả hệ thống mới có rẻ hơn không?Việc này xảy ra khi bạn muốn thay đổi business rule (không hẳn mới mà chỉ là sửa lại cái sai). Ví dụ là muốn thay đổi request/response thôi chẳng hạn. Nguyên lý này được áp dụng trong quá trình thiết kế phần mềm, tức là sẽ phải thiết kế một phần mềm mà “rất khó để chỉnh sửa” đặc biệt chỉnh sửa một cách thô bạo thì gần như không thể. Nhưng khi cần thêm mới thì rất dễ dàng và tiện lợi. Đây kim chỉ nam cho thiết kế phần mềm mà mọi sinh viên học nghiêm chỉnh bộ môn này sẽ hiểu. Phân tích: Các dấu hiệu của một phần mềm đang theo OCP: Phần mềm theo OCP thường có thiết kế lớp lang quỷ củ kỷ luật. Chiến lược chiến thuật phòng bị hầu như không có khuyết điểm, công khó mà thủ dễ. Đơn giản nhất là khi bạn thấy muốn thay đổi 1 hàm sẽ ảnh hưởng tới rất nhiều thành phần khác trong hệ thống, tức là không thể chỉnh sửa đơn giản mà phải thực hiện can thiệp “thô bạo”, thì đó có khả năng là một phần mềm theo OCP, và đó là một phần mềm tốt. Các triệu chứng của hành vi thô bạo: Và khi phần mềm được thiết kế tốt, thì việc chỉnh sửa thô bạo chính là phần sai của lập trình viên. Vậy dấu hiệu hành vi sai trái là gì?    Sa lầy trong cuộc chiến “chỉnh sửa”: Bạn nhận lệnh chỉnh sửa và tổ chức đóng quân thực hiện nhiệm vụ mất vài ngày thậm chỉ vài tuần mà không sửa xong. Không cần biết sửa nhiều hay ít, nhưng sa lầy lâu như vậy vừa tốn tài lực bên trong lại ảnh hưởng tới bên ngoài. Rõ ràng cuộc chiến đang sa lầy.     Nội dung chỉnh sửa lớn: Bạn kiểm tra diff trên Git và thấy hàng ngàn dòng code bị thay đổi, hàng trăm hàm bị thay đổi nội dung, …Vâng, bạn đang thực hiện một cuộc can thiệp vũ trang “thô bạo”.     Sửa nhiều mà không hết lỗi, sửa chỗ này lại phát sinh lỗi ở chỗ khác: Bạn tổ chức “bình định” chỉnh sửa hàm này xong, nhưng tại một component khác trong khu vực chiếm đóng lại xuất hiện lỗi mới. Bạn đang sa lầy trong một cuộc “chỉnh sửa thô bạo” mà chỉ tốn tài lực của phía bạn thôi.  Đừng chỉnh sửa thô bạo hệ thống có sẵn: Không chỉ lúc thiết kế mà ngay cả lúc thực hiện sửa lỗi, nếu các bạn gặp 1 phần mềm mà bạn cảm thấy vô cùng khó để chỉnh sửa, và bạn càng “cố đấm ăn xôi”, càng tìm cách “mang kìm mang búa vào” để thay đổi cưỡng ép nó một cách thô bạo. Thì hành động này của bạn là thiếu suy nghĩ!Bởi vì như vậy kết quả sẽ là tạo ra một thay đổi mang tính “can thiệp vũ trang thô bạo”, và như vậy nhất định sẽ có cái gì đó không đúng: một là thiết kế phần mềm tuân thủ OCP thì bạn chính là kẻ thô bạo vi phạm OCP, hai là thiết kế phần mềm kém. Trường hợp 1 thì bạn sai vì bạn “thô bạo”. Trường hợp 2 thì bạn cũng sai vì nó đã kém bạn còn đề xuất sửa làm gì cho rách việc ra, và trách nhiệm của bạn còn lớn nữa nếu thêm mới lại “rẻ” hơn chỉnh sửa thô bạo, và thường như thế. Tóm lại nguyên lý đóng mở trong thiết kế phần mềm là kim chỉ nam không chỉ trong bước thiết kế ban đầu mà trong mọi bước thiết kế thay đổi về sau và nếu một ai tổ chức chỉnh sửa bằng “can thiệp vũ trang thô bạo”, tức là chỉnh sửa nghiêm trọng, thì người đó chắc chắn sai. Khi bắt đầu công tác thay đổi một cái gì đó trong một phần mềm được thiết kế ngon nghẻ theo nguyên lý OCP (mà thường là thế), việc đầu tiên là điều tra kỹ để tránh việc thực hiện can thiệp vũ trang thô bạo. Hãy mở rộng nâng cấp để thay đổi, hãy thêm mới: Phần mềm được thiết kế tốt nhất định sẽ theo nguyên lý OCP. Vì vậy bạn càng tìm cách chỉnh sửa thô bạo, bạn sẽ càng đi vào ngõ cụt trong nhiệm vụ thay đổi. Lúc này, bạn cần nghĩ tới việc thêm mới bởi vì cửa ngõ duy nhất khiến phần mềm OCP thay đổi theo ý bạn là thay đổi theo hướng mở rộng. Ý nghĩa của chiến lược thêm mới chính là thay vì trực tiếp tổ chức can thiệp thô bạo, thì gián tiếp tác động thông qua việc thêm chức năng mới (tức là đầu tư thêm). Ví dụ, nếu bạn thấy một Controller có lỗi, thay vì sửa thẳng vào đấy, hãy đề xuất 1 Story mới để thêm 1 controller mới với những tính năng mới và hướng lại các presenters vào controller mới. Nhưng thiết kế của controller mới nên đảm bảo là đủ các tính năng của controller cũ, và như thế sau này controller cũ sẽ không còn giá trị nữa. Bạn không phải can thiệp vào controller cũ nữa và sau này khi người ta không cần thì thực hiện quy trình deprecation thôi. Nhờ thêm mới bạn sẽ không phải dính dáng vào những cuộc “sát phạt” vô đạo đức, vi phạm nguyên tắc của ngành. Nguyên lý đóng mở và đời sốngCan thiệp “thô bạo” và đầu tư thay thế: Những cuộc can thiệp vũ trang “thô bạo” xảy ra hàng ngày trên thế giới và đương nhiên khó có thể gọi những người đó là những kỹ sư phần mềm thực thụ. Những cuộc can thiệp trong thế giới thực luôn hàm chứa một lượng “chỉnh sửa lớn”, mà bạn hiểu “chỉnh sửa lớn” trong thế giới thực là gì: giết hại phụ nữ, trẻ em, người già, đốt phá đánh bom các công sở, tòa nhà, …Những cuộc can thiệp đó thường kết thúc với sự phản kháng của phe bị can thiệp và sa lầy của bên tổ chức can thiệp, tốn kém tiền thuế của dân chúng. Khi nhìn vào những sự việc như vậy và suy nghĩ dưới góc nhìn của ngành phần mềm, chúng ta thấy may mắn vì có kim chỉ nam là những nguyên lý để có thể tự tin rằng những người đó không phải là kỹ sư phần mềm chân chính. Tôi tán thành việc đào tạo kỹ sư phần mềm không chỉ đơn giản những người biết code mà phải thấm nhuần tư tưởng và đạo đức của ngành. Đơn giản là code một cách thô bạo thì các nhà quân sự, chính trị gia, những nhà hoạt động văn hóa, họ cũng làm. Một nguyên lý đầu tiên cần phải dạy tôi nghĩ chính là OCP. Bên cạnh can thiệp một cách thô bạo, thì thực ra muốn thay đổi một đối tượng có những cách gián tiếp mà nhẹ nhàng khác như tác động bên ngoài và thường là sẽ là đầu tư thêm tính năng mới. Như đầu tư thêm trường học, tổ chức viện trợ kinh tế, giao lưu văn hóa, …Cách làm này không “chỉnh sửa” cái gì mà chỉ thêm một cái gì đó và cái có sẵn mang tính hữu nghị. Cách làm này ít điều tiếng hơn và “cởi mở” hơn. Thật thú vị khi thấy các hệ thống trong thế giới thực thường sẽ phản kháng khi có can thiệp vũ trang thô bạo, nhưng lại sẵn sàng bắt tay khi có tác động gián tiếp mang tính mở. Hướng dẫn sinh viên: Bạn có một nhóm sinh viên cần hướng dẫn và trong đó có sinh viên rất cứng đầu và chậm tiến. Bạn đứng giữa hai lựa chọn: một là can thiệp một cách thô bạo thông qua kỷ luật và tổ chức; hai là tiếp cận nhẹ nhàng thông qua bạn bè, gia đình và các thủ pháp. Bạn cần nhớ dù rằng bản tính con người có thể cứng đầu chậm tiến, nhưng ở mức độ nào đó khi đã vào đại học thì cũng có khả năng tiếp thu nhất định. Nếu bạn chọn con đường vũ trang thì sẽ vấp phải phản kháng!Tôi nhớ cách làm khá hay của một thày ngày xưa, đó là tổ chức nói chuyện và tìm cách thông qua bạn bè tổ chức để lôi kéo đối tượng vào các hoạt động. Nhưng đó mới chỉ là bước 1, bước 2 sẽ là khi đối tượng có sự cởi mở nhất định thì sẽ thực hiện giao việc. Chính là thêm mới tính năng, thông qua các giao việc đó, nhờ đối tượng làm thêm tính năng mới, nhưng thực chất là thông qua tính năng mới sửa đi cái lỗi của chính đối tượng. Đối tượng tự mình thêm tính năng mới và nhận thức được cái sai của mình. Quá trình diễn ra từ từ, nhưng cuối cùng đối tượng hiểu ra và đây mới là cách làm của người làm phần mềm. Không được giao việc ngẫu nhiên, hoặc những việc không liên quan tới đối tượng, mà phải là những việc mới hẳn nhưng bản chất là những sai lầm cũ của đối tượng. Bộ phim “Giáo sư cờ bạc” và nguyên tắc thêm mới để thay đổi: đôi khi cách trả nợ duy nhất là tiếp tục vay thêm!: Tôi còn nhớ trong bộ phim “Giáo sư cờ bạc” của Mỹ, nhân vật chính nợ một tay anh chị vài trăm ngàn đô, và bị tay anh chị sai đàn em đánh cho một trận tả tơi. Nhưng sau khi “can thiệp thô bạo” như vậy tên anh chị vẫn không đòi được một xu, thậm chí cuối cùng, khi nhân vật chính đưa ra giải pháp cuối cùng là vay thêm tương đương số tiền đã vay để anh ta tiếp tục “khởi nghiệp”, nhà đầu tư anh chị đã phải thốt lên:  Jim, anh định trả nợ cho tôi bằng cách vay thêm sao? Và cuối cùng nhà đầu tư đã phải chấp nhận đầu tư thêm và hắn đã không thất vọng khi nhận lại toàn bộ số tiền một thời gian sau. Nhà đầu tư cuối cùng sau khi can thiệp thô bạo không thành công, đã phải chuyển hướng sang cho vay thêm nhưng yêu cầu làm thêm tính năng mới và đạt kết quả. Tất nhiên, đó là 1 canh bạc, nhưng mọi khởi nghiệp đều phải sống như vậy cho đến khi thành công thực sự mà! Tài liệu tham khảoMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details"
    }, {
    "id": 91,
    "url": "https://wanted2.github.io/synchronized-and-diversity-detail-and-abstract/",
    "title": "Đồng bộ và đa dạng, cũng như chi tiết và chung chung",
    "body": "2021/08/22 - Đồng bộ (tiếng Anh: synchronized) thể hiện sự đồng thuận, thống nhất về trạng thái giữa một hay nhiều thành phần của hệ thống. Trong khi sự đa dạng (tiếng Anh: diversity) thể hiện sự phong phú, đa hình thái, trạng thái cùng tồn tại trong một hệ thống. Mỗi khái niệm đều có tầm ảnh hưởng và ứng dụng riêng và không hề thiếu tầm quan trọng trong cuộc sống hàng ngày. Sự đồng bộ trong hệ thống nhân quảMới sáng nay tôi đọc lại 1 bài báo từ năm 2019 của Báo Khánh Hòa Điện Tử về tình trạng tái xử lý rác thải nhựa ở Việt Nam nói chung và Khánh Hòa nói riêng. Nhìn chung, tôi có 1 cái nhìn khá thông cảm về vấn đề rác thải tại Việt Nam, đặc biệt là rác thải nhựa, nếu bạn biết rằng để 1 miếng rác nhựa nhỏ bé phân hủy trong tự nhiên cũng mất tới hơn 50 năm (nguồn: [1]). Vì vậy, việc xử lý triệt để những loại rác thải “khó tiêu” như nhựa và linh kiện phần mềm điện tử, đòi hỏi phải làm có quy trình quy chuẩn cẩn thận. Bước đầu tiên trong một quy trình xử lý rác thải triệt để là phân loại rác ngay từ nguồn. Ở các nước tiên tiến như Nhật hay Âu Mỹ, chính quyền đã áp dụng những quy trình phân loại rác ngay từ hộ gia đình và ăn sâu vào ý thức của người dân. Và để thói quen phân loại từ nguồn đó có mục đích: khi đưa ra bãi rác, sẽ có những quy trình xử lý tái chế khác nhau với rác khác loại. Tuy nhiên, khi đem về VN thì cái quy trình từ nguồn tới xử lý tái chế ấy, lại chỉ áp dụng được 1 nửa và về sau nhân dân chán nản cũng từ bỏ thói quen đó. Lý do thì như sau:  Công tác tái chế khó khăn một phần do đặc điểm của rác thải Việt Nam là chưa được phân loại từ nguồn. Tuy nhiên, việc phân loại rác hiện nay cũng gặp nhiều khó khăn. … trong 2 năm 2009 - 2010, Sở TN-MT đã triển khai dự án phân loại rác thải từ nguồn, thí điểm tại 2 địa phương ở Cam Ranh, trong đó có Cam Lộc. Dự án kết thúc, người dân thực hiện đúng theo yêu cầu của mô hình, tách rác hữu cơ và vô cơ theo các bao đựng có màu sắc khác nhau. Xe vận chuyển rác thải của Công ty Cổ phần Đô thị Cam Ranh vào lấy rác cũng có hộc riêng, đựng chất thải vô cơ riêng, hữu cơ riêng nhưng lại đem về xử lý chung. Càng về sau, dự án mai một, người dân nhận thấy việc phân loại không còn ý nghĩa vì rác nào cũng đem về chôn lấp tại bãi xử lý nên không mặn mà với việc phân loại nữa. Như vậy là việc phân loại chả có ý nghĩa gì vì nếu đến lúc xử lý không quan tâm tới loại thì còn phân loại làm gì?Và thế là khâu phân loại chỉ được áp dụng ở nguồn mà chẳng đem lại tác dụng gì vì khi đến bãi rác, nhãn phân loại không được dùng!Mình có thể đem nhái lại quy trình của người ta, nhưng lại chỉ áp dụng 1 nửa, còn nửa kia thì bỏ đi, dẫn đến thiếu đồng bộ trong quy trình. Nhái nhưng lại nhái nửa vời!Đây chính là sự thiếu đồng bộ nhân quả: phải dùng tới loại khi xử lý thì yêu cầu phân loại mới có động lực. Nếu không việc phân loại chỉ là một hàm thừa thãi được định nghĩa trong hệ thống, bản thân xử lý thừa thãi ấy cũng thành rác rưởi. Sự đa dạng trong chi tiết và chung chung: 2 mô hình dự án khác nhauCách đây khá lâu, tôi có giới thiệu khá sâu về quy trình quản lý dự án [2]. Thực tế, nghiệp vụ dự án không thể lúc nào cũng đồng bộ như đòi hỏi của người dân Khánh Hòa trong bài toán trước. Bạn hãy suy nghĩ hai kiểu dự án sau nhé:    Dự án 1: làm một cánh cửa vào phòng họpKhi nghe đến yêu cầu của khách hàng như vậy, bạn thấy sao? Bạn có thể lên kế hoạch hành động chi tiết tỷ mỷ, break down thành task nhỏ trong vòng 5 phút? Chuyện đó quá hiển nhiên, vì ai cũng biết cánh cửa như thế nào, và dù có nhiều kiểu dáng, nhưng ai cũng biết là làm ra nó ra sao. Việc lên một cái breakdown cho một yêu cầu kiểu này có khi còn dễ hơn ăn kẹo và dự án làm cánh cửa cứ thế mà chạy. Tuy nhiên, có một điểm chí tử trong kiểu hình dự án này: vì quá chi tiết nên rất khó thay đổi về sau (nếu thay đổi dù chỉ 1 tí cũng bỏ luôn đi làm lại cái mới tinh, sửa chữa có khi chả có tác dụng gì mà chỉ rách việc) và vì vậy, kiểu dự án này bạn sẽ thấy nó xuất hiện nhiều trong những kiểu sản phẩm quá “hiển nhiên” (ví dụ như mấy cái form login của phần mềm). Ví dụ về điểm chí tử cho dễ hình dung là một ngày kia, khách hàng yêu cầu tăng chiều cao của chiếc cửa lên gấp đôi: vậy là phải phá cả phòng họp đi để nâng chiều cao lên gấp đôi!Quản lý tiến trình của dự án kiểu này cũng khá đơn giản: cứ theo trình tự làm maintenance, tuần tự như nước chảy về nguồn.     Dự án 2: làm một cánh cửa vô hìnhBạn nghĩ sao về yêu cầu đến từ khách hàng này?Bạn có thể lên kế hoạch break down cho nó được không?Nếu bạn nghĩ là không, đây chính là lúc bạn từ bỏ cách suy nghĩ “cổ hủ” waterfall với WBS bắt buộc phải có. Và bạn đến với cách suy nghĩ Agile!Với kiểu hình yêu cầu như thế này, có những nguy cơ lớn sau:      Khách sẽ không có sự chắc chắn, mà bản thân họ cũng như bên phát triển sẽ có nhiều thay đổi. đổi chính là tử địa nếu bạn là người cứ khăng khăng là phải làm WBS bằng mọi giá ngay từ đầu. Bạn phải hiểu là sẽ có những tình huống nhưu thế này: tuần này ông khách đi dự hội nghị A về, ông ấy thấy có tính năng X hay có thể tốt cho cánh cửa, ông ấy sẽ yêu cầu làm.  Nhưng tuần sau, ông ấy đi dự triển lãm B về, ông ấy lại thấy để thực hiện việc vô hình phải có tính năng Y, ông ấy lại xóa đi tính năng X hoặc hạ độ ưu tiên của X xuống.    Ảnh hưởng bởi tốc độ thay đổi chóng mặt của công nghệ mới.  Bạn mà cứ khăng khăng là tôi phải làm WBS và tôi sẽ ép khách làm theo WBS thì sẽ không bao giờ làm được cánh cửa vô hình, vì những dự án kiểu này là những dự án cần tiếp thu công nghệ mới vốn thay đổi hàng ngày.    Bạn mà cứ kiểu đòi đồng bộ theo kiểu ép dự án 2 phải chạy theo cách của dự án 1 là sẽ dẫn đến hoàn cảnh là dự án 2 không thể thực hiện thay đổi theo yêu cầu của khách vì cách làm của dự án 1 là không welcome changes. Ở vai trò người quản lý công ty, đương nhiên nếu tất cả các dự án đều cùng 1 kiểu và cứ thế áp dụng cung cách quản lý đồng bộ thì quá tuyệt vời và tiết kiệm công sức. Tuy nhiên, thực tế thì bao giờ cũng là dự án 1 chạy theo kiểu WBS và không chào mừng sự thay đổi, còn dự án 2 thì bản chất lại luôn luôn thay đổi và khách dự án 2 thì luôn muốn thử nghiệm các tính năng mới. Vì vậy mất công sức để quản lý tốt nhiều hình thái dự án trong cùng 1 tổ chức là chuyện bình thường, thậm chỉ kỹ năng quản lý sự đa dạng ấy là cần thiết. Tất nhiên, là cần thời gian và cứng. Tâm thế của người quản lý dự án: Thực ra thì chúng ta có thể thấy sự khác nhau trong tâm thế của người quản lý mỗi loại dự án. Tâm thế của quản lý dự án 1 sẽ luôn đòi hỏi việc theo dõi và đảm bảo quy trình đã định ra chạy đúng. Quản lý dự án 1 cũng sẽ là người không welcome thay đổi, vì thay đổi luôn là hiểm nguy mà nếu cứ chạy theo đám đông là dẫn đến trễ kế hoạch, sai spec, …Nói chung dự án 1 cần quản lý cực kỳ bảo thủ và cứng, sẵn sàng đấm đá tay đôi với thay đổi. Phải là người kỷ luật, không chào đón và chấp nhận cái mới, cái thay đổi cũng ok. Luôn cảnh giác cao độ, thiết lập các form, các quy trình để đảm bảo bên dưới không có gì chạy sai dự định. Gặp cái mới cái thay đổi, nhìn chúng như kẻ thù và “táng” luôn là chuyện bình thường với kiểu quản lý này. Vì nếu không như thế là … vỡ trận!Quản lý dự án 1 như các bạn thấy sẽ đòi hỏi kinh nghiệm thực chiến đặc biệt là kinh nghiệm áp đặt, quản lý theo phong cách kỷ luật sắt. WBS anh đã viết ra rồi mà các chú làm sai dù chỉ 1 ly là ra tòa án binh luôn! Tâm thế của quản lý dự án 2 sẽ phải là người cởi mở, nhẹ nhàng, chấp nhận thay đổi, chấp nhận cái mới. Quản lý dự án 2 sẽ hay gặp những tình huống thay đổi chóng mặt, vì vậy cũng cần là người có hiểu biết và nắm bắt thay đổi nhanh. Gì chứ ít cập nhật công nghệ mới thì khó mà quản lý dự án 2 vì khách tự nhiên đòi hỏi implement tính năng mà ông ấy vừa đi hội nghị A về mà lại trả lời “em dốt, em không biết” thì có mà … giải tán!Luôn luôn phải cởi mở, sẵn sàng làm, sẵn sàng trao đổi để tìm ra giải pháp mới. Kinh nghiệm không cần, vì công nghệ toàn cái mới chả ai biết ở đâu ra (thực ra là khách đi đây đi đó xem rồi thu lượm về cũng nhiều). Người quản lý dự án 2 nên là người mềm mỏng, dĩ hòa vi quý, với cung cách quản lý tạo không gian làm việc thoải mái để anh em thỏa sức sáng tạo. Tài liệu tham khảoIlyas, M. , Ahmad, W. , Khan, H. , Yousaf, S. , Khan, K. and Nazir, S. 2018. Plastic waste as a significant threat to environment–a systematic literature review. Reviews on environmental health. 33, 4 (2018), 383–406. DetailsMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 92,
    "url": "https://wanted2.github.io/idle-time-pricing-models/",
    "title": "Idle Time Pricing Models in the Cloud",
    "body": "2021/08/19 - Batch processing (JP: バッチ処理, VI: Xu Lý Lô) is the processing of jobs that can run without end-user interaction or can be scheduled to run as resources permit. A program that reads a large file and generates a report, for example, is considered to be a batch job. Services such as Azure Batch [1] and AWS Batch [2], [3] support runs of batch jobs with VMs/EC2 instances and serverless container backend. However, running batch jobs requires VMs and instances to run continuously in production with almost zero downtime. It also means that there will be idle time to keep batch jobs running in the background. We review and estimate the idle time pricing of several cloud-based batch processing services.  Batch processing and idle timeWith the advent of cloud computing with virtualization, more and more workloads have been offloaded from conventional private clouds to public clouds in recent years. The greatest issue of these approaches is the pricing models. Virtual machines often use the pay-per-use model, which requires running the VMs continuously, and non-working hours are begin paid. For example, AWS EC2 instances are billed for running durations on an hourly basis. Because the distribution of coming requests is unknown or probabilistic, then the instances must be kept running without downtime. This results in hours without requests are being paid with the same price as hours with requests (real working hours). The issue is called idle time issue, which is well-known in cloud computing [1], [2], [3]. Batch jobs are scheduled jobs with minimal user interactions. It is possible to have some jobs in the systems which cannot be answered in real-time then users want to submit for processing in the background. The submitted jobs are queued and scheduled to process later when the application does not require them to give results soon. Scheduled jobs are popped from the queue and run automatically with interactions to computations resources such as VMs and databases. Due to a large number of jobs, a Massive Parallel Processing (MPP) system can be used for batch jobs when the number of data is too big (1TB or more)!Azure Batch [1] and AWS Batch [2] are fully managed cloud services for batch jobs which give highly scalable without extensive programming efforts. Azure Batch supports Azure VMs while AWS Batch supports EC2 instances based Elastic Container Service (ECS), Fargate-based ECS, and Lambda. Reducing idle time while deploying to these services is a demand, and it requires careful designs. For example, in [4], Qureshi described a design that reduces Azure Batch cost using dynamic allocation and relocation of resources with lookahead technique for deallocating nodes. Their results showed that 30% of the total cost was reduced with the proposed techniques. Graphical Processing Unit (GPU) has helped to accelerate the latest Artificial Intelligence workloads. Many Deep Learning applications require GPUs or even thousands of GPUs for training and inference. Building such a massive GPU system requires tedious setups, maintenance, and oversized budgets. Many public clouds such as Azure and AWS have more flexible pricing models which bill for milliseconds. Their flagship services are AWS Lambda and Azure Functions. In AWS Lambda and Azure Functions, computations are billed for running time only, which is calculated from when the resources are loaded into function memory to when the computation is finished with results is returned to an integration service like AWS API Gateway. However, Lambda and Functions do not support GPU!Hence, the normal uses of these serverless computing mechanisms are often API calls, i. e. , calling to other APIs, not any massive computations. Therefore, deploying GPU-required batch jobs to the cloud often use VM-based approaches. In [5], Risco and Moltó designed a batch processing system using AWS Batch with GPU-enabled ECS. They used 12 vGPUs of type g3s. xlarge. In summary, batch processing in the cloud relies on the pricing models that have idle time if we do not keep the system busy 24 hours per day. AWS Batch supports some serverless mechanisms like Lambda and Fargate-based ECS but only supports CPUs. For the need of GPUs, one must switch to instance-based or VM-based approaches, which turns us back to the idle time problem indeed. The choice of pricing models is whether billing for milliseconds or not? Cloud-based batch processingAWS Batch: A mistake in scale-out design: One of the most interesting examples of how the scale-out and scale-up of instances are done in AWS Batch was this QA. A user ulsa9983 asked for their tough situation of managing EC2 instances in 2017 as follows:  We have low-traffic, but want to keep some spot CPU available for quick response. We set min CPU to 2, which initially starts a reasonably small instance, e. g. an m4. large. However, after some batch requests with varying CPU and memory requirements, the cluster has grown to an m4. 16xlarge, and it stays there day in and day out. This costs us plenty more than we anticipated with the min CPU=2 setting.  Is there any way to make it scale down, or is the only alternative to set min CPU=0? The question was actually somewhat practical: there is a minVcpus parameter in the AWS Batch template1, for which when the system goes into DISABLED mode, then idle instances will be scale down to keep only minVcpus running. Setting minVcpus lower will reduce idle cost but increase the time for starting up, loading, provisioning new vCPUs. That’s why even though there is an option to scale down minVcpus=0, but ulsa9983 wanted to find another solution. Their service is low traffic, but (we) want to keep some spot CPU available for quick response. The most interesting solution came from Jamie@AWS included the following script:  Alternatively, you could achieve your goal by creating two managed to compute environments which are both associated with your job queue. For the first compute environment (with the lowest integer value for order), set min/desired/max vCPUs=2, ensuring that it always has a single EC2 instance with 2vCPUs in it. The second one computes environment could have min vCPUs=0 and max vCPUs set to whatever upper limit you like. With this model, AWS Batch jobs requiring two or fewer vCPUs could be immediately scheduled to the always-on instance while additional capacity is scaled up/down as needed when you have a larger number of concurrent jobs to run. Actually, this answer always keeps an m4. large instance to be run without being modified and having another group of resources that can be scaled up and down according to the situations. Nevertheless, on the trade-off of response time and cost, minVcpus=2 was kept, but the type of the instance was preserved to reduce cost. To find more about the parameters of AWS Batch, one can consult with2. AWS Batch is a batch computing platform with stateful jobs, job queues, GPU jobs, …Jobs are the unit of work invoked by AWS Batch. Jobs can be invoked as containerized applications running on Amazon ECS container instances in an ECS cluster. SCAR: SCAR [5] is a serverless system for GPU-enabled computing. They have their own Functional Definition Language (FDL) for serverless workflows. Their work helps to design a batch system faster and easier. One example can be found below.  This system does object detection and speech transcription for automatic captioning. Some examplar results can be seen as follows.  The above two images are from https://github. com/grycap/scar/raw/master/examples/av-workflow. Azure Batch: Azure Batch [1]3 creates and manages a pool of compute nodes (virtual machines), installs the applications you want to run, and schedules jobs to run on the nodes. There’s no cluster or job scheduler software to install, manage, or scale. Instead, you use Batch APIs and tools, command-line scripts, or the Azure portal to configure, manage, and monitor your jobs. Some important practices for designing a good batch system with Azure Batch:  Always design your Batch application for high availability Always define error handling Always care about security Best practices, . etc. Idle time pricing optionsWe summarize the pricing options. First, container-based serverless options like Fargate ECS are available but do not really support GPU use cases. This is somewhat a pity for deep learning deployments because these options provide millisecond-based billing. We will wait for GPU supports in the near future. Second, even with some parametric methodologies like setting minVcpus VMs/instance-based options still need to face the trade-off between idle time and cost. An example of the use case was given in which a mistake in scale-out design has led to a scale-up option that doubled the bills. Typically, we will need to keep minVcpus$\geq 2$ to have quick responses, but we still do not have scaledown of GPU (no minvGpus parameters in AWS Batch template). Then the idle cost is mostly unavoidable (SCAR [5] described their approach on scaling down with zero resources when no workload, then the startup time and loading time will increase in return, and they are idle costs). ConclusionFinally, we want to cut down our bills every month (but want to increase the income). :yum:This article provides some guidance to help the readers to design their approaches for cloud-based batch processing systems. Future works will direct to the practices of GPU-enabled serverless architecture with less idle time and high efficiency. ReferencesSoh, J. , Copeland, M. , Puca, A. and Harris, M. 2020. Overview of Azure Platform as a Service. Microsoft Azure. Springer. 43–55. DetailsWitte, P. A. , Louboutin, M. , Modzelewski, H. , Jones, C. , Selvage, J. and Herrmann, F. J. 2019. Event-driven workflows for large-scale seismic imaging in the cloud. SEG Technical Program Expanded Abstracts 2019. Society of Exploration Geophysicists. 3984–3988. DetailsBalaji, A. and Allen, A. 2018. Benchmarking automatic machine learning frameworks. arXiv preprint arXiv:1808. 06492. (2018). DetailsQureshi, A. N. 2019. Reduce Cost of Batch Processing Microsoft Azure Cloud. JETIR-International Journal of Emerging Technologies and Innovative Research. (2019), 2349–5162. DetailsRisco, S. and Moltó, G. 2021. GPU-Enabled Serverless Workflows for Efficient Multimedia Processing. Applied Sciences. 11, 4 (2021), 1438. Details      https://docs. aws. amazon. com/batch/latest/userguide/compute_environment_parameters. html &#8617;        What Is AWS Batch? - AWS Batch &#8617;        https://azure. microsoft. com/en-us/services/batch/ &#8617;    "
    }, {
    "id": 93,
    "url": "https://wanted2.github.io/software-maintenance-technical-debt/",
    "title": "Software maintenance and technical debt - Bảo trì phần mềm và 'nợ công nghệ'",
    "body": "2021/08/11 - Tại sao các công ty thường outsource nghiệp vụ bảo trì phần mềm ra cho những công ty chưa từng tham gia phát triển phần mềm đó?Tại sao các dự án cốt lõi có tính mới được giữ lại bên trong nội bộ?  The purpose of software maintenance is defined in the international standard for software maintenance: ISO/IEC/IEEE 14764. The objective of software maintenance is to modify existing software while preserving its integrity. [1] Tại sao cần phải bảo trì sau first-time code?Tại sao lại có technical debt?  Shipping first-time code is like going into debt. A little debt speeds development so long as it is paid back promptly with a rewrite. Objects make the cost of this transaction tolerable. The danger occurs when the debt is not repaid. Every minute spent on not-quite-right code counts as interest on that debt. Nguồn: Ward Cunningham [2] Technical debt: món “nợ công nghệ” của các dự án phần mềmĐầu tiên, chúng ta nên hình dung về bức tranh lớn của ngành công nghệ phần mềm. Tại sao lại có những người được làm cái “mới”, trong khi lại có những người phải ngồi sửa code “của người khác”?Gọi là sửa code của người khác vì cái người đã viết ra code đó đầu tiên (first-time code) thì đã không tiếp tục maintain code đó nữa, dẫn đến công ty phải thuê người khác hoặc outsource cho một công ty bên ngoài làm maintain hộ. Hoặc có thể người first-time đó vẫn còn ở đó, nhưng dự án đã xong bước code lần đầu để xây dựng nền tảng, và vì việc bảo trì sẽ rất mệt nhọc với những việc như fix bug, review, mà nội bộ làm thì lại mất đi tính công tâm, nên giờ họ sẵn sàng thuê bên ngoài để “bảo trì” cái đoạn code ấy. Từ đó, một lực lượng nhân lực của ngành phần mềm được hình thành để nôm na là “vận hành và sửa chữa một đoạn code không do mình viết ra”. Bản chất của việc bảo trì thì như định nghĩa của IEEE 14764: “Bảo trì phần mềm có mục đích sửa chữa lỗi và đảm bảo tính đúng đắn của phần mềm”. Đây là 1 quy trình trong phát triển phần mềm, nhưng về mặt lịch sử ít khi được coi trọng và thường bị đẩy outsource ra bên ngoài (xem thêm Tại sao các công ty chọn outsource?). Tức là cái gì “mới” thì để nội bộ code first-time, xong đâu đấy đến lúc chỉ còn sửa chữa fix bug, chứ không có thêm tính năng mới thì đẩy ra ngoài. Vậy nếu first-time code mà đã good rồi thì chi phí duy trì sẽ nhẹ đi biết bao nhỉ?Theo Cunningham thì đúng như vậy:  Shipping first-time code is like going into debt. A little debt speeds development so long as it is paid back promptly with a rewrite. Nhưng nếu first-time code mà nhiều vấn đề thì người tiếp nhận sẽ phải tốn nhiều công sức để sửa lỗi, tích hợp, … và kết quả là những công số đó bị tính là technical debt. Một định nghĩa gần đây của khái niệm Technical Debt [3] như sau:  in software-intensive systems, technical debt consists of design or implementation constructs that are expedient in the short term, but set up a technical context that can make a future change more costly or impossible. Technical debt is a contingent liability whose impact is limited to internal system qualities, primarily maintainability and evolvability. Các bạn có thể chú ý tới cụm từ “expedient in the short term, but set up a technical context that can make a future change more costly or impossible”, nôm na là “có lợi trước mắt trong ngắn hạn, nhưng tạo ra một tình huống kỹ thuật mà về lâu dài là có thể khiến những thay đổi xảy ra tương lai, rất đắt hoặc hầu như không thể”. Chính vì technical debt đắt đỏ như vậy, nên ít công ty sẽ để lại mà sẽ outsource ra ngoài, vì chi phí bảo trì khi khoán ngoài sẽ control được qua hợp đồng và nhìn chung là outsource sẽ rẻ đi nhiều. Software maintenanceChức năng: Chức năng của bảo trì phần mềm khá đa dạng:  sửa lỗi cải tiến thiết kế cải thiện tính năng tích hợp với các phần mềm khác tích hợp vào các hệ thống khác bao gồm cả phần cứng, phần mềm, tính năng, viễn thông, … nâng cấp phần mềm; và kết thúc dự án. 5 đặc điểm của công việc của người làm bảo trì hệ thống phần mềm:  quản lý bảo trì các tính năng với chu kỳ hàng ngày quản lý bảo trì ứng với mọi thay đổi có thể xảy ra (đối ứng sự cố, …) hoàn thiện các tính năng sẵn có phát hiện nguy cơ bảo mật và sửa lỗi bảo mật lên kế hoạch phòng chống degrading của phần mềm. Chi phí: Chi phí bảo trì thường được cho là chủ yếu do sửa lỗi, nhưng thực ra, theo 1 nghiên cứu nhiều năm thì hơn 80% chi phí bảo trì là rơi vào các hạng mục không phải sửa lỗi như nâng cấp, tích hợp, cải tiến [1]. Các điều kiện về môi trường vận hành (cả phần cứng, phần mềm) cũng như các quy chế, quy định trong công ty cũng ảnh hưởng tới việc vận hành và bảo trì. Các vấn đề: Vấn đề kỹ thuật: Các vấn đề kỹ thuật chính bao gồm:  Limited Understanding (hiểu biết hạn hẹp về phần mềm): điều này cũng dễ giải thích do code có thể không do người bảo trì viết, hệ thống phức tạp nhưng tài liệu không có gì ngoài code, … Testing: đòi hỏi test đầy đủ và chính xác. Điều này lại rất tốn công và thời gian. Regression testing là phương pháp luận thường thấy.  Impact Analysis: khi bạn muốn sửa một cái gì đó trong code, bạn có nghĩ tới những hệ lụy sẽ xảy ra với toàn thể hệ thống nói chung không? Nếu có thì bạn đã làm impact analysis rồi đấy. Để làm impact analysis nghiêm chỉnh thì cần phân tích kỹ change request.  Maintainability: như đã nói ở trên, phần mềm đôi khi rất khó để change hoặc giá để change rất đắt. Vấn đề quản lý: Vấn đề đầu tiên là mục tiêu của công ty: đôi khi công ty muốn giao hàng nhanh để tiết kiệm chi phí, và thế là maintenance trở thành khâu bị “trảm” đầu tiên. Lúc đó bạn nên đề xuất outsource để giảm chi phí. Vấn đề thứ hai là nhân sự: maintenance luôn bị coi là công việc “hạng hai”, làm sao để thu hút và giữ động lực cho đội ngũ bảo trì?Vấn đề thứ ba phân công trách nhiệm trong công ty: khi có nhiều team thì không nhất thiết team viết ra đoạn code phải maintain chính đoạn code đó. Vấn đề thứ tư là quy trình: maintenance có quy trình riêng như review, acceptance, migration, … Quy trình bảo trì phần mềm: Quy trình chung của maintenance thì như hình bên:  process implementation: quá trình này thường là do bên khác đã code sẵn.  problem and modification analysis: với code sẵn từ bên khác, bên maintenance sẽ thực hiện điều tra tìm vấn đề và ra giải pháp (solution).  modification implementation: sửa lỗi sẽ được thực hiện sau khi solution được phê duyệt thông qua.  maintenance review/acceptance: các bên thực hiện review và phê duyệt nếu đạt tiêu chuẩn.  migration: thực hiện nâng cấp hệ thống sau khi implementation được phê duyệt.  software retirement: nếu không tìm được giải pháp phù hợp thì đóng lại và kết thúc vòng đời.   Tài liệu tham khảo Bourque, P. and Fairley, R. 2004. SWEBOK. Nd: IEEE Computer society. (2004). DetailsCunningham, W. 1992. The WyCash portfolio management system. ACM SIGPLAN OOPS Messenger. 4, 2 (1992), 29–30. DetailsBehutiye, W. N. , Rodrı́guez Pilar, Oivo, M. and Tosun, A. 2017. Analyzing the concept of technical debt in the context of agile software development: A systematic literature review. Information and Software Technology. 82, (2017), 139–158. Details"
    }, {
    "id": 94,
    "url": "https://wanted2.github.io/unit-tests-clean/",
    "title": "Professionalism and Test-Driven Development - Sự chuyên nghiệp và kiểm nghiệm định hướng phát triển",
    "body": "2021/08/09 - Unit Tests [1], [2], [3] là đơn vị tests gần gũi nhất với lập trình viên. Vậy vì sao phải làm Unit Tests? Những nguyên tắc để làm UT một cách nghiêm chỉnh và chuyên nghiệp là gì?Hôm nay, xin điểm lại bài báo của R. Martin [2] xuất bản vào năm 2007 cùng một số nguyên tắc trong cuốn sách nổi tiếng Clean Code [1] đồng tác giả, để phần nào trả lời những câu hỏi này. Bài báo học thuật có thể được đọc miễn phí tại: https://fpl. cs. depaul. edu/jriely/450/extras/prof-tdd. pdf Test-Driven Development và 3 luật cơ bản Test code is just as important as production code. Phương pháp test thủ công của những năm 1990s: Khi TDD còn chưa ra đời thì làm thế nào để test được phần mềm?Ví dụ như 1 đoạn code C++ đơn giản như sau: 123std::sort(inputArray);int pivot = inputArray. size() / 2;int pivotValue = inputArray. at(pivot);Nội dung của đoạn code trên đơn giản chỉ là tìm số đứng giữa trong một mảng, hay chính là giá trị median. Như mô tả của Martin [1] thì những năm 1990s, người ta sẽ viết một chương trình có dùng đoạn mã trên với input là một mảng cụ thể và biên dịch, rồi in giá trị lên màn hình để xác nhận. Ví dụ, chúng ta sẽ set giá trị đầu vào là auto inputArray = std::vector&lt;float&gt;({1. 0, 6. 0, 3. 0, 5. 9, 7. 8});. Và rồi khi màn hình in ra giá trị pivotValue là 5. 9 thì test coi như xong. Tại sao nên test tự động và nên dùng TDD?: Các bạn chắc cũng đồng ý với Martin về định nghĩa của 1 lập trình viên chuyên nghiệp như sau chứ nhỉ?  Professional software developers ship clean, flexible code that works—on time. Một nhà phát triển phần mềm chuyên nghiệp sẽ phát hành code sạch, linh hoạt và chạy được - một cách đúng hạn. Lợi ích của phương pháp TDD [2] chính là đảm bảo phát hiện ra vấn đề và bắt nhà phát triển sửa ngay: không chỉ là chạy được mà còn là sạch sẽ và linh hoạt. Nhiều bạn từng ca thán với tôi là test tự động theo TDD thì không đảm bảo deadline được. Thì nguyên nhân chính là vì chưa nắm rõ TDD nên chưa đưa tối ưu hóa được quy trình theo TDD, dẫn đến estimate sai và trễ hạn. Còn về nguyên tắc thực hiện đúng TDD thì càng làm sẽ càng ít vấn đề và cái giá phải trả về sau sẽ nhẹ đi, nên càng về sau sẽ phải càng đúng hẹn lên. 3 luật của TDD: Trích dẫn từ [3]:  First Law You may not write production code until you have written a failing unit test. Luật thứ nhất Khi mà chưa viết ra được 1 unit test chứng tỏ fail, thì đừng viết thêm code sản phẩm.  Second Law You may not write more of a unit test than is sufficient to fail, and not compiling is failing. Luật thứ hai Đừng viết những unit test không chứng tỏ việc fail, chỉ viết những unit test đủ để cho thấy vấn đề.  Third Law You may not write more production code than is sufficient to pass the currently failing test. Luật thứ ba Đừng viết code sản phẩm nhiều hơn mức đủ để chữa những test fail. Các bạn có thể thấy 3 định luật này tạo nên một vòng lặp giữa việc viết unit test và sửa code. Đồng thời, 3 định luật cũng buộc chặt developer và tester phải tập trung viết code để sửa test fail và viết test code để chi ra fail. Chứ việc viết những code và test code không để chỉ ra fail là việc tối kỵ. Thế nên, khi tham gia TDD, bạn lại viết một đoạn code không để chỉ ra thất bại, cũng không để sửa chữa thất bại, mà chỉ để nêu ra thành công thì về nguyên tắc là không tuân thủ luật của TDD và bản thân điều đó lại là một thất bại. Viết test “sạch sẽ”Như Martin [2] đã chỉ ra, test không chỉ là test mà còn là documentation: người ta đọc test code là có thể hình dung ngay được luồng hoạt động của module phần mềm. Vì vậy, lười viết test code chính là việc làm giấu đi cái hướng dẫn sử dụng module phần mềm đó. Ngoài ra viết test “bẩn” thì lại mất đi ý nghĩa: người ta khó hình dung là test case này đang kiểm tra cái gì của module phần mềm, và module phần mềm đang làm gì! Để viết test thật “sạch sẽ” nên chú ý viết thật đơn giản tránh phức tạp. Mỗi test chỉ nên tầm vài dòng. Và nhìn chung là có hai luật kinh nghiệm sau nên được áp dụng:    Mỗi test case chỉ nên có 1 assert.     Mỗi test case chỉ nên kiểm chứng 1 concept.  Quy tắc F. I. R. S. TVề mặt tư tưởng của kiểm nghiệm phần mềm, có thể tìm đọc cuốn của Binder [3] tuy hơi cũ nhưng rất chi tiết về lĩnh vực này. Các tài liệu về kiến trúc xUnit1 hay cách viết code và thiết kế sạch sẽ [4], [1] cũng nên tìm hiểu. Ở đây chúng ta sẽ điểm qua bộ quy tắc FIRST để viết test code:  Fast: Test nên nhanh, vì nếu chậm, bạn sẽ không muốn test nhiều. Mà nếu không chạy test nhiều thì sẽ ít cơ hội phát hiện vấn đề đi.    Independent: Các test không nên phụ thuộc vào nhau. Giả sử kết quả của test A lại phụ thuộc vào 1 dòng code nào đó của test B, thì việc kết quả của A bị B chi phối sẽ dẫn đến nghi vấn là thế tình huống không có B thì A có chạy được không?Một ví dụ về trường hợp kiểu này tôi gặp từ 7-8 năm về trước như sau: bạn lập trình viên khi test để tất cả các test dùng chung kết nối database. Khi chạy xong test này bạn không clear kết quả đi, mà vẫn cho test phía sau dùng lại kết nối ấy và dữ liệu như vậy bị phụ thuộc vào test trước. Kết cục là kết quả test trở thành một mớ hỗn độn không thể tin tưởng được!     Repeatable: Test nên có thể chạy trên mọi môi trường như production, QA, thậm chí là trên máy tính cá nhân của bạn. Nếu không thì cứ mỗi lần vào 1 môi trường mới mà nó fail thì bạn lại phải giải thích tại sao nó fail.     Self-Validating: Ví dụ đơn giản là cách test thủ công từ những năm 1990s. Có lần để test thủ công, bạn tôi bị bắt truy cập vào máy chủ để đọc cả 1 đoạn log truy cập dài hàng ngàn trang. Thế nếu quá trình đọc hàng ngàn trang log ấy bị sót mất trang nào thì sao?Kết quả là bản thân việc kiểm toán đã là một quy trình thủ công và sót, dẫn đến là kết quả có khi chả chỉ ra vấn đề gì, cũng chả sửa gì. Một test tốt nên tự động, binary output (PASS hoặc FAIL thôi). Tốt nhất là không nên để người test bằng tay, hay đọc rồi trả lời câu hỏi.     Timely: Phải viết test nhanh, nên viết trước khi có production code. Cứ thử đợi lên production rồi mới viết test thì bạn thấy ngay là lúc đó production code đã bị đóng gói và rất khó test.  Ví dụ về State Machine ModelMột vấn để cuối cùng tôi muốn đề cập là cách suy nghĩ ra testcases. Theo văn hóa làm việc ngành phần mềm, thì trước khi test phải làm bản kế hoạch test mà tiếng Nhật gọi là テスト計画書. Nhìn chung, theo kinh nghiệm thì các bạn cũng chịu khó làm, nhưng không hiểu sao đôi khi lại cứ nảy sinh vấn đề thiếu này thừa kia. Thì đối với unit test, các bạn có thể tham khảo State Machine Model (SMM) là cách suy nghĩ hiệu quả và khó lọt lưới cases nhất. Mô hình suy nghĩ này tức là bạn phải liệt kê hết các trạng thái cần có cho đối tượng test. Sau đó bạn sẽ lên kế hoạch để test hết tất cả các transition giữa các trạng thái ấy. Đơn cử tôi xin lấy ví dụ về đối tượng test là 1 cấu trúc dữ liệu tên là Stack. Stack là một cấu trúc dữ liệu mà dữ liệu vào muộn nhất sẽ được lấy ra sớm nhất. (Có nhiều bạn có thể liên tưởng tới dịch vụ stackoverflow, nhưng mặc dù cũng là 1 nơi tập trung dữ liệu quan trọng, và cái stack ấy mà sập thì anh em cũng mất nhờ, nhưng mà nó không phải Stack cấu trúc dữ liệu mà mình muốn đề cập tới). Stack class sẽ cần 5 hàm sau:  new để khởi tạo stack mới. Kiểu trả về là Stack.  push để thêm phần tử mới vào đỉnh của stack. Kiểu trả về là Stack.  top để lấy phần tử ở đỉnh (và là mới nhất) mà không xóa nó đi. Kiểu trả về là kiểu dữ liệu của phần tử mà ta sẽ lấy kiểu int.  retract để lấy phần tử mới nhất ở đỉnh ra khỏi stack và trả về stack đã bị sửa. Kiểu trả về là Stack.  is_empty để trả lời xem stack rỗng hay không. Kiểu trả về là boolean. Vậy nếu ta cần làm một Stack có thể chứa tối đa $N$ phần tử, thì tính theo số lượng phần từ, Stack sẽ có bao nhiêu trạng thái? Câu trả lời chính là $N+1$ trạng thái (tính cả rỗng). Dưới đây để cho dễ quan sát, tôi chỉ lấy 2 trạng thái là rỗng (empty) và không rỗng (not empty).  Giả sử $N=1$ cho dễ hình dung, thì các bạn thấy như hình trên sẽ có 2 trạng thái ($N+1=2$). Vậy bây giờ cần thiết kế test ra sao? Theo nguyên lý của SMM thì bạn phải viết hết các testcases để bao trùm toàn bộ các transition giữa các trạng thái. Như vậy, thì khi $N=1$, các bạn sẽ phải viết như hình vẽ trên là 6 transitions, tương ứng 6 quan điểm test cơ bản nhất. Sau đó vào từng cái thì các bạn có thể làm rõ và test thêm tùy ý các trường hợp nhỏ lẻ. Tài liệu tham khảoMartin, R. C. 2009. Clean code: a handbook of agile software craftsmanship. Pearson Education. DetailsMartin, R. C. 2007. Professionalism and test-driven development. IEEE Software. 24, 3 (2007), 32–36. DetailsBinder, R. 2000. Testing object-oriented systems: models, patterns, and tools. Addison-Wesley Professional. DetailsMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details      Hummble Object &#8617;    "
    }, {
    "id": 95,
    "url": "https://wanted2.github.io/srp-ccp/",
    "title": "Luận bàn về nguyên tắc trách nhiệm đơn và tính thuần nhất",
    "body": "2021/08/07 - Quy tắc trách nhiệm đơn (SRP, [1]) là nguyên tắc đầu tiên trong SOLID [1], và có thể khái quát như là:  A module should be responsible to one, and only one, actor. Một module phần mềm chỉ nên phục vụ 1 và chỉ 1 đối tượng sử dụng. Giới thiệuQuy tắc trách nhiệm đơn (SRP, [1]) là nguyên tắc đầu tiên trong SOLID [1], và có thể khái quát như là:  A module should be responsible to one, and only one, actor. Một module phần mềm chỉ nên phục vụ 1 và chỉ 1 đối tượng sử dụng. Hình vẽ bên thể hiện design một class Employee vi phạm SRP: nó phục vụ cùng lúc 4 đối tượng khác nhau là quản lý dự án (PM), kỹ sư cầu nối (BrSE), backend dev và frontend dev. PM có thể cần dùng manageAttendance() để quản lý hoạt động của team. BrSE có thể cần callClient() để liên lạc khách hàng. Backend dev có thể cần accessServer() để làm việc. Và frontend dev có thể cần getToken() từ backend để xác nhận người dùng. Tuy nhiên, để tất cả trong cùng 1 rọ Employee như vậy có vấn đề gì?. Backend dev có thể chặn các kết nối với bên ngoài để hạn chế việc sử dụng accessServer(), nhưng việc này lại khiến BrSE khó làm việc khi mỗi lần callClient() lại phải qua hàng loạt thủ tục xác nhận. Khi 1 nhân vật yêu cầu một thay đổi gì đó trong class Employee thì lại ảnh hưởng tới các nhân vật khác!Một lời giải cho vấn đề này là sử dụng Facade pattern mà ta sẽ giải thích sau. Nếu như quy tắc SRP quan tâm tới cách phân bổ các hàm trong class thì quy tắc thuần nhất (Component Cohesion Principle, CCP) lại quan tâm tới việc phân bổ các class và module phần mềm vào các thành phần phần mềm (software component). Có 3 quy tắc chính trong nhóm nguyên tắc này là:  REP: The Reuse/Release Equivalence Principle CCP: The Common Closure Principle CRP: The Common Reuse PrincipleChúng ta sẽ quan tâm tới CCP. Nội dung quy tắc nói như sau:  Gather into components those classes that change for the same reasons and at the same times. Separate into different components those classes that change at different times and for different reasons.  Tập hợp vào cùng component những class thay đổi cùng lúc và cùng lý do. Tách các class thay đổi không cùng lúc và vì lý do khác nhau thành những component khác nhau. Nhìn chung, nguyên tắc CCP khá gần với SRP nhưng ở cấp độ cao hơn. Cả 2 quy tắc đều yêu cầu sự đồng bộ cần thiết khi lựa chọn các phần tử vào một module hay component phần mềm. Quy tắc trách nhiệm đơn (Single Responsibility Principle)Ví dụ đời sống: hồ sơ xin việc: Cách đây cũng khá lâu, tôi có anh bạn đi phỏng vấn một công ty phần mềm và có vẻ khả quan, nhưng không hiểu sao lúc nhận kết quả lại là “hoặc ý thức kém, hoặc code kém”. Bạn tôi cũng không hiểu nổi phải nhìn nhận kết quả này kiểu gì?Tức là người phỏng vấn đánh giá là 1 trong hai khả năng “ý thức kém”, hoặc “code kém”, chứ không nói là cả hai. Đồng thời, bạn tôi cũng kể lại là lúc phỏng vấn chỉ nói mấy chuyện xung quanh hồ sơ, và chuyện phiếm chứ cũng không bị yêu cầu code hay gì khác. Vậy căn cứ vào đâu mà người phỏng vấn lại kết luận như vậy dù không nhìn một dòng code nào của bạn tôi? Cũng là tình cờ, một thời gian sau, tôi lại có dịp làm việc cùng với chính đội phỏng vấn ấy. Thì một trong những yêu cầu quan trọng của họ là năng lực thiết kế và kỹ năng phát triển phần mềm tốt. Trong bộ câu hỏi của họ có một câu hỏi là Kiểm tra thói quen thiết kế, mindset về thiết kế của ứng viên. Bạn tôi cũng thú thực là có bị hỏi về cách thức sắp xếp các trọng trách của mình và giải thích tại sao lại sắp xếp như vậy. Tất nhiên là cũng có thể chưa có nhiều kinh nghiệm nên sự sắp xếp có thể khiến cho người phỏng vấn cảm thấy hơi bối rối. Cũng như ví dụ về Employee ở trên, nếu bạn không giải thích rõ được tại sao lại gộp hết trách nhiệm vào cùng 1 class, và điều đó thể hiện trong lối sống thì chứng tỏ bạn chưa thấm nhuần tư tưởng của SOLID. Nếu vậy thì tại sao? Một là chưa biết cách code theo SOLID thì bị đánh giá là code kém. Hai là biết rồi nhưng không chịu áp dụng, thì lại thành ý thức kém. Một lĩnh vực tuy mới nhưng vẫn đứng vững và đang trở nên thành ngành phổ biến trên thế giới như phần mềm thì đương nhiên bên trong nó có nội lực rất mạnh. Nội lực đó thể hiện qua thể chế cực kỳ cứng (SOLID), quy trình, triết lý cũng chuẩn chỉnh … chứ không phải là một mớ ô hợp, tùy tiện. Vì vậy, đứng trước những người lâu năm trong ngành như vậy, nếu không thể hiện được những tiêu chí cơ bản nhất thì chỉ có bị đánh giá là “hoặc ý thức kém, hoặc code kém” (hoặc cả hai). Ví dụ code PHP: thiết kế streamers: Trong các framework về lập trình web bằng ngôn ngữ lập trình PHP thì Laravel có lẽ là một ngôn ngữ sở hữu bộ design pattern được implement phong phú nhất. Điều này khiến lập trình viên Laravel nhanh chóng tiếp cận con đường “chính đạo”, tránh việc dòng đời xô đẩy đi theo tà đạo. Vì thế việc đầu tiên khi bắt đầu với Laravel là tìm hiểu bộ pattern được implement trong framework như Command, Observer/Listener, Factories, Facade, …Nói không ngoa thì sở hữu kỹ năng Laravel cũng như bằng chứng rằng đã có tiếp cận các quy tắc thiết kế phần mềm. Hôm nay chúng ta sẽ lấy ví dụ về sử dụng pattern Facade để xây dựng Streamer service bằng Laravel. Một điểm chú ý là bản thân Laravel cũng có một feature tên là Facade. Pattern Facade mà ta nói tới hôm nay cũng na ná như vậy khi cung cấp một interface để ẩn hoạt động của các subsytems đằng sau. Ở đây chúng ta sẽ tham khảo một chút code của dự án koel, một ứng dụng streaming nhạc cá nhân. 1234use App\Services\Streamers\DirectStreamerInterface;use App\Services\Streamers\ObjectStorageStreamerInterface;use App\Services\Streamers\StreamerInterface;use App\Services\Streamers\TranscodingStreamerInterface;Giả sử ta muốn tạo ra 3 streamers cho 3 phương án streaming khác nhau:  Streaming trực tiếp từ file, gọi là DirectStreamerInterface.  Streaming từ Amazon S3 file, gọi là ObjectStorageStreamerInterface.  Streaming với transcoding trước khi streaming từ file, gọi là TranscodingStreamerInterface.  Cả 3 interfaces trên đều extend từ StreamerInterface. Cả 3 đều implement interface method là setSong() và stream(), nhưng TranscodingStreamerInterface sẽ cần thêm setBitrate() và setStartTime(). Khi sử dụng, có 1 logic lựa chọn streamer như sau:  Nếu thông số của S3 được set thì sẽ chọn ObjectStorageStreamerInterface; Nếu không, thì xem tiếp có cần transcoding không? Nếu có thì set bitrate và start time, rồi chọn TranscodingStreamerInterface Nếu không cần transcoding thì chọn DirectStreamerInterface. Một logic bao gồm 3 subsystems khác nhau như vậy sẽ rất phức tạp nếu để người dùng can thiệp trực tiếp vào. Vì vậy, đơn giản nhất là tạo một interface hầu như không chứa code là StreamerInterface để chạy hàm stream(). Chúng ta có thể dùng thêm Factory pattern để ẩn quy trình chọn này đi như sau: 12345678910111213141516171819202122232425262728293031323334353637class StreamerFactory{  private DirectStreamerInterface $directStreamer;  private TranscodingStreamerInterface $transcodingStreamer;  private ObjectStorageStreamerInterface $objectStorageStreamer;    // . . .   public function createStreamer(    Song $song,    ?bool $transcode = null,    ?int $bitRate = null,    float $startTime = 0. 0  ): StreamerInterface {    if ($song-&gt;s3_params) {      $this-&gt;objectStorageStreamer-&gt;setSong($song);      return $this-&gt;objectStorageStreamer;    }    if ($transcode === null &amp;&amp; $this-&gt;transcodingService-&gt;songShouldBeTranscoded($song)) {      $transcode = true;    }    if ($transcode) {      $this-&gt;transcodingStreamer-&gt;setSong($song);      $this-&gt;transcodingStreamer-&gt;setBitRate($bitRate ?: config('koel. streaming. bitrate'));      $this-&gt;transcodingStreamer-&gt;setStartTime($startTime);      return $this-&gt;transcodingStreamer;    }    $this-&gt;directStreamer-&gt;setSong($song);    return $this-&gt;directStreamer;  }}Khi sử dụng, chúng ta chỉ cần quan tâm tới StreamerInterface-&gt;stream() mà không cần quan tâm tới những xử lý chi tiết bên dưới. 12345678910111213141516class PlayController extends Controller{  private StreamerFactory $streamerFactory;  public function __construct(StreamerFactory $streamerFactory)  {    $this-&gt;streamerFactory = $streamerFactory;  }  public function show(SongPlayRequest $request, Song $song, ?bool $transcode = null, ?int $bitRate = null)  {    return $this-&gt;streamerFactory      -&gt;createStreamer($song, $transcode, $bitRate, floatval($request-&gt;time))      -&gt;stream();  }}Thiết kế trên giảm độ phức tạp hệ thống mà khiến cho mỗi streamer chỉ cần quan tâm đến các hàm mà mình cần, không phải chung với các streamer khác. Quy tắc thuần nhất (Common Closure Principle)Ví dụ đời sống: Chung và Riêng: Trong đời sống có khá nhiều ví dụ về CCP. Một team làm việc có cả kế toán, nhân sự, lập trình viên, quản lý, …Vậy lịch họp sẽ trở thành thế nào?Một đội quân ô hợp như vậy thì nếu họp chung thì sẽ mỗi người một ý. Bây giờ giả sử bạn yêu cầu tất cả bỏ phiếu chọn ra 1 ngày để họp thì sẽ thấy ngay:những người quản lý thường sẽ chọn đầu hoặc cuối tuần làm việc để tiện cho nắm bắt cả team. Nhưng những người làm sales thì lại hay phải ra ngoài, họ sẽ bị phụ thuộc vào khách. Lập trình viên thì lại bị chi phối bởi deadline giao hàng. Kết cục, là chuyện không thể tìm được 1 ngày khiến tất cả vui vẻ. Ngoài ra khi đã chọn được một ngày chung, thì sẽ rất khó với vấn đề thay đổi: nhân viên bán hàng có thể thay đổi theo lịch của khách mà quản lý và lập trình viên hầu như không thay đổi. Như vậy, bạn sẽ phải đối ứng với gấp 3 lần sự thay đổi nếu chỉ lên lịch cho 1 team chỉ toàn lập trình viên đang chạy theo 1 dự án cùng nhau. Chung là tốt nhưng phải là chung với những người giống mình về thời điểm thay đổi và lý do thay đổi!Còn nếu không thì phải tách riêng những người khác mình ra. Common Closure Principle là quy tắc khép kín thành phần phần mềm chỉ bao gồm những class hay module phần mềm giống nhau về thời điểm cũng như lý do thay đổi. Một nguyên tắc trong các dự án phần mềm là bảo trì bao giờ cũng quan trọng hơn tái sử dụng. Để cho việc bảo trì và vận hành trở nên dễ dàng, thiết kế các thành phần theo CCP là việc cần làm. Ví dụ kỹ thuật: Tại sao phải phân chia backend/frontend, server/client?: Kiến trúc server/client, hay gần đây đôi khi biến hình thành kiểu backend/frontend cũng không còn xa lạ. Nhìn chung, sẽ có hai nhóm xử lý quan trọng trong dự án phần mềm web: một là những xử lý đặt tại server (hoặc gần đây là serverless, auto-scaling clusters), và một là những xử lý cho client đặt tại thiết bị người dùng (PC, mobile, tablets, …). Theo quy tắc CCP và SRP, chuyện tách riêng hai nhóm này ra là chuyện nên làm. Và trong thực tế, kể cả trên thị trường tuyển dụng, việc tuyển người riêng cho từng thành phần backend và frontend cũng là xu hướng thường thấy, và bị chi phối bởi luật CCP/SRP. Kết luậnHai quy tắc mà chúng ta thảo luận ngày hôm nay là khá cơ bản với mọi kỹ sư phần mềm bài bản. Chúng ta có thể nhìn thấy những điểm sau từ SRP và CCP:  Bảo trì luôn quan trọng hơn tái sử dụng, đặc biệt là khi vận hành.  Nên xếp những thứ giống nhau vào cùng một rọ, chứ đừng tạo ra những cái rọ ô hợp trong phần mềm.  Facade là một thói quen cho lập trình viên hoặc thiết kế phần mềm khi đối đầu với một dự án phần mềm phức tạp. Tài liệu tham khảoMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details"
    }, {
    "id": 96,
    "url": "https://wanted2.github.io/the-dip/",
    "title": "The Abstract Factory",
    "body": "2021/08/05 - The Dependency Inversion Principle (DIP) is the last principle in SOLID [1]. It implies that successful software modules should rely on abstract classes and interfaces, not on concrete implementations. The Abstract Factory implements the DIP to reduce unexpected dependencies on concrete classes. The Dependency Inversion Principle (DIP)DIP tells us that the most flexible systems are those in which source code dependencies refer only to abstractions, not to concretions! In fact, abstract interfaces are always more stable than concrete implementations. Changes made to a concrete implementation don’t affect the abstract interface, but changes made to the abstract interface will lead to severe changes in the concrete implementation. We, therefore, conclude with several rules to favor stable abstract interfaces:  Don’t refer to volatile concrete classes. Refer to abstract interfaces instead.  Don’t derive from volatile concrete classes.  Don’t override concrete functions.  Never mention the name of anything concrete and volatile. The Abstract FactoryTo comply with these rules, the creation of volatile concrete objects requires additional handling. This caution is warranted because, in virtually all languages, the creation of an object requires a source code dependency on the concrete definition of that object. In most object-oriented languages, such as Java, we would use an Abstract Factory to manage this undesirable dependency. In the next section, we will examine an implementation in C++ of the abstract factory pattern. ExamplesUML diagram: We will implement the Abstract Factory in the below UML diagram: The blue line shows the architecture boundary between high-level classes and functions with low-level ones. Low-level parts interact with each other and never interact with main function. The main function only contains the code of Service and ServiceFactory. This made the architecture clean! C++ implementation: 12345678910111213141516171819202122232425262728293031323334353637// solid. hpp#include &lt;iostream&gt;#include &lt;string&gt;#ifndef SOLID_EX#define SOLID_EXclass Service {public:  virtual void run() = 0;};class ServiceImpl : public Service {public:  ServiceImpl(const std::string&amp;);  virtual ~ServiceImpl() = default;  void run() override;private:  std::string _name;};class ServiceFactory {public:  virtual Service* makeSvc(const std::string&amp;) = 0;};class ServiceFactoryImpl : public ServiceFactory {public:  ServiceFactoryImpl();  virtual ~ServiceFactoryImpl() = default;  Service* makeSvc(const std::string&amp;);};#endifNote that, we have the Service and ServiceFactory implemented as pure abstract classes. And we have the implementations: 1234567891011121314151617181920// solid. cpp#include  solid. hpp void Service::run() {}ServiceImpl::ServiceImpl(const std::string&amp; name) {  this-&gt;_name = name;}void ServiceImpl::run() {  std::cout &lt;&lt; this-&gt;_name &lt;&lt; std::endl;}ServiceFactoryImpl::ServiceFactoryImpl() {}Service* ServiceFactoryImpl::makeSvc(const std::string&amp; name) {  ServiceImpl* serviceImpl = (ServiceImpl*)new ServiceImpl(name);  return serviceImpl;}We have the main function: 123456789// main. cpp#include  solid. hpp int main() {  ServiceFactory* serviceFactory = (ServiceFactory*)new ServiceFactoryImpl();  Service* service = serviceFactory-&gt;makeSvc( Hello, Factory! );  service-&gt;run();  return 0;}To run this code, you may make a run script like 123456cmake_minimum_required(VERSION 3. 0)project(solid_example CXX)include_directories((${PROJECT_SOURCE_DIR}))add_executable(${PROJECT_NAME} main. cpp solid. cpp)and run 1234$ cmake . $ cmake --build . $ . /Debug/solid_example. exeHello, Factory!ReferencesMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details"
    }, {
    "id": 97,
    "url": "https://wanted2.github.io/the-humble-object/",
    "title": "The Humble Object Pattern",
    "body": "2021/08/04 - Clean architecture is always the goal for any software design. The Humble Object1, [1] is a design pattern that is helpful for software testing. It is often characterized as the separability between testable and non-testable parts. The Presenter/View model is an example of a clear boundary between hard-to-test and easy-to-test components. The Humble Object [1] The idea is very simple: split the behaviors into two modules or classes. One of those modules is humble; it contains all the hard-to-test behaviors stripped down to their barest essence. The other module contains all the testable behaviors that were stripped out of the humble object.  For example, UIs are hard to unit test because it is very difficult to write tests that can see the screen and check that the appropriate elements are displayed there. However, most of the behavior of a GUI is, in fact, easy-to-test. Using the Humble Object pattern, we can separate these two kinds of behaviors into two different classes called the Presenter and the View. The Presenter/View model [1]   The View is the humble object that is hard to test. The code in this object is kept as simple as possible. It moves data into the UI but does not process that data.   The Presenter is the testable object. Its job is to accept data from the application and format it for presentation so that the View can simply move it to the screen. Anything and everything that appears on the screen, and that the application has some kind of control over, is represented in the &gt;View Model as a string, or a boolean, or an enum. Nothing is left for the View to do other than to load the data from the View Model into the screen. Thus the View is humble.  ExamplesWe take the Vue. JS code as an example. Vue. JS is actually an MVVM framework, but we can find the Humble Object pattern there. A humble view: Let’s see the following example: 123456789101112131415161718192021222324252627282930313233&lt;template&gt; &lt;div&gt;  &lt;div class= message &gt;   { { message } }  &lt;/div&gt;  Enter your username: &lt;input v-model= username &gt;  &lt;div   v-if= error    class= error   &gt;   Please enter a username with at least seven letters.   &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'Foo', data () {  return {   message: 'Welcome to the Vue. js cookbook',   username: ''  } }, computed: {  error () {   return this. username. trim(). length &lt; 7  } }}&lt;/script&gt;This view in &lt;template&gt;&lt;/template&gt; is humble: it only loads processed data, and it doesn’t process data!The test is straightforward now since we only test the presenter’s logic: 1234567891011121314151617181920212223242526272829303132333435363738import { shallowMount } from '@vue/test-utils'import Foo from '. /Foo'const factory = (values = {}) =&gt; { return shallowMount(Foo, {  data () {   return {    . . . values   }  } })}describe('Foo', () =&gt; { it('renders a welcome message', () =&gt; {  const wrapper = factory()  expect(wrapper. find('. message'). text()). toEqual( Welcome to the Vue. js cookbook ) }) it('renders an error when username is less than 7 characters', () =&gt; {  const wrapper = factory({ username: '' })  expect(wrapper. find('. error'). exists()). toBeTruthy() }) it('renders an error when username is whitespace', () =&gt; {  const wrapper = factory({ username: ' '. repeat(7) })  expect(wrapper. find('. error'). exists()). toBeTruthy() }) it('does not render an error when username is 7 characters or more', () =&gt; {  const wrapper = factory({ username: 'Lachlan' })  expect(wrapper. find('. error'). exists()). toBeFalsy() })})An arrogant view: The view in the previous example was humble: it only loads formatted data!But let’s see a view that is not humble: 1234567891011121314&lt;template&gt; &lt;div&gt;  &lt;div class= message &gt;   { { message. split(''). reverse(). join('') + '-'. repeat(Math. floor(Math. random()*100))} }  &lt;/div&gt;  Enter your username: &lt;input v-model= username &gt;  &lt;div   v-if= error    class= error   &gt;   Please enter a username with at least seven letters.   &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;The code works, but the view is not humble now!It processes data inside &lt;template&gt;&lt;/template&gt; instead of only displaying the message. Due to the lack of humility, this code may be hard to test! ReferencesMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details      Humble Object at XUnitPatterns. com &#8617;    "
    }, {
    "id": 98,
    "url": "https://wanted2.github.io/python38-sys-exit/",
    "title": "Termination in Python",
    "body": "2021/07/25 - Every program will come to its end! The termination of a Python process can be the result of “murder” by the system (SIGTERM/SIGKILL), or controlled by the program (using quit(), exit(), os. _exit() or sys. exit()), or by reducing budget. When writing code, programmers may need to determine whether to put a line of code to terminate the process. The termination process itself is complex. And before terminating, we should remember to make a report, an audit log to summarize the life of the Python process. What are kinds of termination in Python?The processes in Python can terminate by themselves or by the system. By the system: by Extinction and Starvation: A Python process can be terminated suddenly without notices. If the system (OS) issued a kill (SIGTERM/SIGKILL) command when the process violated some policies of the system, we call it termination by murder (i. e. , the system murdered the process). To “murder” a process, a Linux system can issue a command like: 1$ kill -SIGTERM PIDSIGTERM cares about termination processes, saving data to prevent losses, but SIGKILL doesn’t. When the system “murders” the processes by SIGKILL, there will be no saved data! Another form of sudden termination is starvation but it is not considered as termination. This happens when the system manager found some processes consuming too many resources, or those processes are not needed anymore. But the manager cannot “murder” the processes, then they decide to let them alive, but reducing the usage of resources. In Linux, the manager can use the tool cpulimit to limit CPU usage by process ID. To install it in Debian, 1$ sudo apt install cpulimitand to limit CPU usage of a process, say 1999, to less than 50% 1$ sudo cpulimit --pid 1999 --limit 50Note that this limit CPU usage during runtime, not at the beginning!Then by this starvation mechanism, we may found some very slow processes in the top command summaries, but they don’t end! By processes: by Add/Integration processes: Extinction and starvation patterns of terminations are in control of the system and out of control of the process itself. Other patterns in this section are about termination defined by the process. In this section, we will discuss two types of such terminations: addition and integration. In the termination by addition, a successful child process terminates, and results are returned to a parent process. Sometimes, calling cleanup handlers or flushing buffers are not needed, and then it is normal to use os. _exit() to terminate the child process. The computed results and resources consumed by the children will be merged into a more full-fledged member in parent processes. In the termination by integration, a successful process that completed its computation can be terminated. Computed results and used resources are returned to be used in other processes of the system. Then it is normal to use sys. exit([args]) in most of these cases. Some audithooks can be called here to ensure the integrity of the audit cycle. Termination processThis section describes the mindset in termination. The termination process has two different parts: (i) decide whether to terminate a process; (ii) implement the termination in code or by command line. For the first part, the information about the performance of the process is collected from related programs in the system such as syslog. Then a criteria database is established as a baseline. The scoring model is then created to evaluate the decisions: terminate or not. Termination rules set are matched with a scoring model to make the final decision. If the decision is uncertain, then further sensitive analysis may be needed to adjust the criteria and their weights. When termination is confirmed in the decision, it will be implemented. Manual closeout like in extinction and starvation can be carried out by system commands. Automatic termination by coding can be implemented by quit(), exit(), os. _exit() and sys. exit(). Note that quit() and exit() require site modules to be imported, and they should not be called in production. os. _exit() is used in child processes only. sys. exit() should be preferred in most cases. Successful processes should call 1sys. exit(0)Coming into the mindset should be: whether the cleanup of data, resources are carried out properly? Were related client processes notified about the termination? Did it determine which records and reports are needed to keep? Were closeups of supports and dependencies scheduled properly?More items are available and should be used in a checklist. The final reportA final report summarizes the lifetime of a process is important for further diagnosis and audits. If it logged its behavior in the code, then logs can be a good summary. Nonetheless, a good final report should include the following criteria:  Process performance: CPU usage and resources information can be monitored and logged.  Administrative performance: errors and exceptions, accesses, and other security events may happen during process lifetime.  Process structure: the structure of code can be logged for diagnosis.  Administrative files: any files and functions that are called or linked into this job should be traceable.  Techniques: planning, control, and errors/exception handling techniques should be logged. "
    }, {
    "id": 99,
    "url": "https://wanted2.github.io/python-amazon-lex-chatbot-elasticsearch/",
    "title": "Amazon Lexと文章連想検索を用いた音声検索チャットボットの実現",
    "body": "2021/07/24 - 音声認識による文章連想検索システムは、ユーザの音声入力に対して、決まった言葉で検索エンジンを起動して、関連する文章の一覧を返すシステムです。本稿では、アマゾンLexやElasticsearchなどの既製部品を使ってシステムを構築する話題を挙げます。 はじめにSlackやTwilioなどで音声認識チャットボットを想定します。  1万冊以上論文を保管する時、「音声認識」に関する文献を検索したいです。その1万冊は別でアマゾンS3にアップロードして保管します。検索エンジンにデータの索引を作る。 ユーザが「私は音声認識について知りたいです。」を音声で入力すると、ボットが「音声認識」を理解して、検索を稼働させる。 入力キーワード「音声認識」に対して検索を行い、結果一覧を返す。1. と3. はアマゾンElasticsearchとS3で実現できます。索引作成処理と検索処理はラムダで実現できます。2. はアマゾンLexで作れます。 既製部品アマゾンLexによる音声チャットボット: アマゾンLexは会話チャットボットを作る支援プラットフォームです。開発者は会話のスクリプトを考えて、そのスクリプトを実現するために、Lexのインテントを使います。チャットボットの作成だけではなく、テストや他のプラットフォームへのインテグレーションもサポートします。英語だけではなく、日本語やスペイン語などの英語以外の言語も対応します。また、音声以外にテキストで入力することも可能です。 ここで、インテグレーションのために重要な概念を説明します。インテントはスクリプトのことですが、インテントの構成と作成方法です。Pythonでコーディングするなら、boto3ライブラリ1のcreate_botメソッドを参考すれば良いでしょう。会話フローを実現するために、重要な要素はコンテキスト、サンプル、プロムプト、スロットとフックです。       要素   説明   例         コンテキスト   入力と出力の時に、状態制御変数のこと。入力コンテキストが有効でない場合、応答インテントをしないなどの制御を実現できる。   ―       サンプル   インテントを起動する言葉。   「こんにちは」、「ピザを注文したいです。」       プロムプト   「はい・いいえ」だけの質問。回答後、終了を確認するか、ユーザが満足できたか確認質問。   「いかがでしょうか」、「回答に満足できましたか。」、「○○ピザを注文しました。いかがでしょうか。」       終了レスポンス   終了時、最後に出す言葉。または、別のインテントを起動する質問を出すことも可能。   「ご注文どうもありがとうございました。今○○製品にキャンペーンを実現していますけれども、追加注文しませんか。」       スロット   インテントを実行するときに必要な既定変数。   ―       フック   インテントの実行関数、あるいは、入力バリデーションなどを設定できる。   ラムダのARN   これだけの情報を入力することで、インテントを作成できます。実は、音声で入力するが、アマゾンの既製音声認識でやっています。特に、音声認識の中身に触れることなく、システム構築できますので、音声認識の知識がなくても安心してください。 Elasticsearchによる連想検索:  Amazon Elasticsearch Service は、Elasticsearch を大規模かつ簡単でコスト効率の良い方法を使用してデプロイ、保護、実行する完全マネージド型サービスです。好きなツールを使用して、必要な規模でアプリケーションを構築、監視、およびトラブルシューティングできます。このサービスは、オープンソースの Elasticsearch API、マネージド Kibana、Logstash とその他の AWS サービスとの統合、組み込みのアラートと SQL クエリのサポートを提供します。Amazon Elasticsearch Service は従量課金制です。前払い費用や最低料金はありません。Amazon Elasticsearch Service では、必要なだけの ELK スタックをランニングコストなしで入手できます。出典：Amazon Elasticsearch 大規模の文章データの高速検索を実現するためのプラットフォームです。Elasticsearch本番では全文検索など、キーワードを連想する検索も実現されています。図で説明するとネットで調べてみたが、はてなBlogを引用しています。これはElasticsearchのクエリ検索の仕組みを良く表現できたと思います。クエリビルダーでは、ANDとOR演算子でキーワード配列を連想して、強力なクエリを新たに定義します。事前に作った索引とmulti_matchで検索します。0件という結果が帰ると、救済処理などを行えますが、それが今回のスコープに入りません。 このようにすると、Lexで認識したインテントをElasticsearchで文章連想検索を実現できます。 実現と結果      部品   説明         Amazon Lex   音声認識と会話フローを実現する       Fulfilment Lambda   「○○について知りたいです。」というクエリが来ると、実行する処理を記述するラムダ関数。その処理は、Elasticsearchのエンドポイントにクエリを投げ、結果が返るのを待機する。       Elasticsearch service   文章データの索引と連想検索を行う。       Indexing Lambda   文章データがアップされると、トリガー処理を行う。データをElasticsearchのエンドポイントに送る処理を行う。       Document Storage S3   文章データが保管されるストレージ。手動でアップする。   文書データベース索引の作成フローの自動化: PDFデータやOfficeデータをS3にアップして、Elasticsearchのエンドポイントへ送信するフローを自動化したいです。つまり、Document Storage S3 ---&gt; Indexing Lambda ---&gt; Elasticsearch Serviceのフローを自動化します。 この部分の設計は下記になります2。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121 ESNS3Access:  Type: 'AWS::IAM::Policy'  Properties:   PolicyName: ESNS3Access   PolicyDocument:    Version: 2012-10-17    Statement:     -       Effect: Allow      Action:       - es:ESHttpGet       - es:ESHttpPost       - es:ESHttpPut      Resource:       !Join       - ''       - - !GetAtt ElasticsearchDomain. Arn        -  /*      -      Effect:  Allow       Action:  s3:GetObject*       Resource: !Sub  arn:aws:s3:::${AWS::StackName}-${AWS::AccountId}-documentstore/*    Roles:    - !Ref IndexDocumentLambdaExecutionRole IndexDocumentLambdaExecutionRole:  Type:  AWS::IAM::Role   Properties:   AssumeRolePolicyDocument:    Version:  2012-10-17     Statement:     -      Effect:  Allow       Principal:       Service:        -  lambda. amazonaws. com       Action:       -  sts:AssumeRole    Path:  /    ManagedPolicyArns:    -  arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole  IndexDocumentLambda:  Type:  AWS::Lambda::Function   Properties:   Code:    S3Bucket:  aws-machine-learning-blog     S3Key:  artifacts/document-search-bot/es-index. zip    Description: This function is used to convert documents to base64 and upload index it in Elasticsearch   FunctionName: !Sub  ${AWS::StackName}-IndexDocumentLambda    Handler: index. handler   MemorySize: 256   Role: !GetAtt IndexDocumentLambdaExecutionRole. Arn   Runtime:  nodejs12. x    Timeout: 25   Environment:    Variables:      ESENDPOINT:       !Join        - ''        - -  https://          - !GetAtt ElasticsearchDomain. DomainEndpoint      AWSREGION: !Ref AWS::Region ElasticsearchDomain:   Type:  AWS::Elasticsearch::Domain    Properties:    ElasticsearchClusterConfig:     DedicatedMasterEnabled: false     InstanceCount: 1     InstanceType: t2. small. elasticsearch     ZoneAwarenessEnabled: 'false'    EBSOptions:     EBSEnabled: true     Iops: 0     VolumeSize: 10     VolumeType:  gp2     ElasticsearchVersion:  6. 2     SnapshotOptions:     AutomatedSnapshotStartHour:  0     AdvancedOptions:     rest. action. multi. allow_explicit_index: 'true'    AccessPolicies:     Version:  2012-10-17      Statement:      -       Effect:  Allow        Principal:        AWS:        - !Sub  arn:aws:iam::${AWS::AccountId}:root        Action:       -  es:*        Resource: '*' S3DocumentStoreBucket:  Type: AWS::S3::Bucket  DependsOn: PermissionForS3ToInvokeLambda  Properties:   BucketName: !Sub  ${AWS::StackName}-${AWS::AccountId}-documentstore    NotificationConfiguration:    LambdaConfigurations:    -     Event:  s3:ObjectCreated:*      Function: !GetAtt IndexDocumentLambda. Arn  PermissionForS3ToInvokeLambda:  Type: 'AWS::Lambda::Permission'  Properties:   Action: 'lambda:InvokeFunction'   FunctionName: !GetAtt IndexDocumentLambda. Arn   Principal: 's3. amazonaws. com'   SourceAccount : !Ref AWS::AccountId   SourceArn:    !Join    - ''    - -  arn:aws:s3:::      - !Join        -  -         - - !Ref AWS::StackName         - !Ref AWS::AccountId         -  documentstore リソースアクセスのためのロールとポリシーも記述されていますが、大まかにDocument Storage S3 ---&gt; Indexing Lambda ---&gt; Elasticsearch Serviceの処理を記述します。毎回PDF文章データがS3DocumentStoreBucketにアップされると、IndexDocumentLambdaというラムダ関数が起動されます。 索引作成処理にはElasticsearchのエンドポイントにドキュメントを送信する処理ですが、下記になります2。 1234567891011121314151617181920212223242526272829303132333435363738394041424344function putDocumentToES(doc, context, key, objURL) {  console. log( Url   + objURL);  console. log( endpoint   + esDomain. endpoint);  var req = new AWS. HttpRequest(endpoint);  var attachKey = key + '?pipeline=attachment';  var bodyString = JSON. stringify({     data : doc,     filePath  : objURL  });  console. log(bodyString);  req. method = 'PUT';  req. path = path. join('/', esDomain. index, esDomain. doctype, attachKey);  console. log('reqPath ' + req. path);  req. region = esDomain. region;  req. body = bodyString;  req. headers['presigned-expires'] = false;  req. headers['Host'] = endpoint. host;  req. headers['content-type'] = 'application/json';  // Sign the request (Sigv4)  var signer = new AWS. Signers. V4(req, 'es');  signer. addAuthorization(creds, new Date());  // Post document to ES  var send = new AWS. NodeHttpClient();  send. handleRequest(req, null, function (httpResp) {    var body = '';        httpResp. on('data', function (chunk) {            body += chunk;    });    httpResp. on('end', function (chunk) {      console. log('completed');      console. log('body' + httpResp. statusCode);    });  }, function (err) {    console. log('Error: ' + err);    context. fail();  });}チャットボットへのインテグレーション: LexへElasticsearchのをインテグレーションするために、ラムダ関数BotFullfilmentLambdaを使います。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 ESAccess:  Type: 'AWS::IAM::Policy'  Properties:   PolicyName: ESAccess   PolicyDocument:    Version: 2012-10-17    Statement:     -       Effect: Allow      Action:       - es:ESHttpGet       - es:ESHttpPost      Resource:       !Join       - ''       - - !GetAtt ElasticsearchDomain. Arn        -  /*    Roles:    - !Ref BotFullfilmentLambdaExecutionRole  BotFullfilmentLambdaExecutionRole:  Type:  AWS::IAM::Role   Properties:   AssumeRolePolicyDocument:    Version:  2012-10-17     Statement:     -      Effect:  Allow       Principal:       Service:        -  lambda. amazonaws. com       Action:       -  sts:AssumeRole    Path:  /    ManagedPolicyArns:    -  arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole  BotFullfilmentLambda:  Type:  AWS::Lambda::Function   Properties:   Code:    S3Bucket:  aws-machine-learning-blog     S3Key:  artifacts/document-search-bot/lex-index. zip    Description:  This function will do the fullfillment of document search bot    FunctionName: !Sub  ${AWS::StackName}-DocumentSearchBotFullfilment    Handler: index. handler   MemorySize: 256   Role: !GetAtt BotFullfilmentLambdaExecutionRole. Arn   Runtime:  nodejs12. x    Timeout: 25   Environment:    Variables:      ESENDPOINT:       !Join        -           - -  https://          - !GetAtt ElasticsearchDomain. DomainEndpoint      AWSREGION: !Ref AWS::Regionインテグレーションの手順は下記になります。  Lexコンソール画面では、新しいチャットボットを作成する。 言語を英語に設定する。 英語言語でインテントを作成する。 インテントの設定画面で「Codehooks」の項目に、BotFullfilmentLambdaのARNを設定する。考察: 試すためには、数十冊の英文PDFをアップしました。Lexコンソール画面からインテントの設定画面に移すと、「構築」ボタンを押し、「テスト」が可能になりました。speech recognitionの要望を示すとボットは結果とダウンロードURLが返ります。 メリット: LexとElasticsearchのアプローチで音声による文章連想検索システムの構築は素早く終わらせました。決まったインテントで音声認識したので、あまり誤差を拡大せず、正確に検索できました。また、連想検索は検証されたElasticsearchのアプローチなので、正確な検索結果を取得でき、ユーザの満足感を向上できます。なお、すべてサーバーレス形式なので、課金制度も柔軟になります。 デメリット: 一番目のデメリットは、アマゾンの内臓音声認識エンジンに好奇心を持っているのに、内臓エンジンに触れたいなら、エンジニアとしてアマゾンに入社するしかありません。二番目は価格です。柔軟な課金制度ですが、しっかり強いシステムを構築するとどれぐらいかかるかみてみましょう。今回のシステムの見積もりを作成しました。Lex以外のサービスの見積はここで確認できます。前提は、毎月1万冊程度、10万リクエストと想定しています。ですので、Lexの単価は1音声リクエストあたり0. 004ドルなので、毎月400ドルが課金されます。上記の見積と合わせると、毎月1286ドルの課金量が発生します。ElasticsearchとLexの課金量はかなりかかります。また、UltraWarmという大規模処理を補う機能を使うと、必ず毎月20万円以上が消費されると思います。 おわりに本稿では、アマゾンElasticsearchやLexなどを使ってすべてクラウドで音声認識による文書連想検索システムを実現しました。1万文献かつ10万音声リクエスト規模のシステムです。 今回のデモは、コアの部分を実現したが、実に稼働するために、システムにセキュリティ機能、ログ処理、CloudWatch設定、例外処理などを追加するとなお良いです。ユーザ認証はCognitoなどで実現すれば結構です。 参考文献      https://boto3. amazonaws. com/v1/documentation/api/latest/reference/services/lexv2-models. html &#8617;        Build a document search bot using Amazon Lex and Amazon Elasticsearch Service - AWS Machine Learning Blog &#8617; &#8617;2    "
    }, {
    "id": 100,
    "url": "https://wanted2.github.io/bitbucket-pipelines-runners/",
    "title": "Bitbucket pipelines for small teams: Bitbucket Cloud or Self-hosted Runners?",
    "body": "2021/07/23 - Bitbucket. org là nền tảng để hợp tác (collaboration) trong phát triển phần mềm. Xây dựng chu trình cung ứng phần mềm mang tính liên tục (continuous integration and delivery/deployment) đòi hỏi sử dụng những nền tảng như Bitbucket để nâng cao năng suất. Hiện nay, việc sử dụng Bitbucket đã khá phổ biến ngay cả với các team nhỏ (xung quanh 5 người, và thường chưa tới 10 người). Tuy nhiên, team nhỏ có những đặc điểm rất khác biệt: giới hạn về tài nguyên, nhân lực, cả utilization rate (tỷ lệ sử dụng). Bài toán kinh tế với Bitbucket quay trở lại điểm trade-off quen thuộc: tiền ít mà muốn chất lượng cao (cũng không hẳn cao quá, chỉ cần tốt hơn hiện tại) thì làm thế nào?Nói đến chất lượng là vì bản thân mục đích của chu trình CI/CD chính là nâng cao chất lượng phần mềm nhờ chuyên nghiệp hóa quy trình. Chúng ta sẽ đi sâu vào vấn đề này trong việc sử dụng Bitbucket, mà cuối cùng như tựa đề sẽ là so sánh hai lựa chọn khả thi nhất: dùng có sẵn (Bitbucket Cloud) và tự dựng (self-hosted Bitbucket runners). Bài toán team nhỏTeam nhỏ là những team chỉ khoảng 5 thành viên, trong đó vì bài toán chúng ta có liên quan tới Bitbucket tức là quản lý sản phẩm phần mềm, nên gọi là 5 thành viên là chỉ tính người có commit code thôi!. Bởi vì chỉ có người commit lên thì mới là chủ chốt sử dụng. Còn viết tài liệu cũng có công, nhưng công số không nhiều. Và quan trọng hơn chỉ code và cấu hình thì mới trigger cái luồng CI/CD cho chạy, còn tài liệu thì chỉ để đọc. Và tài liệu thì ta có thể dùng những giải pháp ít tốn kém khác như Google Drive để quản lý, và đó không thuộc scope của bài viết này. Và đương nhiên, nói tới Bitbucket là phải nói tới các công cụ quản lý phiên bản (version control system) như Git hay mercurial. Vậy tại sao team nhỏ cũng cần những công cụ như Bitbucket?Hiển nhiên là vì họ cũng cần quản lý code và đóng gói thành phẩm chuyển giao cho khách hàng, và quan trọng là hợp tác giữa các thành viên trong dự án. Với dự án có 1 người thì một Bitbucket account cũng là nơi tốt để họ lưu trữ sản phẩm, hơn là lưu trữ tại mỗi local để rồi dễ bị mất mát, không quản lý được. Đặc điểm của team nhỏ là gì?    Nguồn nhân lực ít, vì vậy, nếu có thể dùng CI/CD để tự động hóa sẽ giúp giảm tải khá nhiều. Tuy nhiên, chu trình CI/CD nếu đã được thiết lập thì nên tái sử dụng ở các dự án về sau. Tránh cảnh mỗi lần kickoff lại xây lại chu trình CI/CD từ đầu.     Nguồn tài chính, hỗ trợ hạn hẹp. Thường thì phân phối nguồn lực sẽ tương ứng với team size, vì vậy, budget thường sẽ hạn chế và tài nguyên cũng sẽ không reserved được mà phải on-demand (lúc nào cần thì phải làm đơn xin cấp). Với các team lớn vì nguồn thu của họ ổn định và cũng lớn, nên họ có thể dễ dàng xin cấp số lượng lớn ngay từ đầu để duy trì nguồn thu đó (*).     Chất lượng sản phẩm và chu trình phát triển phụ thuộc vào nhiều biến cố định. Ví dụ như thường sẽ có nhân lực chủ chốt, đó là 1 biến cố định, mà biến ấy mất đi là dự án đi vào khó khăn. Hoặc có 1 cái máy mà cả dự án chỉ có 1 cái, nếu nó hỏng thì … khó khăn! Loại bỏ bớt các biến cố định này, đòi hỏi phải bổ sung thay thế (alternatives) và đương nhiên là cần tiền, và không có tiền thì … giải tán! Hoặc thế nào đó, nhưng tóm lại thường sẽ là phải chấp nhận thôi!  Nói đến đây có thể thấy cloud là một cứu cánh như thế nào với các team nhỏ. Muốn có máy thay thế, có thể thuê cả năm, hoặc trả pay-as-you-go, muôn vàn hình thái chi trả, mà tóm lại không cần mua một cái máy về. Như vậy giảm thiểu được các biến cố định. Ngoài ra, lên cloud tức là lên chuẩn, kiến thức chuyên môn hầu như chỉ có cấu hình nên ai vào thay thế cũng vậy, chỉ cần có cơ bản về cloud. Nhờ vậy mà vấn đề nhân lực cũng theo đó mà suy giảm. Chú ý: (*) Tất nhiên, cũng không hẳn không có cách để team nhỏ xin resource. Nhiều team nhỏ có cùng requirements về resources có thể cùng nhau xin một số lượng lớn, sau đó đem về dùng chung. Biểu đồ về tỷ lệ sử dụng dịch vụ Runner của các teams dưới đây được trích từ công cụ tính giá tiền của AWS. Biểu đồ nào phù hợp với team nhỏ? CI/CD với BitbucketBitbucket cung cấp khá nhiều giải pháp để hỗ trợ tự động hóa CI/CD:  Bitbucket Pipelines1: chu trình CI/CD được viết vào file bitbucket_pipelines. yml và được tự động hóa theo kịch bản được viết ra.  Bitbucket Cloud: hệ thống máy ảo để chạy các pipelines được đặt trên server của Bitbucket.  Bitbucket Runners: các runners do người dùng tự dựng. Có thể cấu hình theo từng project (Project Runners) và workspace (Workspace Runners). Quy trình sẽ là:  Người dùng tạo file kịch bản CI/CD bitbucket_pipelines. yml và để vào nhánh tương ứng.  Người dùng thực hiện thay đổi trong code và commit, push lên Bitbucker repo.  Dựa theo kịch bản đã cho, một runner (server, có thể ở trên Bitbucket Cloud hoặc tự dựng, nếu trong kịch bản không cấu hình, thì mặc định Bitbucket Cloud) sẽ chạy kịch bản đó để thực hiện các bước building, testing và merge code tự động. Để cho dễ giải thích về sau, tôi xin đặt ra một số tiền đề sau cho bài toán team nhỏ:    Giới hạn team là 5 thành viên.   Nhu cầu sử dụng giới hạn trong 8h/ngày. Tính từ 9h sáng tới 17h tối. Nhân viên ít thích OT, có OT cũng rất ít. Nên sau 7h tối hầu như không có ai sử dụng.   Vì lý do bảo mật, có yêu cầu không được sử dụng mà không báo cáo xin phép. Do vậy có thể xem sau 7h tối và trước 9h sáng hầu như nhu cầu dùng rất ít (được luật của cty bảo đảm). Ngày nghỉ ngày lễ đương nhiên phải có cho phép của lãnh đạo mới được làm nên coi như không có nhu cầu dùng.  Bitbucket Cloud: Bitbucket Cloud cung cấp runners theo pipelines, với bảng giá như bên dưới. Hầu như về mặt kỹ thuật, chỉ cần cấu hình file bitbucket_pipelines. yml, nên thủ tục rất tiện lợi cho người dùng. Giá của Bitbucket Cloud này là flat. Giả dụ team có nhiều nhất 5 thành viên, và dùng gói Standard để có 2500 build minutes mỗi tháng (tính trung bình 1 build là 5 phút thì ta có 500 lượt CI/CD mỗi tháng), thì tổng giá là $3\times 5=15$ USD. Sau khi vượt quá mức 2500 build minutes, thì cứ mỗi 1000 build minutes phải trả thêm 10 USD. Vậy với các tiền đề đã nói, chúng ta hãy nhẩm tính, chỉ có 20 ngày làm việc/tháng. Và mỗi ngày thì chỉ làm 8 tiếng, thì với 500 lượt CI/CD, tức là trung bình $500/(20\times 8)=3. 125$ lượt build/giờ. Con số này không quá tồi, vì mỗi lượt CI/CD tương ứng 1 push (push có thể cùng lúc nhiều commit), nên 5 thành viên trong 1 giờ push 3 lần là cũng không quá ít. Tuy nhiên, để đảm bảo mô hình giá trên trở nên hiện thực, cần có luật trong công ty để đảm bảo:  Thời gian 1 lượt CI/CD không quá 5 phút Push nhiều commit cùng lượt thì rebase hoặc merge lại làm 1 cho đẹp history, chứ nghiêm cấm kiểu push nhát một, là rất tốn tiền build CI/CD.  Lên kế hoạch làm việc cụ thể, đến giờ nào thì ai push cái gì. Điều này đảm bảo tỷ lệ 5 người push 3 lần trong 1 giờ!Tránh tình trạng, có dev cứ tí lại push, tí lại push nhát một, tốn tiền build time. Cũng không nên tích lại nhiều commit để push 1 lần quá, vì như thế có thể sót mất một số vấn đề ở các commit giữa chừng, khó review, và lọt lưới vấn đề. Nhìn chung, đòi hỏi dev phải có kinh nghiệm và khả năng quản lý code tốt! Self-hosted solution: Như trên có thể thấy, với tiền đề đã cho, thì nếu dùng Bitbucket Cloud, nếu có kỷ luật mạnh thì chỉ mất 15 USD/tháng. Còn kỷ luật mà yếu thì, giá đội lên khá nhiều. Đơn cử có các đồng chí, cứ 1-2 phút lại push một lần là mô hình giá này … giải tán!Và các nhân vật ấy sẽ không khác gì đang DDoS chính dự án của mình!Tất nhiên con số ở trên là trung bình trong cả 1 thời gian dài, vì có nhiều dev chỉ push nhiều 1 thời điểm nhất định trong ngày, sau đó thì lại không push nữa. Việc nắm bắt được biểu đồ push của anh em là trách nhiệm của nhà quản lý dự án. Song song với bán runners của mình, Bitbucket cũng cho phép người dùng tự cấu hình runners. Tức là quản lý code vẫn trên Bitbucket, nhưng CI/CD thì sẽ có 1 docker chạy trên runners của khách hàng. Cứ mỗi lần push lên là runner ở nhà hoặc trên cloud nào đó sẽ tự động clone code về và thực hiện kịch bản. Máy đặt tại nhà: Giải pháp đặt runner tại nhà (hoặc tại máy công ty) thì có vẻ tránh được ràng buộc của Bitbucket Cloud. Cho chạy 24/7 cũng OK nếu chịu được tiền điện!Theo như giá điện tại Hà Nội chả hạn, cứ tính là 2000 một số. Thì một server cho chạy 24/7 thì $24\times 30\times 2000=1440000\approx 62. 5$ USD/tháng. Tuy nhiên, với tiền đề đã cho, thì thực ra là $20\times 8\times 2000=320000\approx 13. 9$ USD/tháng. Nhưng như vậy, mỗi ngày đều phải có người đến bật lên và lúc ra về phải nhớ tắt đi. Tiền upfront (giá máy, chi phí lắp đặt ban đầu) thì sẽ phát sinh tầm vài chục triệu, nên hy vọng sẽ lâu để hoàn lại. Giả dụ là mua server RAM 8GB, 2 core mạnh để chạy CI/CD thì giá sẽ vào tầm: 45 triệu VND$\approx 1956. 5$ USD. Giả dụ dùng được 5 năm là phải nâng cấp thì mỗi tháng cũng mất tầm 32. 5 USD. Tổng cộng với tiền điện là $32. 5+13. 9=46. 4$USD/tháng. Ngoài ra, nên nhớ hệ điều hành bắt buộc là Linux2. Cách cấu hình khá đơn giản, nhưng lời khuyên là nên cấu hình Workspace Runner để nhiều dự án khác nhau có thể dùng chung. Nhờ vậy mà tỷ lệ sử dụng sẽ tăng, tránh lãng phí. Như vậy, so với Bitbucket Cloud thì giá thành của giải pháp đặt runner tại nhà là gấp 3 lần. Lợi ích lớn nhất là không bị giới hạn bởi số lượng build minutes, nhưng sẽ có một số thủ tục bắt buộc như về là phải tắt điện. Vì tiền điện phải tự trả mà! Máy trên cloud: Giải pháp cuối cùng là cũng tự dựng, nhưng là dựng trên một dịch vụ cloud khác như AWS EC2. So với lời giải đặt tại nhà thì sẽ bớt tiền điện và upfront. Với tiền đề đã cho, tôi chọn biểu đồ sử dụng Daily spike traffic, với baseline là 0 máy, và thời điểm peak là 1 máy, 1 ngày dùng chỉ 8 tiếng, thứ 7, chủ nhật không dùng. Tôi cũng không thấy lý do phải lưu snapshot ra với kiểu nhu cầu CI/CD (mỗi lần chạy lại một sản phẩm khác hẳn), nên dùng EBS nhưng không để lựa chọn Snapshot. Ngoài ra, để theo dõi runner và tự động tắt bật runner, tôi thêm một instance nhỏ giá rẻ (chỉ 5 USD/tháng) vào. monitor thì sẽ cho chạy 24/7 vì giá rẻ. Với monitor, tôi sẽ cấu hình cài AWS CLI và chạy cron định kỳ cứ thứ 2 đến thứ 6, đúng 9h sáng thì bật và 5h chiều thì tự động tắt. Các sử dụng ngoại lệ như OT là phải có phê duyệt!Chi tiết của estimate có thể xem tại: https://calculator. aws/ Thì tổng giá là 17. 30 USD/tháng. Nhìn chung là ngang dùng Bitbucket Cloud mà không tốn thêm phí gì khác. Tuy nhiên, để đảm bảo việc chỉ dùng 8 tiếng/ngày, thì phải đảm bảo bật tắt tự động, tôi giải quyết bằng thêm instance monitor và set quyền truy cập vào monitor giới hạn. Vậy với giải pháp này, tôi thêm được bao nhiêu build minutes?Hãy nhẩm tính là 160 tiếng có 60 phút là tôi đã có $160\times 60=9600$ build minutes. Rộng rãi hơn so với dùng Bitbucket Cloud, mặc dùng vẫn phải cưỡng chế thời gian bật tắt cho chỉ dùng 8h/ngày. Kết luận      Solution   Price [USD/tháng]   # Build minutes   Nhược điểm         Dùng Bitbucket Cloud   15. 0   2500   Phải lên lịch push. Giới hạn 5 thành viên push 3 lần 1 giờ. Không cho OT, ngày làm chỉ 8h, ngày nghỉ không cho làm.        Tự dựng EC2 runner   17. 3   9600   Cài đặt monitor tự bật tắt runner vào lịch đã hẹn. Ngoài thời gian 8h làm việc của ngày làm việc là monitor sẽ tắt runner.        Tự dựng server runner   46. 4   9600   Phải tự quản lý và có việc gì phải gọi bảo trì hoặc tự sửa chữa. Khi không dùng phải tắt điện đi. Nếu theo cách này thì phải dùng chung với vài dự án thì mỗi dự án sẽ rẻ đi, và sẽ chậm đi nếu chạy nhiều tác vụ cùng lúc.     Mặc dù, cách đặt tại nhà có thể cải tiến bằng cách cho nhiều dự án dùng chung, nhưng cách đặt trên EC2 cũng có thể áp dụng dùng chung, nên cách đặt tại nhà sẽ vẫn thua cách cấu hình EC2.  Nếu dự án cần không quá nhiều build minutes (dưới 2500) thì Bitbucker Cloud là giải pháp yên tâm và nhanh chóng nhất. Không cần cài đặt gì nhiều và chu trình CI/CD có thể bắt tay vào ngay.  Nếu cần nhiều build minutes và chia sẻ nhiều dự án, thì có lẽ dựng trên EC2 vẫn tốt hơn đặt tại nhà. Vì 2 cách sau khá mất công cấu hình, nên để CI/CD không chậm trễ, thì roadmap hợp lý nhất:  Khởi đầu bằng Bitbucket Cloud, nhưng có theo dõi số lượng build minutes.  Khi pipelines đã hoàn thiện, nếu phát hiện thấy số lượng build minutes lớn hơn 2500 quá nhiều thì chuyển sang dùng EC2 vẫn với pipelines đã hoàn thiện. Nói chung, với dev có nhiều kinh nghiệm thì chuyện push nhát một liên tục là sẽ ít xảy ra, thường là 1 buổi sáng hoặc 1 buổi chiều làm 1-2 push thôi. Cả ngày chắc được 2-3 push. Do đó có lẽ cũng sẽ không đến 2500 build minutes nếu đội dev có kinh nghiệm!và vì vậy giải pháp Bitbucket Cloud có lẽ là lời giải nhanh chóng hơn. Tài liệu tham khảo      https://support. atlassian. com/bitbucket-cloud/docs/configure-bitbucket-pipelinesyml/ &#8617;        https://support. atlassian. com/bitbucket-cloud/docs/set-up-and-use-runners-for-linux/ &#8617;    "
    }, {
    "id": 101,
    "url": "https://wanted2.github.io/xops/",
    "title": "Tản mạn về XOps",
    "body": "2021/07/18 - Các thuật ngữ ITOps, CloudOps, DevOps, DevSecOps, NoOps, BizDevOps và AIOps mà gọi chung là XOps đã không còn xa lạ với giới công nghệ. Bài viết này đưa ra một hình dung chung về các khái niệm này. ITOpsLà viết tắt của “Information Technology Operations”, ITOps bao phủ toàn bộ các hoạt động nghiệp vụ IT trong doanh nghiệp như quản trị mạng, quản trị hệ thống, vận hành và bảo trì, và hỗ trợ kỹ thuật. ITOps phân chia độc lập các teams theo từng domain (khu vực chuyên môn) và chuyên môn hóa, cũng như quy trình hóa nghiệp vụ mà kết quả cuối cùng là đi vào mô hình waterfall quen thuộc. Một đặc điểm của ITOps là vốn dĩ không đề cập tới sự tồn tại của team R&amp;D, và đương nhiên đây là hậu quả của lối suy nghĩ “ăn xổi”, cho mọi thứ đều lên product ngay mà không cần qua giai đoạn prototype, không cần kiểm chứng ở quy mô nhỏ trước mà cứ đưa lên quy mô lớn để ăn ngay. Nhược điểm hiển nhiên của ITOps là:  Không đối ứng kịp với sự thay đổi của business hàng ngày hàng giờ; Không đối phó kịp với sự thay đổi của công nghệ trong vài thập kỷ qua; và Không đáp ứng kịp nhu cầu cần phản ứng nhanh với sự thay đổi của business. Vì thế, ITOps và các nhóm nghiên cứu và phát triển (R&amp;D) đã đề xuất ra DevOps mà chúng ta sẽ nói đến bên dưới, để thay thế ITOps như giải pháp real-time hơn. CloudOpsCloudOps là tập hợp các nghiệp vụ liên quan tới Cloud, mà theo đó những nghiệp vụ ITOps vốn xử lý ở data center của khách hàng, nay chuyển hết lên cloud. Các nghiệp vụ như quản lý người dùng, quản trị hệ thống, quản trị mạng, . v. v… đều được chuyển hết lên điện toán đám mây. Vì thế, quy trình chuyển đổi lên điện toán đám mây sẽ xuất hiện bài toán migration (di chuyển dữ liệu, データ移行) từ data center của khách hàng lên đám mây. Ở cấp độ này, khởi tạo tài nguyên đám mây, lên kế hoạch scaling và tự động hóa các thành phần riêng lẻ của đám mây được thực hiện. Ví dụ như khởi tạo 1 EC2 instance thì quy trình khởi tạo cài đặt OS đã được tự động hóa bên trong. Điều này đem lại sự tiện lợi, tin cậy và tiết kiệm cho người dùng. Tuy nhiên, với sự ra đời của DevOps, CloudOps nhanh chóng bị trở nên bị lãng quên. Khoảng năm 2014-2015, tôi chứng kiến khá nhiều người theo chiều hướng này nhưng gần đây hầu như mọi người đã chuyển lên DevOps để có thể nhận được lợi ích từ một cấp độ cao hơn của tự động hóa. DevOpsDevOps (Development and Operations) có lẽ là phần chờ đón nhất. DevOps là sự kết hợp những yếu tố tốt nhất của phát triển phần mềm (Software development), quản lý chất lượng (QA) và ITOps. Sự kết hợp 3 trong 1 này nhắm tới  Rút ngắn chu trình phát triển phần mềm.  Nhanh chóng đáp ứng nhu cầu thị trường đang thay đổi từng phút Rút ngắn thời gian đưa sản phẩm ra thị trường. Chu trình DevOps bao gồm: Lên kế hoạch (Plan) -&gt; Xây dựng (build) -&gt; Continuous Integration (CI) -&gt; Triển khai (deploy) -&gt; Vận hành (Operate) -&gt; Continuos Feedback. Tự động hóa chính là công cụ cốt lõi của DevOps: mọi bước trong chu trình đều đi kèm một hệ thống các tools để nhanh chóng tự động hóa chu trình. Điều này đem lại lợi ích không thể chối bỏ:  Giảm thời gian delivery Tăng chất lượng release Giảm thời gian giữa hai releases liên tục Giảm thời gian chuẩn bị release. Sẽ có 3 teams tham gia vào DevOps: team R&amp;D (bao gồm cả kỹ sư phát triển phần mềm), team QA và team Ops. Trong đó 2 teams đầu sẽ lo về tự động hóa, build, test và deployment. Còn team Ops sẽ chịu trách nhiệm về hạ tầng (cả phần cứng và phần mềm), quản trị hệ thống, policy và quản lý sự cố. Sự khác biệt trong trách nhiệm của team R&amp;D và QA dần sẽ trở nên mờ hơn và test engineer sẽ phải tham gia vào nhiều nghiệp vụ đòi hỏi code và hệ thống như automated testing, chuẩn bị môi trường test/staging, và xây dựng chu trình CI/CD. Nhược điểm của DevOps là chi phí và giá cả cho resources và nhân lực chuyên gia DevOps khá khan hiếm. Ví dụ, 1 server EC2 hạng tốt hoặc RDS hạng lớn sẽ ngốn kha khá tiền hàng tháng, nhất là nếu dịch vụ có đến hàng triệu người dùng. Do vậy nhiều cty đã quyết định đi đến một cực mới của DevOps, đó chính là NoOps mà chúng ta sẽ đề cập bên dưới. DevSecOpsNguồn: OWASP DevSecOps DevSecOps là sự tích hợp các chức năng bảo mật vào chu trình DevOps. Tất nhiên về tinh thần chung vẫn là công cụ và script, nhưng cụ thể cần tích hợp cái gì thì có 1 danh sách như bên dưới: Nguồn: OWASP DevSecOps  Đầu tiên là việc quản lý thông tin bảo mật trên Git. Nếu phát hiện ra cái gì như mật khẩu hay token mà lại có trên Git là phải có biện pháp xử lý ngay.  SAST (Static Application Security Test) hay là test tĩnh. Trong devops có thể tham chiếu tới các công cụ linter, review code tự động. Còn với DevSecOps thì là các công cụ Threat Modeling. Thường thì kiểu test sẽ là white-box.  IAST (Interactive Application Security Testing) hay là tương tác. Tức là để người dùng và chuyên gia tương tác với hệ thống để tìm ra lỗ hổng. Một nghiệp vụ liên quan là pen test.  DAST (Dynamic Application Security Test) hay là black-box test để kiểm tra hệ thống bảo mật chạy trong điều kiện mô phỏng gần production nhất hoặc ngay trên production.  Infrastructure scanning là phần quan trọng trong triển khai và vận hành. Audit tiến hành ở cấp độ này sẽ phát hiện ra lỗ hổng bảo mật ngay khi hệ thống đang chạy thực tế.  Compliance check là kiểm chứng và xác nhận độ tuân thủ các quy định về bảo mật. Công cụ thường dùng là các checklist, và có thể tự động hóa vào CI/CD với OWASP ZAP. Về hệ thống công cụ có thể dùng, các bạn có thể tham khảo hướng dẫn của OWASP. BizDevOpsNguồn: Stackify BizDevOps là sự cân chỉnh (alignment) giữa DevOps và business teams. Lợi ích thì cũng như định nghĩa, nó giúp giảm gap (sự sai lệch trong nhận thức) giữa team DevOps và team business. Giảm thiểu tình trạng phải làm lại vì không hiểu nghiệp vụ business, tăng độ hài lòng của khách hàng. Một điểm lợi nữa là ý kiến của nhà phát triển cũng được tích hợp vào luồng đưa ra quyết định, dẫn tới sự đồng thuận cao giữa các team nghiệp vụ. Với BizDevOps, mọi kế hoạch DevOps (plan) sẽ phải có thông qua xác nhận của team business, và mọi quy trình nghiệp vụ business sẽ phải thích ứng (adapt) vào các team DevOps. Nhược điểm là quy trình chuẩn bị và triển khai sẽ mất thời gian hơn, từ lúc lên kế hoạch tới phê duyệt và cuối cùng là đi vào triển khai. NoOpsNoOps là viết tắt của No IT Operations, tức là loại bỏ mọi nghiệp vụ IT ra khỏi chu trình phát triển, và nhà phát triển sẽ tập trung vào các nghiệp vụ phát triển như viết code. Để hiện thực hóa điều này, NoOps kế thừa và nâng cấp độ tự động hóa của DevOps lên một mức cao hơn. Một ví dụ điển hình chính là Serverless Computing. Serverless Computing: Serverless Computing được hỗ trợ ở hầu hết các nền tảng đám mây hiện tại với Lambda, Azure Functions, GCP Functions, AWS Serverless Application Model (SAM), các dịch vụ micro services như AWS Elasticsearch, DynamoDB, AI services như Comprehend, Rekgonition, . v. v… Lợi ích chính là  Mô hình giá (pricing model) trở nên mềm (flexible) hơn, người dùng trả cho đúng thời gian sử dụng dịch vụ (pay-as-you-go).  Không có upfront nên quy trình khởi tạo và chạy sẽ đơn giản hơn, và tiết kiệm giá khởi tạo upfront.  Chi phí quản lý, bảo trì cũng đơn giản hơn nhờ phụ thuộc vào nền tảng đám mây. Bạn sẽ chỉ phải cấu hình Lambda để chạy, chứ việc log hay xử lý lỗi sẽ không phải viết code mà cấu hình luôn.  Nhà phát triển tập trung vào việc code mà bớt lo nghĩ về các vấn đề Ops. Nhược điểm là  Giới hạn chỉ trong những service được serverless hóa.  Vì là trọn gói nên cái gì có là chỉ có thể thôi, người dùng không thể tùy tiện customize nhiều được. Nhìn chung NoOps vẫn đang trong quá trình hình thành và độ phủ dịch vụ sẽ tăng lên, do đó việc giới hạn về nội dung services hy vọng sẽ được cải thiện sớm. Ngược lại đối với các startup muốn khởi đầu nhỏ, đây rõ ràng là một hướng đi có lợi nhờ giảm bớt overhead vào các nghiệp vụ khác. Tuy nhiên, bài toán migration sẽ phát sinh với các doanh nghiệp lớn đã triển khai hệ thống data center hoặc devops từ lâu. Do vậy, tôi nghĩ nếu vào năm nay năm 2021, nhận nhiệm vụ xây dựng một cái gì chưa hề có sẵn, tôi chắc chắn chọn start small với NoOps. AIOpsNguồn: Gartner AIOps hay Artificial Intelligence for IT Operations, liên quan khá trực tiếp tới BizDevOps. BizDevOps đòi hỏi các quyết định business cần được đưa ra dựa trên nền tảng dữ liệu thu thập từ nhiều nguồn, và dữ liệu đó thường sẽ là dữ liệu lớn. Làm thủ công là không thể, vì vậy, đòi hỏi một giải pháp tự động hóa, xử lý được dữ liệu nhiều nguồn, đó chính là AIOps. Các ứng dụng như phát hiện bất thường, phân tích dữ liệu lịch sử, phân tích hiệu suất, . v. v… không những góp phần tự động hóa quy trình đưa ra quyết định, mà là quyết định với cơ sở là dữ liệu lớn. Hai trụ cột chính để hiện thực hóa AIOps chính là dữ liệu lớn (Big Data) và máy học (Machine Learning). Kết luậnITOps và CloudOps là những quy trình cũ (ít nhất từ 2014-2015 là thời cực thịnh), hiện tại đang là DevOps (ra đời năm 2008), DevSecOps (vẫn đang được OWASP chuẩn bị guideline) và BizDevOps. Hiện tại đang có mặt và sẽ là chủ đạo tương lai có lẽ sẽ là NoOps và AIOps. Chúng ta hy vọng vào một tương lai của các dịch vụ công nghệ mới, quy trình phát triển mới mà độ tự động hóa và DX sẽ gia tăng hơn nữa. "
    }, {
    "id": 102,
    "url": "https://wanted2.github.io/python-38-sys-audit/",
    "title": "Code audit processes and the new feature of Python 3.8 - sys.audit",
    "body": "2021/07/17 - Auditing is actually a quality assurance feature. The purpose of audits is to identify and report problems, not to respond, prevent or act on them. If you are using Python 2. x or even Python 3. 7, this feature may not be available. Calls like sys. audit and sys. addaudithook are only available from version 3. 8. The new feature allows the audits to run at runtime level, or in other words, to keep track of changes in system calls and standard libraries while the application’s running. Two proposals that made this available are PEP 578[^4] and PEP 551[^3]. [^3]: PEP 551 - Security transparency in the Python runtime - Python. org[^4]: PEP 578 – Python Runtime Audit Hooks | Python. org Overview of technical auditsCode review vs. code audit: First of all, code audit differs from code reviews. First, while code reviews can be done by internal teams, each member reviews code of and focus on the specific part of the project, code audit is relevant to the whole project, a big picture and must be done objectively (often done by an outsider who is not a member of the project, or even from another company). Second, code reviews can be done by only reading and find suspicious pieces in the code, but code audit requires to see the product running in actions. Therefore, to support the audit, language tools, and utilities to ascertain the changes made by the programs during the run is helpful. General audit aspects and tools: There are several aspects to assess when auditing a software project. Code management: Most software projects adopt some kinds of code management tools like Git and Mercurial to reduce operation mistakes. Check your target projects if they have their own code repositories and governance policies for code management. Some items to include in the checklist:  Branch naming convention Workflow like Gitflow to determine the process of creating PR, merging, . etc.  Release management: Git tags systems may help to version different releases in project lifetime.  and any recommendations that help to improve code management. Architectures: Software products take many forms: desktop applications, mobile apps, websites, infrastructures, and even combinations to make a far complicated system. But whatever the forms they take, they will have architectural choices. Integrability: A product can be a mixture of many components: a website written with Django framework with PostgresSQL. First, please pay attention to the compatibilities among components: are all versions work well together? Are all of them well-tested together (system test or integration tests are sufficient)? Deployment and delivery: How is the product (system) deployed and delivered to the customer? Please pay attention to the deployment process. If the project has a continuous mechanism to deliver outcomes to the customer, it should make everything’s smoother. Especially if CI/CD processes have not been well-tested, then it should be good advice to construct such a pipeline. Container systems like Docker or Kubernetes are helpful. Then, it’s time to check whether the README file contains all the necessary elements:  instructions for configuration, instructions for installation, a user’s manual, a manifest file (with an attached list of files), information on copyrights and licenses, contact details for the distributors and developers, known bugs and malfunctions, a problem-solving section, a changelog (for developers). Maintenance and incident responses: The system may be down for many reasons: operating mistakes, system errors, human errors, and so on. Does the project has such a mechanism to report and respond to the errors?An error tracking tool like Bugsnag is good advice. Coding best practices: Using code analysis tools: Static code analyzers like linting tools are helpful to detect early bugs, bottlenecks, performance issues, security vulnerabilities, and threats connected with maintaining the application. Tips to improve code quality: Code reviews between team members (developers) help to detect early problems. Using githooks can enforce your local development. Standardize the configurations and formats among team members like IDE configurations can prevent several human errors. Finally, remember to share knowledge among the team. Python audit toolsProposals PEP 578 and PEP 551: Since code audit is inevitable in software development, the Python programming language also has its own tools to do the job. Remember that code audit differs from code reviews in the way that code audit reviews the running system!Then if it’s not running, this is not audit. In this post, we focus on the new features for runtime audit: proposals PEP 578 [^4] and PEP 551[^3]. These proposals were made available only from Python 3. 8. PEP 578 was only a part of PEP 551, which concerns the security transparency of Python programs: the lack of internal audits leads to the fact that many threats can bypass audits. Python up to 3. 7 in isolation could manage to audit all events in codes. However, no runtime events have been captured, and this led to a poor audit. Defenders have a need to audit specific uses of the Python programming language in order to detect abnormal or malicious usage. With PEP 578, the Python runtime gains the ability to provide this. The aim of this PEP is to assist system administrators with deploying a security-transparent version of the Python programming language that can integrate with their existing auditing and protection systems. sys. audit and sys. addaudithook: Runtime audithooks [^4] comes with two different APIs: CPython and standard way. You can use sys. audit/sys. addaudithooks in Python code and PySys_Audit/PySys_AddAuditHook in native C code PEP 5781. The backport to Python 3. 7 and older versions can be found in a third-party sysaudit2A list of auditable events can be found in 3. Some discussions on bypassing audit features in Python 3. 8 can be found in4. Native C interface: If you want native audits, you may need to re-compile the python binary. An example is the NetworkPromtHook example. We will write an example spython. c: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;Python. h&gt;#include &lt;opcode. h&gt;#include &lt;string. h&gt;#include &lt;locale. h&gt;static int my_hook(const char* event, PyObject* args, void* userData) {  /* Only care about 'socket. ' events */  if (strncmp(event,  socket.  , 7) != 0) {    return 0;  }  PyObject* msg = NULL;  /* So yeah, I'm very lazily using PyTuple_GET_ITEM here.    Not best practice! PyArg_ParseTuple is much better! */  if (strcmp(event,  socket. getaddrinfo ) == 0) {    msg = PyUnicode_FromFormat( WARNING: Attempt to resolve %S:%S ,      PyTuple_GET_ITEM(args, 0), PyTuple_GET_ITEM(args, 1));  }  else if (strcmp(event,  socket. connect ) == 0) {    PyObject* addro = PyTuple_GET_ITEM(args, 1);    msg = PyUnicode_FromFormat( WARNING: Attempt to connect %S:%S ,      PyTuple_GET_ITEM(addro, 0), PyTuple_GET_ITEM(addro, 1));  }  else {    msg = PyUnicode_FromFormat( WARNING: %s (event not handled) , event);  }  if (!msg) {    return -1;  }  fprintf(stderr,  %s. Continue [Y/n]\n , PyUnicode_AsUTF8(msg));  Py_DECREF(msg);  int ch = fgetc(stdin);  if (ch == 'n' || ch == 'N') {    exit(1);  }  while (ch != '\n') {    ch = fgetc(stdin);  }  return 0;}#ifdef MS_WINDOWSintwmain(int argc, wchar_t** argv){  PySys_AddAuditHook(my_hook, NULL);  return Py_Main(argc, argv);}#elseintmain(int argc, char** argv){  PySys_AddAuditHook(my_hook, NULL);  return _Py_UnixMain(argc, argv);}#endifIn Windows, this requires Visual Studio C, and compilation can be done: 123456789101112131415161718192021222324252627@setlocal@echo offif not defined PYTHONDIR echo PYTHONDIR must be set before building &amp;&amp; exit /B 1if exist  %PYTHONDIR%\PCbuild  (  set _PYTHONINCLUDE=-I %PYTHONDIR%\PC  -I %PYTHONDIR%\include   if  %VSCMD_ARG_TGT_ARCH%  ==  x86  (    set _PYTHONLIB=%PYTHONDIR%\PCbuild\win32  ) else (    set _PYTHONLIB=%PYTHONDIR%\PCbuild\amd64  )) else (  set _PYTHONINCLUDE=-I %PYTHONDIR%\include   set _PYTHONLIB=%PYTHONDIR%\libs)if exist  %_PYTHONLIB%\python_d. exe  (  set _MD=-MDd) else (  set _MD=-MD)@echo on@if not exist obj mkdir objcl -nologo %_MD% -c spython. c -Foobj\spython. obj -Iobj -Zi -O2 %_PYTHONINCLUDE%@if errorlevel 1 exit /B %ERRORLEVEL%link /nologo obj\spython. obj /out:spython. exe /debug:FULL /pdb:spython. pdb /libpath: %_PYTHONLIB% @if errorlevel 1 exit /B %ERRORLEVEL%Save this bat script as make. cmd and run it in a VC console to compile spython. exe. Then we can confirm the audit is in effect: Standard interface: To do audits in pure Python language, we can use sys. audit/sys. addaudithook as follows: 1234567891011121314151617181920import sysdef my_hook(event, args):  # Only care about 'socket. ' events  if not event. startswith( socket.  ):    return  if event ==  socket. getaddrinfo :    msg =  WARNING: Attempt to resolve {}:{} . format(args[0], args[1])  elif event ==  socket. connect :    addro = args[0]    msg =  WARNING: Attempt to connect {}:{} . format(addro[0], addro[1])  else:    msg =  WARNING: {} (event not handled) . format(event)  ch = input(msg +  . Continue [Y/n]\n )  if ch == 'n' or ch == 'N':    sys. exit(1)sys. addaudithook(my_hook), then confirm the effects similarly. ConclusionThis article introduced two important features in a software project. The first part showed an overview of code audit in practice. The second part discussed a new audit feature in the Python programming language: sys. audit/PySys_Audit. Although this feature is new in the Python programming language but the merit of capturing runtime events can be helpful. Benchmark in PEP 578 [^4] showed that adding this audit did not change the performance or cost then it is nice to have this in your project. References      sys - System-specific parameters and functions - Python 3. 9. 6 documentation &#8617;        sysaudit – sysaudit documentation &#8617;        Audit events table - Python 3. 9. 6 documentation &#8617;        Bypassing Python3. 8 Audit Hooks [Part 1] · daddycocoaman &#8617;    "
    }, {
    "id": 103,
    "url": "https://wanted2.github.io/python-chain-of-responsibility/",
    "title": "Pythonによる責任の連鎖",
    "body": "2021/07/15 - 責任の連鎖1はオブジェクト思考のデザインパターンで、送信者に対して複数の受信者を順次にデータを渡してあげます。受信側の処理を一連の部品に分けてして部品化いくため、独立性が高められ、コードの記述も簡略化されます。もちろん、処理が長くなるか、デバッグが困難になるというデメリットはあるけれども、連鎖を避けれない場合、ぜひなれたいパターンです。 責任の連鎖責任の連鎖: 責任の連鎖は一つのオブジェクト思考のデザインパターンで、送信者に対して複数の受信者を順次にデータを渡してあげます。例えば、あるリクエストに対して、複数の処理が必要です。  リクエスト情報を使って認証局に認証する 業務データベースに接続して、トランザクションを開始する 業務データを取得して、レスポンスを作る トランザクションを終了し、レスポンスを返す。もちろん、処理に不具合が発生する場合、エラー処理なども必要です。これらの処理は順序が決まっています。ですので、一つの流れを複数のハンドラーに分けて、実行時、最初のハンドラーを呼び出すだけで、連鎖の全体を順次に実行していきます。それが、責任の連鎖です。 メリットはいくつかあります。  Loose Coupling: 送信側と受信側の結び付けは弱めているので、処理の独立性が高まります。各部品（ハンドラー）には、自身の責任で対応しているので、部品化はしやすい。 Clean code: 受信側は連鎖を内部に記述しているため、連鎖全体を実行するのに、最初の部品を呼び出すだけで大丈夫ですので、記述が簡略になります。事例: 下記の連鎖を考えます。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from typing import Anyfrom abc import abstractmethodclass AbstractCaller:       抽象クラス       @abstractmethod  def __init__(self, name) -&gt; None:    pass  @abstractmethod  def next(self, caller) -&gt; Any:    pass  @abstractmethod  def __call__(self, x) -&gt; Any:    passclass Caller(AbstractCaller):  _caller = None  def __init__(self, name) -&gt; None:    super(). __init__(name)    self. _name = name  def next(self, caller: AbstractCaller) -&gt; Any:           次のコーラーをセットする           self. _caller = caller    return caller  @abstractmethod  def process(self, x):    pass  def __call__(self, x) -&gt; Any:    x = self. process(x)    print(f'{self. _name}: {x}')    if self. _caller is not None:      return self. _caller(x)    return xclass FirstCaller(Caller):  def process(self, x):    x += 1    return xclass SecondCaller(Caller):  def process(self, x):    x += 2    return xclass ThirdCaller(Caller):  def process(self, x):    x += 3    return xfirst = FirstCaller( First Stage's result )second = SecondCaller( Second Stage's result )third = ThirdCaller( Third Stage's result )# Chain of Responsibilitiesfirst \  . next(second) \  . next(third)print( Process 1:\n--------------- )print(f ---------------\nFinal result: {first(0)} )print( Process 2:\n--------------- )print(f ---------------\nFinal result: {second(0)} )連鎖の定義は下記になります。 123first \  . next(second) \  . next(third)firstの次にsecondを実行します。secondの次にthirdを実行します。連鎖の実行は単純に最初の要因を呼び出すだけです。 1first(0) # 連鎖の実行連鎖の要因とするfirst, second, thirdはCallerを継承しています。それぞれはCaller. __call__を継承しているので、processを実行して、値を修正してから連鎖の次の要因に渡します。 このようにすると、連鎖の各要因を実行する記述を隠蔽できるため、コードがすっきりになります。 課題独立性が高くて記述がすっきりでも弱点があります。連鎖が長くなると、実行は遅くなります。さらに、バグがある場合、連鎖の途中でどこの部品にバグがあるかを知りたいなら、最初から最後までデバッグしないといけないのです。 結論責任の連鎖を紹介しました。連鎖の操作がある場合、強力なパターンです。長い連鎖を部品化し、最初の部品を実行するだけで、連鎖全体を稼働できます。 参考文献      3. Data model — Python 3. 9. 6 documentation - Metaclasses &#8617;    "
    }, {
    "id": 104,
    "url": "https://wanted2.github.io/controlling-creative-activities/",
    "title": "Controlling Creative Activities",
    "body": "2021/07/11 -  Control is not necessarily the enemy of creativity; nor, popular myth to the contrary, do creative activity imply complete uncertainty. Source: Meredith et al. [1] Creative Activities like research and development projects, design projects, and similar processes depend strongly on the creativity of individuals and teams. Controlling such activities is quite sensitive: creative activities involve a high degree of uncertainty, but tight controlling inhibits the creativity. Such a dilemma can be controlled by looking carefully at processes, payoff, and risk management. Process ReviewProcess Review vs. Result Review: In creative activities, processes are often more certain than the results (Meredith et al. [1]). Therefore, controlling the processes is more realistic than controlling the outcomes. In contrast, outsource projects often certain enough then PMs only need to care about the results, and processes are not reviewed in those projects. The contract in an outsourcing request often takes the form:  In outsourcing, we provided a given input and desired outputs. The deliverable should be a software module that transforms the input into the outputs, and that’s all! Yes, that’s all, and any further requests can be filtered by the contract terms. However, creative projects don’t have such certain requirements. The outcomes can be undetermined or vaguely determined. Therefore, controlling creative activities often review the processes rather than the final deliverable: we know the process to produce the deliverables, but we cannot know what deliverables are at the contract time.  Even when we know the methods, we don’t know what the final results are. Phase-Gate Control: Because processes are more reliable to control in creative projects, there is a need to divide the process in these creative activities into measurable milestones. Assessments at each milestone should care about the progress so far, the potential value that can be achieved in the next step, and the desirability to change the research designs. That is the phase-gate control. Personnel ReassignmentFor such an uncertain type of project, it is valuable to reassign people: productive personnel are kept, while the rest is reassigned to other projects or other organizations. It is not difficult to perform this control, but there are several issues:    If the favored personnel doesn’t change, then there is a possibility to create an elite group: new personnel becomes demotivated when joining, while old personnel is already highly motivated for further achievements.     It is difficult to rank the middle quartile.  Control of Input ResourcesCreativity does not equal the generosity of resources. Resource controlling helps to enhance the efficiency of creative projects. Outcomes in R&amp;D projects often come in batches: sometimes, there are no visible results, but suddenly many outcomes can be delivered. ReferencesMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 105,
    "url": "https://wanted2.github.io/python-decorator/",
    "title": "Pythonによるメタプログラミングーデコレーター・パターンについてー",
    "body": "2021/07/10 - 関数を実行するときに、実行時間とリソース管理を監視し、レポートすることは不可欠です。さらに、異常や例外が検知された時に、実行を制御できるとなお嬉しくなります。例えば、途中経過時間が長くなると、スレッド数を増やし、実行時間を短縮させるとか、リソースが足りないとか、コミュニケーションがうまくいかないなどの不具合が発生するときの対応を行う方法が必要となります。単に、関数毎に処理を記述すると、制御を組織のすべての関数に正則化できないといった問題があるため、メタプログラミングで制御の論理をすべての関数で対応できます。前回で紹介したメタクラス1について、クラスの初期化で一つのパターンを記述すると、実行時に条件に応じて複数のパターンのクラスを生成できますが、今回で実行時の制御処理にもひとつのメタパターンを紹介します。それデコレーターパターン2です。 デコレーター・パターン単に　「関数を戻り値として返す関数」と解釈すれば良いですが、具体的にPython言語でみてみます。ある関数fを実行する時に、関数の中身に記述せずに監視や認証・制御などの別の処理を行ってから関数fを実行するようなパターンです。 12345import numpy as npdef f(x): y = x + 1 return yこの関数の実行時間を測定したい場合、下記に2案あります。 123456789101112131415161718192021222324252627282930313233import timedef f(x): y = x + 1 return y# 案1：関数の中身を変更し、実行時間を測定するロジックを記述def f2(x): start = time. time() y = x + 1 end = time. time() print(f 実行時間：{end-start:. 5f}秒 ) return y# 案2：デコレーター・パターンを利用def measure_time(f): def wrapper(*args):  start = time. time()  y = f(*args)  end = time. time()  print(f 実行時間：{end-start:. 5f}秒 )  return y return wrapper@measure_timedef f3(x): y = x + 1 return y# 下記の3パターンは同じ結果になります。f2(1) # 内部を変えてしまうmeasure_time(f)(1) # デコレーターを展開すると同等の記述f3(1) # pythonのデコレーター上記の案1だと、外部要件に対して内部組織を変えてしまう感じですね。悪くもないけれども、毎回毎関数をこんなに内部変更を行ういくつかの問題が発生します。  品質の低下。修正工夫が大きくなり、コードが「スパゲッティ」になりうる。 コード管理が難しくなります。実行関数に外部による微細修正が加えたので、問題発生時、毎関数を見ないといけない。 スコープ管理が緩和された。外部要件によって変更しやすいので、スコープも良く変わってしまう。 内部修正は危険。プロジェクト内に関数は1つの場所に使用されない場合、その関数を内部修正するとほかの場所にも影響を与えてしまう。事例12345678import numpy as npdef solve(A, x, b): return A * x + bdef solve_double(A, x, b): y = solve(A, x, b) return solve(A, y, b)監視と制御を行うと、  solve_doubleの各ステップの実行時間を計測したいです。ベンチマークや監視に使われます。 yの実行時間が長くなると、solve(A, y, b)の実行時間を短縮できるように制御したいです。といった単純な関数実行の制御の要件を実現したいです。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import timeimport numpy as npfrom typing import Anyclass Controller: reports = [] def __init__(self, action_trigger_threshold:float=0. 4) -&gt; None:  self. action_trigger_threshold = action_trigger_threshold def receive_report(self, t: float) -&gt; None:  self. reports. append(t) def is_slow(self) -&gt; bool:  return len(self. reports) &gt; 0 and \   self. reports[-1] &gt;= self. action_trigger_threshold# コントローラー記述project_controller = Controller(action_trigger_threshold=0. 0005)class Decorator:  @staticmethod def on_slow() -&gt; Any:       遅いタスクが発見された場合、制御するデコレーターパターン。       def decorator(f):    def wrapper(*args):      if project_controller. is_slow():        print( 遅い実行が発見。次の実行をよくするためにポストコントロールしてください。 )        # 高速化するために工夫を記述する        # 入力のサイズを縮小するか、環境変数を調整するか、スレッド数を増えるか        pass      return f(*args)    return wrapper  return decorator  @staticmethod def measure_time() -&gt; Any:  def decorator(f):    def wrapper(*args: object):      # 実行時間を測定      start = time. time()      result = f(*args)      end = time. time()      # ログを残す      print(f'実行時間は{end-start:. 5f}秒です。')      # 制御インスタンスに実行時間をレポートする      project_controller. receive_report(end-start)      return result    return wrapper  return decorator# デコレーターパターンを追加@Decorator. measure_time()@Decorator. on_slow()def solve(A, x, b): return A * x + bdef solve_double(A, x, b): y = solve(A, x, b) return solve(A, y, b)A = np. eye(1000, 1000)x = np. ones((1000,1))b = np. ones((1000,1))solve_double(A, x, b)print(project_controller. reports)実行結果はこんなになります。 12345$ python decorator. py実行時間は0. 00900秒です。遅い実行が発見。次の実行をよくするためにポストコントロールしてください。実行時間は0. 00900秒です。[0. 008999109268188477, 0. 009003400802612305]複数デコレーター: 上記のコードには下記のデコレーターパターンを利用しています。 12@Decorator. measure_time()@Decorator. on_slow()実行順序は下記になります。  プロジェクトコントローラーproject_controllerに問い合わせis_slowで実行歴には、最終実行で遅くなったことがあるか確認する。遅くなった場合、ログも残し、プロジェクトの内容に応じてそれぞれの制御を行う。 実行時間を測定し、ログを残し、プロジェクトコントローラーproject_controllerに実行記録を更新する。 solve本体を実行する。プロジェクト・コントローラー: project_controllerの重要性は、プロジェクトによって変わるけど、今回の要件では、重要だと仮定しましょう。その制御変数は、実行時間をキープしたり遅いプロセスを発見したりしますので関数を監視するために役立つ情報を管理しています。 課題よく使うと良い武器になるが、悪く使うと制御できない武器になりえます。メタプログラミングは勢いため、悪用はもちろんあります。 12345678910111213def plus_one(f): def wrapper(*args):  return f(*args) + 1 return wrapper@plus_onedef identity(x): return x# 別のファイルでidentity(1)# 出力# 2identityを呼ぶのに1を足した結果になります。出力、あるいは、レスポンスを変えてしまう利用になりますので、スコープが変更されてしまうという課題です。 コードを難読化するために、利用すると良い場面もあります。ですが、実用であまりこの「出力を変更してしまうパターン」を利用しない方がいいかもしれません。 結論勢いパターンであるデコレーター・パターンを紹介しました。実行時の制御や監視などに役に立します。内部変更を防ぎ、スコープ管理やコード管理をよくするパターンです。しかし、出力を変更してしまう場合、気をつけて利用しましょう。Pythonのフレームワークにはこのパターンはよく使われます。DjangoやFlaskのフレームワーク・コードをよく見ると必ず出会うパターンですので、悪用せずによく利用するとメリットばかりです。 参考文献      https://docs. python. org/3/reference/datamodel. html#metaclasses &#8617;        https://refactoring. guru/design-patterns/decorator &#8617;    "
    }, {
    "id": 106,
    "url": "https://wanted2.github.io/attendance-bot-microsoft-teams/",
    "title": "Hệ thống chấm công AttendanceBot tích hợp vào công cụ chat Microsoft Teams để quản lý và theo dõi nhân sự dự án",
    "body": "2021/07/10 - Quy trình thực thi dự án luôn đòi hỏi phân tích real-time các dữ liệu dự án như nhân sự và communication trong team và với khách hàng (Meredith et al. [1]). Việc theo dõi, phân tích, thậm chí lên cảnh báo này hỗ trợ các thành viên team được giữ “tỉnh táo” ở mức độ cao ví dụ như thông tin hôm nay nhân viên nào nghỉ và cần bổ sung nhân lực làm thay. AttendanceBot là một hệ thống chấm công thông minh sử dụng phân tích dữ liệu ngôn ngữ tự nhiên để đăng ký tự động các kỳ nghỉ cũng như thời gian in/out hàng ngày của thành viên, qua đó giảm tải cho quản lý dự án. Hệ thống tích hợp vào luồng quản lý dự án thông qua các ứng dụng chat như Slack, Microsoft Teams và Google Chat. Dịch vụ AttendanceBotChức năng chính: Quản lý thời gian lao động (timesheet). Nhân viên thực hiện chat với AttendanceBot bằng cụm từ quy định in để check in. Thời gian checkin được tính từ thời điểm này và được lưu trữ vào bảng timesheet của AttendanceBot. Bằng việc nhập lệnh out vào cửa sổ chat, nhân viên thực hiện checkout và cập nhật timesheet. Nhân viên có thể xem timesheet của chính mình trực tiếp trên công cụ chat bằng lệnh timesheet. Để tải timesheet của bản thân mình, nhân viên nhập lệnh timesheet report và AttendanceBot sẽ trả về đường link tới file timesheet CSV. Đồng thời hành vi check in/out của nhân viên cũng được thông báo trên channel của team, do đó PM có thể nhanh chóng nắm bắt nhân sự và nhận cảnh báo. Với quyền quản lý, PMs còn có thể truy cập và quản lý timesheet của các nhân viên. Quản lý kỳ nghỉ. Nhân viên có thể đăng ký lịch nghỉ dài hạn hoặc ngắn hạn, hoặc Work From Home (WFH) từ cửa sổ chat. PM và các quản lý cấp cao có thể nhận cảnh báo ngay lập tức với các kỳ nghỉ của nhân viên. Mẫu câu để đăng ký nghỉ có thể phong phú hơn từ khóa in/out, ví dụ, nhân viên có thể nhập WFH on next Monday, và AttendanceBot có thể nhận ra thời gian là thứ 2 tới ngày 12/7 và nội dung là Work from home. Quản lý phiên làm việc. Lên kế hoạch làm việc cho nhân viên trong cả tuần tiếp theo là một việc làm thường xuyên của PM. PM sẽ thực hiện lên kế hoạch trên công cụ lịch của AttendanceBot và từ đó, AttendanceBot sẽ lên schedule để gửi thông báo (notification) cho nhân viên về time shift. Khi gần tới phiên làm việc sẽ có cảnh báo/thông báo gửi về từng nhân viên để làm việc. Đơn giá: Báo giá của AttendanceBot có thể tìm thấy tại https://www. attendancebot. com/pricing/. Có 2 phiên bản là bản AttendanceBot và AttendanceBot Pro. Mức giá tính theo user và theo tháng là 4 đô và 6 đô tương ứng. Sử dụng cùng Microsoft Teams Một điểm hay của AttendanceBot là ngoài việc dùng trực tiếp dịch vụ tại trang chủ attendancebot. com thì các nhà phát triển chủ động một cách thông minh tích hợp dịch vụ dưới dạng chatbot tự động vào các nền tảng communication cho dự án nổi tiếng như Slack, Teams và Google Chat. Chính sự chủ động này đã khiến cho việc tích hợp AttendanceBot vào dự án đang chạy vô cùng dễ dàng cho người dùng. Để cài đặt bạn chỉ cần mở tab App và gõ tên app AttendanceBot vào khung tìm kiếm để lọc ra app. Sau đó theo các hướng dẫn để cài đặt vào thư mục dự án. Đăng ký lịch nghỉ: Sau khi cài đặt, AttendanceBot sẽ chủ động chat với bạn về cách sử dụng app. Có hai cách để tương tác với AttendanceBot:    Trong thư mục của team dự án, bạn có thể gửi tin lên @AttendanceBot Vacation from 12 December to 14 December.     Bạn mở một chat trực tiếp với AttendanceBot với nội dung Vacation from 12 December to 14 December.  Dù làm theo cách nào, thì AttendanceBot cũng sẽ chat lại cho bạn với nội dung như trên. Cú pháp xin nghỉ khá đa dạng nhưng phần lớn tuân theo một số kiểu câu: 12345&lt;Loại nghỉ&gt; from &lt;ngày bắt đầu&gt; to &lt;ngày kết thúc&gt;Ví dụ:Vacation from 12 December to 14 DecemberWFH from 12 December to 14 DecemberNhững mẫu câu về ngày nhất định như Vacation today hoặc WFH tommorrow cũng được bot hiểu. Check in/check out: Quản lý check in/out có thể được thực hiện khá đơn giản bằng các câu lệnh sau:  in: nhân viên bạn quản lý phải nhập câu lệnh này để checkin.  out: khi ra về nhân viên phải nhập lệnh này để checkout và cập nhật timesheet. Nếu bạn là quản lý trực tiếp của nhân viên, thông tin timesheet sẽ được thông báo để bạn quản lý và theo dõi.  timesheet: nhân viên có thể dùng lệnh này để tự xem timesheet của mình.  timesheet report: nhân viên có thể dùng lệnh này để tải CSV của timesheet về máy.  change manager: lần đầu đăng ký timesheet (in/out), nhân viên sẽ bị hỏi nhập thông tin quản lý trực tiếp. Trong quá trình dự án, nếu có thay đổi quản lý trực tiếp, nhân viên dùng lệnh này chat cập nhật quản lý. Chức năng này giúp luồng báo cáo của nhân viên tới quản lý dự án được tự động hóa và thông suốt.  my manager: nhân viên có thể xem tên quản lý trực tiếp của mình bằng lệnh này. Quản lý báo cáo: Source: AttendanceBot. com Công cụ hữu ích cuối cùng là dashboard để quản trị thông tin nhân sự. Với công cụ này, quản lý dự án có được cái nhìn tổng thể mà chi tiết (big picture) của dự án. Kết luậnAttendanceBot là công cụ tự động hóa (RPA) hữu ích, đồng thời cũng là công cụ có tích hợp trí tuệ nhân tạo. Mặc dù các xử lý ngôn ngữ tự nhiên bên trong khá đơn giản như quyết định bằng rule gắn với keyword (in/out) hay phân tích mẫu câu có sẵn template đơn giản như vacation from . . . to . . . , đây là một công cụ hữu ích và trong tương lai có thể kỳ vọng vào những bước tiến xa hơn. ReferencesMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 107,
    "url": "https://wanted2.github.io/student-syndrome/",
    "title": "The Student Syndrome: Hiệu Ứng Sinh Viên "Lười"",
    "body": "2021/07/04 - Hiệu ứng Sinh Viên “lười” là hiện tượng sinh viên hoặc nhân viên khi đến thời hạn nội bài, nộp hàng thì mới bắt đầu “chạy”, và nếu không kịp thì xin trì hoãn và yêu cầu quản lý dự án cũng như khách hàng đợi thêm một thời gian (1-2 ngày, có khi 1-2 tuần). Đây là một vấn đề không mới, nhưng là “bệnh nan y” mà quản lý dự án phải đối mặt theo cách này hay cách khác. Hậu quả của hiệu ứng này là khiến cho dự án “muộn” hơn so với kế hoạch và hầu hết trong các giáo trình quản lý dự án (như cuốn của Meredith et al. [1]) đều coi việc quản lý hiệu ứng này như một phần của quản trị rủi ro. Nguyên nhânCó nhiều nguyên nhân dẫn tới hiệu ứng Sinh Viên “lười” như sau:    Quản lý dự án yếu kém: PM thiếu tổ chức và thiếu trách nhiệm, trốn tránh việc giao tiếp thường xuyên với team dự án có thể là nguyên nhân dẫn đến hiệu ứng này. Khi team dự án bị “bỏ bê” nhiều, các bạn trở nên mất phương hướng, thụ động, ngại hành động, dẫn đến tiến độ chậm. Thế rồi, quản lý dự án đột nhiên xuất hiện và yêu cầu các bạn nộp bài để cho stakeholders xem, mà không hướng dẫn kịp vì bỏ bê lâu ngày, thì … toang!     Yêu cầu dự án không rõ ràng: Dự án chứa nhiều yêu cầu không rõ ràng để tạo ra một “big picture” có độ phân giải cao. Các bạn thử tưởng tượng khi xem một bức ảnh có độ phân giải thấp, thì những vùng khó nhìn trên ảnh sẽ có thể hiểu nổi không? Nếu không thể hiểu được thì bạn có xem vùng đó nữa không? Tất nhiên là không! và thành viên dự án cũng vậy. Khi có một nhiệm vụ không rõ ràng, các bạn sẽ có xu hướng tránh nhận nhiệm vụ đó. Và dự án trì trệ theo chiều hướng đó thôi!     Chạy song song nhiều dự án, thiếu tập trung: Cùng 1 team dự án phải care nhiều dự án quá cùng lúc dẫn tới dàn trải sức và kết cục sẽ phải ưu tiên 1 dự án nào đó hơn các dự án khác. Các dự án còn lại sẽ rơi vào “stagination mode” mà nói chung sẽ phải chờ!     Bản tính con người là “lười”: “Nhân chi sơ, tính bản thiện” nhưng ai cũng “lười”.  Hậu quả của hiệu ứng Sinh Viên “lười” là giảm chất lượng sản phẩm, khiến cho scope có nhiều chỗ không kịp làm, giảm niềm tin của stakeholders, team dự án thì lúc nào cũng rơi vào tình trạng “áp lực”, và rủi ro cho kế hoạch thì gia tăng. Thuốc trị   Định kỳ thường xuyên theo dõi, giám sát team dự án: Không bao giờ bỏ bê các bạn. Phải hỏi ngay khi có dấu hiệu không hiểu, phải chỉnh lại ngay khi có dấu hiệu “lạc lối”.     Định kỳ thường xuyên báo cáo và liên lạc với stakeholders: báo cáo tiến độ kịp thời và nhận feedback kịp thời khi có vấn đề.     Theo dõi sát sao thời gian rảnh của team (“slack”) và lên cảnh báo ngay khi có dấu hiệu thời gian rảnh nhiều hoặc khi tiến độ chậm lại.     Chia nhỏ đầu việc, đến mức có thể nắm được: Tránh giao 1 đầu việc to vì như vậy rủi ro chậm dự án là rất lớn. Lý do bởi vì thời gian để detect được hiệu ứng SInh Viên “lười” luôn không vượt quá thời gian công số của task. Vì vậy nếu để đầu việc công số lớn thì thời gian để phát hiện ra hiệu ứng cũng lớn theo. Do vậy, không nên để đầu việc nào có thời gian dài quá 1 ngày, hoặc vài giờ, tùy theo dự án.  ReferencesMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 108,
    "url": "https://wanted2.github.io/python-meta-programming/",
    "title": "Pythonによるメタプログラミングーmetaclassについてー",
    "body": "2021/07/03 - 日々でPythonコードを書いています。根拠なくコードを書くことが絶対良くないのです。自信を持って根拠強いコードを書くとハイクオリティを保証できます。メタプログラミングはPythonのクラスとメソッドの生成過程を制御できるので、その自信を確保できます。今回はメタプログラミングのデザインパターンの中に強力なメタクラス（metaclass）について解説します。 メタプログラミングとは: プログラミングはコードでロジックを実装します。メタプログラミングはそのロジックをプログラミングします。つまり、高次ロジックを制御することになります。 例えば、シンプルな例ですが、プロジェクトの実行フェーズに入ると、各行を監視したいですね。インスタンスが稼働する時にクラスのコードにロガーを追加することで、運用時の挙動を監視することができます。しかし、例えば、毎回再起動時にクラスが再生成されるが、クラス生成過程も監視したい場合、クラスよりメタ段階で生成過程を定めてログ処理を記述することで、プロジェクトの監視が一貫で行えます。 Python言語のメタプログラミングについて: Python言語では、メタプログラミングを行う道は主に2つあります。一つ目は、ｍetaclassを利用することで、2つ目は、decoratorsを利用することです12。 クラスはオブジェクトを生成するが、metaclassはクラスを生成します。例えば、下記のクラスを監視したいとしましょう。 123456789101112131415161718192021import mathclass Sigmoid: def __init__(self):  pass def preprocess(self, x):  x = x + 1  return x def postprocess(self, x):  x = math. exp(x)  return x / (1 + x) def __call__(self, x):  x = self. preprocess(x)  return self. postprocess(x)sigmoid = Sigmoid()sigmoid(-1)# 0. 5このようなクラスだと、そのクラスの生成過程は隠蔽されますので、クラス生成過程を監視できません。次に、preprocessとpostprocessを抽象化して隠蔽したいです。つまり、前処理と後処理はメタクラスで隠蔽して、動的に変更できるようにしたいです。 Python3では、クラスの定義でtypeを継承すればメタクラスを定義できます。新クラスの生成過程を定義でき、自由に監視ロガーを追加できます。また、前処理や後処理を属性として管理できます。例えば、シグモイドクラスの場合、preprocessとpostprocess関数を利用しますが、線形クラスの場合、identity関数を利用すると条件文で指定できます。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106import math, loggingdef setup_logger(name: str, level:int=logging. DEBUG):     ロガーを設定し、インスタンスを返す。     logger = logging. getLogger(name) logger. setLevel(logging. DEBUG) ch = logging. StreamHandler() ch. setLevel(level) formatter = logging. Formatter(   '%(asctime)s - %(name)s - %(levelname)s - %(message)s') ch. setFormatter(formatter) logger. addHandler(ch) return loggerdef preprocess(self, x):     前処理     x = x + 1 return xdef postprocess(self, x):     後処理（中身はシグモイド関数）     x = math. exp(x) return x / (1 + x)def identity(self, x):     線形アクティベーション関数     return xclass MetaActivation(type):     メタクラス     @classmethod def __prepare__(cls, name, bases, **kwargs):       新クラス生成の前処理  クラスの種類で、前処理関数と後処理関数を動的に定める。       print(f'MetaActivation. __prepare__({cls}, {name}, {bases}, {kwargs})')  attrs = {}  attrs['preprocess'] = preprocess if name == 'Sigmoid' else identity  attrs['postprocess'] = postprocess if name == 'Sigmoid' else identity  return attrs def __new__(cls, name, bases, attrs, **kwargs):       ここでも属性を見直すことができます。  今回はロガーを設定する処理を追加します。       print(f'MetaActivation. __new__({cls}, {name}, {bases}, {attrs}, {kwargs})')  if 'logger' not in attrs:   attrs['logger'] = setup_logger(name=name)  return super(). __new__(cls, name, bases, attrs) def __init__(cls, name, bases, attrs, **kwargs):       あまり意味ない設定関数です。  通常一般クラスで__init__する。       print(f'MetaActivation. __init__({cls}, {name}, {bases}, {attrs}, {kwargs})')  return super(). __init__(name, bases, attrs) def __call__(cls, *args, **kwargs):       インスタンス生成時で呼び出されます。  ここで、引数を見直すことができます。       print(f'MetaActivation. __call__({cls}, {args}, {kwargs})')  # 引数を渡すとエラーを返す  if len(kwargs) &gt; 0:   raise ValueError( The class must have no variable constructor )  return super(). __call__(*args, **kwargs)# サンプルクラスclass Sigmoid(metaclass=MetaActivation): def __call__(self, x):  self. logger. debug('Main run started with x = {}'. format(x))  x = self. preprocess(x)  x = self. postprocess(x)  self. logger. debug('Main run finished with x = {}'. format(x))  return xclass Linear(metaclass=MetaActivation): def __call__(self, x):  self. logger. debug('Main run started with x = {}'. format(x))  x = self. preprocess(x)  x = self. postprocess(x)  self. logger. debug('Main run finished with x = {}'. format(x))  return x# 実行print( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  )sigmoid = Sigmoid() # Sigmoid(x=1)などでエラーが発生する。print(sigmoid(-1))print( . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  )linear = Linear()print(linear(-1))出力は下記になります。この出力からわかるように、実行だけではなくクラス生成過程も監視できます。モニタリングとオーディティングは簡易になれます。 12345678910111213141516MetaActivation. __prepare__(&lt;class '__main__. MetaActivation'&gt;, Sigmoid, (), {})MetaActivation. __new__(&lt;class '__main__. MetaActivation'&gt;, Sigmoid, (), {'preprocess': &lt;function preprocess at 0x000001784EC26160&gt;, 'postprocess': &lt;function postprocess at 0x000001784EC261F0&gt;, '__module__': '__main__', '__qualname__': 'Sigmoid', '__call__': &lt;function Sigmoid. __call__ at 0x000001784EC265E0&gt;}, {})MetaActivation. __init__(&lt;class '__main__. Sigmoid'&gt;, Sigmoid, (), {'preprocess': &lt;function preprocess at 0x000001784EC26160&gt;, 'postprocess': &lt;function postprocess at 0x000001784EC261F0&gt;, '__module__': '__main__', '__qualname__': 'Sigmoid', '__call__': &lt;function Sigmoid. __call__ at 0x000001784EC265E0&gt;, 'logger': &lt;Logger Sigmoid (DEBUG)&gt;}, {})MetaActivation. __prepare__(&lt;class '__main__. MetaActivation'&gt;, Linear, (), {})MetaActivation. __new__(&lt;class '__main__. MetaActivation'&gt;, Linear, (), {'preprocess': &lt;function identity at 0x000001784EC26280&gt;, 'postprocess': &lt;function identity at 0x000001784EC26280&gt;, '__module__': '__main__', '__qualname__': 'Linear', '__call__': &lt;function Linear. __call__ at 0x000001784EC26670&gt;}, {})MetaActivation. __init__(&lt;class '__main__. Linear'&gt;, Linear, (), {'preprocess': &lt;function identity at 0x000001784EC26280&gt;, 'postprocess': &lt;function identity at 0x000001784EC26280&gt;, '__module__': '__main__', '__qualname__': 'Linear', '__call__': &lt;function Linear. __call__ at 0x000001784EC26670&gt;, 'logger': &lt;Logger Linear (DEBUG)&gt;}, {}). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MetaActivation. __call__(&lt;class '__main__. Sigmoid'&gt;, (), {})2021-07-04 03:23:51,474 - Sigmoid - DEBUG - Main run started with x = -12021-07-04 03:23:51,474 - Sigmoid - DEBUG - Main run finished with x = 0. 50. 5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MetaActivation. __call__(&lt;class '__main__. Linear'&gt;, (), {})2021-07-04 03:23:51,476 - Linear - DEBUG - Main run started with x = -12021-07-04 03:23:51,476 - Linear - DEBUG - Main run finished with x = -1-1属性を自動的に追加したり管理したりすることもメタプログラミングで行えます。例えば、ロガーやデータ準備・整理・正則化などを属性としてメタクラスで追加できます。これにより、動的クラスや動的関数を作成できます。運用に未知のクラスを動的に生成できるので、未知物体がある場合、静的クラスのアプローチと比べるとより柔軟な対応ができます。 metaclassについて: 上記の例の生成フローは下図に示します。 __prepare__: クラス生成の最初に実行される関数です。記述される場合、クラス生成の準備処理を行えます。属性配列を返す関数なので、自動的に属性を追加したり管理したりすることができます。今回の例では、前処理などを隠蔽しました。メタプログラミングのコードを読むときに、下記のクラスだけを読むとpreprocessやloggerはどこに定義するか新人にとっては不思議ですが、フレームワークなどのコードではよくあるパターンであろう。 1234567class Sigmoid(metaclass=MetaActivation): def __call__(self, x):  self. logger. debug('Main run started with x = {}'. format(x))  x = self. preprocess(x)  x = self. postprocess(x)  self. logger. debug('Main run finished with x = {}'. format(x))  return x__new__と__init__: __new__はクラスを生成するが、未初期化の状態です。ここで、属性の見直すのもできます。__prepare__が返したものが属性の辞書とみなされクラスを生成します。__init__は__new__のクラスを初期化します。 __call__: この関数はインスタンス作成時に呼び出されます。例えば、sigmoid = Sigmoid(引数)で呼び出されるので、クラスのコンストラクタの引数を制限するなど行えます。 結論: インスタンスのイベントではなく、クラスのイベントを容易に管理できるようにメタクラスの概念を紹介しました。Pythonのmetaclassでクラス生成を制御する事例も紹介しました。__prepare__, __new__, __init, __call__などで属性を自由に管理できるので、動的に新クラスを定義できるため、実際にいろいろな条件に応じて新クラスを生成できます。メタプログラミングをよく知って、使いこなせると強い武器になると思われます。 特に、新クラス、未知クラス、珍クラスなどをメタクラスで制御できるときたいできるのであろう？ 参考文献      https://docs. python. org/3/reference/datamodel. html#metaclasses &#8617;        https://refactoring. guru/design-patterns/decorator &#8617;    "
    }, {
    "id": 109,
    "url": "https://wanted2.github.io/the-customer-interface/",
    "title": "The Customer Interface",
    "body": "2021/06/20 - Risks are quite often in project management [1]. Financial risk and schedule risk are among the most common risks in the project. One of the most popular situations is when the technical content turns to be more complex than planned. It adds a delay to the completion of the projects. In this post, we review one of the exemplar stories (cases) from the PM book of Meredith et al. [1, pp. 472], in which the unaddressed risks led to the loss for all sides in terms of costs and delays in schedules. The storyThe whole story can be found in [1]. The scene graph for this story is as follows. The organization. SPIS personnel: RB=Reggie Brown, RR=Roger Robert, JD=Jacqueline Doyle, RC=Rus Clemons. RB is the site manager, RR and JD are contract managers, and RC is the Vice President. NLP personnel: RJ=Rick James, LM=Lou Mayhew, SS=Sly Simmons, SG=Stand Goodsen, BJ=Bill Jones, MC=Mel Carter. SG is the on-site personnel but is not in charge of contracts. LM is the PIC for contracts. Here we brief the case: NLP ordered SPIS for an outage in 1989. SPIS’s site manager, RB speculated some delays, and it had become real. RB reported to SG as the technical personnel of NLP and asked for changes in contracts to change it to pay-as-you-go. SG replied as OK, but the real person in charge of contract management, LM, didn’t get any request! NLP had changed the procedures for purchasing from 1986, but SPIS and RB didn’t know about that. After the delays, bills were one million bucks higher than the original deal (500,000$). Key persons in NLP found that the price went higher without their notices due to the fact that SPIS reported to the wrong personnel (RB). In fact, they must ask LM for changes in contracts. SPIS and NLP arranged some meetings to resolve the issues, but finally, one thing to keep in mind is that NLP still has four outages remaining!If SPIS cannot handle well this customer relationship, they may lose the 4 outages that worth more. Lessons   QA to the right person: RB didn’t ask LM although, LM is the true PIC for contract, not SG.     Know the decision maker: LM or the boss is the right decision-maker, not SG.     Control the customer: SPIS wasn’t aware of changes in purchasing procedures of NLP.     Get money for work performed: I think SPIS should have divided work into smaller units and get paid for each done. In fact, they waited until everything of the big piece was done for purchasing.     Persevere  ReferencesMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 110,
    "url": "https://wanted2.github.io/crystal-ball-monte-carlo-risk-analysis/",
    "title": "Using Oracle CrystalBall for Monte Carlo Simulation based Risk Analysis",
    "body": "2021/06/17 - Uncertainty is everywhere in the lifecycle of a project. The duration of activities, resources to assign to actions, and schedules are all uncertain. Project managers can manage to reduce the risks associated with uncertainties, but they cannot be eliminated. Risk analysis helps to identify and sometimes visualize the risks, not to remove risks from projects. Oracle CrystalBall (OCB)1 is a simulation software for risk management. It utilizes Monte Carlo Simulation to consider every possibility that can happen when calculating and predicting the outcomes of their occurrences, thereby giving you the critical part of your model in which to concentrate. IntroductionRisk Analysis and Monte Carlo Simulation: Uncertainty is everywhere in the lifecycle of a project. The duration of activities, resources to assign to actions, and schedules are all uncertain. Project managers can manage to reduce the risks associated with uncertainties, but they cannot be eliminated. Risk analysis helps to identify and sometimes visualize the risks, not to remove risks from projects. To apply risk analysis, one must make assumptions about the probability distributions that characterize key parameters and variables associated with a decision and then use these to estimate the risk profiles or probability distributions of the outcomes of the decision. Monte Carlo Simulation (MCS) works upon an established assumption about distributions of risks and evaluating the risks by computing the values of multiple samples. Under statistical analysis, a range of values of interests can be selected with confidence. For example, let’s see when we have to estimate the duration of a task, which follows normal distributions with the mean is 5 days, and the standard deviation is about 1 day. Then MCS generates a set of 1,000 samples following the assumptions. We can find the worst cases and the best cases from simulated results, as the minimum values and the maximum values out of 1,000 samples. Oracle CrystalBall: Oracle CrystalBall (OCB) is a simulation software for risk management. It utilizes Monte Carlo Simulation to consider every possibility that can happen when calculating and predicting the outcomes of their occurrences, thereby giving you the critical part of your model in which to concentrate. With the Monte Carlo Simulation, your working processes are simplified, including the statistics and the necessary information at its side in split-view format. The software is an Excel add-in and useful for Excel professionals. Get a trial version: Because a license of OCB costs nearly $1,000, we experience the trial version. Please visit here and follow the instruction to download trial versions. To see the download link, you may need to register for an Oracle account. Oracle reviews your download request. In my case, it took less than 24 hours. Thanks to Oracle!The download came after the approval. That download content was: Installation: Read the guides and then double-click on crystalballsetup-x64. exe and follow the instructions for installation. The CrystalBall add-in is installed inside Excel and is available for every workbook. ExamplesProject Selection: Somewhere in the past, we have discussed about Discounted Cash Flow (or Net-Present Value). Problem: A company has been planning to develop a new product. The project has a hurdle rate $K=0. 12$ and prospective inflation rate $p=0. 03$. The most likely cash flows in ten years are as follows. Recall that the Discounted factor of $t$-th year is $d_t=\frac{1}{(1+K+p)^t}$. Note that the above table is the most likely number. Now let us assume that the expenditures in this example are fixed by contract with an outside vendor. Thus, there is no uncertainty about the outflows, but there are, uncertainties about the inflows. Assume that the estimated inflows are as shown in the below table and include a most likely estimate, a minimum (pessimistic) estimate, and a maximum (optimistic) estimate. Both the beta and the triangular statistical distributions are well suited for modeling variables with these three parameters, but fitting a beta distribution is complicated and not particularly intuitive. Therefore, we will assume that the triangular distribution will give us a reasonably good fit for the inflow variables.       Year   Minimum Inflow   Most Likely Inflow   Maximum Inflow         2008   $35,000   $50,000   $60,000       2009   95,000   120,000   136,000       2010   100,000   115,000   125,000       2011   88,000   105,000   116,000       2012   80,000   97,000   108,000       2013   75,000   90,000   100,000       2014   67,000   82,000   91,000       2015   51,000   65,000   73,000       2016   30,000   35,000   38,000         Total   $621,000   $759,000   $847,000   Register assumption and forecast variables: To do Monte Carlo simulation, we must register our assumptions on the uncertainties of inflows. Select cell B5, and then select the Define Assumption button from the ribbon menu of CrystalBall. Note that the project was assumed that the project generates no profit in the first three years. Therefore, we start from 2008 (or cell B5). Then choose the three-point estimates. Adjust the parameters of the triangular distribution according to the table in the previous section. Then click OK(O). Repeat the process for cells B6:B13. To run the simulation, we still need to select a forecasting goal: the total DCF the project will generate after 10 years, or the value of cell F15. Select cell F15 and the button Define Forecast from the ribbon menu. Input the name and the unit as follows. Simulation: Click button Start in the ribbon menu of CrystalBall to start MCS. The default number of simulations is 1,000. After done the simulation, we observe some results like: To find a robust estimation, we can see the median: 10,869 USD after 10 years is the expected DCF the project will generate. Not so much! References      Oracle Crystal Ball Downloads &#8617;    "
    }, {
    "id": 111,
    "url": "https://wanted2.github.io/microsoft-project-scheduling/",
    "title": "Planning Waterfall Project Schedule and Resources with Microsoft Project 2019",
    "body": "2021/06/13 - Scheduling is the conversion of a project action plan to actionable timetables [1]. Resource utilization and availability are needed to keep the schedule on time and sometimes help to shorten the runs. Networks techniques such as Activity-on-Arrow (PERT analysis) and Activity-on-Node (Critical Path Model, or CPM) help establish the schedule. Resource allocation considers labor, material, facilities, and equipment to achieve the action plan. Resource allocation requires adjusting the loading and leveling, while constrained planning can be solved using heuristic methods or optimization methods such as linear programming. While computer-aided scheduling and resource allocation are popular, Microsoft Project1 is among the best choices for project management software because it supports all the above techniques. Introduction: Scheduling: Given a Work-Breakdown Structure (WBS), project scheduling concerns the timeline of all activities and events that must be speculated to ensure the delivery to customers is on time and with high quality. Formal methods for this task establish a network of activities and events, which also highlights the critical events in the network. The critical activities and events delay the completion of a project if they are delayed. The order of tasks and dependencies between tasks can be seen in the schedule. The probability of various completion dates can be estimated then the schedule can be re-evaluated and reworked. A simple form of the network is the Gantt charts which can be seen in the above figure. The Gantt chart shows planned and actual progress for a number of tasks displayed as bars against a horizontal time scale. Program Evaluation and Review Technique (PERT) and Critical Path Model (CPM) are two different networking methods for estimate a project schedule. With preliminary knowledge from WBS, CPM constructs an Activity-on-Node (AON) network where each node represents an action. From a start node, with source nodes are predecessors, and target nodes are given tasks, we can construct a graph of tasks. Each task is assigned an expected time $t_e$. The longest path $\mathcal{L}$ in the network (with sum of time $T_e=\sum_{e\in\mathcal{L}} t_e$) show the shortest time needed to complete the project. $\mathcal{L}$ is the critical path and $T_e$ is the critical time of a project. The right-hand-side figure shows the CPM of 5 tasks with assigned resources (workers and equipment). Each activity can have some uncertainty in estimating durations; therefore, a stable predictor can be made using a three-point analysis: optimistic estimates ($a$), pessimistic estimates ($b$), and most likely estimates ($m$). The expected time can be estimated as follows. $t_e=\frac{a+4m+b}{6}$ The estimates $a,b,m$ can be found when the distribution of duration for an activity is known as a priori. Then the most likely time $m$ is the mode of the distribution. PMs select $a$ at the actual time required by the activity will be $a$ or greater about 99 percent of the time. Similarly, PMs select $b$ at the actual time required is $b$ or less for 99% of the time. The standard deviation of $t_e$ can be estimated as $\sigma_e=\frac{b-a}{6}$. For an activity, the slack is the difference between the latest possible start date (LS) and the earliest possible start date (ES), i. e. , slack = LS - ES. With these statistics, the PM can determine the uncertainty of the completion date. For example, the PM promised that a project finishes in $D$ days. Then we model the uncertainty by a normal distribution $Z=\frac{D-T_e}{\sigma_e}\sim \mathcal{N}(0, 1)$ For example, $D=50$ days, $T_e=39$ days, and $\sigma_e=6. 15$ days, then the likelihood that the PM can keep the promise is $\mathcal{N}^{-1}(Z)=\mathcal{N}^{-1}\left(\frac{50-39}{6. 15}\right)=\mathcal{N}^{-1}(1. 79)\approx 96\%$ In other words, the probability of lateness is only 4%. The risk assessment, another aspect of the schedules, will be addressed in another article. Resource allocation: CPM can be used for resources as well. However, to accelerate the speed, introducing more resources (or crashing with resources) can be useful. It is equal to trading time with cost, and this is the essence of resource allocation problem. Resource loading is the amount of a specific resource to finish an activity in a duration. For example, to finish a task, we need 2 labor-hour, then that is the resource loading. In reality, the resource loading can be uneven during the project lifetime. Resource leveling concerns making the loading even throughout the project time. It is done by shifting the tasks within their slack allowances. In other words, leveling helps to stabilize the workloads over time. One concern for the PMs is how many workers should be hired for a given workload?If the workload is uneven, then the number of workers and other resources must be sufficient to handle the peak. In other time, when the workload is much lower than the peak workload, the worker pool size can be adjusted to avoid waste. Microsoft Project: MSP is a multi-scale project management software that handles from small projects to large ones. It supports multiple views such as grid view and board view for better reporting. Statistical analyses like CPM and PERT are supported. Gantt charts are supported in both cloud and desktop solutions. In this practice, we use MSP for scheduling and resource allocation with analytics. We use the priority rule of As Soon as Possible (ASAP). Preparation: Sample data: In this practice, we use the sample project WBS is from a typical software development project. Please download the data from the following resource.  Software development project WBS (from University of Ohio): downloadThe sample project consists of 6 phases:  Phase 1: Requirements Definition Phase 2: Logical Design Phase 3: Physical Design Phase 4: Programming and Unit Testing Phase 5: System Testing Phase 6: InstallationFor simplicity, we only take a subset for example: 123456781	Requirements Definition (Phase 1) 1. 01	Requirements funding  1. 01. 01	Review project request  1. 01. 02	Establish preliminary justification  1. 01. 03	Fund Phase 1  1. 01. 04	Prioritize project  1. 01. 05	Establish project teamRegister for an evaluation version of Project: Unfortunately, Microsoft Project is non-free. Price for monthly users starts from 10$/user. Fortunately, they provide us an evaluation version for 30 days. Please visit https://www. microsoft. com/en-us/evalcenter/evaluate-project and choose Project Plan 3 and click Continue button.  Follow the step-by-step instructions to have an account set up. Visit https://portal. office. com/account/?ref=MeControl#subscriptions you can see as follows.  Visit https://portal. office. com/account/?ref=MeControl#installs and click Install Project button to get the download file. After downloading, please double-click and follow the guides to install. After installation, you may need to log in to your Microsoft account registered in the previous steps.  Run: From the Start screen of MSP, please select Waterfall project. Although Sprint project is available, we will explain it in another article.  Register tasks: In the Waterfall project, the default view is the Gantt chart. Now, we need to input by hands the WBS tasks. Note that the default unit for Waterfall projects works is days. Then workers will be awarded daily. Is this a good cost performance? In reality, it is better to make payrolls by precisely to hours and even by minutes. For example, worker A registers a task S for 1 working day. But in fact, nobody spends all 8 working hours for work!A study has shown that about 12% of working hours have been spent on other things like personal time. Estimating task durations by minutes or hours helps to reduce such uncertainty. MSP supports measurements by minutes, but hourly payment is good enough. Select Options &gt; Schedule &gt; Input worktime unit to minutes. To view the state of distributed tasks, you can switch the view Assigned workers and resources can be shown, too. Register resources: Please switch to Resource Sheet view.  There are two kinds of resources: noncurrent assets like equipment which price is fixed with one-time payments, and other resources which must be paid by hourly or monthly payments such as labors of office rental fees. , office rental fees aren’t counted in many cases, especially for remote workers. In this case, we have one worker and one PC (laptop) that are assigned to the tasks. For this case, we assign the price of 2,000 JPY per hour (tax included) to the payment. Overtime is paid by 2,500 JPY per hour. Note that when the tasks of the same person are run parallel, the total time is a sum!Then the working time is doubled, and any hours exceed 8 hours per day will be counted as overtime. To avoid payments for such concurrency caused overtime, the PMs should:  Break down the tasks smaller and make them run sequentially within the 8-hour frame.  Pay by hours or minutes for non-contract workers. If they are contract workers, PMs don’t need to care about the units but need to break down tasks. After registering the resources, switch back to the Gantt chart view, and assign the resources to tasks. Under the Report tab, there are many other views that are useful. Readers are recommended to explore by themselves.  View Critical Paths: Switch to the Gantt chart view, and from the View tab in the ribbon, choose Critical as the filter. We will observe the Critical Paths automatically as follows.  PERT Analysis: We use the macro from https://github. com/flametron/MSProject-2019-PERT. With the weight combinations $(1,4,1)$ we have the estimates of Duration: Conclusion: We explained several concepts in project scheduling and resource allocations. We also experienced with the Microsoft Project 2019 to handle scheduling problems and resource assignments. We applied some tips to reduce project costs. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details      Project Management Software - Microsoft Project &#8617;    "
    }, {
    "id": 112,
    "url": "https://wanted2.github.io/commitments-expenses-cash-flows/",
    "title": "Commitments, Expenses, and Cash Outflows - Three Perceptions of Project Costs",
    "body": "2021/06/13 - Different roles in a project can have different perceptions about cost [1], [2]. Commitments, expenses, and the organization’s cash flows are vital to consider about the cost. Losses incurred in the projects include finance expenses and taxes, too. Project managers can use their understanding of these perceptions to impact the cost of the project. Deferring and delaying are techniques the PMs can use to enhance their contributions to improve project expenditures. Three perceptions of project cost: Each key person in a project cares about different aspects of cost. To the project managers, commitments must be high to prove efficiency. However, accountants implemented their accounting systems that do not identify commitments. Accountants care more about expenses for labor cost, materials, and equipment. Senior and top managers care more about the cash outflows of the organization. If the project does not terminate on time, another tax term may start, and more money for taxes must be spent. Usually, a project should reduce all these factors, but the relationships of these costs often take the form in this figure. Commitment leads, cash outflow lags, and expenses fall in the middle. The bill: A typical bill of the project (profit and loss terms) is as follows.       Revenue (project sell price)   +$1,000,000       Cost of goods sold (project costs)   -$750,000       Gross-margin   +$ 250,000       Selling, general &amp; administrative expenses   -$180,000       Profit before interest and taxes   +$ 70,000       Financial expense   -$ 30,000       Profit before taxes   +$ 40,000       Taxes   -$ 20,000       Net profit   +$ 20,000   Project cost (cost of goods sold): In the exemplar bill, they sold the project at 1M dollars, but the actual cost of goods sold was about 75% of the selling price. Then the gross-margin (includes sales/GA costs, financial expenses, and taxes) was about $250,000. The net profit after tax was 20,000 dollars. While the cost for Sales and GA is almost fixed in many companies, the cost of goods sold can be reduced by the actions of PMs. An effective PM can reduce it by carefully making the choices:  Considering alternatives for design concepts and taking “trade-offs” can reduce the cost.  QA is expensive: QA or any safety factors can enhance the quality but also makes the product more expensive. Then PMs must be careful when thinking about doing excessive QA in their projects. Usually, after having safety at standard, it is possible to stop doing QA to prevent the cost go higher.  Control work: developers and technicians can require excessive time for development and testing. This only increases the cost of the project. PMs must control the cost of development by setting deadlines for any tasks and tests.  Cost of changes: changes are often in projects. The PMs must be aware of the contingency budget. Unexpected problems ultimately arise, at which time the funds are needed. Uses of this budget to finance a scope change is neither advantageous to the project manager nor to management.  Changes must be made under client or management approval Procurement of material, equipment, and services should be planned in the project schedule. The procurement that bases on incomplete requirements should be avoided.  Economy phenomena like inflation can impact the project negatively. The PMs must handle such impacts effectively. Finance expenses and taxes: Finance expenses include the order placements during the project’s life cycle, withheld funds, financial supports from vendors. Internal projects may not experience outside pressures like client’s payment considerations. However, they live with the pressure to shorten the lifetime and to pay back faster to the organization. In both cases, PMs need to care about delivering the outcomes faster to keep profits and reduce costs. In other words, it always means to stop early. Taxes are also another aspect for the top management to “kill” a project earlier. Sometimes, it can be an order to reduce the lifetime to a half. The reason behind the order can be the risk they feel about not meet a tax term deadline. In some cases, PMs can insist company’s top managers by changing the schedule to shift benefits and expenses from a tax term to the next. Defer when possible but don’t delay the termination: When thinking about the timing to place purchase orders, the PMs can use deferring techniques to avoid buying equipment and material when the price is high. The work of placing orders can be deferred until the price becomes low. However, this technique comes with risks: if the price does not go lower or if the item goes unavailable, it becomes a backfire to the project. Deferring order placements can be helpful, but delaying the termination of a project can be dangerous. Many organizations expect a project finishes on time to meet a tax deadline. Delaying the termination for only a few days can result in a new tax term starts, and lots of money will be spent. Indeed, a PM should insist the organization by shortening the lifetime of the project when it is possible and do not delay the end. Conclusion: An effective project manager can help the organization reducing project costs, expenses, and cash outflows a lot by the techniques described in this article. Financial expenses and taxes can be other factors for the top managers to shorten project lifetime. PMs must be aware of these risks and use techniques like deferring and delaying at the right time to mitigate. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. DetailsHamburger, D. H. 1986. Three perceptions of project cost. Project Management journal. (1986), 372–378. Details"
    }, {
    "id": 113,
    "url": "https://wanted2.github.io/design-structure-matrix/",
    "title": "The Design Structure Matrix and Information Flow",
    "body": "2021/06/12 - Tracking the information flow throughout the project is required for planning the coordinating flows. Traditional tools like the Gantt chart are good at tracking the interdependency between runs of concurrent tasks but fail to describe the causal dependencies between flows. This problem is magnified when multidisciplinary teams join the stage. Hence, the problem becomes increasingly serious while handling many domains at once. To cope with this issue, Design Structure Matrix (DSM), Domain Mapping Matrix (DMM), and Multiple Domain Matrix (MDM) have been proposed [1]. While DSM tracks the information flows, DMM can be used to show the person-task assignments. MDM combines both DSM and MDM in multiple domains, i. e. , coordination of not only tasks but also people. It can track all structures of the team, tasks, and assignments at once. Design structure matrix: Integration management is important because it makes the deliverables regarding customer needs but lacks coordination of flows. For instance, in the below figure, task 1 needs to gather information from task 3 before running. Such information flow does not appear in previous tools like the Gantt chart. While not aware of such flow, planning tasks, like which task should be done before a particular task, can result in a mess. With this schedule, then task 3 is completed after task 1. However, task 1 requires information flow from task 3, thus task 1 must be revisited and reworked after task 3’s completed. Source: dsmweb. org Concurrency: Concurrent engineering helps to enhance the performance but makes the system is hard to design due to the complexity of dependencies and reworks. Let’s see the example. The PM can plan to let task 2, task 3, and task 4 run concurrently. What happens here is that there are two situations needing reworks. The first one was already explained in the first section: task 1 must be updated according to information from task 3. The second one is task 4 that needs information from task 5, but task 5 also completes after task 2 running concurrently with task 4. Let’s see whether we can eliminate the reworks. The first solution is to move the mark X above the diagonal: for example, let task 5 inputs directly to task 6, and instead of task 3, task 6 inputs to task 1. The second solution is to add more activities into the concurrency: for example, add task 5 and task 1 to the concurrency, and all first 5 tasks run concurrently. Domain Mapping Matrix: Instead of working in only one domain, sometimes we may need to map two different domains, such as assignments between people and tasks. Domain Mapping Matrix shows such assignments in a matrix form. For example, by watching the above DMM, one can find who is the Person In Charge (PIC) of a task in the project. Multiple Domain Matrix: Single-class tracking can be done with DSM in a domain like tasks. Mapping different domains are done using DMM, then attributions of tasks to identities can be done, too. But when we have multiple classes (domains), the problem becomes complex. Multiple Domain Matrix helps to solve the multi-class tracking problem.  MDM is helpful here because it can demonstrate all inter-class and intra-class relationships at once. Not only the hierarchy of the tasks but also of people in the team can be seen. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 114,
    "url": "https://wanted2.github.io/matrix/",
    "title": "The Matrix Organization",
    "body": "2021/06/10 - Pure project organization emphasizes the role of project managers, while functional organization emphasizes the importance of functional departments such as accounting, finance, marketing, R&amp;D, . etc. Both have their own advantages and disadvantages. The combination is the Matrix Organization [1] which takes the strengths of both predecessors in a “balanced” form between project managers and functional ones. The Matrix organizations: Formation: Matrix Organization can be represented as tabular data, where each row for a PM and each column for a functional department. The president directly manages the departments, and the PM deal directly with the clients. PMs have flexibilities to handle the clients, while they also need to deal with the functional managers for many aspects of their projects: human resources, technology, resources, schedules, . etc. PMs decide where, when and what the project members achieve, but the functional managers select those people for the project and which technology is used in the project. Each cell represents the effort the PMs need to borrow from the departments for the aspects of their projects. For example, PM1 needs a half man-month from the R&amp;D department, but PM2 does not need to borrow effort from R&amp;D. Advantages: Since its flexibility, PMs are the center of the organization while functional departments are the supporters. Responses to the clients are rapid, and knowledge sharing among the organization and across projects/departments is possible. Disadvantages: First, it is the conflicts between PMs and functional managers when dealing with resources. The workers always feel like they have two bosses, and that is another conflict. Second, it is a myth about who is in charge of the project. Since PMs control only people and tasks, functional managers control everything else like resources, schedules. Third, people in these projects tend to resist the termination of the projects. The first and second disadvantages often result in political fighting inside the organization. Imbalanced organizations: Pure project organization is like its name, purely project-oriented. Functional departments, if any, are just small offices, even with single members. Since there is no functional department, each project must carry out responsibilities with all functional aspects such as budgets and costs (accounting), finance, technology, and everything else. The project managers are then powerful in their projects. They decide every aspect of the projects, deal directly with clients, and only report to the top management (president or the program managers). Even if there are functional managers in the organizations, PMs do not care about them. The pros include the freedom of each project and the PMs, less time spending on negotiating resources with functional departments. But the disadvantages are also many. Since each PM need to decide everything in the project, the burden is heavy. Since each project is independent, less collaboration and knowledge sharing between projects is expected. In short, PMs here are quite soloers. Another imbalanced organization is the Functional Organization. In this formation, it is usually no PM, and no project, of course. The organization is of many departments, and there can be some internal projects in each department. But as each department has its technical focus, projects that require collaborations across departments are rare. Dealing with clients is slow because of the deep structure of management. In other words, processes in this form are slow. Conclusion: Imbalanced options like pure project organizations and functional organizations are different extremes of the project spectrum. The Matrix Organization (TMO) takes a more balanced form and comes with many strengths. However, a source of conflicts is the political fighting between PMs and FMs, and the myth of finding who is actually in charge of the project should care are problems. Pure project options may fit more small companies like startups, while functional ones fit big organizations like universities and corporations. TMO is the middle and fits mid-size ventures. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 115,
    "url": "https://wanted2.github.io/the-fisher-method/",
    "title": "The Fisher's method",
    "body": "2021/06/08 - Conciliating the conflicts with allies is an important step to maintaining a good alliance. Key persons in the organization may be stubborn, especially the functional managers. When both the functional managers and the project managers are technical people, cooperation between them often results in conflicts. Fisher’s method (1983, [1]) is an effective way, often named by the “principled negotiation” or win-win method, applying in situations where allies disagree in some aspects of the project. Deal with the problem, not with the person: Separating the people from the issues allows the parties to address the issues without damaging their relationship. It also helps them to get a clearer view of the substantive problem. Three sources of problems in parties involved in negotiation:    The multi-view problem: each party has its viewpoints, with different perceptions of the facts. The parties should not assume that their worst confusions become the actions of the other party. Nor should one side blame the other for the problem. One should put itself in the stand of others.     The emotion problem: Getting the task done is a frustrating process, sometimes. When their interest is threatened, people often react with confusion or anger. When seeing a man with anger expression, we may dismiss the man’s feeling, but it is likely to provoke more intense emotional responses. Dealing with emotions requires a calm mind. Symbolic gestures such as apologies or an expression of sympathy can help to defuse strong emotions.     The communication problem: A party may not listen to what another one’s speaking. The listeners should pay full attention to the speakers. However, misunderstanding can happen even the listeners hear carefully.  Dealing carefully with these issues may start by preventing them from arising. Allies should think about their counterparts as partners, not as adversaries. Focus on the interest, not the positions:  Your position is something you have decided upon. Your interests are what caused you to so decide. Fisher and Ury [1] Then interests are the root cause to solve the conflicts. Fisher argued that negotiations must focus on such interests. The first step is to identify the interests by asking the reasons for the positions to hold. For example, when the PM asks the functional managers that “We need to finish the task by June 25th”, and the functional manager responds that “We are busy until the end of June”. This conversation is no end without expected output. The PM starts with a position way then the functional manager also ends with a position argument. Removing the position may be like starting with “Can we talk about the schedule?”. The scheduling problem touches on the mutual interests of PM and functional manager, and it works. Once the parties have identified their interests, they must discuss them together and in a positive way. Generate more options for mutual gain: More options allow parties to achieve safer decisions. However, there are four problems for multi-party scenarios to have mutual gain:  Parties rely strongly on only one option and fail to consider alternatives.  Parties have the intent to narrow the set of options to find limited answers.  Parties have a bias on finding the options which bring advantages for them (bias on win-lose terms).  Parties do not feel the responsibility to find the solution and decide to let others do so. To generate meaningful options, parties should come together and brainstorm for creative proposals. Evaluation should be separated and start from the most useful proposals. Use Objective Criteria: When interests come in opposing directions, using objective criteria brings better resolution. These criteria ground parties into the same standard and resolve the differences. This leads to an agreement or a situation in which parties can agree. Scientific findings, professional standards, or legal precedent are possible sources of objective criteria. One way to test for objectivity is to ask if both sides would agree to be bound by those standards.  For example, children may fairly divide a piece of cake by having one child cut it while the other choose their pieces. Some notes on using objective criteria:  First, each issue should be approached as a shared search for objective criteria.  Second, each party must keep an open mind and reasonability.  Third, while they should be reasonable, negotiators must never give in to pressure, threats, or bribes. References: Fisher, R. and Ury, W. 1983. Getting to Yes: Negotiating Agreement Without Giving In. New York: Penguin Books. Details"
    }, {
    "id": 116,
    "url": "https://wanted2.github.io/never-let-the-boss-surprised/",
    "title": ""Never Let The Boss Be Surprised!"",
    "body": "2021/06/07 - A prime law in projects is that “Never Let The Boss Be Surprised!” [1]. As such a phrase, team members must be honest to the Project Manager (PM), and the PM must also be honest to the senior managers in a parent organization. Never hide the troubles nor the risks from the PM! Introduction: On the need of a man with high self-esteem: At the beginning of a project, the PM must select the best personnel for the team by cooperating with the functional managers. Besides personal characteristics such as technical skills, political sensitivity, problem or goal orientation, another aspect which is usually expected to be found in the team member is high self-esteem. That translates into 日本語 as “自尊心” or into Vietnamese as “lòng tự tôn. ” In practice, there are two signs of lack of self-esteem:    The members worry about their own errors and try to hide the troubles from the eyes of the PM. They are threatened by their own mistakes, though everyone can have mistakes!     The members try to point out the mistakes of others. They try to take advantage of others’ troubles and cover their own.  A man who is high self-esteem will never hide badness nor laugh at other mistakes. The man faces troubles as challenges and helps others to improve. Then a man without self-esteem always makes the boss surprised! And “Bad News Travels Fast”: You are the PM in a company amongst other PMs. You try to hide the unexpectedness in your project spilling out, but ultimately, the “bad news travels fast” when it happens. Then the boss (senior manager) won’t be surprised because of the news, but because you hide the news. Not only the team members must be honest to their boss, but also the boss (PM) must be honest to the senior manager and the top management. Under the corporate culture, not so many things can be a secret. The right actions should include timely reports, being self-esteem, and being straightforward. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 117,
    "url": "https://wanted2.github.io/weighted-score-model/",
    "title": "Weighted Score Models",
    "body": "2021/06/06 - In an attempt to overcome some of the disadvantages of profitability models, with one example, the Discounted Cash Flow we have discussed last time, which particularly their focus on a single decision criterion, a number of evaluation/selection models that use multiple criteria to evaluate a project has been developed. Such models vary widely in their complexity and information requirements. Weighted Factor Scoring Model is such one flexible method we discuss today. Weighted Score Model (WSM): The main disadvantage of a profitability model like the DCF is the limitation to a single decision criterion at a time. A solution to integrate multiple criteria into the selection process is to make a project evaluation form, in which each row presents a different criterion of the project with a specific score. A project council is established with experts to give a score on each criterion. Unweighted Binary Factor Model gives 0-1 score $b_i\in{0,1}$ to the project for each criterion. Unweighted Score Factor Model gives a numeric score $s_i\in\mathbb{R}$ with normally at a scale of 5 for each criterion. The total score of a project is given by$S=\sum_{i=1}^ns_i,$where $n$ is the number of criteria. A sample evaluation form can be as follows (taken from [1]).  Project _________________________  Rater ____________ Date _____________       Criterion   Qualifies   Does Not Qualify         No increase in energy requirements   x           Potential market size, dollars   x           Potential market share, percent   x           No new facility required   x           No new technical expertise required       x       No decrease in quality of final product   x           Ability to manage the project with current personnel       x       No requirement for reorganization   x           Impact on workforce safety   x           Impact on environmental standards   x           Profitability               Rate of return more than 15% aftertax   x           Estimated annual profits more than $250,000   x           Time-to-break-even less than 3 years   x           Need for external consultants       x       Consistency with the current line of business       x       Impact on company image               With customers   x           With our industry       x       Totals   12   5   A project with more qualified points is more likely to be accepted. And the Weighted Score Factor Model, each criterion is associated with a weight value $w_i$. The final score for a project is then $S_w = \sum_{i=1}^nw_is_i. $ This model gives flexibility to the evaluation method because the importance of each criterion is modeled by $w_i$ and then differently. The list of criteria and weight values can be chosen by experts based on their experiences. Problem and Discussion: The following problem is taken from the PM text [1]:  Use a weighted score model to choose between three methods $(A, B, C)$ of financing the acquisition of a major competitor. The relative weights for each criterion are shown in the following table as the scores for each location on each criterion. A score of 1 represents unfavorable, 2 satisfactory, and 3 favorable.       Category   Weight   A   B   C         Consulting costs   20   1   2   3       Acquisition time   20   2   3   1       Disruption   10   2   1   3       Cultural differences   10   3   3   2       Skill redundancies   10   2   1   1       Implementation risks   25   1   2   3       Infrastructure   10   2   2   2   In practice, MS Excel is enough to get this done, but we usually use some statistical tools like the Oracle CrystalBall software1. In this case, a higher score means a better method to implement. Let’s compute the score for each method: $S_A = 20 \times 1 + 20 \times 2 + 10 \times 2 + 10 \times 3 + 10 \times 2 + 25 \times 1 + 10 \times 2 = 175$ $S_B = 20 \times 2 + 20 \times 3 + 10 \times 1 + 10 \times 3 + 10 \times 1 + 25 \times 2 + 10 \times 2 = 220$ $S_A = 20 \times 3 + 20 \times 1 + 10 \times 3 + 10 \times 2 + 10 \times 1 + 25 \times 3 + 10 \times 2 = 235$ Method C has the highest final score and is then acceptable. Choosing the most probable weight values may be a deep problem, then to aggregate them into the best decision to the next stages, adjusting the value by trial-and-error may be approached. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details      Oracle Crystal Ball Downloads &#8617;    "
    }, {
    "id": 118,
    "url": "https://wanted2.github.io/discounted-cash-flow/",
    "title": "Discounted Cash Flow and The Hiring of a Postdoc",
    "body": "2021/06/01 - In the winter of 20xx, a professor was asked to evaluate a postdoc profile for hiring. The offer is about 4. 2 million per year (currency is not revealed). An average postdoc lifetime is about 5 years (and then moving towards tenure or perish). The inflation rate is about 3% per year. The sponsors of the professor’s project can give 50 million for five years. What is the hurdle rate that makes the profile (together with the project) is acceptable? Assumptions:    Assume that the profile is trusted that the holder can produce sufficient works in a year worths 4. 2M.     Although 50M flow to the project, other stuff also needs money. Hence, the professor needs to pay themselves. Then let’s say about 21M are secured for paying the postdoc.  Discounted Cash Flow: Also referred to as the net present value (NPV) method, the discounted cash flow method determines the net present value of all cash flows by discounting them by the required rate of return (also known as the hurdle rate, cutoff rate, and similar terms) as follows: $\mbox{DCF} = I_0 + \sum_{t=1}^T\frac{F_0}{(1+h+p)^t},$where,  $I_0&lt;0$ is the initial investment, and in this case, $I_0=-21$M, which is a negative value $T=5$ is the total number of phases.  $F_0$ is the net cash flow in phase $t$ $h$ is the hurdle rate, i. e. , the percentage of gross-income which must be returned to the investors (project’s sponsors) $p=0. 03$ is the inflation rate. Now, by substitution in to their case, $\mbox{DCF} = -21 + \sum_{t=1}^5\frac{F_0}{(1+h+0. 03)^t}$. If the DCF is non-negative then the postdoc is deemed acceptable, else rejected. When $h=15$% then how much the postdoc must produce every year?: Let’s say the sponsors require 15% of the gross-income made by the work of the postdoc. Then which $F_0$ must be to make the DCF non-negative? We would solve the equation: $DCF =-21 + F_0\sum_{t=1}^5\frac{1}{(1+0. 15+0. 03)^t} \geq 0$or,$-21+F_0\times 3. 127 \geq 0$ Then the money the postdoc is expected to make each year in the following 5 years is $F_0\geq \frac{21}{3. 217}=6. 7$M. In other words, if the sponsors require 15% of gross-income, the postdoc must make more 2. 5M than the cash they are paid. When $h=5$% then how much the postdoc must produce every year?: We would solve the equation: $DCF =-21 + F_0\sum_{t=1}^5\frac{1}{(1+0. 05+0. 03)^t} \geq 0$or,$-21+F_0\times 3. 993 \geq 0$ Then the money the postdoc is expected to make each year in the following 5 years is $F_0\geq \frac{21}{3. 993}=5. 2$M. In other words, if the sponsors require only 5% of gross-income, the postdoc must make 1M than the cash they are paid every year. What if the offer is only 3. 5M in gross-income and the initial investment is only 17. 5M?: In case of hurdle rate is 5%, then the value the postdoc must make is $\frac{17. 5}{3. 993}=4. 4M$. The expectation is low, and matching the ability of the postdoc (their profile can make 4. 2M per year as evaluated), then lower salary lower duty, straight life! In case of hurdle rate is 15%, then the value the postdoc must make is $\frac{17. 5}{3. 127}=5. 6M$. The expectation is higher than the ability of the postdoc (their profile can make 4. 2M per year as evaluated). Conclusion: what’s the strategy?: The best deal for their postdoc is to negotiate with the professor and the sponsors to have:  hurdle rate is 5%; and annual income is only 3. 5 million. Otherwise, overtime is frequent, and their life will be destroyed. "
    }, {
    "id": 119,
    "url": "https://wanted2.github.io/mOCR-mlkit-androidx-example/",
    "title": "mOCR: A real-time application of OCR with Google MLKit and Android CameraX",
    "body": "2021/05/31 - Google MLKit1 is a software solution for Machine Learning problems in mobile devices (Android and iOS). It supports computer visions and natural language applications. The library is optimized for mobiles and is convenient. In this example, we build a mobile OCR solution with Android CameraX API2. The demo code can be seen at my Github. Introduction: We build a real-time demo shown in the left figure.    When a user tap on the screen, a bounding box surrounding the location being tapped is the detection area from which text characters are recognized.   The recognized text is detected in real-time and displayed at the bottom sheet.  CameraX API supports invocations of image analysis whenever an image is captured. Setup: Google MLKit setup: Since Text Recognition API does not support local models, we have to make the application downloading the model weights through Play services (i. e. , Firebase). This can be done by using the presets in build. gradle: 1  implementation 'com. google. android. gms:play-services-mlkit-text-recognition:16. 2. 0'CameraX setup: We also need to add presets for CameraX. With this API, all events such as start capturing, handling when an image’s available, stop capturing are bound automatically to the cameraX lifecycle. The lifecycle is initialized with a camera selector, an image analyzer, and a Preview. 1234  def camerax_version =  1. 1. 0-alpha04   implementation  androidx. camera:camera-core:${camerax_version}   implementation  androidx. camera:camera-camera2:${camerax_version}   implementation  androidx. camera:camera-lifecycle:${camerax_version} Camera lifecycle: Initializing the lifecycle: CameraX API binds all events in a camera into a lifecycle. To preview captured image sequence, we need to add to the main layout a PreviewView component: 1234  &lt;androidx. camera. view. PreviewView    android:id= @+id/fs_cam_preview     android:layout_width= match_parent     android:layout_height= match_parent  /&gt;The captured images will be displayed here, and in the code of the main activity, we retrieve the view: 1mContentView = findViewById(R. id. fs_cam_preview);For concurrency, there is a com. google. common. util. concurrent. ListenableFuture class to wrap up the ProcessCameraProvider to multi-processing: 1private ListenableFuture&lt;ProcessCameraProvider&gt; cameraProviderListenableFuture;The process provider is initialized as follows (in onCreate()): 123456789    cameraProviderListenableFuture = ProcessCameraProvider. getInstance(this);    cameraProviderListenableFuture. addListener(() -&gt; {      try {        ProcessCameraProvider processCameraProvider = cameraProviderListenableFuture. get();        bindPreview(processCameraProvider);      } catch (ExecutionException | InterruptedException e) {        Logger. e(e,  ERROR:  );      }    }, ContextCompat. getMainExecutor(this));Heavyweight resources such as image analysis, previewing, and camera selection can be bind to the lifecycle owner in the bindPreview function: 12345678  private void bindPreview(ProcessCameraProvider processCameraProvider) {    Preview preview = new Preview. Builder(). build();    CameraSelector cameraSelector = new CameraSelector. Builder()        . requireLensFacing(CameraSelector. LENS_FACING_BACK)        . build();    preview. setSurfaceProvider(mContentView. getSurfaceProvider());    Camera camera = processCameraProvider. bindToLifecycle(this, cameraSelector, imageAnalysis, preview);  }For ease of use, we initialized the back camera with CameraSelector. LENS_FACING_BACK. And the preview is bound to a view mContentView. With the ProcessCameraProvider, the heavyweight resources are managed efficiently by the lifecycle owner. Using the CameraX API, we pay less for managing the lifecycles and with more reliable processing. Image Analyzer: A useful tool in CameraX API is the ImageAnalyzer interface, which is used to define custom Image Processing pipelines. Here, whenever an image capture is returned in the form of ImageProxy instances, we need to get the image, crop the region of interest and do real-time text recognition. Such a pipeline can be done in ImageAnalysis. Analyzer. analyze’s body. To keep track of the location user touched in the screen, we have a posisition: 1private Point position;And the custom image analysis pipeline: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657  private class MLKitAnalyzer implements ImageAnalysis. Analyzer {    TextRecognizer textRecognizer;    public MLKitAnalyzer() {      textRecognizer = TextRecognition. getClient(TextRecognizerOptions. DEFAULT_OPTIONS);      Logger. i( Loaded text recognition models! );    }    @Override    public void analyze(@NonNull ImageProxy image) {      @SuppressLint( UnsafeOptInUsageError ) Image image1 = image. getImage();      Logger. i(position. toString());      Logger. i( Canvas size:   + overlayView. getWidth() +  ,   + overlayView. getHeight());      int width = overlayView. getWidth();      int height = overlayView. getHeight();      int mx = Math. min(width, height) / 4;      int my = mx / 2;      int left = position. x - mx;      int top = position. y - my;      int right = position. x + mx;      int bottom = position. y + my;      if (image1 != null) {        ImageConvertUtils convertUtils = ImageConvertUtils. getInstance();        InputImage inputImage = InputImage. fromMediaImage(image1, image. getImageInfo(). getRotationDegrees());        Bitmap bitmap;        try {          bitmap = convertUtils. convertToUpRightBitmap(inputImage);        } catch (MlKitException e) {          Logger. e(e,  ERROR );          return;        }        Rect rect = new Rect(Math. max(0, left * bitmap. getWidth() / width),            Math. max(0, top * bitmap. getHeight() / height),            right * bitmap. getWidth() / width,            bottom * bitmap. getHeight() / height);        Bitmap crop = Bitmap. createBitmap(bitmap, rect. left, rect. top,            Math. min(bitmap. getWidth(), rect. right) - rect. left,            Math. min(bitmap. getHeight(), rect. bottom) - rect. top);        inputImage = InputImage. fromBitmap(crop, 0);        Logger. i( Image size:   + inputImage. getWidth() +  ,   + inputImage. getHeight());        Task&lt;Text&gt; results = textRecognizer. process(inputImage)            . addOnSuccessListener(text -&gt; {//              overlayView. setLatestText(text);              overlayView. setPosistion(position);              Logger. i( Found text:  + text. getText());              overlayView. invalidate();              TextView txtDetected = sheetView. findViewById(R. id. txt_detected);              txtDetected. setText(text. getText());            })            . addOnFailureListener(e -&gt; {              Logger. e(e,  ERROR );            })            . addOnCompleteListener(task -&gt; image. close());      }    }  }The recognizer is initialized in the constructor, and it will take some seconds for the first time since the model needs to be downloaded from Firebase. After the text blocks are detected, we will display the results in a bottom sheet. A green rectangle with an aspect ratio of 2:1 is drawn around the touched location after passing the position to the OverlayView class. Google MLKit: The Machine Learning pipeline running in the Image Analyzer is real-time, and the precision is quite good. By limiting the search region to a 256 x 128 rectangle with the center is the touched location seems to be helpful to reduce the false alarms (noisy text). It can be a good interaction since apps like searching for words in English often need to focus only on one compound word. For example, foreign tourists (people who don’t use English), when looking for unknown words, may only need to adjust the rectangle to the word they don’t know. Conclusion: I used Google MLKit and Android CameraX API to build a lightweight mobile OCR application, the mOCR. The result is a real-time recognition achieved by CameraX and MLKit. The accuracy is good with a focused design in human user interaction. A promising application should look into dictionaries for signs in metro stations or airports, and it is helpful in the Olympics. The next step is to add the dictionary lookup to mOCR. References:       ML Kit - Google Developers &#8617;        CameraX - Android デベロッパー - Android Developers &#8617;    "
    }, {
    "id": 120,
    "url": "https://wanted2.github.io/ai-intellience-stupidity/",
    "title": "Máy móc thông minh hay ngu ngốc: Hiện trạng và kỳ vọng năm 2021",
    "body": "2021/05/29 - Ảnh bởi Alex Knight trên tuần san Unsplash  Biết, nhưng nói không biết … ấy là biết. (Lão Tử)  Ta dại, ta về nơi vắng vẻ;Người khôn, người đến chốn lao xao. (Ngạn ngữ cổ Việt Nam)  Machines are still very, very stupid. The smartest AI systems today have less common sense than a house cat. (Yann Lecun, Turing Award 2019 winner) Cái sự “lập lờ” giữa ngu-khôn và cái gọi là “common sense” (kiến thức sống): Các cụ ta ngày xưa “khiêm tốn” nhận mình ngu bằng câu “Ta dại, ta về nơi vắng vẻ. Người khôn, người đến chỗ lao xao”. Tuy nhiên, thực ra câu này hàm ý mỉa mai sự đời, tức là chúng chửi các cụ ngu thì các cụ “chuồn” về nơi thanh tịnh các cụ sống, còn kệ chúng nó xô bồ bon chen. Thế các cụ có thực sự là “ngu” không?Chắc là không, mà câu này chỉ hàm ý mỉa mai, vì các cụ đã trải qua hết các đắng cay ngọt bùi của sự đời, kinh nghiệm còn thiếu gì mà các cụ phải nhận dại. Cái sự “lập lờ” (ambiguity) này không chỉ có trong văn hóa Việt Nam mà có trong phần lớn văn hóa của người Á Đông. Lão Tử ở Trung Quốc có câu “Biết, nhưng nói là không biết … ấy là biết”. Điều này thể hiện một bộ phận không nhỏ trong xã hội đương thời của Lão Tử có hiện tượng “biết, nhưng cứ nói là không biết” (known but pretend to be unknown), và Lão Tử hàm ý cũng mỉa mai, có lẽ những người đó mới thực sự là người “biết”. Suy nghĩ của người Á Đông là như vậy, chỉ cần nhìn thấy được sự lập lờ để né tránh, còn chấp nhận cho nó tồn tại. Tuy nhiên, người phương Tây thì thường có khuynh hướng làm rõ ràng ra. Như giáo sư Yann Lecun, người nhận giải Turing năm 2019, một giải tương đương giải Nobel cho ngành khoa học máy tính, đã thẳng thắn phê “kém, ngu” cho ứng dụng máy móc hiện tại1. Tôi xin tạm dịch nguyên văn lời phát biểu của ông trong lễ nhận giải cách đây 2 năm có lẻ:  Máy móc (hiện tại năm 2019) vẫn rất, rất ngu. Hệ thống AI thông minh nhất ngày nay còn yếu kiến thức sống hơn cả một chú mèo nhà. Cái kiến thức sống, tiếng Anh là common sense mà giáo sư Lecun nói đến thực ra cũng phải là điều xa lạ với trí tuệ con người. Trong mảng video game, khi chơi game Montezuma’s Revenge, việc gặp phải một cánh cửa bị khóa thì bước tiếp theo cần làm theo kiến thức sống thông thường (common sense) đó chính là nếu cửa không mở -&gt; đi tìm chìa khóa để mở. Nhưng cái suy luận đơn giản này (reasoning) lại không có trong AI nếu không có người chỉ dẫn. Hoặc như trong ứng dụng xe tự hành, một kiểm chứng gần đây cho thấy nếu đặt một ký hiệu phù hợp trên đường, người ta có thể khiến xe tự hành đi vào làn không cho phép1. Con người với hệ suy luận đơn giản có thể biết để không bao giờ đi vào làn không cho phép cho dù có bất cứ ám hiệu nào đặt trên đường. Như vậy, AI vẫn còn rất nhiều việc phía trước, với những nhiệm vụ cấp thấp như định hướng cho xe tự hành hoặc đưa quyết định về hành động tiếp theo. Còn những suy luận cấp cao như nhìn nhận phân biệt ngu-khôn trong những nền văn hóa Á Đông, có lẽ còn ở một tương lai xa hơn. Trí tuệ nhân tạo vẫn cần giải quyết vấn đề common sense (kiến thức sống) trước khi tiến tới những vấn đề sâu xa hơn. Ví dụ như đơn giản trong bài toán game, mà để mở được cửa là một bài kiểm tra đưa ra quyết định có nên di chuyển nhà máy rời khỏi thành phố để về nông thôn mở trang trại hay không, thì có lẽ AI sẽ không thể tái hiện được hết các khả năng. Lý do cũng giống hệt như việc phân biệt ngu-khôn ở trên, giả dụ ở lại thành phố là ngu, còn mang về nông thôn là khôn, nhưng nó sẽ dẫn đến tương lai là người ta sẽ tìm cách hạ thu nhập cơ bản ở nông thôn xuống, thế là quyết định trước được đánh nhãn là khôn, giờ lại trở thành ngu. AI hiện trạng năm 2021, dù của bất cứ tập đoàn công ty nào trên thế giới, thiếu sự hỗ trợ sau lưng của con người thì vẫn chỉ là một cỗ máy vô hại, dễ bị đánh lừa. Bài kiểm tra Turing và cách phân biệt giữa AI ngu dốt với AI thông minh: Bài kiểm tra Turing1 là một phần không thể thiếu của lịch sử AI. Alan Turing là một nhà khoa học máy tính vĩ đại trong lịch sử với kỳ công phá giải mật mã của phát xít Đức. Sau thể chiến II, ông đã dành khá nhiều công sức để phát triển lên cỗ máy tính thông minh, mà đó chính là nền móng của trí tuệ nhân tạo hiện đại (chủ yếu thuật toán). Đương nhiên với tư cách một nhà khoa học máy tính và cũng là một kỹ sư hệ thống, Turing không chỉ phát triển (develop) thuật toán và mã chương trình, ông còn phải thiết kế về mặt quản lý chất lượng sản phẩm, và đương nhiên một câu hỏi đã xoáy vào tâm trí ông khi nghĩ đến sự an toàn của người dùng khi sử dụng AI: Làm thế nào để đánh giá sự thông minh của AI? Sau một thời gian dày công đào sâu vấn đề, ông đi đến kết luận trong luận văn nổi tiếng “Computing Machinery and Intellignce” [1], đó là cái cần kiểm tra chính là “AI có thực sự đang suy nghĩ?”. Các bạn đều biết, việc không suy nghĩ mà vẫn trả lời được bài kiểm tra chỉ có 2 khả năng: một là may mắn, hai là ngu nhưng copy giỏi. Khả năng hai tức là cỗ máy không suy nghĩ, nhưng nó đơn giản copy lại câu trả lời từ đâu đó. Turing thì vẫn là người phương Tây, nên có thể vì thế ông không chấp nhận sự “lập lờ” ngu-khôn mà người Á Đông thường chấp nhận (tức là ngu nhưng bắt chước giỏi thì cứ tính là khôn vì vẫn được việc, và lại rẻ nữa chứ, vì mức giá của công việc AI ở phương Đông hiện tại chắc chỉ bằng 1/10 tới 1/5 của Silicon Valley), và sâu hơn, ông không tính cái trường hợp hai là trí tuệ thông minh. Tất nhiên, với suy nghĩ của phương Tây, Turing cũng đúng. Vì ngày nay chúng ta đang phải đối phó với nguy cơ DeepFake1 chẳng hạn. Đó chính ví dụ rõ ràng của trường hợp 2, mà Turing muốn loại. Và bài kiểm tra Turing chính là để AI thực hiện phỏng vấn với con người và con người sẽ tìm cách hỏi đáp theo một kịch bản mở (chứ không phải lời thoại soạn trước), để thử thách AI. Sau đó, dựa vào sự xác nhận của con người đã tương tác với AI để đánh giá độ hoàn hảo của AI. Và quay lại vấn đề ở mục 1. , vấn đề thử thách nhất với AI trong màn phỏng vấn này chính là làm sao đánh giá được những quyết định của chính AI đưa ra là ngu hay khôn, là biết hay chưa biết về lâu dài. Vì những quyết định đó sẽ ảnh hưởng tới lợi ích về sau, như vị trí làm việc tương lai, đãi ngộ, …Còn về phía người thực hiện phỏng vấn, thì họ cũng như nhà tuyển dụng, chuyện ngu hay khôn, biết hay không cũng không quan trọng bằng việc tìm ra AI tốt có khả năng tự suy nghĩ, chứ không phải ngu nhưng bắt chước giỏi. Đó chính là ý nguyện của Alan Turing! Kết luận: Qua bài tiểu luận nhỏ, hy vọng đã có thể đưa đến các bạn một cái nhìn đúng về sự thông minh của AI hiện tại. Thực ra cái quan trọng nhất của công việc AI, là đảm bảo di chúc của Alan Turing được thực hiện, tức là, AI phải có khả năng tự suy nghĩ, cái đó thì thông qua conversation là bài kiểm tra Turing (mà thực ra là phỏng vấn). Một việc chắc chắn không thể tránh khỏi đó là hiện tại các phát triển AI chỉ mang tính cấp thấp như nhận dạng, tracking, phát hiện, cảnh báo có điều kiện, … trong tương lai, cùng với Deep Learning (thuật toán học dựa trên hoạt động của não người, để cho mô hình AI học từ hàng ngàn, hàng triệu thậm chí hàng tỷ nơ rôn não), thì chúng ta sẽ nhanh chóng đưa AI xử lý hết các nhiệm vụ cấp thấp, để tiến tới những nhiệm vụ cấp cao hơn, như quyết định có lợi cho tương lai, phân biệt ngu-khôn, biết-chưa biết, … References: Turing, A. 1950. Computing machinery and intelligence. Mind. 49, 236 (1950), 433. Details      Không chỉ là video khiêu dâm bóng ma Deepfake đang khiến cả thế giới lo sợ vì ngày càng khó kiểm soát &#8617; &#8617;2 &#8617;3 &#8617;4    "
    }, {
    "id": 121,
    "url": "https://wanted2.github.io/on-limitation-mediapipe/",
    "title": "On Limitation of MediaPipe Holistic Face Detection Module",
    "body": "2021/05/22 - Source 1, Source 2, Credit: Microsoft Google AI announced MediaPipe Holistic1 as a simultaneous face, hand, and pose inference engine for on-device AI. As we knew, on-device AI works in a specialized environment such as Edge devices (Arduino, Raspberry Pi, Jetson Nano) and mobile devices (Android/iOS/…). These environments are characterized by limited computing power (except Jetson Nano, all are low-end CPUs), often no Internet (wifi modules maybe not embedded). Then MediaPipe is a great offer. It provides a consistent interface for working with deep learning models and computer vision models in various programming languages (Java, Swift, Python, Javascript, …). The solution is also end-to-end, so the code can be done by calling ready-to-use functions. But wait, such a big deal?This post provides a fairer view of the MediaPipe library for on-device face detection. The result is that, although MediaPipe is fast, however, the accuracy is limited for crowded scenes. It works best when there is only one person in the frame. Lesson 1: MediaPipe works poorly in multi-faces scenario: Let’s start with the famous selfie photo made by Microsoft Lumina 730.  My code is as follows. 1234567891011121314151617181920212223242526272829import cv2import mediapipe as mpfrom time import timefile_list = ['selfie. jpg', 'selfie-3. jpg', 'selfie-small. jpg']mp_face_detection = mp. solutions. face_detectionmp_drawing = mp. solutions. drawing_utils# For static images:with mp_face_detection. FaceDetection(  min_detection_confidence=0. 5) as face_detection: for idx, file in enumerate(file_list):  print(f'Processing {file} . . . ')  image = cv2. imread(file)  # Convert the BGR image to RGB and process it with MediaPipe Face Detection.   t1 = time()  results = face_detection. process(cv2. cvtColor(image, cv2. COLOR_BGR2RGB))  t2 = time()  print(f'Processed {file} in {t2-t1:. 5f} second(s)')  # Draw face detections of each face.   if not results. detections:   continue  annotated_image = image. copy()  for detection in results. detections:   print('Nose tip:')   print(mp_face_detection. get_key_point(     detection, mp_face_detection. FaceKeyPoint. NOSE_TIP))   mp_drawing. draw_detection(annotated_image, detection)  cv2. imwrite('. /annotated_image' + str(idx) + '. png', annotated_image)I downloaded the selfie photo and named it selfie. jpg. Since I speculated that MediaPipe does not work in such a scenario (More than 1,000 faces in one photo), I cropped two different versions: one with less than 100 faces (selfie-small. jpg) and one with only 3 big faces (selfie-3. jpg).      Original photo       Cropped image with less than 100 faces       Cropped image with 3 big faces  Results is as follows. 123456789101112131415161718192021python run_mediapipe. pyProcessing selfie. jpg . . . INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: Replacing 162 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 2 partitions. Processed selfie. jpg in 0. 02069 second(s)Processing selfie-3. jpg . . . Processed selfie-3. jpg in 0. 00496 second(s)Nose tip:x: 0. 62226236y: 0. 37454954Nose tip:x: 0. 23528881y: 0. 65063083Nose tip:x: 0. 43768775y: 0. 32625142Processing selfie-small. jpg . . . Processed selfie-small. jpg in 0. 00852 second(s)What this means is that, MediaPipe did not detect any faces in the original scenario (1,000 faces) and second scenario (about 100 faces). But when there are 3 big faces, the result is promising (no miss in the 3 faces I wanted to detect). Then for on-device AI, we cannot expect too much. The limits of them which I obtained from this example:    Miss all faces in crowded scenes.     Miss all small faces: even with selfie-3. jpg, only big faces (area is more than 10% of the whole image) can be detected well. Other small faces are missed.  With low budgets, we cannot expect too much. One bright side of MediaPipe from this result is that because small faces are often missed, then false positives (spoofers) is not a serious problem. For critical applications such as face authentication, making wrong decisions (false positives) can lead to spoofers getting in the system, but if MediaPipe misses too many small faces, our small spoofers are omitted hopefully in return. Lesson 2: MediaPipe is fast: For the original image, MediaPipe took about 20 ms on average. For the small image (100 faces), it took about 10-15 ms. And for the smallest image, it took about 7-8 ms. Since MediaPipe is not good at crowded scenes, then we can speculate that it is used in scenarios with a few faces. In such scenarios, then 7-8ms/image, or 125 FPS is not bad. Conclusion: MediaPipe is fast but works poorly in crowded scenes. From this observation, one recommendation is to only use it in one-person or few-people scenarios. The source code can be found at my Github. References:       Google AI Blog: MediaPipe Holistic – Simultaneous Face, Hand and Pose Prediction, on Device &#8617;    "
    }, {
    "id": 122,
    "url": "https://wanted2.github.io/understanding-object-detection-nms/",
    "title": "Understanding Object Detection Algorithms: the Non-maximum-suppression (NMS)",
    "body": "2021/05/20 - The non-maximum-suppression (NMS) is one of the most common parts of object detection methods. The idea is to remove the overlapping bounding boxes and keep only the separated boxes in the detection result. Given a threshold and starting from a particular box, any other boxes which have the overlappings with the reference larger than the threshold will be opted out. The process is repeated until all boxes are visited. Basic algorithms and speed-up: Let the threshold be $\theta=0. 33$ and start from the red box. Then we can remove the dark and the blue ones because they have overlappings with the reference larger than 0. 33. Here, we chose the overlap is the rate between intersection and union of the boxes. It is illustrated as follows.  As illustrated above, if the reference box (green one) has more than $t$ of the area of the target box (dark box), then the target box is overlapped with the reference and that the target should be removed.  Algorithm parameters: threshold $\theta \in (0, 1]$ A list of detected bounding boxes $L$.  Procedure $NMS(L, \theta)$:  $\quad$1. Sort the box $L$, for example, by the bottom right coordinate values. $\quad$2. Initialize the picked boxes list $P\leftarrow \phi$ $\quad$3. Loop until $L$ become empty:  $\qquad$3. 1 Choose the last box in the sorted list, push the index to the picked indices $P$  $\qquad$3. 2 Find all overlapping boxes with the reference (the last box in $L$), and remove all of them from the list $L$  $\qquad$3. 3 Remove the reference box from $L$ To implement an efficient (fast) NMS, it is best to avoid using nested for loops. For example, in some reference implementations, step 3. 2 can be implemented as a for loop. However, if we use numpy matrix operations such as numpy. where or numpy. multiply, we can perform faster the NMS. An example is from Adrian Rosebrock’s implementation which resulted in a 100x faster NMS. A careful implementation of NMS is ready to use is the tf. image. non_max_suppression. I also used it in the inference of YOLOv5. Discussions: Why sorting the boxes before running nms?: It is quite trivial, and often omitted issue in NMS. Let’s see the first example we explained. What if we don’t start from the red box but the dark box?What happens is as follows.  The result has changed, according to the changes in the choice of the reference. Therefore, we need to sort the box list before running NMS. Should we prune low-confidence boxes before or after running NMS?: In my implementation of inference of YOLOv5, I prune low-confidence boxes before NMS. Because NMS takes time, pruning low-probability candidates beforehand is preferred. After NMS, if the candidates is still too many, then we can merge boxes again using another overlapping measurements such as the IoU. In this implementation, the NMS often takes about 10-20 ms to opt out some hundreds boxes. Conclusion: We discussed about the vital part of object detection to remove redundancies in detected results: the non-maximum-suppression algorithm. To enhance the accuracy with NMS, some improvements such as the Soft NMS [1] should be the idea. References: Bodla, N. , Singh, B. , Chellappa, R. and Davis, L. S. 2017. Improving Object Detection With One Line of Code. CoRR. abs/1704. 04503, (2017). Details"
    }, {
    "id": 123,
    "url": "https://wanted2.github.io/ban-ve-thiet-ke-he-thong-co-quan-ly-user-non-english/",
    "title": "Bàn về thiết kế hệ thống có quản lý users: Cây nhà lá vườn hay ăn sẵn?",
    "body": "2021/03/13 - Quản lý người dùng (Identity and Access Management, IAM hay user management) luôn là một mục quan trọng trong mọi thiết kế hệ thống. Vấn đề bảo vệ dữ liệu riêng tư người dùng, quản lý phân quyền và quản lý truy cập, đều là các vấn đề có thể nói là đau đầu không khác gì “bệnh ung thư”. Tuy vậy, cần chú ý rằng việc quyết định lựa chọn giải pháp tự build lấy (tôi xin dùng thuật ngữ “cây nhà lá vườn” của hội nông dân để chỉ giải pháp này) và giải pháp sử dụng nhà cung cấp có sẵn, thì cái nào hơn?Câu trả lời là tùy thuộc vào bài toán business của bạn, nhưng theo ý kiến cá nhân thì nhìn chung, với các khởi nghiệp nhỏ và các business mà bạn muốn tập trung vào chuyên môn hơn là lo những vấn đề không phải chuyên môn mà lại dễ tạo ra “bệnh ung thư” trong cả tổ chức như quản lý người dùng, thì giải pháp sử dụng nhà cung cấp có sẵn đôi khi rất fair. Bạn vẫn quản lý được người dùng với quy trình chất lượng thỏa mãn các tiêu chuẩn của IETF (1 tổ chức kiểu ISO dành cho Internet và bảo mật) nhưng vẫn không phải tốn thời gian cho những thứ ngoài chuyên môn. Giới thiệu: Quản lý người dùng (Identity and Access Management, IAM hay user management) là 1 phần quan trọng trong mọi hệ thống quản trị thông tin. Về cơ bản, bạn vẫn thường thấy như khi tạo tài khoản cho 1 dịch vụ như Facebook, bạn sẽ được cung cấp 1 định danh duy nhất đi kèm với thông tin bạn đã cung cấp (email và địa chỉ, . v. v…), bạn sẽ được tạo 1 profile, được phép chỉ định thông tin nào hiển thị và không hiện thị công khai hay là giới hạn quyền truy cập vào thông tin cá nhân của bạn. Về phía nhà cung cấp dịch vụ Facebook, họ sẽ lưu trữ dữ liệu của bạn vào user store của họ và phân quyền truy cập (đọc, ghi đè, chỉnh sửa, xóa) lên dữ liệu của bạn. Bạn bắt buộc phải tạo tài khoản và đăng nhập để sử dụng dịch vụ. Rất nhiều tình huống kinh doanh đòi hỏi IAM và quản lý người dùng:  B2B: Dịch vụ của bạn cho phép người dùng sử dụng định danh của riêng cty họ. Như Trello cho phép người dùng liên kết sử dụng tài khoản cty.  B2C: Dịch vụ của bạn cho phép người dùng sử dụng định danh của 1 dịch vụ xã hội khác như Gmail, Facebook, Twitter, . v. v… B2E: Dịch vụ của bạn cho phép nhân viên đăng nhập 1 lần duy nhất. Hậu quả của việc quản lý người dùng kém chính là việc thông tin người dùng bị lọt ra ngoài và trong môi trường kinh doanh chuyên nghiệp, 1 SLA cơ bản của mọi dịch vụ chính là cam kết bảo vệ riêng tư người dùng. Tuy nhiên, có nhiều nguyên nhân dẫn đến những sự vụ lọt dữ liệu cá nhân riêng tư của người dùng như quản lý yếu kém, thực thi thiếu, . v. v…Để đảm bảo những yếu tố trên, bắt buộc bên thực thi chức năng hệ thống quản lý người dùng cần có làm bài bản và công phu. Tuy nhiên, với khá nhiều business cases, chủ yếu các công ty khởi nghiệp quy mô nhỏ hoặc các công ty viện nghiên cứu, nơi yếu tố tập trung vào chuyên môn, ý tưởng nghiệp vụ quan trọng hơn những vấn đề không phải chuyên môn của họ, việc sử dụng giải pháp của nhà cung cấp bên thứ 3 trở nên cấp thiết. Và bên thứ 3, bên cung cấp giải pháp có sẵn bắt buộc phải đáng tin cậy, với chất lượng thể hiện qua SLA phải thật cao. (SLA sẽ quan trọng hơn SLO/SLI hay KPI rất nhiều, vì SLO/SLI hay KPI chỉ là các chỉ số nội bộ vô giá trị với người dùng, còn SLA mới là cái cam kết với người dùng là dịch vụ của bạn sẽ cung cấp những hạng mục này với chất lượng như thế này). Các hình thái quản lý người dùng: Đầu tiên, chúng ta xem lại một vài khái niệm về định danh và quản lý người dùng.    Định danh liên bang (Federated Identity): dữ liệu định danh được di chuyển giữa các server (A và B) mà không vi phạm kiểm tra nguồn gốc (origin). Việc này được thực hiện bởi server IAM bên ngoài. Nhờ vào hình thái này, người dùng của dịch vụ A có thể đăng nhập và dịch vụ B mà chưa hề đăng ký ở B.     Single Sign-On: đăng nhập 1 lần. Giả sử có 1 hệ sinh thái bao gồm nhiều dịch vụ khác nhau, thì đăng nhập 1 lần tức là người dùng của dịch vụ này chỉ cần đăng nhập và dịch vụ đã đăng ký là có thể tự do truy cập vào mội dịch vụ trong hệ sinh thái. Để thực hiện hình thái này, đòi hỏi trong hệ sinh thái có 1 dịch vụ IAM để thực hiện định dạng liên bang, tức là vận chuyển dữ liệu định danh của user giữa các dịch vụ.     Định danh liên bang trong doanh nghiệp (Enterprise Federated): Định danh liên bang được dùng với các kết nối dịch vụ trong doanh nghiệp như Active Directory, SAML, LDAP, Google Apps, . v. v…  Các hình thức định danh kể trên giúp việc quản lý người dùng được đồng bộ trong toàn doanh nghiệp cũng như hệ sinh thái. Trong tiếng Nhật, gọi đó là 一元管理, tức là thay vì quản lý theo nhiều dịch vụ nhiều phương diện khác nhau, sự quản lý được đơn giản hóa thành quản lý 1 chiều thống nhất. Quản lý người dùng là lĩnh vực đang biến đổi với sự gia tăng nhanh chóng của các thiết bị nhỏ gọn như điện thoại di động. Do đó, các công nghệ mới trong toàn lĩnh vực này bao gồm Đăng nhập 1 lần (Single Sign-On), Đăng nhập không mật khẩu (Passwordless) và Định danh đa hình thái (Multi-factor Authentication) đang trở nên nóng hổi. Multifactor Authentication (MFA) thực hiện định danh nhiều cửa để tăng tính bảo mật. Passwordless sử dụng các biện pháp định danh ngoài mật khẩu truyền thống như bio-metrics (vân tay, khuôn mặt, . v. v…). Thêm vào đó, các ứng dụng sử dụng dịch vụ đám mây như Amazon Web Services (AWS) hay Google Apps, cũng cần thực thi IAM do họ lưu trữ dữ liệu người dùng trên các server trên đám mây. Ngoài ra, social authentication cũng là một nhu cầu do các ứng dụng B2C đòi hỏi liên kết với các mạng xã hội để nâng cao hiệu quả quảng cáo, marketing, cũng như gia tăng trải nghiệm người dùng. Với các ứng dụng B2E thì ngược lại, họ không phải quan tâm đến quá nhiều phân quyền và nhiều cấp độ truy cập. Nhưng nhà quản lý (admin) sẽ phải chịu trách nhiệm tạo tài khoản khi có thành viên mới gia nhập và vô hiệu tài khoản khi thành viên rời đi. “Của nhà trồng được” và “Đồ ăn sẵn”: Lợi ích của các giải pháp IAM bên thứ 3 (có sẵn): Như các bạn đã thấy ở trên, so với quản lý người dùng truyền thống (tự build và tự update khi bắt buộc), các giải pháp quản lý trong một hệ sinh thái lớn đòi hỏi nhiều phương thức định danh mới như định dang liên bang mà vì thế việc cân nhắc đưa một giải pháp IAM vào tổ chức là cần thiết cho mọi nhà quản lý hệ thống IT hiện nay. Chúng ta xem lại lợi ích của các giải pháp này: Đối với mọi hệ thống IT:    Giảm chi phí kỹ thuật: Sử dụng giải pháp IAM bên thứ 3 tương đương với giảm thiểu chi phí kỹ thuật cho việc thực hiện các chức năng IAM bên trong tổ chức của bạn. Nói ngắn gọn, là bạn sẽ tập trung được vào các chức năng chuyên môn thay vì bận rộn lo thủ tục giấy tờ mà hay gọi là các công việc mang tính chất overhead. Như vậy đồ ăn sẵn nói một cách fair thì rẻ hơn nhiều so với của nhà trồng được. Nếu không tính chi phí phát sinh trong quá trình vận hành, sửa lỗi, quản lý chất lượng.     Tăng độ tin cậy và bảo mật: Lưu trữ dữ liệu trên server của bên thứ 3 giúp dữ liệu của người dùng của bạn an toàn khỏi … chính bạn. Đơn cử như việc dùng lại mật khẩu để tránh phải nhớ nhiều mật khẩu dẫn đến quản lý dữ liệu yếu kém. Ủy thác công việc “đau đầu” này cho bên thứ 3 giúp bạn nhẹ tay khỏi những nghiệp vụ này, đồng thời an ninh được đảm bảo hơn nhờ những công nghệ xác thực mới như Passwordless hay MFA.  Đối với nhóm hệ thống B2B:    Gia tăng tốc độ hiện đại hóa doanh nghiệp: Với chức năng định danh doanh nghiệp (Enterprise Federation), người dùng được “chắp thêm cánh” với hàng loạt công nghệ bảo mật mới, đồng thời an toàn kết nối với hàng loạt dịch vụ tiện lợi như Active Directory, Google Apps, . v. v… Với SSO, người dùng thậm chí không cần nhớ username hay password, thậm chí dù bạn vẫn muốn user dùng mật khẩu, vẫn có những giải pháp IAM an toàn và ready-to-use.     Tăng lợi nhuận: Bằng việc cung cấp dịch vụ IAM thuận tiện và an toàn, những dịch vụ bên thứ 3 khiến tăng tỷ lệ engagement của khách hàng khiến bạn có thêm khách hàng doanh nghiệp trung thành, đồng thời giảm thiệt hại cho việc khách hàng bỏ dùng dịch vụ giữa chừng.     Rút ngắn quá trình sales và onboarding (huấn luyện nhân viên mới): Các tiêu chuẩn bảo mật đã được ship sẵn trong gói dịch vụ bên thứ 3 như của Auth0 hay AWS Cognito khiến cho đội ngũ sales tập trung vào công việc chuyên môn, giúp ngắn thời gian xây dựng và triển khai giải pháp. Đối với khách hàng B2B có đội ngũ nhân viên mới lớn, giải pháp “ăn sẵn” của bên thứ 3 giúp nhân viên mới nhanh chóng nắm bắt và tuân thủ các quy định bảo mật, và đương nhiên nhanh chóng tiếp cận công việc chuyên môn chính.  Đối với nhóm hệ thống B2C:  Hút khách hàng: Bằng việc cung cấp giải pháp tiêu chuẩn, đồng bộ (form login, các quy trình xử lý), giải pháp IAM giúp khách hàng của doanh nghiệp nhanh chóng tiếp cận dịch vụ, bỏ bớt nhiều công đoạn mang tính thủ tục. Đối với nhóm hệ thống B2E:    Rút ngắn thời gian tác vụ ngoài chuyên môn với SSO: Với SSO, người dùng có thể truy cập nhiều dịch vụ khác nhau chỉ thông qua 1-click. Kết nối tới các giải pháp ERP, CRM, Saleforces, Office 365, . v. v… đã đều ở trạng thái ready-to-use.     Hỗ trợ phân quyền: Quản lý truy cập của người dùng mới và người dùng đã kết thúc được quy chuẩn hóa.  Các giải pháp tự build: Nếu tổ chức của bạn đã có sẵn một giải pháp “tự” quản lý, tôi hiểu rất khó để “chuyển đổi số”. Tuy vậy, bắt đầu chuyển đổi số nên bắt đầu từ những cấp quản lý, vì đây là nghiệp vụ mang tính overhead khá cao. So với giải pháp bên thứ 3 đang ngày càng phong phú, hầu như giải pháp tự build khó có ưu điểm nào vượt trội, đặc biệt trong các lĩnh vực mà có nhiều “cá mập” như security. Do đó nếu doanh nghiệp mới bắt đầu tìm cách thực thi giải pháp security, hầu như chúng tôi không tư vấn để thực hiện bằng build với code bao giờ, vì giá thành vừa cao mà dễ bị lỗi do nhân tố con người (human errors). Các yếu tố cần xem xét khi lựa chọn dịch vụ IAM và các dịch vụ IAM khuyên dùng: Các yếu tố nên xem xét: Như đã nói, thị trường security là đã “bão hòa” và lắm cá mập. Nhưng ngay cả việc chỉ việc lựa chọn trong đám cá mập để xài thì cũng có rất nhiều. Nên chúng tôi cung cấp dưới đây danh sách yếu tố nên quan tâm khi lựa chọn nhà cung cấp giải pháp.       Yếu tố   Giải thích         Hình thái triển khai   Nên lựa chọn giải pháp có thể triển khai ở bất cứ đâu. Ví dụ, giải pháp có thể triển khai trên cloud của cty giải pháp, trên cloud của khách hàng, hoặc trên data center của chính khách hàng.        Khả năng tích hợp   Tìm kiếm giải pháp có SDKs tốt, tài liệu hướng dẫn chuẩn chỉnh, APIs mạnh, các tính năng hoàn chỉnh, phong phú và quan trọng nhất là dễ cấu hình cho nhân lực của cty bạn.        Sự hỗ trợ với các nhà cung cấp dịch vụ định danh khác   Microsoft Active Directory, ADFS, Office 365,Google Apps và SAML là những nhà cung cấp dịch vụ mà 1 giải pháp security tốt nên hỗ trợ. Bạn nên kiểm tra kỹ xem các nhà cung cấp dịch vụ định danh mà giải pháp của bạn hỗ trợ có phù hợp nhu cầu cuả cty bạn không.        Khả năng mở rộng   Giải pháp security bạn lựa chọn có dễ dàng cho bạn khi muốn customize quy trình đăng nhập và đăng ký không? Nếu business của bạn thường xuyên thay đổi theo nhu cầu người dùng, bạn nên cân nhắc giải pháp có hỗ trợ mạnh việc customize (như trong e-commerce, tôi vẫn khuyên anh em xài wordpress vì nó nhiều plugin, khả năng mở rộng mạnh mà ít phải code).        Bộ tính năng   Bạn nên chọn giải pháp có bộ tính năng tốt nhất trên thị trường.        Khả năng dịch chuyển   Khả năng dịch chuyển dữ liệu giữa các server, hoặc từ data centers của khách hàng lên cloud của giải pháp là những yếu tố nên có.        Dịch vụ hỗ trợ   Dịch vụ hỗ trợ tốt, nhanh, 24/7 là yếu tố nên có.    Giới thiệu sơ qua về các dịch vụ định danh nổi bật trên thị trường: Nhìn chung mảng bảo mật và dịch vụ định danh khá là nhiều “cá mập”. Vì vậy, tôi nghĩ sẽ không khó để tìm được 1 dịch vụ có đầy đủ các yếu tố kể trên, cộng thêm reviews tốt nữa để nhấn nút đặt hàng. Nếu bạn đã triển khai dịch vụ của bạn trên cloud, đương nhiên tích hợp giải pháp định danh mà nhà cung cấp cloud có sẵn là cách đồng bộ nhất. Ví dụ bạn đang dùng AWS thì tốt nhất nên dùng Cognito. Còn nếu bạn đang triển khai trên data center của riêng cty bạn hoặc bạn vì lý do nào đó không thích giải pháp mà cloud offer, bạn có thể xem xét một số nhà cung cấp thứ 3 khác như Auth0. Ngoài ra, chú ý là Auth012 cũng hỗ trợ định danh đến từ Cognito3, Microsoft Active Directory4 hay Google Identity Platform5. Còn 1 danh sách dài khác các ứng cử viên (cả cá mập, cá lớn lẫn cá bé) trong lĩnh vực security này, tôi xin phép đề cập khi có dịp. Tổng kết: Nhìn chung, sự ra đời của hàng loạt dịch vụ định danh và bảo mật tính theo chiều dài lịch sử của Internet từ bong bóng Dotcom những năm 2000 đã tạo nên cục diện ngày hôm nay. Hầu như các giải pháp bên thứ 3 đã cung cấp nền tảng đầy đủ tính năng và phong phú về ứng dụng cho người dùng trong mảng định danh và bảo mật. Vì có quá nhiều nên cuộc cạnh tranh đã khiến cho người dùng có thể thoải mái lựa chọn giải pháp phù hợp nhất cho nhu cầu của mình. Đứng về góc độ business, tôi tin tưởng tương lai chuyển đổi số sẽ đứng về phía các dịch vụ có sẵn. Dù xây thì thực chất bên dưới vẫn là các nền tảng có sẵn. Công việc của người làm quản lý sẽ được rút ngắn, tránh overhead cho những nghiệp vụ người dùng. Người quản lý sẽ tập trung vào chuyên môn của doanh nghiệp và phần thời gian cho nghiệp vụ người dùng sẽ ở mức độ thao tác trên giao diện web ở mức kéo thả và thao tác cơ bản. Tài liệu tham khảo:       https://auth0. com/user-management &#8617;        Build vs Buy: Guide to Identity Management &#8617;        https://aws. amazon. com/vi/cognito/ &#8617;        https://docs. microsoft. com/en-us/windows-server/identity/identity-and-access &#8617;        https://cloud. google. com/identity-platform/ &#8617;    "
    }, {
    "id": 124,
    "url": "https://wanted2.github.io/animal-vision-series-part-1/",
    "title": "Animal Vision Part 1: Course Introduction",
    "body": "2021/03/06 - The purpose of this learning-by-doing course is to get familiar with a cross-discipline topic: Computer Vision for Agriculture. Specifically, we introduce Animal Vision (AniV), targeting canine animals. Learners get through several useful applications of Computer Vision (CV) into Animal Farming:  Object Recognition, Detection and Segmentation (including class, instance and panoptic segmentation [1]) Methods to adapt to new domains in farming such as Transfer Learning and Domain Adaptation.  Image enhancement methods such as de-raining, super-resolution, . etc.  Applications such as Dog counting, animal crowd density estimation, anomaly detection. The objective of this course is to build a proper Animal Vision system below. Introduction: Automation is changing the world with growing conventional technologies such as Information Tech, Software, Hardware, and frontier domains such as Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), Internet of Things (IoT), Computational Linguistics (CL), and Computer Vision (CV). Cloud platforms like Amazon Web Services (AWS) provide convenient solutions to handle many industrial problems at hand, such as Equipment Defect Detection using AWS Lookout for Equipment1. For Agriculture Automation, the uses of CV are also growing2 [2] in two sectors in agriculture: crops and farming.       Applications   Sub-tasks   Description   Technologies         Crop Management   Crop Monitoring   AI engine mounted into a UAV can detect crops and monitor their growth for human diagnostic. Human experts and farmers can perform further investigations and change the plan of feeding crops.    UAV/Drones, object detection/classification/segmentation           Yield Prediction   Using deep learning with satellite imagery, we can gather various information like soil conditions, nitrogen levels, moisture, seasonal weather, historical yield information of crops for precise farming.    Time series forecasting, fruit object counting       Food Safety   Environment Management   From historical information of crop environment, such as soil conditions, nitrogen levels, moisture, seasonal weather, spotting the outliers to judge the quality of the crops and fruits.    Time series anomaly detection           Spraying pesticides   We use AI-enabled drones to monitor the infected crops and spray the pesticides to prevent crops from insects and pests. The computer vision allows drones to precisely detect the infected crops and spray the pesticides accordingly.    Image anomaly detection, UAV/Drones, object detection/classification/segmentation           Automatic quality grading and sorting   Using the deep learning techniques, we calculate the percentage of infection. The grading and sorting of the fruit image helping farmers to reduce the crop damages due to storage.    Image classification, image anomaly detection, object detection/classification/segmentation           Weight estimation   CV enables automatic weight detection to ensure that animals grow well before being sent to the slaughterhouse.    Image weight estimation, UAV/Drones, object detection/classification/segmentation       Farming Management   Livestock Management   Automatic counting of animals in the farms helps the farmers better manage their hounds. When an individual has a sickness, the AI-enabled system can alert the farmers to separate that individual from the rest.    Animal counting, density estimation, image anomaly detection, UAV/Drones, object detection/classification/segmentation   The applications in this field include Forestry Management, which is similar to Crop Management but has some other issues such as burning prevention. Our targets include Farming Management (and weight estimation). System: A system for farming management:    A camera captures an image and sends it to the server. In the server, a Global Anomaly Detector (GAD) detects whether if there is a non-targeted class in the image. For example, if the monitor scenario only accepts dogs and sheep, the system must raise an alert (5) when a wolf or a human thief appears.     If GAD detects only accepted classes, (4) a 1-class detector/segmentation model (or limited-domain model, LDM) detects the (7) annotations (bounding boxes and polygons). Here, we find other benefits of GAD: it reduces the load and mistakes of LDM. Why? Because in the course, we see that 1-class models and 1-vs-rest learning mechanisms often result in over-confidence problem: the 1-class model makes many false-positives (FPs) on the rest classes. Therefore, using a GAD reduces the chance of such over-confidence by early detection of intrusions.     For each detection of LDM, it is passed to a wide-domain classifier (WDC) such as an ImageNet pre-trained model. GAD does early intrusion detection, WDC performs late detection of intrusions. Because a 1-class model can give many FPs, GAD is insufficient. Since WDC learned in 1000 classes, a larger knowledge base than LDM, it is less over-confident. WDC firmly prevents non-target classes from being intruded into the Analytics Database.     If the detection surpasses RAD, it has less chance to be a false alarm. We want to obtain further analytics information: (10) Regional Anomaly Detector (RAD) which finds if the instance region has sickness or not; (11) Fine-Grained Classifier (FGC) returns the detailed classes such as breed names of the instances (an Egyptian dog rather than a dog); and (12) Weight Estimation Model (WEM) returns the instance weight for further diagnostics.     The above information is inserted by timestamp to the Analytics Database to display in the Dashboard to the farm owners.  In this course, we learn the CV and ML technologies to implement this system. Technologies: Image classification: As said before, Global Anomaly Detector (GAD) for early intrusion detection, as well as the Wide-Domain Classifier for late intrusion detection, are implemented by Wide-domain models such as pre-trained ImageNet models. This technology is well known today, especially after 2012 with the boost of the Deep Learning era. Object detection and segmentation: We apply these classes of technology to 1-class models and RAD models above. Image Anomaly Detection: This technology aims at delivering the infected instances in an image. The deliverables can be bounding boxes, polygons, or binary labels (sickness/noon-sickness). GAD and RAD use this technology. Gathering training data can be a problem due to the frequency of rare events. Machine Learning: Data augmentation, balancing training data, domain adaptation, and transfer learning can be the core set of ML technologies we need to learn. Image Processing: Some worst cases in practice require further care about image quality:  Bad weather such as raining and snowing can result in low detection quality. De-raining models can be useful.  Camera movement can be complement by calibration.  Low-resolution imagery requires up-scaling or super-resolution. The models can be run on the input images before fed to GAD. Attribute Detection: For farming, they feed most animal breeds for meat. Therefore, attributes which affect the quality of food are inevitable. They are animal health conditions such as weight and sickness. Fine-Grained Categorization: FGC helps to identify the breeds and origins of the animals. This technology contributes to quality insurance to keep traceability of food origin. About this course: In this course, we implement the Animal Vision System (AVS) using frontier technologies. Tools:       Language   English       OS   Ubuntu, macOS, Windows x64       Programming Language   Python (C/C++)       Deep Learning frameworks   PyTorch       Computer Vision and Machine Learning libraries   opencv-contrib-python, scikit-learn, scipy, scikit-image   Posts:  Introduction (this post) Introduction about Computer Vision (using scikit-image and opencv)     Image filters, edge detection, calibration, artifact removals, . etc.     Object detection, classification and segmentation     Image features   Conventional object detection methods like the Viola-Jones cascade method.    Convolutional neural networks, convolutional/pooling/upscaling/dilated operations, etc.     Advanced problems in image enhancement (1)     Generative Adversarial Networks, Auto-Encoders    Advanced problems in image enhancement (2)     Haze removal, artifact removal, reconstruction   Super-resolution, de-raining methods.     Machine learning (1)     The role of data augmentation   Common learning problems: over-fitting/under-fitting   Over-confidence problems: Open-world recognition    Machine learning (2)     Learning in new domains   Transfer learning vs. Domain Adaptation    Machine learning (3)     One-vs-rest learning   Limited-domain learners (experts) vs. Wide-domain learners (generalists) and over-confidence problems.     Machine learning (4)     Anomaly Detection    Animal Vision (1)     Object counting   Density Estimation    Animal Vision (2)     Attribute detection: weight estimation and anomalous individual detection    Conclusion     The whole system   References: Kirillov, A. , He, K. , Girshick, R. , Rother, C. and Dollar, P. 2019. Panoptic Segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Jun. 2019). DetailsTian, H. , Wang, T. , Liu, Y. , Qiao, X. and Li, Y. 2020. Computer vision technology in agricultural automation –A review. Information Processing in Agriculture. 7, 1 (2020), 1–19. DOI:https://doi. org/10. 1016/j. inpa. 2019. 09. 006. Details      Amazon Lookout for Equipment - Amazon Web Services &#8617;        Application of Computer Vision in Precision Agriculture \&amp; Farming - by Vikram Singh Bisen - VSINGHBISEN - Medium &#8617;    "
    }, {
    "id": 125,
    "url": "https://wanted2.github.io/aws-vpc-aurora-postgresql-odoo-crm-deployment/",
    "title": "Deploy Odoo to AWS EC2 and Aurora with best practices",
    "body": "2021/02/15 - IntroductionWhat is Odoo?: Odoo is an open-source software platform and eco-system for business apps that cover all your company needs: CRM, e-commerce, accounting, inventory, point-of-sale, project management, so on. The suite of open source apps is very easy-to-use and fully integrated: the customers deploy the base Odoo distribution in the Odoo cloud with few clicks or their on-premises server/cloud, and integration of the apps is without pains. Because every app needed for business as CRM and point-of-sale are available in the eco-system, installing can be done in some single steps. Which Odoo edition should I use?: Comparisons: Odoo has two different editions: Odoo Enterprise and Community. The enterprise edition provides better support and a richer set of features. With community edition, the company pays less for a usable solution but with lack of some useful features:  No mobile app.  No integration to commercial platforms like Amazon and eBay.  No digital sign, subscription, gift programs, VoIP supports.  No custom supports such as field services, AI/IoT solutions, payroll OCR, scheduling, maintenance.  No social and marketing automation. Odoo Enterprise: There are several aspects to care about the pricing of Odoo Enterprise to your organization:  The number of users: Odoo license is applied to the number of employees. Note, employees who report their timesheets are counted as users. New customers obtain a 15% discount for each user at the time of this post.  The number of apps: Only CRM app is coming with the base Odoo Enterprise. The price of each additional app ranges from 4 to 8 dollars.  Where do you host Odoo?: There are 3 hosting types: Cloud hosting, on-premises, and Odoo. sh platform. The first two options add no extra costs. You pay by yourself to the Cloud provider (AWS, IBM Cloud) or server costs. For the third option, the Odoo. sh platform, you can find a simulation below with a 50-employee company, using 50GB of data every month, 1 staging environment, and they pay about 207. 20 dollars monthly. I follow the rule-of-thumb as discussed here. 1 worker would be equal to 25 users. Therefore with 50 employees, we would like to have 3 workers. Source: https://www. odoo. sh/pricing  How do you implement Odoo?: After purchasing the license to Odoo Enterprise, you can implement it in 3 ways: by yourself, using Odoo Success Packs, or by asking help from a partner company. The first option charges you nothing. The second one is available only if Odoo has some branches in your neighborhood. It charges at least 975 dollars (after a 15% discount for new customers). The final option is to check the list of Odoo partner companies in your region and asking them for installation and maintenance supports. Every option to install and maintain a distribution of Odoo Enterprise requires at least 1000$/year, except the first one. If you put Odoo Enterprise in your server, Odoo Success Packs are not applied. Let’s get some example of pricing model for Odoo Enterprise in a medium-sized company: Example 1: Company A has 50 employees with only 15 users (Human Resource teams, accountants, sales) use Odoo. They use 37 apps (excluding CRM) and Odoo. sh platform with Odoo Success Packs Standard. It requires 1 Odoo Cloud Worker, with 100GB data storage and 1 staging environment. This plan costs A 92 dollars per month.       Item   Quantity   Price   Cost ($)         Employees usage   15   8$/user   120       Discount on employees usage   12   -2$/user   -30       App usage   37 apps   8. 22$/app   304       Hosting (Odoo. sh)   1   92   92       Implementation (Success Packs Standard)   1   333. 33   333. 33       Discount on Success Packs Standard   1   -50   -50         Total           769. 33/month   Source: https://www. odoo. sh/pricing Example 2: Company B has 50 employees with only 15 users (Human Resource teams, accountants, sales) use Odoo. They use 37 apps (excluding CRM). They choose to buy the Enterprise pack but buy no Odoo. sh or Success Packs.       Item   Quantity   Price   Cost ($)         Employees usage   15   8$/user   120       Discount on employees usage   15   -2$/user   -30       App usage   37 apps   8. 22$/app   304       Hosting (Odoo. sh)   0   207. 20   0       Implementation (Success Packs)   0   333. 33   0       Discount on Success Packs   0   -50   0       Customer hosting service   1   200   200       Development and maintenance   1   200   200         Total           794. 00/month   Source: https://www. odoo. sh/pricing The development and maintenance which company B pay for is to the developers who implement and maintain the Odoo distribution in the company. In usual cases, there are one or two people in charge of this stuff. Note, this is the lowest price the author found. Comparing to example 1, company B pays a little bit in total, and the processes rely more on human workers and are more prone to human errors. Huh, a ready-to-use solution is cheaper than a self-service solution, right? Odoo Community: The pricing model becomes affordable, but you have to implement and find hosting services yourself. Then, a sufficient background on the Odoo Community edition and deployments are required. You only pay for hosting services and people who implement Odoo on your business. Note that, although the community edition does not come with some custom apps, there is a third-party community with many alternatives for free and paid: https://apps. odoo. com/. Furthermore, when the app you need in the given third-party community, you can start adding an idea to the community since the development of Odoo is fully open-source. (However, it might be better to have ready-to-use solutions). The process is highly dependent on human and is prone to human errors. Compared to the second example, this solution costs less, but the customer hosting price and Development/Maintenance costs might not be changed. So will similar quality, it only reduces about 60% for Odoo apps. And because you need specialized personnel to implement custom Odoo apps, the cost for Development/Maintenance can increase. Deploying Odoo to Amazon Web Service (AWS)After understanding the pricing model and the merits and demerits of each edition, we go to the deployment with Odoo Community to AWS EC2, Aurora for better understanding. Setup VPC in a defense-in-depth style: Setup SSH Bastion server for further investigation: Setup Aurora instance: Deploy Odoo 14. 0: ConclusionIn the first section, we compare available solutions for deployments of Odoo (both paid and free editions). Surprisingly, we found that paid solution, the Odoo Enterprise can be cheaper than using the free solutions if we count all development and maintenance fees. And because this is a reliable and stable solution, Odoo Enterprise with full support from Odoo Cloud Platform is more advantageous with less effort (you and your people can sleep well). From the view of business, the winner is determined. In the second part, although using the community edition does not offer any real benefit in business, we still explored the way to deploy by ourselves to AWS. In conclusion, ready-to-use solutions like Odoo Enterprise with Odoo Cloud seem to be the winner. If you like this, please give :+1: "
    }, {
    "id": 126,
    "url": "https://wanted2.github.io/hybrid-cloud-vmware-on-aws/",
    "title": "On Hybrid Clouds: VMware Cloud on AWS",
    "body": "2021/02/14 - Note: this post is about building Customer Data Center (CDC) for enterprise, not about applications for end-users. For many years, enterprise customers have been deployed their infrastructure into VMware cloud with powerful and secure virtualization technologies like vSphere for computing resources (VMs, . etc. ), vSAN (storage) and NSX (networking). While new and leading public cloud solutions like Amazon Web Services (AWS) offer lower cost tiers, with greater opportunities in building innovative solutions, transforming from the existing VMware cloud to AWS is a trade-off between reliability (on existing solutions) with better business offers. Building a hybrid cloud using VMware Cloud on AWS is a way to achieve both goals: maintaining customers’ trust while migrating to novel solutions. Introduction: Migration to AWSEnterprise customers refer to the companies who have more than 500 personnel each (See the report 1, pdf). These customers have some characteristics in common: they have adopted a stable and reliable infrastructure for years and they are refraining from making a sudden changes in what is already running well. For example, a company who already have their infrastructure running on VMware Cloud and the system have established and have been running well for 20 years, then now it would be difficult t persuade the executives to change to AWS and stop all what already been running well so far. The problems arise when persuading customer to migrate to a new platform like AWS:  The cost of destroying what have been running well: Our infrastructure in VMware have been running well for 20 years, now if we stop everything, we need to investigate on re-training our staffs to be familiar with new systems. The cost of migrating our products to new platform is also high and the waste should be considerable. This is a common customer claim.  The gain of new business opportunities in new platform: Does new platform offer us genuine solutions? Is the gain better than the cost?Solving these trade-offs is a hurdle for migration to new businesses. VMware has offered a great solution to this problem: hybrid clouds with VMware Cloud on AWS. In short, this means the customers deploy their data center to AWS Cloud using VMware Cloud technologies. All of the resources will be allocated in AWS but the management will be done using VMware Cloud solutions such as vSphere, vSAN and NSX23. VMware Cloud on AWSBenefits:  Access to a public cloud environment that is consistent with an on-premises environment and can be operated with the same tools and skill sets as customers’ on-premises VMware environments, allowing migration, operations, and integration with customers’ on-premises environments Ability to add or remove resources on demand within minutes and use resources with hourly pay-as-you-go pricing, enabling agility and flexibility with customers’ VMware environments Access to AWS public cloud services, including databases, data analytics services, and emerging technologies such as artificial intelligence/machine learning (AI/ML) Delivered as a completely managed service by VMware, with pay-as-you-go pricing and no up-front commitmentsLearning resources: Tutorials:    Migrating MS SQL Server to AWS using VMware Cloud     Managing Oracle Database using VMware Cloud on AWS     Performance Characterization of Microsoft SQL Server Using VMware Cloud on AWS     Optimize Virtual Machine Configurations in VMware Cloud on AWS for Enterprise Applications Workload     DNS Strategies for VMware Cloud on AWS     Using an On-Premises DHCP server     Understanding Integrations with AWS Services  Official guides:  User guides AWS guidesOthers:  Series on how-to VMC on AWS VMware Cloud on AWS Tech Zone Introducing the HashiCorp Terraform Provider for VMware Cloud on AWSConclusionSmall business customers are likely to accept the solutions using new platforms like AWS because they don’t possess an existing solution their own. However, medium and large enterprises will need to consider the waste when migrating to new business platforms. Therefore, a solution like VMware Cloud on AWS is a big deal. Enterprises will not need to stop their existing infrastructures, they only move the underlying resources which they rarely need to interact with to AWS, while they still use the same tools (vSphere, vSAN, NSX) for management in VMware Cloud. References      The Business Value of Running Applications on VMware Cloud on AWS in VMware Hybrid Cloud Environments &#8617;        VMware Cloud on AWS: Tech Deep Dive Webinar &#8617;        VMware Cloud on AWS: Deployment, Migration, and Configuration of Oracle Workloads &#8617;    "
    }, {
    "id": 127,
    "url": "https://wanted2.github.io/oracle-cloud-infra/",
    "title": "About Oracle Cloud Infrastructure (OCI)",
    "body": "2021/02/12 - I’ve created this diagram to summarize the services in Oracle Cloud Infrastructure (OCI). I usually use AWS for building applications but apparently, OCI is often used in enterprise solutions. For example, Zoom also chose OCI to deploy their infra. Unsurprisingly, I found that the structure of services in OCI is similar to AWS in several foundational categories: Management, Security, Identity, Database, Compute, Storage, . etc. However, in terms of Artifical Intelligence and IoT applications, it feels like OCI has less showcases and developers who use OCI will be likely to have more tedious work to work. For example, building a chatbot is a relatively affordable task for IBM Watson users or AWS ML users. Quite recently, OCI has announced some of their tutorials in Fraud Detection, and released their Oracle Machine Learning (OML) solution. Anyway, all cloud platforms have their own upsides and downsides. Developers would learn to use and making their own choice. For me, for building AI/IoT applications, I still feel good with Amazon ;) "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});