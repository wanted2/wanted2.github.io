
var documents = [{
    "id": 0,
    "url": "https://wanted2.github.io/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "https://wanted2.github.io/about",
    "title": "Tuan Nguyen-Anh",
    "body": "Contacts: tuan. nguyenanh. brse@gmail. com| Curriculum Vitae: English Experience: Bridge System Engineer/Project Management/Product Owner/Scrum Master/Techlead/TeamleadSystem engineering/Web development      Backend: Java SpringBoot, Scala Playframework, PHP Laravel, Python Flask/Django, . etc.     Frontend: VueJS, ReactJS, AngularJS, Svelte, jQuery, . etc.     Cloud: AWS, Google GCP, Azure    DevOps, SRE, . etc.     Artificial Intelligence/Internet of Things/Computer Vision/Machine Learning "
    }, {
    "id": 2,
    "url": "https://wanted2.github.io/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "https://wanted2.github.io/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                           Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices                              :               Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự. . . :                                                                                                                                                                       AiFi                                12 Feb 2022                                                                                                                                                                                                                                                                                                                        Chia tay 2021!                              :               Đây là bài post thứ 61 của blog AiFi trong năm 2021, cũng là bài viết chia tay 2021, trong tâm thế đón chờ 2022 tươi mới hơn. Theo quan điểm làm việc scrum, thì coi như đây là thời điểm kết thúc 1 chu kỳ, cũng là lúc làm một số việc để nhìn lại mộ. . . :                                                                                                                                                                       AiFi                                31 Dec 2021                                                                                                                                                                                                                                                                                                                  Nghịch ngợm Svelte và câu chuyện code JS thuần 10 năm trước                              :               Svelte1 là một framework của ngôn ngữ Javascript, với mô hình bi-direction data binding (MVVM) khiến cho trạng thái và hiển thị luôn đồng bộ, cũng giống như những người anh em khác như Vue. js, ReactJS hay AngularJS. Tuy nhiên, một ưu điểm của Svel. . . :                                                                                                                                                                       AiFi                                10 Dec 2021                                                                                                                                                                                                                                                                                                                                  All Stories:                                                                                                     Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices              :       Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự. . . :                                                                               AiFi                12 Feb 2022                                                                                                                                    RapidAPI and RapidAPI Hub              :       Image Credit: FinanceFeedsRakuten launched RapidAPI Marketplace in 2018 as a result of the collaboration between Japan’s Rakuten Inc and San Francisco-based startup RapidAPI. The API marketplace aims to provide software developers in Japan and A. . . :                                                                               AiFi                09 Jan 2022                                                                                                                                    Chia tay 2021!              :       Đây là bài post thứ 61 của blog AiFi trong năm 2021, cũng là bài viết chia tay 2021, trong tâm thế đón chờ 2022 tươi mới hơn. Theo quan điểm làm việc scrum, thì coi như đây là thời điểm kết thúc 1 chu kỳ, cũng là lúc làm một số việc để nhìn lại mộ. . . :                                                                               AiFi                31 Dec 2021                                                                                                                                    Nghịch ngợm Svelte và câu chuyện code JS thuần 10 năm trước              :       Svelte1 là một framework của ngôn ngữ Javascript, với mô hình bi-direction data binding (MVVM) khiến cho trạng thái và hiển thị luôn đồng bộ, cũng giống như những người anh em khác như Vue. js, ReactJS hay AngularJS. Tuy nhiên, một ưu điểm của Svel. . . :                                                                               AiFi                10 Dec 2021                                                                                                                                    Quả trứng và con gà: cái nào có trước? - Bất bình đẳng về lương và sản lượng lao động              :       Con gà và quả trứng là một bài toán kinh điển trong thống kê học cổ điển liên quan tới nhân quả (causality) và luân hồi (feedback) của các chuỗi thời gian (time series). Năm 1969, Granger (Nobel kinh tế 2003) xuất bản một seminar paper [3] định . . . :                                                                               AiFi                20 Nov 2021                                                                                                                                    So sánh git merge và git rebase              :       git merge và git rebase là 2 câu lệnh quen thuộc với lập trình viên chỉ để giải quyết cùng 1 bài toán: với 2 branches được phát triển song song, nay cần migrate các tính năng của branch feature vào branch chính (main). Vấn đề là làm thế nào? Một c. . . :                                                                               AiFi                08 Oct 2021                                               &laquo; Prev       1        2        3        4        5        6        7      Next &raquo; "
    }, {
    "id": 4,
    "url": "https://wanted2.github.io/projects",
    "title": "Projects and Demos",
    "body": "Projects: Demos: AI demos "
    }, {
    "id": 5,
    "url": "https://wanted2.github.io/bibliography/abstraction1988hierarchy/",
    "title": "Data Abstraction and Hierarchy",
    "body": "  Liskov, B. 1988. Data Abstraction and Hierarchy. SIGPLAN Notices. 23, 5 (1988), 17–34.                                                                                             @article{abstraction1988hierarchy, author = {Liskov, Barbara}, journal = {SIGPLAN Notices}, number = {5}, pages = {17--34}, title = {Data Abstraction and Hierarchy}, volume = {23}, year = {1988}}                                                        "
    }, {
    "id": 6,
    "url": "https://wanted2.github.io/bibliography/balaji2018benchmarking/",
    "title": "Benchmarking automatic machine learning frameworks",
    "body": "  Balaji, A. and Allen, A. 2018. Benchmarking automatic machine learning frameworks. arXiv preprint arXiv:1808. 06492. (2018).                                                                                             @article{balaji2018benchmarking, author = {Balaji, Adithya and Allen, Alexander}, journal = {arXiv preprint arXiv:1808. 06492}, title = {Benchmarking automatic machine learning frameworks}, year = {2018}}                                                        "
    }, {
    "id": 7,
    "url": "https://wanted2.github.io/bibliography/behutiye2017analyzing/",
    "title": "Analyzing the concept of technical debt in the context of agile software development: A systematic literature review",
    "body": "  Behutiye, W. N. , Rodrı́guez Pilar, Oivo, M. and Tosun, A. 2017. Analyzing the concept of technical debt in the context of agile software development: A systematic literature review. Information and Software Technology. 82, (2017), 139–158.                                                                                             @article{behutiye2017analyzing, author = {Behutiye, Woubshet Nema and Rodr{\'\i}guez, Pilar and Oivo, Markku and Tosun, Ay{\c{s}}e}, journal = {Information and Software Technology}, pages = {139--158}, publisher = {Elsevier}, title = {Analyzing the concept of technical debt in the context of agile software development: A systematic literature review}, volume = {82}, year = {2017}}                                                        "
    }, {
    "id": 8,
    "url": "https://wanted2.github.io/bibliography/bourque2004swebok/",
    "title": "SWEBOK",
    "body": "  Bourque, P. and Fairley, R. 2004. SWEBOK. Nd: IEEE Computer society. (2004).                                                                                             @article{bourque2004swebok, author = {Bourque, Pierre and Fairley, R}, journal = {Nd: IEEE Computer society}, title = {SWEBOK}, year = {2004}}                                                        "
    }, {
    "id": 9,
    "url": "https://wanted2.github.io/bibliography/brown2013software/",
    "title": "Software architecture for developers",
    "body": "  Brown, S. 2013. Software architecture for developers. Coding the Architecture. (2013).                                                                                             @article{brown2013software, author = {Brown, Simon}, journal = {Coding the Architecture}, title = {Software architecture for developers}, year = {2013}}                                                        "
    }, {
    "id": 10,
    "url": "https://wanted2.github.io/bibliography/cleanarch/",
    "title": "Clean Architecture - A Craftman’s Guide to Software Structure and Design",
    "body": "  Martin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall.                                                                                             @book{cleanarch, author = {Martin, Robert C. }, publisher = {Prentice Hall}, title = {Clean Architecture - A Craftman's Guide to Software Structure and Design}, year = {2017}}                                                        "
    }, {
    "id": 11,
    "url": "https://wanted2.github.io/bibliography/cunningham1992wycash/",
    "title": "The WyCash portfolio management system",
    "body": "  Cunningham, W. 1992. The WyCash portfolio management system. ACM SIGPLAN OOPS Messenger. 4, 2 (1992), 29–30.                                                                                             @article{cunningham1992wycash, author = {Cunningham, Ward}, journal = {ACM SIGPLAN OOPS Messenger}, number = {2}, pages = {29--30}, publisher = {ACM New York, NY, USA}, title = {The WyCash portfolio management system}, volume = {4}, year = {1992}}                                                        "
    }, {
    "id": 12,
    "url": "https://wanted2.github.io/bibliography/DBLP_journals/corr/BodlaSCD17/",
    "title": "Improving Object Detection With One Line of Code",
    "body": "  Bodla, N. , Singh, B. , Chellappa, R. and Davis, L. S. 2017. Improving Object Detection With One Line of Code. CoRR. abs/1704. 04503, (2017).                                                                                             @article{DBLP:journals/corr/BodlaSCD17, archiveprefix = {arXiv}, author = {Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S. }, eprint = {1704. 04503}, journal = {CoRR}, title = {Improving Object Detection With One Line of Code}, url = {http://arxiv. org/abs/1704. 04503}, volume = {abs/1704. 04503}, year = {2017}}                                                        "
    }, {
    "id": 13,
    "url": "https://wanted2.github.io/bibliography/feige1979casual/",
    "title": "The casual causal relationship between money and income: Some caveats for time series analysis",
    "body": "  Feige, E. L. and Pearce, D. K. 1979. The casual causal relationship between money and income: Some caveats for time series analysis. The Review of Economics and Statistics. (1979), 521–533.                                                                                             @article{feige1979casual, author = {Feige, Edgar L and Pearce, Douglas K}, journal = {The Review of Economics and Statistics}, pages = {521--533}, publisher = {JSTOR}, title = {The casual causal relationship between money and income: Some caveats for time series analysis}, year = {1979}}                                                        "
    }, {
    "id": 14,
    "url": "https://wanted2.github.io/bibliography/fisher1983getting/",
    "title": "Getting to Yes: Negotiating Agreement Without Giving In",
    "body": "  Fisher, R. and Ury, W. 1983. Getting to Yes: Negotiating Agreement Without Giving In. New York: Penguin Books.                                                                                             @misc{fisher1983getting, author = {Fisher, Roger and Ury, William}, publisher = {New York: Penguin Books}, title = {Getting to Yes: Negotiating Agreement Without Giving In}, year = {1983}}                                                        "
    }, {
    "id": 15,
    "url": "https://wanted2.github.io/bibliography/granger1969investigating/",
    "title": "Investigating causal relations by econometric models and cross-spectral methods",
    "body": "  Granger, C. W. J. 1969. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: journal of the Econometric Society. (1969), 424–438.                                                                                             @article{granger1969investigating, author = {Granger, Clive WJ}, journal = {Econometrica: journal of the Econometric Society}, pages = {424--438}, publisher = {JSTOR}, title = {Investigating causal relations by econometric models and cross-spectral methods}, year = {1969}}                                                        "
    }, {
    "id": 16,
    "url": "https://wanted2.github.io/bibliography/hamburger1986three/",
    "title": "Three perceptions of project cost",
    "body": "  Hamburger, D. H. 1986. Three perceptions of project cost. Project Management journal. (1986), 372–378.                                                                                             @article{hamburger1986three, author = {Hamburger, DH}, journal = {Project Management journal}, pages = {372--378}, title = {Three perceptions of project cost}, year = {1986}}                                                        "
    }, {
    "id": 17,
    "url": "https://wanted2.github.io/bibliography/Kirillov_2019_CVPR/",
    "title": "Panoptic Segmentation",
    "body": "  Kirillov, A. , He, K. , Girshick, R. , Rother, C. and Dollar, P. 2019. Panoptic Segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Jun. 2019).                                                                                             @inproceedings{Kirillov_2019_CVPR, author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Dollar, Piotr}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = jun, title = {Panoptic Segmentation}, year = {2019}}                                                        "
    }, {
    "id": 18,
    "url": "https://wanted2.github.io/bibliography/kuznetsova2020open/",
    "title": "The open images dataset v4",
    "body": "  Kuznetsova, A. , Rom, H. , Alldrin, N. , Uijlings, J. , Krasin, I. , Pont-Tuset, J. , Kamali, S. , Popov, S. , Malloci, M. , Kolesnikov, A. , Duerig, T. and Ferrari, V. 2020. The open images dataset v4. International Journal of Computer Vision. 128, 7 (2020), 1956–1981.                                                                                             @article{kuznetsova2020open, author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and Duerig, Tom and Ferrari, Vittorio}, journal = {International Journal of Computer Vision}, number = {7}, pages = {1956--1981}, publisher = {Springer}, title = {The open images dataset v4}, volume = {128}, year = {2020}}                                                        "
    }, {
    "id": 19,
    "url": "https://wanted2.github.io/bibliography/lin2014microsoft/",
    "title": "Microsoft coco: Common objects in context",
    "body": "  Lin, T. -Y. , Maire, M. , Belongie, S. , Hays, J. , Perona, P. , Ramanan, D. , Dollár, P. and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. European conference on computer vision (2014), 740–755.                                                                                             @inproceedings{lin2014microsoft, author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence}, booktitle = {European conference on computer vision}, organization = {Springer}, pages = {740--755}, title = {Microsoft coco: Common objects in context}, year = {2014}}                                                        "
    }, {
    "id": 20,
    "url": "https://wanted2.github.io/bibliography/machinery1950computing/",
    "title": "Computing machinery and intelligence",
    "body": "  Turing, A. 1950. Computing machinery and intelligence. Mind. 49, 236 (1950), 433.                                                                                             @article{machinery1950computing, author = {Turing, Alan}, journal = {Mind}, number = {236}, pages = {433}, title = {Computing machinery and intelligence}, volume = {49}, year = {1950}}                                                        "
    }, {
    "id": 21,
    "url": "https://wanted2.github.io/bibliography/meredith2017project/",
    "title": "Project management: a strategic managerial approach",
    "body": "  Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons.                                                                                             @book{meredith2017project, author = {Meredith, Jack R and Shafer, Scott M and Mantel Jr, Samuel J}, publisher = {John Wiley \&amp; Sons}, title = {Project management: a strategic managerial approach}, year = {2017}}                                                        "
    }, {
    "id": 22,
    "url": "https://wanted2.github.io/bibliography/qureshi2019reduce/",
    "title": "Reduce Cost of Batch Processing Microsoft Azure Cloud",
    "body": "  Qureshi, A. N. 2019. Reduce Cost of Batch Processing Microsoft Azure Cloud. JETIR-International Journal of Emerging Technologies and Innovative Research. (2019), 2349–5162.                                                                                             @article{qureshi2019reduce, author = {Qureshi, Asfak N}, journal = {JETIR-International Journal of Emerging Technologies and Innovative Research}, pages = {2349--5162}, title = {Reduce Cost of Batch Processing Microsoft Azure Cloud}, year = {2019}}                                                        "
    }, {
    "id": 23,
    "url": "https://wanted2.github.io/bibliography/risco2021gpu/",
    "title": "GPU-Enabled Serverless Workflows for Efficient Multimedia Processing",
    "body": "  Risco, S. and Moltó, G. 2021. GPU-Enabled Serverless Workflows for Efficient Multimedia Processing. Applied Sciences. 11, 4 (2021), 1438.                                                                                             @article{risco2021gpu, author = {Risco, Sebasti{\'a}n and Molt{\'o}, Germ{\'a}n}, journal = {Applied Sciences}, number = {4}, pages = {1438}, publisher = {Multidisciplinary Digital Publishing Institute}, title = {GPU-Enabled Serverless Workflows for Efficient Multimedia Processing}, volume = {11}, year = {2021}}                                                        "
    }, {
    "id": 24,
    "url": "https://wanted2.github.io/bibliography/soh2020overview/",
    "title": "Overview of Azure Platform as a Service",
    "body": "  Soh, J. , Copeland, M. , Puca, A. and Harris, M. 2020. Overview of Azure Platform as a Service. Microsoft Azure. Springer. 43–55.                                                                                             @incollection{soh2020overview, author = {Soh, Julian and Copeland, Marshall and Puca, Anthony and Harris, Micheleen}, booktitle = {Microsoft Azure}, pages = {43--55}, publisher = {Springer}, title = {Overview of Azure Platform as a Service}, year = {2020}}                                                        "
    }, {
    "id": 25,
    "url": "https://wanted2.github.io/bibliography/thurman1988chickens/",
    "title": "Chickens, eggs, and causality, or which came first",
    "body": "  Thurman, W. N. , Fisher, M. E. and others 1988. Chickens, eggs, and causality, or which came first. American journal of agricultural economics. 70, 2 (1988), 237–238.                                                                                             @article{thurman1988chickens, author = {Thurman, Walter N and Fisher, Mark E and others}, journal = {American journal of agricultural economics}, number = {2}, pages = {237--238}, publisher = {Agricultural and Applied Economics Association}, title = {Chickens, eggs, and causality, or which came first}, volume = {70}, year = {1988}}                                                        "
    }, {
    "id": 26,
    "url": "https://wanted2.github.io/bibliography/TIAN20201/",
    "title": "Computer vision technology in agricultural automation –A review",
    "body": "  Tian, H. , Wang, T. , Liu, Y. , Qiao, X. and Li, Y. 2020. Computer vision technology in agricultural automation –A review. Information Processing in Agriculture. 7, 1 (2020), 1–19. DOI:https://doi. org/10. 1016/j. inpa. 2019. 09. 006.     DOI                                                                                            @article{TIAN20201, author = {Tian, Hongkun and Wang, Tianhai and Liu, Yadong and Qiao, Xi and Li, Yanzhou}, doi = {10. 1016/j. inpa. 2019. 09. 006}, issn = {2214-3173}, journal = {Information Processing in Agriculture}, keywords = {Computer vision, Image processing, Agricultural automation, Intelligent detection}, number = {1}, pages = {1-19}, title = {Computer vision technology in agricultural automation --A review}, url = {https://www. sciencedirect. com/science/article/pii/S2214317319301751}, volume = {7}, year = {2020}}                                                        "
    }, {
    "id": 27,
    "url": "https://wanted2.github.io/bibliography/witte2019event/",
    "title": "Event-driven workflows for large-scale seismic imaging in the cloud",
    "body": "  Witte, P. A. , Louboutin, M. , Modzelewski, H. , Jones, C. , Selvage, J. and Herrmann, F. J. 2019. Event-driven workflows for large-scale seismic imaging in the cloud. SEG Technical Program Expanded Abstracts 2019. Society of Exploration Geophysicists. 3984–3988.                                                                                             @incollection{witte2019event, author = {Witte, Philipp A and Louboutin, Mathias and Modzelewski, Henryk and Jones, Charles and Selvage, James and Herrmann, Felix J}, booktitle = {SEG Technical Program Expanded Abstracts 2019}, pages = {3984--3988}, publisher = {Society of Exploration Geophysicists}, title = {Event-driven workflows for large-scale seismic imaging in the cloud}, year = {2019}}                                                        "
    }, {
    "id": 28,
    "url": "https://wanted2.github.io/bibliography/dosovitskiy2020image/",
    "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
    "body": "  Dosovitskiy, A. , Beyer, L. , Kolesnikov, A. , Weissenborn, D. , Zhai, X. , Unterthiner, T. , Dehghani, M. , Minderer, M. , Heigold, G. , Gelly, S. , Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010. 11929. (2020).                                                                                             @article{dosovitskiy2020image, title = {An image is worth 16x16 words: Transformers for image recognition at scale}, author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uskoreit, Jakob and Houlsby, Neil}, journal = {arXiv preprint arXiv:2010. 11929}, year = {2020}}                                                        "
    }, {
    "id": 29,
    "url": "https://wanted2.github.io/bibliography/carion2020end/",
    "title": "End-to-end object detection with transformers",
    "body": "  Carion, N. , Massa, F. , Synnaeve, G. , Usunier, N. , Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. European conference on computer vision (2020), 213–229.                                                                                             @inproceedings{carion2020end, title = {End-to-end object detection with transformers}, author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey}, booktitle = {European conference on computer vision}, pages = {213--229}, year = {2020}, organization = {Springer}}                                                        "
    }, {
    "id": 30,
    "url": "https://wanted2.github.io/bibliography/gao2021fast/",
    "title": "Fast convergence of detr with spatially modulated co-attention",
    "body": "  Gao, P. , Zheng, M. , Wang, X. , Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. Proceedings of the IEEE/CVF International Conference on Computer Vision (2021), 3621–3630.                                                                                             @inproceedings{gao2021fast, title = {Fast convergence of detr with spatially modulated co-attention}, author = {Gao, Peng and Zheng, Minghang and Wang, Xiaogang and Dai, Jifeng and Li, Hongsheng}, booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages = {3621--3630}, year = {2021}}                                                        "
    }, {
    "id": 31,
    "url": "https://wanted2.github.io/bibliography/ge2021yolox/",
    "title": "Yolox: Exceeding yolo series in 2021",
    "body": "  Ge, Z. , Liu, S. , Wang, F. , Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107. 08430. (2021).                                                                                             @article{ge2021yolox, title = {Yolox: Exceeding yolo series in 2021}, author = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian}, journal = {arXiv preprint arXiv:2107. 08430}, year = {2021}}                                                        "
    }, {
    "id": 32,
    "url": "https://wanted2.github.io/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 33,
    "url": "https://wanted2.github.io/page2/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 34,
    "url": "https://wanted2.github.io/page3/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 35,
    "url": "https://wanted2.github.io/page4/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 36,
    "url": "https://wanted2.github.io/page5/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 37,
    "url": "https://wanted2.github.io/page6/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 38,
    "url": "https://wanted2.github.io/page7/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 39,
    "url": "https://wanted2.github.io/seq2seq-cv/",
    "title": "Seq2Seq for Computer Vision: Three SOTAs and 8x GPU server choices",
    "body": "2022/02/12 - Trong các bài viết trước, chúng ta đã xem xét kha khá về seq2seq cho NLP/Vision-Language1 và âm thanh2. Trong bài viết này, ta sẽ tập trung vào 2 vấn đề: một vài SOTA của seq2seq trong computer vision và vấn đề giá thành xây dựng tài nguyên cho dự án cần seq2seq. Vision TransformersViT: Về nguyên lý chung của seq2seq thì chúng ta có hai bài viết trước nói khá nhiều rồi12 nên các bạn tham khảo nhé. Nhà cũng đang có cái “eo hẹp” là chỉ được tối đa 7 citations và 9 phút đọc thôi nên các bạn cần thì đọc lại hai bài viết12 để xem thêm kiến thức về attention, Transformer, …Vision Transformer (ViT, [1]) đưa khái niệm seq2seq vào vision. Nếu các bạn đã quen seq2seq thì hiểu ngay là ta cần chuyển hình ảnh thành 1 chuỗi: Source: Google Để làm được việc này, các tác giả đề xuất:  Image patches: Hình ảnh được chia ra thành patches và đánh số thự tự để input vào transformer.  Position embedding: thứ tự chỉ đơn giản là chuỗi 1D.  MLP với GELU activation: MLP sử dụng GELU để kích hoạt. Model không chứa CNN, có tối thiểu 12 tầng, với kích cỡ hidden size từ 768, ngoài ra có 12-16 heads.  Large-scale pre-training and fine-tuning: Một điểm đáng chú ý khác là pre-training dạng supervised trên một dataset 300 triệu ảnh tạo ra 1 model rất mạnh. Các tác giả báo cáo cải tiến trên khá nhiều bộ dữ liệu lớn, và ngoài ra cả con số TPUv3-core-days = số lượng TPUv3 cores x sô lượng ngày train. Cái con số TPUv3-core-days thì cứ mỗi thí nghiệm là vài ngàn tới vài chục ngàn, mà mỗi core thì cứ 10$/ngày thì các bạn cứ nhẩm tính xem budget của hội con nhà giàu này đầu tư vào nó lớn cỡ nào đấy. DETR [2, 3]: YOLOX [4]: Xây dựng tài nguyên GPUĐầu tiên chúng ta sẽ xem SOTA ở mảng này xài máy móc thế nào?Rồi mình lại xem giá thành bộ đó qua đủ các phương án: tự xây và thuê xem có “vừa túi tiền” không nhá. Giá thành khi train model State-of-the-art (SOTA): Nhìn chung là nếu chỉ hình ảnh với bộ dữ liệu nhỏ nhỏ như COCO [5] tầm trăm ngàn ảnh thì có bảng giá dưới đây: chúng ta lấy ví dụ từ báo cáo của 1 state-of-the-art thì họ dùng 8 cái V100, train tầm 6 ngày liên tục (\(6\times 24\) giờ) thì tổng tiền cho một lượt trên tầm ngàn Mỹ kim cho 6 ngày, 1 tháng cứ tầm 5 ngàn Mỹ kim. Mà các bạn cũng nhớ giá này là giá Spot tức là có thể bị interrupt giữa chừng nên mới rẻ thế. Chứ nếu bạn mà chọn on-demand thì có mà gấp 10 lần.  Nhưng ở trên mới chỉ là giá bộ COCO có hơn 100k ảnh nhé. Bộ Open Images [6] với 1. 7 triệu ảnh thì còn máu nữa. Search trên Kaggle mà có đồng chí chịu khó bỏ tiền ra ngồi train và báo cáo kết quả cho anh biết (xin cám ơn đồng chí): Kaggle Open Images 2019 challenge 6th place solution. Thì kết quả là đồng chí ấy báo cáo:  train 8 models trên V100 (chắc lại EC2 P3 thôi thì mình cứ dùng p3. xlarge để làm phân tích giá nhé) rồi ensemble.  mỗi model train mất 18-36 ngày (tùy model). Thì đồng chí này train 8 GPUs khác nhau.  sau khi train xong các model thì mất thêm 1 ngày nữa để inference và 1 ngày nữa để ensemble (dùng NMS).  Vậy tổng thể đã tiêu tốn \(36\times 8+1\times 8+1=297\) ngày train, tức là \(297\times 24=7128\) giờ train. p3. xlarge thì giá mềm nhất là Spot cũng tầm $0. 918/h. Tức là để train được accuracy tầm 60% đã mất \(7128\times 0. 918\) tức là tầm 6543 Mỹ kim và hơn tháng ngồi monitor màn hình train. Xây dựng hệ thống 8~16 GPU: Nhìn chung thì theo dòng lịch sử có 3 loại NVIDIA GPU dành cho cloud khá thông dụng như sau (tôi không nói tới hai dòng GTX và RTX nhé):  NVIDIA V100 hay Volta: nói đến dòng này chúng ta có những sự lựa chọn chủ yếu liên quan tới V100 Tensor Core mà đại diện cho thuê là p3. 16xlarge và p3dn. 24xlarge.  Với băng thông mạng của phiên bản P3. 16xlarge cao hơn tới 4 lần, phiên bản P3dn. 24xlarge của Amazon EC2 là sự bổ sung mới nhất cho dòng phiên bản P3, được tối ưu hóa cho machine learning phân tán và các ứng dụng HPC. Các phiên bản này cung cấp thông lượng kết nối mạng lên tới 100 Gbps, 96 vCPU Intel® Xeon® Có thể mở rộng (Skylake) tùy chỉnh, 8 GPU NVIDIA® V100 Tensor Core với 32 GB bộ nhớ mỗi GPU và 1,8 TB ổ lưu trữ SSD cục bộ chuẩn NVMe. Các phiên bản P3dn. 24xlarge cũng hỗ trợ Elastic Fabric Adapter (EFA). Giao diện này tăng tốc các ứng dụng machine learning phân tán sử dụng Thư viện giao tiếp chung NVIDIA (NCCL). EFA có thể mở rộng quy mô lên đến hàng nghìn GPU, cải thiện đáng kể thông lượng và khả năng mở rộng của các mô hình huấn luyện deep learning, từ đó cho kết quả nhanh hơn. Source: Amazon Web Service  NVIDIA T4 hay Turing: với AWS EC2 thì bạn có thể thuê g4dn. metal.  NVIDIA A100 hay Ampere: Với AWS EC2 thì có thể thuê p4d. 24xlarge, với Azure HPC thì có thể thuê Standard_ND96amsr_A100_v4. GCP thì có a2-highgpu-8g hoặc bản 16 GPU là a2-highgpu-16g. Thì về mặt spec Ampere là khỏe nhất nếu nói về TFLOPS. Dưới đây là bảng giá thành của NVIDIA 8x A100 Tensor Core. Trong bảng này có 2 cột mà các bạn nên để ý là giá thành thuê theo giờ (Hourly cost) và tỷ lệ GFLOPS/USD (đáng giá thế nào). Giả định chung là hệ thống được xây dựng tối thiểu 4x GPU và được dùng ít nhất 24 tháng, mỗi tháng dùng 22 ngày (T7/CN nghỉ ngơi).  Nói chung tự build thì các bạn có thể tham khảo cấu hình của DGX-13, DGX-24, và DGX-A1005 để mua các bộ phận về tự ráp thì sẽ tiết kiệm công lắp ráp, nhưng nhìn chung tôi nghĩ cũng phải 50 ngàn Mỹ Kim. Các cloud solutions: Trong trường hợp bạn có bài toán train dữ liệu mà mất hàng tháng trời train với GTX/RTX thì bạn sẽ nghĩ phải thuê GPUs trên data center (8x-16x GPU). Thì ngoài AWS/Azure/GCP là khá cùng rank nên bảng giá không chênh lệch nhau mấy, bạn có thể tham khảo thêm các trang cho thuê GPU bên ngoài để tìm được chỗ thuê hợp lý hơn. Như kết quả tìm kiếm của AIFI thì hiện tại có trang vast. ai cung cấp khá nhiều sự lựa chọn cho thuê ở mức giá thấp hơn 5 USD/hour. Còn lời giải nào khác?: Nhìn chung tự build thì có hai khả năng:  Mua đồ sẵn như DGX345 thì các bạn cứ chuẩn bị 100k Mỹ kim trở lên.  Mua bộ phận về tự ráp thì các bác tham khảo cấu hình của DGX rồi độ lại tùy theo nhu cầu. Tuy nhiên, chắc chỉ giảm được tiền công, và tối ưu một chút kiểu DGX dùng nhiều RAM thì mình giảm RAM xuống. Nói chung chắc cũng phải 50K Mỹ Kim. Về cá nhân, tôi thiên về thuê!Nếu tự build thì mua mấy cái RTX/GTX dòng Ti là ổn rồi. Tuy nhiên nếu bài toán lớn thì bạn bắt buộc phải dùng data center GPU thì lúc ấy phải có TIỀN!  Vấn đề của đi thuê là tiền tính theo giờ nên bạn cần phải ước lượng được số giờ sử dụng. Nếu tầm trên 200h/tháng, tôi nghĩ nên thuê theo năm hoặc 3 năm. Spot price thì cũng tàm tạm thôi, vì mất công chờ với nó ngắt điện (interupt) mình cũng phải chịu ấy, nên là rẻ nhưng lại mất thời gian chờ và bị ngắt. Mà vấn đề với Spot là nó không có luôn ấy (phải chờ đến khi cái server ấy nó open mình mới được dùng). Kết luậnTài liệu tham khảoDosovitskiy, A. , Beyer, L. , Kolesnikov, A. , Weissenborn, D. , Zhai, X. , Unterthiner, T. , Dehghani, M. , Minderer, M. , Heigold, G. , Gelly, S. , Uskoreit, J. and Houlsby, N. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010. 11929. (2020). DetailsCarion, N. , Massa, F. , Synnaeve, G. , Usunier, N. , Kirillov, A. and Zagoruyko, S. 2020. End-to-end object detection with transformers. European conference on computer vision (2020), 213–229. DetailsGao, P. , Zheng, M. , Wang, X. , Dai, J. and Li, H. 2021. Fast convergence of detr with spatially modulated co-attention. Proceedings of the IEEE/CVF International Conference on Computer Vision (2021), 3621–3630. DetailsGe, Z. , Liu, S. , Wang, F. , Li, Z. and Sun, J. 2021. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107. 08430. (2021). DetailsLin, T. -Y. , Maire, M. , Belongie, S. , Hays, J. , Perona, P. , Ramanan, D. , Dollár, P. and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. European conference on computer vision (2014), 740–755. DetailsKuznetsova, A. , Rom, H. , Alldrin, N. , Uijlings, J. , Krasin, I. , Pont-Tuset, J. , Kamali, S. , Popov, S. , Malloci, M. , Kolesnikov, A. , Duerig, T. and Ferrari, V. 2020. The open images dataset v4. International Journal of Computer Vision. 128, 7 (2020), 1956–1981. Details      https://wanted2. github. io/seq2seq/ &#8617; &#8617;2 &#8617;3        https://wanted2. github. io/speech/ &#8617; &#8617;2 &#8617;3        https://www. nvidia. com/en-us/data-center/dgx-1/ &#8617; &#8617;2        https://www. nvidia. com/en-us/data-center/dgx-2/ &#8617; &#8617;2        https://www. nvidia. com/en-us/data-center/dgx-a100/ &#8617; &#8617;2    "
    }, {
    "id": 40,
    "url": "https://wanted2.github.io/rapidapi/",
    "title": "RapidAPI and RapidAPI Hub",
    "body": "2022/01/09 - Image Credit: FinanceFeeds Rakuten launched RapidAPI Marketplace in 2018 as a result of the collaboration between Japan’s Rakuten Inc and San Francisco-based startup RapidAPI. The API marketplace aims to provide software developers in Japan and Asia unified access to more than 8,000 APIs with localized documentation and resources in Japan’s language and English. The API marketplace platform will connect API providers and developers. Developers in Japan and across Asia will be able to find, test, and connect to thousands of APIs for their applications. The marketplace will also allow API providers to connect with the global developer community through personalized API portals.  What is RapidAPI?Let us assume that you have an API that is ready for production. You need to add authentication like API key, OAuth 2, or something else. You need to deploy your API to somewhere that is stable and reliable.  What is the shortest path to achieving your goal? You are an application developer, and you need to manage the records of some data for the app. For example, you need to maintain the list of public holidays in your app. You don’t want to hardcode those things in the code. Note that the public holidays change between countries and sometimes due to the law it will change between years. It is somewhat troublesome to maintain the records in your database as it will make you allocate some effort and human resources there.  What is the most convenient way to maintain such data? In both scenarios, Rakuten RapidAPI Marketplace gives you excellent solutions. Either maintenance of the data (public holidays) or publishing a new API, you can do all of the lifecycles in one platform. For example, when you want to check a day is a holiday or not, you can thus search for a free API like this one and make a request. Because all maintenance is up on the providers, this solution costs you nothing: you don’t need to worry about maintaining the records of holidays data (which shouldn’t be your matter in any way) and focus on your own application logic. Note that the Public Holidays API has low latency (59ms) and is completely free. Another solution is to build an endpoint in your own API like /api/v1/holidays to validate the holidays, but while such a ready-to-use solution is there, why should you waste time and money to build/manage/maintain on your own? RapidAPI helps your API to distribute and monetize. Adding your API to the RapidAPI Hub gets you instant exposure to our growing user base, a search-engine-optimized profile page for your API, as well as features like user management and billing services. RapidAPI also serves functional testings, API monitoring dashboards, and many other premiere features like API authentication. RapidAPI for API VendorsThe workflow between an app developer’s client to a vendor API can be as follows:  An API Key is generated and appended to the request’s header to RapidAPI servers.  RapidAPI authenticate the request (using API Key and optionally a configured authentication method like OAuth 2). Then it modifies the requests header to append X-RapidAPI-* headers.  The vendor API (destination API in the diagram) checks the X-RapidAPI-* headers and authenticates the modified requests.  A response is generated according to the requested information and is then returned to RapidAPI.  RapidAPI modifies the response from vendor servers. It appends Rapid API headers (for example, headers about rate limits) or generates a new response. As you can see, RapidAPI Marketplace acts as a proxy between app servers (client in the diagram) and the vendor API servers. The vendors register their APIs and fine-tune the settings in RapidAPI dashboard. All API endpoints are relative to a base URL, which is added as a “prefix” to all API endpoints. This approach avoids the need to define absolute URLs for endpoints every time and increases API portability by changing the base URL. API vendors can add basic authentication or OAuth 2 to their APIs. RapidAPI supports automatic API provisioning using OpenAPI and custom transformations. RapidAPI has basic plan options so app developers can choose among these options to pay:       API Type   Description         Free APIs   APIs that do not require a credit card or subscription to consume.        Pay Per Use   APIs that don’t have a subscription fee associated with them. A credit card is required as you pay for what you use on the API.        Freemium APIs   Paid APIs that also include a limited free tier. These require a credit card, even for the free plan.        Paid APIs   APIs that require a paid subscription plan and credit card to consume.    Some notes on security: RapidAPI supports secret headers and parameters:  RapidAPI allows you to add secret headers and/or query string parameters to API requests. The RapidAPI proxy adds these secrets to every request but is hidden from the API consumers. Note that even the consumers who make the requests do not know about these secrets. This differs from header and query authentication methods where consumers know all secrets in the requests they make to RapidAPI. Users should configure RapidAPI security features like firewalls, threat protection, schema validation, and request size limit (which returns error code 413). Vendors can set their API to private where only invited users can access. Audit and marketing tools: RapidAPI provides Provider Dashboard where vendors can monitor their API usages. Another nice thing is that as a vendor, you can make your monetization more useful using Marketing API. When you have an API, you should make sure you don’t miss a checklist when publishing your solution: This checklist helps you have a better SEO for your API. API Testing: Testing is quite tedious!RapidAPI helps vendors reduce testing costs with their API testing feature.  If you are already familiar with postman-tool you are ready to go with RapidAPI advanced testing.  RapidAPI for App DevelopersAs an app developer, you can find that RapidAPI Hub now has more than 10,000 APIs. Even you want to develop an OCR app or a Translation app, you can find your API right away. All you need is to register a RapidAPI account, choose your API and then make a payment. Finally, you can connect to your paid API using the API key.  ConclusionIt is worth noting that RapidAPI supports not only REST API but also GraphQL, SOAP, and Kafka APIs. We did not touch RapidAPI for Teams, but it might be useful at the organization level. "
    }, {
    "id": 41,
    "url": "https://wanted2.github.io/year-end/",
    "title": "Chia tay 2021!",
    "body": "2021/12/31 - Đây là bài post thứ 61 của blog AiFi trong năm 2021, cũng là bài viết chia tay 2021, trong tâm thế đón chờ 2022 tươi mới hơn. Theo quan điểm làm việc scrum, thì coi như đây là thời điểm kết thúc 1 chu kỳ, cũng là lúc làm một số việc để nhìn lại một năm đã qua (bao gồm cả GKPT hay Good, Keep, Problem, Try). 2021年中61番目の投稿です．2021年と別れて，2022年を迎える時期の投稿です．一年間を1スプリントとすると，いろいろなことができたと思いますので，スクラムの行事として，レビューとレトロ会をここで開催したいと思います．Good, Keep, Problem, Try も含めてやります． Nhìn lại năm 2021 của blog AiFiNhìn từ thống kê người dùng: Hiện tại AiFi blog sử dụng Google Analytics để track và lấy thống kê người dùng. Các sự kiện như view, scroll, referal, … được báo cáo theo phút lên server của Google. Đầu tiên là thống kê về người dùng và nguồn giới thiệu. Trong năm 2021, blog tuy mới ra mắt và còn nhiều khó khăn vất vả nhưng đã thu hút được 552 user mới từ khắp nơi trên thế giới. 552 người dùng này đã ghi lại 7309 sự kiện. Một con số đáng khích lệ với blog mới 1 năm tuổi đời.  Một điểm đáng chú ý là dù facebook. com là nơi tác giả hay chia sẻ bài viết, nhưng user lại phần lớn đến từ 2 nguồn: google và direct. Về yếu tố địa lý thì đa phần người dùng đến từ Việt Nam, Mỹ và Nhật Bản. Các nước khác vẫn chưa đóng tỷ trọng lớn trong cơ cấu người dùng của AiFi.  Tỷ lệ người dùng của AiFi gia tăng tính từ tháng 7. Trong năm 2021, số lượng sự kiện user engagement là 1852, và số page view là 2622 lượt. Ngoài ra, 3 bài viết đạt số lượng truy cập cao nhất (không tính trang chủ) là:  mOCR: A real-time application of OCR with Google MLKit and Android CameraX Adobe Creative Cloud: An All-in-One Platform for Creators Implementing a complex system in AWS Lambda: Should or shouldn’t?Sự “vùng lên” của bài viết Adobe Creative Cloud: An All-in-One Platform for Creators thật thú vị vì bài viết được xuất bản trên blog AiFi vào tháng cuối năm nhưng lại đứng thứ nhì.  Về hệ điều hành, trình duyệt và ngôn ngữ đầu vẫn là Windows, Chrome và English. Theo sau lần lượt là MacOS, Safari và tiếng Nhật.  Nhìn từ kết quả tìm kiếm: Kết quả tìm kiếm về “AiFi Caineng” trên google. com và Bing Search trong ngày 31 tháng 12 năm 2021 như sau: Kết quả tìm kiếm từ khóa “aifi” và thậm chí “aifi caineng” quả là hơi nghèo nàn và dễ bị lẫn vào các từ khóa tìm kiếm khác như “wifi” chẳng hạn. Đây cũng là 1 thiếu sót do blog mới chỉ 1 năm, và tác giả vẫn đang bận bịu công việc chính cuả tác giả. Tuy nhiên, từ năm 2022, ở mức độ nhất định việc nâng rank trong các cỗ máy tìm kiếm từ khóa sẽ được tối ưu hóa nhằm đưa tri thức của AiFi đến với đông đảo bạn đọc và nâng cao chất lượng phục vụ. Good, Keep, Problem, TryViệc chạy sprint kéo dài 1 năm quả là hơi lạ, tuy nhiên là cũng dễ hiểu vì viết blog chỉ là việc phụ làm trong thời gian rảnh rỗi của tác giả.       Good   Keep   Problem   Try         Đã tạo được và thu hút lượng người dùng nhất định.    Duy trì tần suất chia sẻ bài viết.    Thứ hạng trên search engine chưa cao.    Tối ưu hóa SEO                   Tối ưu hóa từ khóa                   Tối ưu thẻ HTML, …               Chưa tạo ra thu nhập từ blog   Xem xét đưa vào và tối ưu hóa quảng cáo.        Các nguồn Google và Facebook đã đem đến lượng người dùng nhất định.    Tiếp tục duy trì quảng bá trên Google và Facebook.    Nguồn Facebook chưa đem lại nhiều người dùng mới.    Tối ưu hóa quảng bá blog trên Facebook.                Một số nguồn cấp khác như Twitter và LinkedIn vẫn chưa đem lại nhiều người dùng.    Lên chiến lược quảng bá trên các nền tảng này.    Tổng kếtKết thúc Sprint 2021, hướng tới Sprint 2022, blog AiFi xin cám ơn đông đảo bạn đọc, đặc biệt là 552 người dùng đã có, vì sự quan tâm và thịnh tình trong năm qua. Trong năm 2022, AiFi sẽ tiếp tục cập nhật và mong muốn lan tỏa tri thức cho anh em, với phương châm, troll trước học sau. "
    }, {
    "id": 42,
    "url": "https://wanted2.github.io/svelte-and-reset-checkboxes-10years-ago/",
    "title": "Nghịch ngợm Svelte và câu chuyện code JS thuần 10 năm trước",
    "body": "2021/12/10 - Svelte1 là một framework của ngôn ngữ Javascript, với mô hình bi-direction data binding (MVVM) khiến cho trạng thái và hiển thị luôn đồng bộ, cũng giống như những người anh em khác như Vue. js, ReactJS hay AngularJS. Tuy nhiên, một ưu điểm của Svelte là cấu trúc ngôn ngữ rất thanh thoát, khiến cho lượng code mà người lập trình cần viết rất ít. Lật lại lịch sử của các JS Framework, có thể lần lại từ khoảng 2006, khi JQuery ra đời với một bước tiến trong xử lý JS dạng embedding (nhúng) vào trong trang HTML. Với JQuery, mô hình lập trình là bạn có thể nhúng những snippet code JS vào trong HTML một cách thuận tiện và control hiển thị cũng như xử lý tại frontend. Cách làm này nhìn chung là thủ công, và rất nhiều lập trình viên JS đã hiểu rằng đây là con đường code thuần và hầu như là buggy (rất nhiều bug có thể phát sinh)!Lịch sử sang trang mới vào những năm 2010-2011, cùng với sự ra đời của hàng loạt trào lưu mới như Lean startups, Agile development, … với những JS framework theo luồng MVVM (Model-View-ViewModel). Backbone và AngularJS hầu như ra đời trong giai đoạn này đã mở ra kỷ nguyên mới, khi dữ liệu và hiển thị được binding với nhau khiến cho sự đồng bộ thống nhất trên frontend được đảm bảo. Công ty tôi thời đó khởi đầu cũng làm JQuery thuần, code vô cùng nhiều bugs, mà kỹ sư OT ngày đêm lo sửa. Nhưng đó là chuyện của hơn 10 năm trước các bạn ạ. Sau đó nhiều kỹ sư trong công ty đã cực lực phản đối việc tiếp tục dùng JQuery và đề xuất dùng những con đường bug-free hơn như AngularJS. Đó là những chuyện xảy ra vào năm 2013. Nhưng cùng lúc đó, 2013, Facebook, gã nhà giàu mới nổi, bắt đầu “o bế” khá nhiều công nghệ Web mới như ngôn ngữ lập trình Hack (hậu duệ của PHP) và Meteor một kiểu JS framework cũng thực hiện MVVM, data binding để đồng bộ hiển thị và dữ liệu tại frontend. Và cùng năm 2013, ReactJS, cũng là một JS framework được FB o bế cũng ra đời và làm mưa làm gió. Hồi đó cũng có tranh luận là lên tiếp tục chịu đựng JQuery hay chuyển sang data binding là AngularJS hoặc ReactJS mà cuối cùng có vẻ là AngularJS đã được chọn. Bẵng đi ít lâu, 2013, VueJS ra đời và nhanh chóng được chào đón. Sau đó gần đây là Svelte. Một ví dụ: hộp checkbox Check All/Uncheck All và bài toán reset hiển thị đúng cáchMột ngày đẹp trời năm 2012, một yêu cầu khách hàng tới: Hồi ấy có 1 câu chuyện là thế này. Nói thì dài dòng nhưng chủ yếu là làm chức năng Check All/Uncheck All mà bạn có thể nhìn thấy ở hình ảnh bên trên. Nó là chức năng bổ trợ cho Datatables mà chúng tôi có dùng. Cũng 10 năm rồi, tôi thú thực là cũng chả nhớ chi tiết lắm, nhưng đại để là có yêu cầu thế này:  Mỗi dòng phải có 1 checkbox, và mỗi checkbox là riêng nhau, không có chuyện click checkbox ở dòng 1 mà lại thay đổi hiển thị của dòng 2 là … giải tán.  Có nút Check All mà khi click vào thì tất cả các checkbox con chuyển thành ON hết.  Có nút Uncheck All mà khi chọn thì tất cả các checkbox con của mỗi dùng đều OFF hết.  Nút Check All và Uncheck All có thể làm một để tối ưu trải nghiệm người dùng.  Nếu tất cả các nút con đều ON thì CheckAll cũng phải ON.  Nếu có ít nhất 1 nút con là OFF thì CheckAll phải OFF.  Ngược lại với Uncheck ALL (nếu tách hai hộp check ra). Đấy, yêu cầu có thế rồi chuyển xuống cho đội hakken làm. Tức là phải reset chuẩn, không được để chọn Check All nhưng lại có dòng nào không được check (dù đã check rồi). Không được để check dòng này nhưng dòng khác lại bị ảnh hưởng. Lời giải 10 năm trước: Scope cũng khá nhẹ nhàng, nhưng lúc đó anh em lại chọn dùng JQuery và code thuần luôn. Đó quả thực là một bước đi gian khổ: thứ nhất, là vấn đề data binding, chuyện bind checkbox từng dòng vào hiển thị từng dòng mà lại làm bằng JS thuần thì đòi hỏi phải chơi trò gắn nhãn và selector. Tức là với dòng thứ n gắn nhãn id= row-n  chẳng hạn, sau đó dùng selector của JS hoặc JQuery như var el = document. querySelector( #row-n ); hoặc như JQuery thì đơn giản hơn $('#row-n'), để switch hiển thị.  Bạn nghĩ sao?  Thế tức là gắn nhãn bằng tay còn gì?  Vâng, đúng thế. Gắn nhãn bằng tay và select nhãn theo tên đã đặt.  Thế nếu miss một phát là toang, select nhầm nhãn là hiển nhiên có khả năng vì là con người làm. Rồi còn đặt tên mà không có quy tắc gây nhầm lẫn lúc select là TOANG nặng! Vâng, thú thực là lúc ấy mấy công nghệ MVVM frameworks như AngularJS/ReactJS vẫn đang trong vòng thai nghén của Google và Facebook, anh em cũng chưa có điều kiện tiếp cận, mới bắt buộc phải dùng JS thuần mà code. Chuyện 10 năm trước cơ mà!(Chứ còn năm 2021 này mà còn dùng là … không nên! Phải MVVM hết. ). Cách làm 1: Làm bằng JS thuầnĐánh nhãn bằng tay đương nhiên là không tốt. Để hiểu được lý do tại sao nên dùng framework và patterns có sẵn, chúng ta nên bắt đầu từ việc định tính hóa bài toán. Bài toán: Giả sử có $n$ dòng trong bảng. Mỗi dòng có một checkbox $c_i\in\{0,1\}$ (ở đây 0 là OFF còn 1 là ON). Trạng thái của Check All là $C\in\{0,1\}$ và tương ứng của Uncheck all là $\overline{C}=1-C$. Thì bài toán đồng bộ trạng thái ở đây là đảm bảo rằng ở mọi thời điểm:  $C=\bigcap_{i=1}^n c_i$                     (1) Điều này khá thú vị, việc đảm bảo phương trình (1) luôn xảy ra đòi hỏi phải quản lý trạng thái một cách chính xác. Để làm được điều này với JS thuần, đòi hỏi  Đánh nhãn chính xác, khó nhầm lẫn.  Không select nhầm nhãn.  Quản lý trạng thái ON/OFF phải đồng bộ.  Phải set checked ngay khi có onclick. Nhìn chung là một cách làm khá tồi tệ. Đặc biệt, vì số lượng biến trạng thái là $n+1$ nên khi $n$ càng lớn việc quản lý trạng thái càng khó khăn, dễ xảy ra bug, mistakes, … Cách làm 2: Làm bằng SvelteVới Svelte’s binding, việc quản lý trạng thái trở nên dễ dàng và ít mistakes hơn. Bạn chỉ cần định nghĩa biến trạng thái của checkboxes và bind chúng vào định nghĩa của &lt;input type= checkbox &gt; nhờ từ khóa bind:checked={}. Ví dụ như sau: &lt;input  type= checkbox   class= form-check-input   aria-checked= false   bind:checked={checkedAll}  on:click={onCheckedAll}/&gt;Như trên biến checkedAll đã được bind vào checkbox. Thậm chí cũng chả cần định nghĩa id cho textbox (tức là đánh nhãn) làm gì, bạn chỉ cần quản lý biến trạng thái checkedAll trong code của mình. Mà thao tác ấy chắc chắn sẽ tự động và ít bugs hơn đánh nhãn bằng tay kiểu JS thuần. Khi biến trạng thái thay đổi, hiển thị checkbox sẽ tự động thay đổi theo và bạn chẳng phải làm mấy cái thao tác set props hay gì mất thời gian mà lại dễ lỗi. Ví dụ bạn để set prop nhầm chỗ hoặc assign nhầm label một phát vì cách đặt tên của lập trình viên cũ quá tồi thì ôi thôi có mà … toang nặng. Đến đây, việc còn lại quá nhẹ nhàng với vài dòng code chơi: bạn định nghĩa một mảng checked cho trạng thái của $n$ biến trạng thái còn lại và bind:checked={} vào các checkbox ở từng dòng. Để thực hiện logic chuyển trạng thái, bạn chỉ cần lo viết code tương tác giữa biến $C$ và $c_i, i=1,n$ chứ chả phải lo ngồi set prop, hoặc set text làm gì mất công và dễ bug. Kết luậnNhìn chung cách làm bằng JS thuần là cách làm thiếu tính trừu tượng hóa, thủ công và đánh nhãn mệt nghỉ. Thay vào đó sự tiến bộ của công nghệ những năm 2010-2014 đã cho phép lập trình frontend web có những framework kiểu MVVM đủ mạnh như AngularJS, VueJS, ReactJS và gần đây là Svelte. Sử dụng Svelte để trừu tượng hóa trạng thái hiển thị thành biến (qua binding) và quản lý trạng thái thông qua quản lý biến là một điểm chốt khiến cho việc sử dụng Svelte (cũng như VueJS, ReactJS, AngularJS) chiếm ưu thế vượt trội so với cách làm thủ công JS thuần mà 10 năm trước chúng tôi hay làm. Tài liệu tham khảo      Docs • SvelteKit &#8617;    "
    }, {
    "id": 43,
    "url": "https://wanted2.github.io/chicken-and-egg-problem/",
    "title": "Quả trứng và con gà: cái nào có trước? - Bất bình đẳng về lương và sản lượng lao động",
    "body": "2021/11/20 - Con gà và quả trứng là một bài toán kinh điển trong thống kê học cổ điển liên quan tới nhân quả (causality) và luân hồi (feedback) của các chuỗi thời gian (time series). Năm 1969, Granger (Nobel kinh tế 2003) xuất bản một seminar paper [1] định tính hóa nhân quả và luân hồi giữa các chuỗi sự kiện thời gian (temporal time series). Việc kiểm tra nhân quả và luân hồi giữa các chuỗi thời gian được định tính và định lượng thông qua Granger verification. Năm 1979, Feige và Pearce [2] nghiên cứu về mối quan hệ luân hồi giữa tiền tệ và thu nhập, có sử dụng Granger verification. Năm 1988, Thurman và Fisher [3] nghiên cứu chuỗi thời gian về sản lượng trứng cũng như chuỗi dữ liệu về số lượng gà trên toàn nước Mỹ để tìm ra quan hệ nhân quả giữa trứng và gà. Họ sử dụng công thức của Granger và lần đầu tiên kết luận mang tính thống kê rằng trứng có trước và là nguyên nhân sinh ra gà. Phân tích các chuỗi sự kiện thời gian (time series analysis) và nhân quả/luân hồi giữa các chuỗi là một chủ đề truyền thống của thống kê và kinh tế học. Gần đây, những nghiên cứu cũng cho thấy sự tồn tại nhân quả giữa sản lượng lao động và bất bình đăng thu nhập hay bài toán Productivity-Pay Gap1. Thật thú vị rằng trong 40 năm qua, mặc dù sản lượng lao động bình quân năm tăng 61. 8%, nhưng lương của người lao động không hề tăng cao, dẫn tới bất bình đẳng thu nhập gia tăng (giá trị tạo ra thay vì đi vào túi người lao động lại tập trung vào túi của tầng lớp chóp bu trong xã hội Mỹ). Granger verificationCon gà và quả trứng [3], cũng như Productivity-Pay gap1 là hai ứng dụng cơ bản của Granger method [1]. Giả sử ta có chuỗi thời gian $\mathbf{X}=\{X_t\}_{t=-\infty}^{+\infty}$ với thời điểm $t=0$ là thời điểm bắt đầu quan sát. Ta kí hiệu chuỗi tín hiệu quá khứ của thời điểm $t$ là $\overline{\mathbf{X_t}}=\{X_{t-i}\}_{i=1}^{+\infty}$. Ngoài ra, chuỗi tín hiệu quá khứ và hiện tại của thời điểm $t$ là $\overline{\overline{\mathbf{X_t}}}=\{X_{t-i}\}_{i=0}^{+\infty}$. Ta cũng ký hiệu giá trị ước đoán điều kiện của $A_t$ trong một chuỗi $\mathbf{A}$ theo least-square errors khi có quan sát là chuỗi $\mathbf{B}$ là $P_t(\mathbf{A}\mid\mathbf{B})$. Chuỗi giá trị lỗi của một dự đoán là $\epsilon_t(\mathbf{A}\mid\mathbf{B})=A_t-P_t(\mathbf{A}\mid\mathbf{B})$. Ta hãy gọi $\sigma^2_t(\mathbf{A}\mid\mathbf{B})$ là phương sai của chuỗi giá trị lỗi $\epsilon_t(\mathbf{A}\mid\mathbf{B})$. Trong trường hợp này ta hãy giả sử tất cả các chuỗi đều là sóng dừng. Bây giờ, ta có thể thêm ký hiệu $\mathbf{U}$ là chuỗi thông tin vũ trụ (universe time series) mô tả mọi trạng thái trong quá khứ, hiện tại và tương lai của thế giới. Thì ứng với một chuỗi sự kiện cụ thể $\mathbf{Y}$ thì $\mathbf{U}-\mathbf{Y}$ là chuỗi thông tin toàn cầu mà bỏ đi thông tin của $\mathbf{Y}$.    Định nghĩa nhân quả: Nếu $\sigma^2(\mathbf{X}\mid\overline{\mathbf{U}})\leq\sigma^2(\mathbf{X}\mid\overline{\mathbf{U}-\mathbf{Y}})$ thì $\mathbf{Y}$ là nguyên nhân gây ra $\mathbf{X}$. Định nghĩa này của Granger khá là trực quan, nói nôm na là nếu xóa thông tin của $\mathbf{Y}$ khỏi hệ thống thông tin toàn cầu thì dự đoán sẽ có sai lệch lớn hơn, thì rõ ràng $\mathbf{Y}$ là 1 trong các nguyên nhân gây ra $\mathbf{X}$     Định nghĩa luân hồi: Nếu đồng thời $\sigma^2(\mathbf{X}\mid\overline{\mathbf{U}})\leq\sigma^2(\mathbf{X}\mid\overline{\mathbf{U}-\mathbf{Y}})$ và $\sigma^2(\mathbf{Y}\mid\overline{\mathbf{U}})\leq\sigma^2(\mathbf{Y}\mid\overline{\mathbf{U}-\mathbf{X}})$ thì $\mathbf{Y}$ là luân hồi $\mathbf{X}$. Định nghĩa này của Granger cũng khá là trực quan, nói nôm na là nếu $\mathbf{Y}$ là nguyên nhân của $\mathbf{X}$ mà $\mathbf{X}$ cũng là nguyên nhân của $\mathbf{Y}$, thì hai chuỗi có luân hồi nghiệp quả.     Nghiệp quả xa: ta nói $\mathbf{Y}$ là nghiệp quả xa của $\mathbf{X}$ nếu có thêm thông tin quá khứ và hiện tại của $\mathbf{Y}$ thì kết quả dự đoán trở nên chính xác hơn: $P(\mathbf{X}\mid\overline{\mathbf{U}},\overline{\overline{\mathbf{Y}}})\leq P(\mathbf{X}\mid\overline{\mathbf{U}})$.     Độ lệch của nghiệp quả: là giá trị số nguyên dương nhỏ nhất thỏa mãn  $m={\arg\min}_k\{\sigma^2(\mathbf{X}\mid\mathbf{U}-\mathbf{Y}(k))\leq\sigma^2(\mathbf{X}\mid\mathbf{U}-\mathbf{Y}(k+1))\}$, tức là chỉ cần biết tối đa $m$ giá trị gần nhất trong chuỗi quá khứ và hiện tại là đủ để dự đoán. Xem xét hai chuỗi theo mô hình white-noise như sau: $X_t=\sum_{j=1}^ma_jX_{t-j}+\sum_{j=1}^mb_jY_{t-j}+\epsilon_t,$ $Y_t=\sum_{j=1}^mc_jY_{t-j}+\sum_{j=1}^md_jX_{t-j}+\eta_t,$ Theo định nghĩa của nhân quả, nếu $\exists j~\mbox{s. t. }~b_j\neq 0$ thì $\mathbf{Y}$ là nguyên nhân của $\mathbf{X}$. Và ngược lại $\exists j~\mbox{s. t. }~d_j\neq 0$ thì $\mathbf{X}$ là nguyên nhân của $\mathbf{Y}$. Nếu đồng thời hai điều kiện trên xảy ra thì chúng là luân hồi của nhau. Kiểm tra giả thuyết $H_0: b_j=0\forall j$ chính là Granger verification. Trứng và GàNăm 1988, Thurman và Fisher [3] đã thực hiện Granger verification trên bộ dữ liệu chứa hai chuỗi dữ liệu: sản lượng trứng hàng năm $\mathbf{X}$và số lượng gà hàng năm $\mathbf{Y}$ trên nước Mỹ từ 1930–1983. Vì cả 2 chuỗi trên đều có thể giả định là iid, nên ta có thể viết lại mô hình nhân quả như sau: $X_t=\sum_{j=1}^mb_jY_{t-j}+\epsilon_t,$ $Y_t=\sum_{j=1}^md_jX_{t-j}+\eta_t,$ Chúng ta kiểm định hai giả thiết null như sau: $H_{0x}: b_j=0\forall j$ $H_{0y}: d_j=0\forall j$ Giả thiết $H_{0x}$ chính là trứng không là nguyên nhân của gà. Giả thiết $H_{0y}$ chính là gà không là nguyên nhân của trứng. Trên bộ dữ liệu trứng Mỹ và gà Mỹ, có thể bác bỏ giả thuyết đầu, nhưng không thể bác bỏ giả thuyết sau. Tựu chung lại là trứng Mỹ có trước và sinh ra gà Mỹ. Productivity-Pay GapHầu hết người Mỹ đều tin rằng kinh tế phát triển thì người lao động sẽ nhận được reward. Nhưng theo 1 nghiên cứu gần đây1 thì có vẻ tình hình thu nhập của người lao động Mỹ không phải như vậy. Từ năm 1979, nước Mỹ đã có những thay đổi lớn về chính sách kinh tế, trong đó có nhiều chính sách phục vụ cho người giàu:  Starting in the late 1970s, policymakers began dismantling all the policy bulwarks helping to ensure that typical workers’ wages grew with productivity. Excess unemployment was tolerated to keep any chance of inflation in check. Raises in the federal minimum wage became smaller and rarer. Labor law failed to keep pace with growing employer hostility toward unions. Tax rates on top incomes were lowered. And anti-worker deregulatory pushes—from the deregulation of the trucking and airline industries to the retreat of anti-trust policy to the dismantling of financial regulations and more—succeeded again and again. Những chính sách như giảm thuế cho những người thu nhập cao, … đã khoét sâu “hố ngăn” thu nhập giữa tầng lớp giàu và nghèo. Lấy mốc 1979 là 100% cho cả tổng sản lượng quốc nội $\mathbf{X}$ lẫn thu nhập bình quân của tầng lớp lao động $\mathbf{Y}$. Tức là $X_{1979}=Y_{1979}=1$. Thì rõ ràng là từ năm 1948 tới 1979, lương và sản lượng quốc nội đều cùng chiều tăng. Nhưng từ mốc 1979 với sự thay đổi về chính sách, tổng sản lượng quốc nội vẫn tăng tuyến tính nhưng lương của tầng lớp lao động đã chững lại. Nên nhớ tầng lớp lao động nói đến trong nghiên cứu này chiếm tới 80% dân số lao động của Mỹ (production and unsupervisory workers). Cũng nhưu ví dụ trứng và gà, cả 2 chuỗi trên đều có thể giả định là iid, nên ta có thể viết lại mô hình nhân quả như sau: $X_t=\sum_{j=1}^mb_jY_{t-j}+\epsilon_t,$ $Y_t=\sum_{j=1}^md_jX_{t-j}+\eta_t,$ Tuy nhiên, điểm mốc 1979 gần như là mốc thay đổi tất cả. Trước 1979, hai chuỗi song song tuyến tính phát triển và quan hệ nhân quả theo phép thử Granger sẽ cho kết quả chấp nhận (không bác bỏ). Từ 1979, chuỗi thu nhập của người lao động chững lại, do đó quan hệ nhân quả luân hồi giảm đi. Đây là một ví dụ cho thấy chính sách đủ mạnh có thể thay đổi quan hệ giữa hai chuỗi, khiến cho nhân quả luân hồi biến mất. Hai bên đi theo hai hướng độc lập riêng biệt, một bên tiếp tục phát triển lên cao, còn một bên chững lại và phát triển chậm. Kết luậnGranger verification là một câu chuyện thú vị. Hai ứng dụng cơ bản là trứng và gà đã được các nhà khoa học Mỹ phát kiến ra trong thế kỷ 20: trứng Mỹ sinh ra gà Mỹ (chứ không phải ngược lại). Và hầu như không có luân hồi trong quan hệ giữa trứng và gà trên đất Mỹ. Câu chuyện thứ hai là về sự thay đổi của chính sách đủ mạnh để làm mất nhân quả: năm 1979 là năm cột mốc với những người lao động Mỹ, khi bắt đầu từ đó, sản lượng quốc nội vẫn tăng tuyến tính nhưng tiền lương của người lao động tăng chậm. Một điểm lưu ý là các chuỗi thời gian trong giả thiết của Granger đều là sóng dừng nhưng gần đây có những nghiên cứu tổng quát cho mọi loại sóng. Tài liệu tham khảoGranger, C. W. J. 1969. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: journal of the Econometric Society. (1969), 424–438. DetailsFeige, E. L. and Pearce, D. K. 1979. The casual causal relationship between money and income: Some caveats for time series analysis. The Review of Economics and Statistics. (1979), 521–533. DetailsThurman, W. N. , Fisher, M. E. and others 1988. Chickens, eggs, and causality, or which came first. American journal of agricultural economics. 70, 2 (1988), 237–238. Details      The Productivity–Pay Gap - Economic Policy Institute &#8617; &#8617;2 &#8617;3    "
    }, {
    "id": 44,
    "url": "https://wanted2.github.io/git-merge-rebase/",
    "title": "So sánh git merge và git rebase",
    "body": "2021/10/08 - git merge và git rebase là 2 câu lệnh quen thuộc với lập trình viên chỉ để giải quyết cùng 1 bài toán: với 2 branches được phát triển song song, nay cần migrate các tính năng của branch feature vào branch chính (main). Vấn đề là làm thế nào? Một cách chi tiết thì chỉ có 2 cách tương ứng với 2 câu lệnh ở trên tiêu đề bài viết mà chúng ta sẽ đi sâu trong bài này. Đôi lời mởCó khá nhiều hướng dẫn về cách tích hợp 2 nhánh công việc khác nhau, với nhiều cái tên “mỹ miều” như migration, integration và những tài liệu dài hàng trang chỉ để giải thích các cách làm. Nhưng nếu là LTV lâu năm đều nhìn ra “tư duy” đằng sau và biết câu lệnh đơn thuần nhất để giải quyết: người mới có khi phải làm “bằng tay” cả khối công việc đó trong một vài tháng để chỉ hiểu được “tư duy” ẩn giấu, nhưng người đã biết thì chỉ cần 1 câu lệnh như git merge là giải quyết xong công việc. Bạn thử nghĩ xem, nếu chỉ cần 1 vài giây là chạy xong git merge nhưng nếu bạn không biết và phải làm theo nguyên tắc: tức là chi tiết hóa và làm toàn bộ chi tiết của git merge bằng tay thì liệu 1 tháng có đủ không? Vì vậy cách suy nghĩ đúng là chi tiết hóa rồi trừu tượng hóa. Chi tiết hóa giúp nhận ra vấn đề là gì, sau khi nắm rõ vấn đề, trừu tượng hóa giúp nhận ra “à với công việc này, thì sử dụng công cụ này sẽ chỉ cần 1 vài dòng lệnh là giải quyết vấn đề”. Chỉ chi tiết hóa thôi thì sẽ bị sa đà vào tiểu tiết, lãng phí công số. Chỉ trừu tượng hóa thôi thì sẽ có rủi ro là bị sai hướng vì không nắm rõ chi tiết. Trừu tượng hóa trước thì sẽ bị sai hướng dẫn đến chi tiết hóa hoàn toàn sai lệch. Vì vậy chỉ có cách suy nghĩ đúng là chi tiết hóa rồi trừu tượng hóa. The devil is in the detailsTại sao trừu tượng hóa trước thì không tốt? Đơn giản thôi: bởi để migrate tính năng, ngoài merge thì còn có rebase, nếu không chi tiết bài toán cũng như sự khác biệt giữa 2 câu lệnh thì có thể đi sai hướng và dùng sai công cụ. git merge: Đầu tiên chúng ta sẽ tìm hiểu cách thông dụng nhất và hay được khuyến khích dùng nhất để giải bài toán của chúng ta. Bản chất của git merge thì cũng như hình vẽ bên: tạo 1 commit mới hẳn và chuyển tất cả tính năng của feature/1 vào đó. Đây là lựa chọn mặc định của git merge, bạn có thể thay đổi bằng thêm --squash. Chi tiết câu lệnh merge thì như bên dưới có 2 options mặc định là --ff cho fast-forward và --commit (tức là tạo commit mới hay là merge commit). 1234567891011121314151617181920212223242526272829303132333435363738$ git merge -husage: git merge [&lt;options&gt;] [&lt;commit&gt;. . . ]  or: git merge --abort  or: git merge --continue  -n          do not show a diffstat at the end of the merge  --stat        show a diffstat at the end of the merge      --summary       (synonym to --stat)  --log[=&lt;n&gt;]      add (at most &lt;n&gt;) entries from shortlog to merge commit message  --squash       create a single commit instead of doing a merge  --commit       perform a commit if the merge succeeds (default)  -e, --edit      edit message before committing  --cleanup &lt;mode&gt;   how to strip spaces and #comments from message  --ff         allow fast-forward (default)  --ff-only       abort if fast-forward is not possible  --rerere-autoupdate  update the index with reused conflict resolution if possible  --verify-signatures  verify that the named commit has a valid GPG signature  -s, --strategy &lt;strategy&gt;             merge strategy to use  -X, --strategy-option &lt;option=value&gt;             option for selected merge strategy  -m, --message &lt;message&gt;             merge commit message (for a non-fast-forward merge)  -F, --file &lt;path&gt;   read message from file  -v, --verbose     be more verbose  -q, --quiet      be more quiet  --abort        abort the current in-progress merge  --quit        --abort but leave index and working tree alone  --continue      continue the current in-progress merge  --allow-unrelated-histories             allow merging unrelated histories  --progress      force progress reporting  -S, --gpg-sign[=&lt;key-id&gt;]             GPG sign commit  --autostash      automatically stash/stash pop before and after  --overwrite-ignore  update ignored files (default)  --signoff       add a Signed-off-by trailer  --no-verify      bypass pre-merge-commit and commit-msg hooksVậy quá trình sẽ xảy ra suôn sẻ? Đương nhiên conflict có thể xảy ra và dev sẽ phải resolve bằng tay, sau đó thì dùng git merge --continue để tiếp tục merge. Trong trường hợp đang merge mà gặp phải tình huống conflict không thể giải quyết được thì git merge --abort sẽ đưa bạn về thời điểm trước khi merge. --abort đương nhiên là điều không mong muốn, mà điều mong muốn là --continue đến khi xong. git merge nhìn chung là 1 quy trình an toàn nếu bạn chịu khó làm đến cùng. Nếu mới gặp conflict đầu tiên bạn đã sợ hãi --abort ngay thì sẽ không bao giờ đi đến cùng được. Đồng thời quy trình sẽ rất tự động nếu 2 branches không conflict nhiều. Vì vậy, trước khi quyết định có merge hay không thì tốt nhất nên điều tra sơ xem có nhiều file trùng nhau giữa lịch sử 2 branches với base không?Nếu nhìn git diff sơ mà thấy khác nhau nhiều quá là có lẽ … không nên cho phép merge. Bởi như vậy không khác gì làm bằng tay (cứ tí lại gặp conflict phải resolve)! Tuy nhiên, so với giải pháp tiếp theo mà chúng ta giới thiệu thì git merge có ít rủi ro gặp conflict hơn và nhìn chung số lần resolve chỉ là $\leq 1$, nên git merge luôn luôn là giải pháp ưu tiên cho bài toán của chúng ta. git rebase: Một lựa chọn khác cho bài toán của chúng ta là git rebase. git rebase sẽ không tạo ra commit mới nào cả mà sẽ sửa lại lịch sử git của main. Như hình vẽ bên trái, bạn thấy rõ là các commit của feature/1 sẽ được bố trí xen kẽ theo thứ tự thời gian vào main. Bắt đầu từ commit đầu tiên của feature branch thì thực hiện merge và nếu gặp conflict thì dev phải resolve bằng tay rồi --continue. Cũng như git merge bạn có thể --abort để quay lại trạng thái ban đầu nếu … sợ!Bạn cũng có thể --quit, nhưng nhớ là nếu --abort dọn dẹp để đưa branch main về trạng thái ban đầu thì --quit sẽ để lại mớ hỗn độn nguyên xi đó. Trong git merge thì dù 2 branches conflict nhiều thế nào thì cũng chỉ cần 1 merge commit để chứa tất cả. Nhưng bạn thấy đấy với git rebase số lần conflict có thể tỷ lệ thuận với số commit của hai branches. Và công việc sẽ không khác gì bằng tay nếu hai branches conflict nhiều!Nhìn chung đây là cách làm nguy hiểm hơn git merge rất nhiều. Bạn có thể dùng git rebase -i để chọn lựa những commit muốn bỏ vào main nếu không phải tất cả mọi commit trên feature/1 đều có giá trị với bạn. Sau khi chỉnh sửa lịch sử thì bạn có thể cập nhật branch bằng git push -f. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ git rebase -husage: git rebase [-i] [options] [--exec &lt;cmd&gt;] [--onto &lt;newbase&gt; | --keep-base] [&lt;upstream&gt; [&lt;branch&gt;]]  or: git rebase [-i] [options] [--exec &lt;cmd&gt;] [--onto &lt;newbase&gt;] --root [&lt;branch&gt;]  or: git rebase --continue | --abort | --skip | --edit-todo  --onto &lt;revision&gt;   rebase onto given branch instead of upstream  --keep-base      use the merge-base of upstream and branch as the current base  --no-verify      allow pre-rebase hook to run  -q, --quiet      be quiet. implies --no-stat  -v, --verbose     display a diffstat of what changed upstream  -n, --no-stat     do not show diffstat of what changed upstream  --signoff       add a Signed-off-by trailer to each commit  --committer-date-is-author-date             make committer date match author date  --reset-author-date  ignore author date and use current date  -C &lt;n&gt;        passed to 'git apply'  --ignore-whitespace  ignore changes in whitespace  --whitespace &lt;action&gt;             passed to 'git apply'  -f, --force-rebase  cherry-pick all commits, even if unchanged  --no-ff        cherry-pick all commits, even if unchanged  --continue      continue  --skip        skip current patch and continue  --abort        abort and check out the original branch  --quit        abort but keep HEAD where it is  --edit-todo      edit the todo list during an interactive rebase  --show-current-patch show the patch file being applied or merged  --apply        use apply strategies to rebase  -m, --merge      use merging strategies to rebase  -i, --interactive   let the user edit the list of commits to rebase  --rerere-autoupdate  update the index with reused conflict resolution if possible  --empty &lt;{drop,keep,ask}&gt;             how to handle commits that become empty  --autosquash     move commits that begin with squash!/fixup! under -i  -S, --gpg-sign[=&lt;key-id&gt;]             GPG-sign commits  --autostash      automatically stash/stash pop before and after  -x, --exec &lt;exec&gt;   add exec lines after each commit of the editable list  -r, --rebase-merges[=&lt;mode&gt;]             try to rebase merges instead of skipping them  --fork-point     use 'merge-base --fork-point' to refine upstream  -s, --strategy &lt;strategy&gt;             use the given merge strategy  -X, --strategy-option &lt;option&gt;             pass the argument through to the merge strategy  --root        rebase all reachable commits up to the root(s)  --reschedule-failed-exec             automatically re-schedule any `exec` that fails  --reapply-cherry-picks             apply all changes, even those already present upstreamNói chung chỉnh sửa lịch sử là quá trình rất nguy hiểm!Vì vậy cần chi tiết hóa trước để tránh đi theo những cách bất thường, sau đó trừu tượng hóa để tìm ra công cụ tối ưu. Lời kếtBài viết lấy ví dụ về bài toán migrate tính năng từ feature/1 vào main. Bạn nên nhớ những Gitflow thực tế sẽ còn phức tạp hơn với những quy tắc như chỉ develop mới được merge vào main, tuy nhiên đó là chủ đề khác. Ngoài ra, nếu bài toán của bạn là bất thường, đòi hỏi phải edit history thì có thể bạn phải dùng rebase. Nhưng nhìn chung trong các trường hợp bình thường, tôi thấy hầu như đều dùng git merge tức là tạo ra merge commit mới hẳn và chuyển hết tính năng cần thiết vào. Thêm nữa, đây là migrate tính năng chứ không chỉ là files hay tài nguyên, tức là các tính năng được implement trên feature/1 phải tái hiện như thế trên main.  Nói chung tạo cái mới thì tốt hơn! "
    }, {
    "id": 45,
    "url": "https://wanted2.github.io/c4-diagrams-software-design/",
    "title": "C4 diagrams for software architecture visualization - Context, Containers, Components, and Code -",
    "body": "2021/09/03 - Using MS Word and Excel, an engineer can describe the system in language and visuals. Such conventional methodologies can be sufficient for small architecture with few components. When the software system scales and changes frequently, maintaining records of the architecture by documents can be tedious. Hence, the lack of interaction in conventional document methods may present a stiff learning curve for a new member to the project team. To cope with these challenges, the C4 diagram model12, [1] was created to give interactive views of the software architectures. Systems are modeled at four different levels: system context, containers, components, and code. These levels are represented by diagrams. Intuitively, we can see the C4 diagrams as large visualization systems in which engineers can zoom in and out to see the details and the big picture. The C4 modelThe cure of representation: Knowledge sharing is important in software projects. Using representations like documents and diagrams, teams can keep the understanding of the product identically among team members. Also, having such powerful representations can keep stakeholders motivated. A good representation of the software projects, especially the architectures, helps audiences to capture the mechanism of the project at a glance.  A good representation should be lightweight but doesn’t omit any aspects of the software product. It should contain as many details as possible but also must be compact and fast. I had experiences in several projects where knowledge sharing is important. People realized the need, and they tried to share by different means: verbal conversations (i. e. , meetings or discussions), visual communications (presentations and diagrams), and textual things (documents by MS Office, notes, …). However, things will be like “a ton of documents” have been produced, and new members need to read all when joining the project. Then when a business staff wants to have a short and compact description to put in presentations (and will give the presentation to the clients or investors), someone will need to do the summarization after reading the whole bundle. Such textual summary was a good means for knowledge sharing, but the compression ratio was not quite good: several sentences can capture only some aspects of the projects, and then QA sections always come for clients and members to understand more. 1913 Piqua Ohio Advertisement - One Look Is Worth a Thousand Words. Source: Wikipedia Another direction to find a compact representation is using visual information.  A picture is worth a thousand words. Diagrams and interactions are always good things. Some studies have shown that children will learn faster by visual representations like pictures and interesting pictures. Finding such a compact and meaningful visual representation also needs to compress the whole architecture into small diagrams which are organized hierarchically. In other words, architects who find such representations also need to refine, prune and search for compact architectures to put into the compact diagrams. Source: c4model. com Therefore, the search for good representations faces the tradeoff between the learning cost and the compactness of representations. What C4 model provides is a compact, interactive and hierarchical representation of any software architecture. The model divides the architectural diagrams into four levels: context, containers, components, and code. Low-level representations such as code and components represent details for implementations and maintenance. Thus, they should be used by developers. High-level representations such as context and containers represent an overview of the architecture such as system diagrams and container architecture (container here is not Docker!). Another aspect of a good representation is handling of changes. Projects always change: requirements change, designs change, people change, code change, architectures change, . etc. Then the representations will change. Having a general model which is valid for many software architectures is a challenge. Neither searching for such an architecture can be done soon, but for the short-term thinking, we need a representation that needs to change only a little even when the architectures change a lot.  A good representation must be not only robust to changes in micro-level details (code and components designs) but also persistent to changes at macro-level architectures and management. Metamodel and notations: The author of C4 model Simon Brown once talked about the invention as follows.  The C4 model was created as a way to help software development teams describe and communicate software architecture, both during up-front design sessions and when retrospectively documenting an existing codebase. It’s a way to create maps of your code, at various levels of detail, in the same ways you would use something like Google Maps to zoom in and out of an area you are interested in.  Although primarily aimed at software architects and developers, the C4 model provides a way for software development teams to efficiently and effectively communicate their software architecture at different levels of detail, telling different stories to different types of audiences when doing up-front design or retrospectively documenting an existing codebase.  The C4 model consists of a hierarchical set of software architecture diagrams for context, containers, components, and code. Elements and relationships: The following elements and relationships form the diagrams in C4. Notations and metamodel are described.       Terms   Description   Notation   Parent   Properties         Person   A person represents one of the human users of your software system (e. g. , roles, personas, etc. ).       -   Name, Description, Location (Internal or External)       Software system   A software system is the highest level of abstraction and describes something that delivers value to its users, whether they are human or not. It should be something large, contains all smaller levels of abstractions such as software containers.       -   Name, Description, Location (Internal or External), The set of containers that make up the software system       Container   An application or data store. A container is essentially a context or boundary inside which some code or some data is stored.       A software system   Name, Description, Technology, The set of components within the container       Component   A component is a grouping of related functionality encapsulated behind a well-defined interface.       A container   Name, Description, Technology, The set of code elements (e. g. classes, interfaces, etc. ) that the component is implemented by       Code   This is the lowest level in C4. The diagrams here show the details of code elements (e. g. , classes, interfaces, objects, functions, etc. ).    -   A component   Name, Description, Fully qualified type       Relationship   Relationships are permitted between any elements in the model in either direction.       -   Description and Technology   Views: The C4 model consists of 4 basic views with respect to 4 levels of diagrams.       View type   Scope   Permitted elements   Examples         1. System Context   A software system.    Software systems, People          2. Container   A software system   Software systems, People, Containers within the software system in scope          3. Component   A container   Software systems, People, Other containers within the parent software system of the container in scope, Components within the container in scope          4. Code   A component   Code elements (e. g. , classes, interfaces, etc. ) that are used to implement the component in scope       System Context diagrams describe business usecases in which the interaction between users (people) and the software system is visualized. By seeing system context, stakeholders catch the key use-cases of the system, how end-users will use the system, and so on.  Container diagrams visualize the architecture of each container. Formally, it is a diagram of users and containers.  Component diagrams visualize the internal architecture of a container with components are atomic elements.  Code diagrams show classes, interfaces, objects, and relationships. One can use UML diagrams to visualize. The class diagram or the ER diagram can be examples. Supplementary diagrams: Besides four basic views, there are several supplementary views for the C4 model to capture the dynamic and the big picture of software systems.  The System Landscape diagram shows the target software systems in a landscape with other related systems. For example, the target system is an Internet Banking System. It needs to interact with other systems in the banks like Email Systems, Mainframe Banking Systems, and other roles like Customer Service Staff and Back Office Staff. To do this, add another diagram that sits “on top” of the C4 diagrams to show the system landscape from an IT perspective. Source: c4model. com    The Dynamic diagram considers how elements in a static model collaborate at runtime to implement a user story, use case, feature, etc. One can reuse the UML communication diagram to show it.     The Deployment diagram is based on the UML deployment diagram. It illustrates how software systems and/or containers in the static model are mapped to infrastructure.  ExamplesTools: Tools for diagramming can be a lot. For only diagramming, I would like to recommend the Diagrams. net, which is very convenient for drawing system architecture. It has many toolboxes for drawing deployment diagrams, AWS/GCP/Azure Cloud architectures, etc. For modeling the software architectures with diagramming, one can refer to Archi2. A tutorial: One way to learn to draw is through the tutorial video. The following tutorial introduces all about C4 model by the author with a part of PlantUML use-case with C4. ConclusionThe C4 model solved the problem of a compact representation for software architectures without disregarding any aspects of the product. The model was designed for Agile projects and to keep the team communicating better with team members as well as stakeholders. ReferencesBrown, S. 2013. Software architecture for developers. Coding the Architecture. (2013). Details      The C4 model for visualising software architecture &#8617;        Archi – Open Source ArchiMate Modelling &#8617; &#8617;2    "
    }, {
    "id": 46,
    "url": "https://wanted2.github.io/liskov-substisution-principle/",
    "title": "The Last Pillar: The Liskov Substitution Principle",
    "body": "2021/08/29 - In 1988, Barbara Liskov [1] wrote about the substitutions of software modules:  What is wanted here is something like the following substitution property: If for each object $o_1$ of type $S$ there is an object $o_2$ of type $T$ such that for all programs $P$ defined in terms of $T$, the behavior of $P$ is unchanged when $o_1$ is substituted for $o_2$ then $S$ is a subtype of $T$. The Liskov Substitution Principle (LSP, [2], [1]) states that a software module should be built up from interchangeable parts. Any violations of the principle lead to confusion and horrible mistakes in production. In object-oriented designs, if different classes and modules behave differently, then one should not be the abstraction of the others. The principle is helpful for designs of inheritance and REST API. Liskov Substitution PrincipleThe following diagram shows a design that conforms LSP. The interface License has two implementations: PersonalLicense and BusinessLicense. Although BusinessLicense has a custom property named users, both implementations do not have custom behaviors compared to behaviors of License (which has only one method: calculateFee()). Therefore, these implementations are interchangeable as License. They are subtypes of License.  The Billing class does not depend on the concrete implementations of the interface License, and this is one of the benefits of LSP. Both are substitutable to License. Examples of violations Violations are bad! We should learn to avoid these negativities. In the previous section, we learn a positive example of the Liskov Substitution Principle (LSP). In this section, we will learn negative examples where LSP is violated. Ducks and Toys: The following design violates the LSP. In this design, a DuckToy can quack() like a Duck. However, when the remainBatteryAmount=0, it can raise an exception!Neither the Duck can raise such an exception, so the behaviors are different. The DuckToy is not a subtype of Duck. So consider the following code: 1234567891011121314class Duck { public:  Duck() {}  virtual ~Duck() = default;  void quack() {   std::cout &lt;&lt;  Just quacking . . .   &lt;&lt; std::endl;  }};Duck duck;EXPECT_NO_THROW({ duck. quack();});This code will work fine with Duck but let us substitute it with DuckToy: 1234567891011121314151617181920class DuckToy : public Duck { public:  DuckToy(int battery) : remainBatteryAmount(battery) {}  DuckToy() : remainBatteryAmount(0) {}  virtual ~DuckToy() = default;  void quack() {   if (this-&gt;remainBatteryAmount &lt; 1) {    // throw an exception   }   std::cout &lt;&lt;  Just quacking . . .   &lt;&lt; std::endl;  } private:  int remainBatteryAmount = 0;};DuckToy duckToy; // remainBatteryAmount = 0EXPECT_NO_THROW({ duckToy. quack(); // this will throw an exception});In class design, conforming LSP is more than drawing a diagram but also enforces the code: programmers must write a clean inheritance. An API design: Assume that we have a taxi driver management system. Each driver identity has a dispatch URI in the driver database. For example, we have a driver Bob, and the dispatch URI is: 1aificorp. in/driver/BobWhen there is a new request from a customer assigned to Bob, the system dispatches all information needed for a pickup like 12aificorp. in/driver/Bob/ pickupAddress/%s/pickupTime/%s/destination/%sNote that aificorp. in is the domain of a partner company, and it is different among partners. Assume that developers in aificorp. in are doing their job good. The problem arises when a new developer joins the team in a partner acme. com, and the new member dispatches the destination by the abbreviation dest! 12acme. com/driver/Alice/ pickupAddress/%s/pickupTime/%s/dest/%sWhat is the problem?That is, now we need to add an exception in our system for this partner only: 123if (partnerDomain. rfind( acme. com , 0) == 0) {  // handle the exception}Now our dispatches table is as follows.       URI   Dispatch format         Acme. com   /pickupAddress/%s/pickupTime/%s/dest/%s       *. *   /pickupAddress/%s/pickupTime/%s/destination/%s   And so, our architect has had to add a significant and complex mechanism to deal with the fact that the interfaces of the restful services are not all substitutable. ReferencesLiskov, B. 1988. Data Abstraction and Hierarchy. SIGPLAN Notices. 23, 5 (1988), 17–34. DetailsMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details"
    }, {
    "id": 47,
    "url": "https://wanted2.github.io/aggregation-segregation/",
    "title": "Aggregation and Segregation",
    "body": "2021/08/28 - Interface Segregation Principle (ISP) [1] is one of the five pillars in SOLID design principles. The main spirit of ISP is that user interfaces shouldn’t rely on features or operations they don’t need. Therefore, instead of designing an aggregated interface that contains functions for various member classes, we should segregate the functions into several sub-interfaces, for which each type of user only needs to a specific feature. When other features change, the current feature is not affected, and the user service will not be interrupted. AggregationLet’s consider the following interface. There is a HumanInst class which has three operations calculate to calculate money, walk for walking, and ride for riding. A Police class only needs calculate operation to calculate the money they collected from criminals. A Pedestrian class only needs walk, and a Rider only needs ride. However, in this aggregated design HumanInst, all actors have access to functions they don’t need. The implementation of this aggregation can be found below: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#ifndef _AGGREGATION_H_#define _AGGREGATION_H_#include &lt;iostream&gt;#include &lt;string&gt;class Human {public:  Human() {}  virtual ~Human() = default;  virtual void calculate() = 0;  virtual void walk() = 0;  virtual void ride() = 0;};class HumanInst : public Human {public:  HumanInst() {}  virtual ~HumanInst() = default;  void calculate() {    std::cout &lt;&lt;  Calculating . . .   &lt;&lt; std::endl;  }  void walk() {    std::cout &lt;&lt;  Walking . . .   &lt;&lt; std::endl;  }  void ride() {    std::cout &lt;&lt;  Riding . . .   &lt;&lt; std::endl;  }};class PoliceInst : public HumanInst {public:  PoliceInst() {}  virtual ~PoliceInst() = default;};class PedestrianInst : public HumanInst {public:  PedestrianInst() {}  virtual ~PedestrianInst() = default;};class RiderInst : public HumanInst {public:  RiderInst() {}  virtual ~RiderInst() = default;};#endifAnd the main function: 1234567891011#include  aggregation. hpp int main() {  PoliceInst police;  police. calculate();  PedestrianInst pedestrian;  pedestrian. walk();  RiderInst rider;  rider. ride();  return 0;}We can find the result: 123456$ mkdir build &amp;&amp; cd buld &amp;&amp; cmake . . $ cmake --build . $ . /Debug/aggregation. exeCalculating . . . Walking . . . Riding . . . SegregationThe Interface Segregation Principle (ISP, [1]) states that the aggregated HumanInst class in the previous section was not good enough:whenever changes happen in one function, it will force the whole to be re-compiled, and then affect all other functions and actors. The ISP guides us that the following design will be better. A Police interface (actually, in C++ we don’t have interfaces, but we can use abstract class instead), or a PoliceInst class can have only one calculate function and doesn’t rely on walk and ride. The aggregation can be persisted by making HumanInst class inherited from all Police, Pedestrian and Rider. Let’s see the implementation: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#ifndef _SEGREGRATION_HPP_#define _SEGREGRATION_HPP_#include &lt;iostream&gt;#include &lt;string&gt;class Police {public:  Police() {}  virtual ~Police() = default;  void calculate() {    std::cout &lt;&lt;  Calculating . . .   &lt;&lt; std::endl;  }};class PoliceInst : public Police {public:  PoliceInst() {}  virtual ~PoliceInst() = default;};class Pedestrian {public:  Pedestrian() {}  virtual ~Pedestrian() = default;  void walk() {    std::cout &lt;&lt;  Walking . . .   &lt;&lt; std::endl;  }};class PedestrianInst : public Pedestrian {public:  PedestrianInst() {}  virtual ~PedestrianInst() = default;};class Rider {public:  Rider() {}  virtual ~Rider() = default;  void ride() {    std::cout &lt;&lt;  Riding . . .   &lt;&lt; std::endl;  }};class RiderInst : public Rider {public:  RiderInst() {}  virtual ~RiderInst() = default;};class HumanInst : public Police, public Pedestrian, public Rider {public:  HumanInst() {}  virtual ~HumanInst() = default;};#endifAnd the main function: 1234567891011121314151617#include  segregation. hpp int main() {  PoliceInst police;  police. calculate();  PedestrianInst pedestrian;  pedestrian. walk();  RiderInst rider;  rider. ride();  std::cout &lt;&lt;  Human also can calculate, walk and ride:  &lt;&lt; std::endl;  HumanInst human;  human. calculate();  human. walk();  human. ride();  return 0;}we see the result again: 12345678910$ mkdir build &amp;&amp; cd buld &amp;&amp; cmake . . $ cmake --build . $ . /Debug/segregation. exeCalculating . . . Walking . . . Riding . . . Human also can calculate, walk and ride:Calculating . . . Walking . . . Riding . . . This design is perfect as it keeps the aggregation and resolves the segregation issues at the same time. A sample CMakeLists. txt: 1234567cmake_minimum_required(VERSION 3. 0)project(isp CXX)include_directories(${PROJECT_SOURCE_DIR})add_executable(aggregation main. cpp)add_executable(segregation main_seg. cpp)ReferencesMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details"
    }, {
    "id": 48,
    "url": "https://wanted2.github.io/mobile-and-web/",
    "title": "Phân tích xu hướng: Tại sao các hiện tượng thành công gần đây lại ít có phiên bản web hơn ứng dụng mobile?",
    "body": "2021/08/26 - Cũng phải 9-10 năm trước đây, khi mới bắt đầu làm các nền tảng với mong muốn xây dựng “một phút huy hoàng rồi vụt tắt”, tôi hay thấy các nhà phát triển lựa chọn cùng lúc phát triển 3 phiên bản: Web, mobile (Android/iOS) và một bản có thể desktop. Đó là những năm 2012 và việc đó chủ yếu để tăng độ phủ của dịch vụ. Nhưng năm nay là năm 2021, và chúng ta không thể hướng dẫn các em trẻ bằng bộ não của những “ông già” mà thời huy hoàng từ những năm 2012. Năm 2021, chúng ta cũng cần cập nhật và nhìn lại những ứng dụng thành công trong 2 năm qua của kỷ nguyên COVID, tôi chợt nhận ra:ứng dụng ClubHouse thành công gọi vốn vòng C tương đương 100 triệu đô lẻ nhưng không hề cung cấp giao diện web, chỉ toàn ứng dụng mobile. Ứng dụng na ná thế của Việt Nam là onMic cũng không thèm chơi với web luôn!Ồ, chúng ta đã già rồi với giới trẻ có lẽ đã không còn mặn mà với việc xây một phiên bản cho desktop browser nữa. Năm 2021 phải là thời đại của mobile. Chúng ta có lẽ đã sai, và cũng nên nhìn nhận lại khả năng của mobile apps (native chứ không phải nhúng mấy cái Javascript vớ vẩn)! Responsive web apps và mobile appsTổng quan: By 2020, 90% dân số thế giới trên 6 tuổi (6,1 tỷ người) có sử dụng điện thoại di động. Nguồn: TheNextWeb Chúng ta luôn tự hỏi tại sao ngày nay lại phải học làm những thứ có thể hiển thị và hoạt động được trên mobile?Câu trả lời là vì chỉ trong vài năm, số lượng thuê bao di động trên thế giới đã phủ khắp đến 90% dân số thế giới, tính tới 2020. Điện thoại di động không phải quá khứ hay tương lai mà là hiện tại ngay trước mắt. Giải pháp để khiến dịch vụ của bạn phủ rộng tới tất cả người dùng di động hiện tại chỉ có hai nhánh chính: responsive web apps và mobile apps. Những năm 2012, chúng ta, những “ông già” phải phát triển song song 3 phiên bản vì lúc đó tỷ lệ mobile chỉ tầm 1. 3 tỷ/6 tỷ dân, tức là khoảng 23% hơn. Responsive web apps thì chủ yếu là HTML5/CSS3 có những tinh chỉnh để vừa vặn vào màn hình mobile. Lợi thế cũng khả rõ ràng:  Một phiên bản cho tất cả người dùng. cách làm web apps thì chỉ cần browser và quan trọng nhất là kết nối Internet. Ngắt mạng là … giải tán! Vận hành và bảo trì đơn giản hơn vì trang web dễ nâng cấp, bảo trì và fix lỗi. Người dùng thậm chí còn không nhận được thông báo khi xảy ra nâng cấp.  Phiên bản web rẻ hơn. SEO cũng dễ dàng hơn, và bảo trì cũng sẽ rẻ hơn. Mobile apps là những ứng dụng có thể cài đặt lên điện thoại di động và máy tính bảng. Ứng dụng di động ưu thế nổi trội sẽ là trải nghiệm người dùng tốt hơn. Nếu như trang web nhắm tới tìm kiếm người dùng mới (vì vậy không tạo barrier trong cài đặt nâng cấp), thì ứng dụng mobile tập trung vào người dùng trung thành. Thường không nên nghĩ ứng dụng mobile chỉ là bản clone của web app, nó sẽ là một suy nghĩ sai lầm!Và cuối cùng cái quan trọng là push notification, đó là ưu thế không thể từ chối của mobile apps. Ưu thế của mobile apps tập trung vào:    Tập trung vào tính năng đặc biệt của nền tảng mobile. Ví dụ người dùng Instagram có thể xem ảnh trên web vô tư nhưng để upload họ phải dùng mobile apps.   Mobile apps hoạt động trực tiếp với nền tảng, đòi hỏi cài đặt và nhận được những hỗ trợ tốt hơn về security cũng như tình năng nền tảng OS Mobile apps có thể hoạt động offline. Còn nói thực, tôi cũng làm nhiều với đủ kiểu web apps, web socket, mà nói chung cứ ngắt net là phiền! Không có Internet, đám webapps chỉ để nhìn.  Tối ưu hóa trải nghiệm người dùng Cung cấp các component native của mobile phone như network, voice, …. Lập trình web hiện đại với Vue. JS hay reactJS có khá nhiều web component, nhưng nhìn chung cũng có giới hạn và hoạt động cũng không tốt ví dụ khi ngắt net!Summary: Tạo tạm một bảng so sánh tính năng giữ hai sự lựa chọn để bạn đọc tham khảo.       Perspective   Responsive website   Mobile app         Compatibility   Mobile version of the site is equally displayed in all browsers, despite the device model.    Requires development of several applications for various platforms.        Audience   All devices that have access to the Internet.    Only smartphones and tablets.        Cost of entering the market   Payments for domain and hosting.    Developer licenses in the app stores.        Ease of use   Doesn’t require download and installation.    Requires downloading and installation.        Working offline   Not all devices support.    Possible.        Support, updates and bug fixing   Easy to update, maintain and fix bugs.    Difficult to update and monitor the application after it’s downloaded. Bugs will be fixed only in the next version.        Convenience of regular using   Average.    Good for regular using.        Personalization   Average. Mobile site is more focused on the service.    Good. App is more aimed at the individual user.    Giới thiệu ứng dụng voice chat room ClubHouseThế giới đang chuyển dần từ service-centric sang user-centric, từ web-based sang mobile-based. Hiện tượng ClubHouse nổi lên từ thàng 4/2020, tức là mới hơn 1 năm. Chúng ta sẽ nhìn nhận lại giá trị của ClubHouse trong ngành dọc: tức là các mạng xã hội âm thanh. Mạng xã hội âm thanh: Mạng xã hội âm thanh là mạng xã hội dựa trên việc chia sẻ âm thanh chứ không phải hình ảnh hay video. Các mạng xã hội chia sẻ âm nhạc như Zing MP3, hay Spotify đã tồn tại từ lâu và với hình thức chia sẻ file. Một đặc điểm của ClubHouse chính là việc chia sẻ thông qua nói chuyện tức là âm thanh cuộc nói chuyện được chia sẻ. Người tham gia không nhìn thấy mặt nhau là đặc điểm khiến cho người dùng có thể tập trung vào nội dung nói chuyện hơn là nhìn sắc mặt nhau. Hiện tượng ClubHouse: Tại sao ClubHouse lại thành hiện tượng như vậy?Bạn nên nhớ khi đạt giá trị 100 triệu đô, Facebook đã sở hữu 5,5 triệu người dùng vào năm 2005. Nhưng cùng mức giá ấy, ClubHouse chỉ có vỏn vẹn 5000 người dùng vào năm 2020. Thời cổ đại nếu một đội quân 5000 người dù dũng mãnh thế nào mà được xếp ngang hàng với đội quân 5,5 triệu người thì đó cũng là chuyện khá hiếm. Twitter khi đạt giá trị 100 triệu đô cũng có 1,2 triệu người dùng, và Instagram cũng có 1,7 triệu MAU.  Vậy cái gì làm nên giá trị của ClubHouse?ClubHouse đã thiết kế UX tức là trải nghiệm người dùng theo nguyên lý FOMO (Fear of missing out, tức là cảm giác bị lãng quên), khiến cho engagement của người dùng được nâng cao. Thiết kế này đòi hỏi 3 tính năng nhất quyết bị loại bỏ (không được implement) là: chức năng mời, chức năng lưu trữ archive, và chức năng nhận xét comment. Việc thiếu 3 chức năng, cùng với việc thiết kế room chat để chỉ có thể nghe người mình muốn nói chuyện vào lúc nửa đêm khiến người dùng càng gắn bó với app. Bởi vì họ luôn ở trạng thái cảm thấy bị miss mất cái gì đó nên sẽ không dám đóng app. Vậy tiềm năng của ClubHouse có thực sự đáng giá?Cái này có lẽ phải để thời gian trả lời. Cá nhân tôi khi dùng thử phát hiện ra một điểm tuyệt vời là thiết kế UX rất đơn giả và ưu việt. Họ không thiết kế kiểu ôm đồm mọi tính năng, mà tất cả đều tuân theo một thiết kế UX có trọng tâm. FOMO là một ví dụ về việc: không phải cứ implement hết mọi tính năng thì sẽ tốt mà đôi khi bỏ đi không thèm implement một số tính năng lại đem lại trải nghiệm tuyệt vời. Thế nên cũng không cần làm hết đâu!Ví dụ họ cũng không cần web vì đúng là với chiến thuật của họ thì mấy cái Vue với React cũng chả để làm gì cả (90% dân số thế giới đã có mobile trên tay và native app thì còn lo gì ngắt mạng) :yum: Tại thời điểm ClubHouse gọi được 100 triệu đô vốn đầu tư thì cũng có nhiều ý kiến phản hồi là tại sao lại giá cao thể trong khi chỉ có 5000 users?Hãy để thời gian minh chứng cho kết cục của startup này. Tuy nhiên, cái chúng ta học được ở đây đó là tối ưu trải nghiệm người dùng trên mobile đang là xu hướng, và web sẽ không làm được việc đó. "
    }, {
    "id": 49,
    "url": "https://wanted2.github.io/idle-time-pricing-models/",
    "title": "Idle Time Pricing Models in the Cloud",
    "body": "2021/08/19 - Batch processing (JP: バッチ処理, VI: Xu Lý Lô) is the processing of jobs that can run without end-user interaction or can be scheduled to run as resources permit. A program that reads a large file and generates a report, for example, is considered to be a batch job. Services such as Azure Batch [1] and AWS Batch [2], [3] support runs of batch jobs with VMs/EC2 instances and serverless container backend. However, running batch jobs requires VMs and instances to run continuously in production with almost zero downtime. It also means that there will be idle time to keep batch jobs running in the background. We review and estimate the idle time pricing of several cloud-based batch processing services.  Batch processing and idle timeWith the advent of cloud computing with virtualization, more and more workloads have been offloaded from conventional private clouds to public clouds in recent years. The greatest issue of these approaches is the pricing models. Virtual machines often use the pay-per-use model, which requires running the VMs continuously, and non-working hours are begin paid. For example, AWS EC2 instances are billed for running durations on an hourly basis. Because the distribution of coming requests is unknown or probabilistic, then the instances must be kept running without downtime. This results in hours without requests are being paid with the same price as hours with requests (real working hours). The issue is called idle time issue, which is well-known in cloud computing [1], [2], [3]. Batch jobs are scheduled jobs with minimal user interactions. It is possible to have some jobs in the systems which cannot be answered in real-time then users want to submit for processing in the background. The submitted jobs are queued and scheduled to process later when the application does not require them to give results soon. Scheduled jobs are popped from the queue and run automatically with interactions to computations resources such as VMs and databases. Due to a large number of jobs, a Massive Parallel Processing (MPP) system can be used for batch jobs when the number of data is too big (1TB or more)!Azure Batch [1] and AWS Batch [2] are fully managed cloud services for batch jobs which give highly scalable without extensive programming efforts. Azure Batch supports Azure VMs while AWS Batch supports EC2 instances based Elastic Container Service (ECS), Fargate-based ECS, and Lambda. Reducing idle time while deploying to these services is a demand, and it requires careful designs. For example, in [4], Qureshi described a design that reduces Azure Batch cost using dynamic allocation and relocation of resources with lookahead technique for deallocating nodes. Their results showed that 30% of the total cost was reduced with the proposed techniques. Graphical Processing Unit (GPU) has helped to accelerate the latest Artificial Intelligence workloads. Many Deep Learning applications require GPUs or even thousands of GPUs for training and inference. Building such a massive GPU system requires tedious setups, maintenance, and oversized budgets. Many public clouds such as Azure and AWS have more flexible pricing models which bill for milliseconds. Their flagship services are AWS Lambda and Azure Functions. In AWS Lambda and Azure Functions, computations are billed for running time only, which is calculated from when the resources are loaded into function memory to when the computation is finished with results is returned to an integration service like AWS API Gateway. However, Lambda and Functions do not support GPU!Hence, the normal uses of these serverless computing mechanisms are often API calls, i. e. , calling to other APIs, not any massive computations. Therefore, deploying GPU-required batch jobs to the cloud often use VM-based approaches. In [5], Risco and Moltó designed a batch processing system using AWS Batch with GPU-enabled ECS. They used 12 vGPUs of type g3s. xlarge. In summary, batch processing in the cloud relies on the pricing models that have idle time if we do not keep the system busy 24 hours per day. AWS Batch supports some serverless mechanisms like Lambda and Fargate-based ECS but only supports CPUs. For the need of GPUs, one must switch to instance-based or VM-based approaches, which turns us back to the idle time problem indeed. The choice of pricing models is whether billing for milliseconds or not? Cloud-based batch processingAWS Batch: A mistake in scale-out design: One of the most interesting examples of how the scale-out and scale-up of instances are done in AWS Batch was this QA. A user ulsa9983 asked for their tough situation of managing EC2 instances in 2017 as follows:  We have low-traffic, but want to keep some spot CPU available for quick response. We set min CPU to 2, which initially starts a reasonably small instance, e. g. an m4. large. However, after some batch requests with varying CPU and memory requirements, the cluster has grown to an m4. 16xlarge, and it stays there day in and day out. This costs us plenty more than we anticipated with the min CPU=2 setting.  Is there any way to make it scale down, or is the only alternative to set min CPU=0? The question was actually somewhat practical: there is a minVcpus parameter in the AWS Batch template [6], for which when the system goes into DISABLED mode, then idle instances will be scale down to keep only minVcpus running. Setting minVcpus lower will reduce idle cost but increase the time for starting up, loading, provisioning new vCPUs. That’s why even though there is an option to scale down minVcpus=0, but ulsa9983 wanted to find another solution. Their service is low traffic, but (we) want to keep some spot CPU available for quick response. The most interesting solution came from Jamie@AWS included the following script:  Alternatively, you could achieve your goal by creating two managed to compute environments which are both associated with your job queue. For the first compute environment (with the lowest integer value for order), set min/desired/max vCPUs=2, ensuring that it always has a single EC2 instance with 2vCPUs in it. The second one computes environment could have min vCPUs=0 and max vCPUs set to whatever upper limit you like. With this model, AWS Batch jobs requiring two or fewer vCPUs could be immediately scheduled to the always-on instance while additional capacity is scaled up/down as needed when you have a larger number of concurrent jobs to run. Actually, this answer always keeps an m4. large instance to be run without being modified and having another group of resources that can be scaled up and down according to the situations. Nevertheless, on the trade-off of response time and cost, minVcpus=2 was kept, but the type of the instance was preserved to reduce cost. To find more about the parameters of AWS Batch, one can consult with1. AWS Batch is a batch computing platform with stateful jobs, job queues, GPU jobs, …Jobs are the unit of work invoked by AWS Batch. Jobs can be invoked as containerized applications running on Amazon ECS container instances in an ECS cluster. SCAR: SCAR [5] is a serverless system for GPU-enabled computing. They have their own Functional Definition Language (FDL) for serverless workflows. Their work helps to design a batch system faster and easier. One example can be found below.  This system does object detection and speech transcription for automatic captioning. Some examplar results can be seen as follows.  The above two images are from https://github. com/grycap/scar/raw/master/examples/av-workflow. Azure Batch: Azure Batch [1, 8] creates and manages a pool of compute nodes (virtual machines), installs the applications you want to run, and schedules jobs to run on the nodes. There’s no cluster or job scheduler software to install, manage, or scale. Instead, you use Batch APIs and tools, command-line scripts, or the Azure portal to configure, manage, and monitor your jobs. Some important practices for designing a good batch system with Azure Batch:  Always design your Batch application for high availability Always define error handling Always care about security Best practices, . etc. Idle time pricing optionsWe summarize the pricing options. First, container-based serverless options like Fargate ECS are available but do not really support GPU use cases. This is somewhat a pity for deep learning deployments because these options provide millisecond-based billing. We will wait for GPU supports in the near future. Second, even with some parametric methodologies like setting minVcpus VMs/instance-based options still need to face the trade-off between idle time and cost. An example of the use case was given in which a mistake in scale-out design has led to a scale-up option that doubled the bills. Typically, we will need to keep minVcpus$\geq 2$ to have quick responses, but we still do not have scaledown of GPU (no minvGpus parameters in AWS Batch template). Then the idle cost is mostly unavoidable (SCAR [5] described their approach on scaling down with zero resources when no workload, then the startup time and loading time will increase in return, and they are idle costs). ConclusionFinally, we want to cut down our bills every month (but want to increase the income). :yum:This article provides some guidance to help the readers to design their approaches for cloud-based batch processing systems. Future works will direct to the practices of GPU-enabled serverless architecture with less idle time and high efficiency. ReferencesSoh, J. , Copeland, M. , Puca, A. and Harris, M. 2020. Overview of Azure Platform as a Service. Microsoft Azure. Springer. 43–55. DetailsWitte, P. A. , Louboutin, M. , Modzelewski, H. , Jones, C. , Selvage, J. and Herrmann, F. J. 2019. Event-driven workflows for large-scale seismic imaging in the cloud. SEG Technical Program Expanded Abstracts 2019. Society of Exploration Geophysicists. 3984–3988. DetailsBalaji, A. and Allen, A. 2018. Benchmarking automatic machine learning frameworks. arXiv preprint arXiv:1808. 06492. (2018). DetailsQureshi, A. N. 2019. Reduce Cost of Batch Processing Microsoft Azure Cloud. JETIR-International Journal of Emerging Technologies and Innovative Research. (2019), 2349–5162. DetailsRisco, S. and Moltó, G. 2021. GPU-Enabled Serverless Workflows for Efficient Multimedia Processing. Applied Sciences. 11, 4 (2021), 1438. Details      What Is AWS Batch? - AWS Batch &#8617;    "
    }, {
    "id": 50,
    "url": "https://wanted2.github.io/software-maintenance-technical-debt/",
    "title": "Software maintenance and technical debt - Bảo trì phần mềm và 'nợ công nghệ'",
    "body": "2021/08/11 - Tại sao các công ty thường outsource nghiệp vụ bảo trì phần mềm ra cho những công ty chưa từng tham gia phát triển phần mềm đó?Tại sao các dự án cốt lõi có tính mới được giữ lại bên trong nội bộ?  The purpose of software maintenance is defined in the international standard for software maintenance: ISO/IEC/IEEE 14764. The objective of software maintenance is to modify existing software while preserving its integrity. [1] Tại sao cần phải bảo trì sau first-time code?Tại sao lại có technical debt?  Shipping first-time code is like going into debt. A little debt speeds development so long as it is paid back promptly with a rewrite. Objects make the cost of this transaction tolerable. The danger occurs when the debt is not repaid. Every minute spent on not-quite-right code counts as interest on that debt. Nguồn: Ward Cunningham [2] Technical debt: món “nợ công nghệ” của các dự án phần mềmĐầu tiên, chúng ta nên hình dung về bức tranh lớn của ngành công nghệ phần mềm. Tại sao lại có những người được làm cái “mới”, trong khi lại có những người phải ngồi sửa code “của người khác”?Gọi là sửa code của người khác vì cái người đã viết ra code đó đầu tiên (first-time code) thì đã không tiếp tục maintain code đó nữa, dẫn đến công ty phải thuê người khác hoặc outsource cho một công ty bên ngoài làm maintain hộ. Hoặc có thể người first-time đó vẫn còn ở đó, nhưng dự án đã xong bước code lần đầu để xây dựng nền tảng, và vì việc bảo trì sẽ rất mệt nhọc với những việc như fix bug, review, mà nội bộ làm thì lại mất đi tính công tâm, nên giờ họ sẵn sàng thuê bên ngoài để “bảo trì” cái đoạn code ấy. Từ đó, một lực lượng nhân lực của ngành phần mềm được hình thành để nôm na là “vận hành và sửa chữa một đoạn code không do mình viết ra”. Bản chất của việc bảo trì thì như định nghĩa của IEEE 14764: “Bảo trì phần mềm có mục đích sửa chữa lỗi và đảm bảo tính đúng đắn của phần mềm”. Đây là 1 quy trình trong phát triển phần mềm, nhưng về mặt lịch sử ít khi được coi trọng và thường bị đẩy outsource ra bên ngoài (xem thêm Tại sao các công ty chọn outsource?). Tức là cái gì “mới” thì để nội bộ code first-time, xong đâu đấy đến lúc chỉ còn sửa chữa fix bug, chứ không có thêm tính năng mới thì đẩy ra ngoài. Vậy nếu first-time code mà đã good rồi thì chi phí duy trì sẽ nhẹ đi biết bao nhỉ?Theo Cunningham thì đúng như vậy:  Shipping first-time code is like going into debt. A little debt speeds development so long as it is paid back promptly with a rewrite. Nhưng nếu first-time code mà nhiều vấn đề thì người tiếp nhận sẽ phải tốn nhiều công sức để sửa lỗi, tích hợp, … và kết quả là những công số đó bị tính là technical debt. Một định nghĩa gần đây của khái niệm Technical Debt [3] như sau:  in software-intensive systems, technical debt consists of design or implementation constructs that are expedient in the short term, but set up a technical context that can make a future change more costly or impossible. Technical debt is a contingent liability whose impact is limited to internal system qualities, primarily maintainability and evolvability. Các bạn có thể chú ý tới cụm từ “expedient in the short term, but set up a technical context that can make a future change more costly or impossible”, nôm na là “có lợi trước mắt trong ngắn hạn, nhưng tạo ra một tình huống kỹ thuật mà về lâu dài là có thể khiến những thay đổi xảy ra tương lai, rất đắt hoặc hầu như không thể”. Chính vì technical debt đắt đỏ như vậy, nên ít công ty sẽ để lại mà sẽ outsource ra ngoài, vì chi phí bảo trì khi khoán ngoài sẽ control được qua hợp đồng và nhìn chung là outsource sẽ rẻ đi nhiều. Software maintenanceChức năng: Chức năng của bảo trì phần mềm khá đa dạng:  sửa lỗi cải tiến thiết kế cải thiện tính năng tích hợp với các phần mềm khác tích hợp vào các hệ thống khác bao gồm cả phần cứng, phần mềm, tính năng, viễn thông, … nâng cấp phần mềm; và kết thúc dự án. 5 đặc điểm của công việc của người làm bảo trì hệ thống phần mềm:  quản lý bảo trì các tính năng với chu kỳ hàng ngày quản lý bảo trì ứng với mọi thay đổi có thể xảy ra (đối ứng sự cố, …) hoàn thiện các tính năng sẵn có phát hiện nguy cơ bảo mật và sửa lỗi bảo mật lên kế hoạch phòng chống degrading của phần mềm. Chi phí: Chi phí bảo trì thường được cho là chủ yếu do sửa lỗi, nhưng thực ra, theo 1 nghiên cứu nhiều năm thì hơn 80% chi phí bảo trì là rơi vào các hạng mục không phải sửa lỗi như nâng cấp, tích hợp, cải tiến [1]. Các điều kiện về môi trường vận hành (cả phần cứng, phần mềm) cũng như các quy chế, quy định trong công ty cũng ảnh hưởng tới việc vận hành và bảo trì. Các vấn đề: Vấn đề kỹ thuật: Các vấn đề kỹ thuật chính bao gồm:  Limited Understanding (hiểu biết hạn hẹp về phần mềm): điều này cũng dễ giải thích do code có thể không do người bảo trì viết, hệ thống phức tạp nhưng tài liệu không có gì ngoài code, … Testing: đòi hỏi test đầy đủ và chính xác. Điều này lại rất tốn công và thời gian. Regression testing là phương pháp luận thường thấy.  Impact Analysis: khi bạn muốn sửa một cái gì đó trong code, bạn có nghĩ tới những hệ lụy sẽ xảy ra với toàn thể hệ thống nói chung không? Nếu có thì bạn đã làm impact analysis rồi đấy. Để làm impact analysis nghiêm chỉnh thì cần phân tích kỹ change request.  Maintainability: như đã nói ở trên, phần mềm đôi khi rất khó để change hoặc giá để change rất đắt. Vấn đề quản lý: Vấn đề đầu tiên là mục tiêu của công ty: đôi khi công ty muốn giao hàng nhanh để tiết kiệm chi phí, và thế là maintenance trở thành khâu bị “trảm” đầu tiên. Lúc đó bạn nên đề xuất outsource để giảm chi phí. Vấn đề thứ hai là nhân sự: maintenance luôn bị coi là công việc “hạng hai”, làm sao để thu hút và giữ động lực cho đội ngũ bảo trì?Vấn đề thứ ba phân công trách nhiệm trong công ty: khi có nhiều team thì không nhất thiết team viết ra đoạn code phải maintain chính đoạn code đó. Vấn đề thứ tư là quy trình: maintenance có quy trình riêng như review, acceptance, migration, … Quy trình bảo trì phần mềm: Quy trình chung của maintenance thì như hình bên:  process implementation: quá trình này thường là do bên khác đã code sẵn.  problem and modification analysis: với code sẵn từ bên khác, bên maintenance sẽ thực hiện điều tra tìm vấn đề và ra giải pháp (solution).  modification implementation: sửa lỗi sẽ được thực hiện sau khi solution được phê duyệt thông qua.  maintenance review/acceptance: các bên thực hiện review và phê duyệt nếu đạt tiêu chuẩn.  migration: thực hiện nâng cấp hệ thống sau khi implementation được phê duyệt.  software retirement: nếu không tìm được giải pháp phù hợp thì đóng lại và kết thúc vòng đời.   Tài liệu tham khảo Bourque, P. and Fairley, R. 2004. SWEBOK. Nd: IEEE Computer society. (2004). DetailsCunningham, W. 1992. The WyCash portfolio management system. ACM SIGPLAN OOPS Messenger. 4, 2 (1992), 29–30. DetailsBehutiye, W. N. , Rodrı́guez Pilar, Oivo, M. and Tosun, A. 2017. Analyzing the concept of technical debt in the context of agile software development: A systematic literature review. Information and Software Technology. 82, (2017), 139–158. Details"
    }, {
    "id": 51,
    "url": "https://wanted2.github.io/the-dip/",
    "title": "The Abstract Factory",
    "body": "2021/08/05 - The Dependency Inversion Principle (DIP) is the last principle in SOLID [1]. It implies that successful software modules should rely on abstract classes and interfaces, not on concrete implementations. The Abstract Factory implements the DIP to reduce unexpected dependencies on concrete classes. The Dependency Inversion Principle (DIP)DIP tells us that the most flexible systems are those in which source code dependencies refer only to abstractions, not to concretions! In fact, abstract interfaces are always more stable than concrete implementations. Changes made to a concrete implementation don’t affect the abstract interface, but changes made to the abstract interface will lead to severe changes in the concrete implementation. We, therefore, conclude with several rules to favor stable abstract interfaces:  Don’t refer to volatile concrete classes. Refer to abstract interfaces instead.  Don’t derive from volatile concrete classes.  Don’t override concrete functions.  Never mention the name of anything concrete and volatile. The Abstract FactoryTo comply with these rules, the creation of volatile concrete objects requires additional handling. This caution is warranted because, in virtually all languages, the creation of an object requires a source code dependency on the concrete definition of that object. In most object-oriented languages, such as Java, we would use an Abstract Factory to manage this undesirable dependency. In the next section, we will examine an implementation in C++ of the abstract factory pattern. ExamplesUML diagram: We will implement the Abstract Factory in the below UML diagram: The blue line shows the architecture boundary between high-level classes and functions with low-level ones. Low-level parts interact with each other and never interact with main function. The main function only contains the code of Service and ServiceFactory. This made the architecture clean! C++ implementation: 12345678910111213141516171819202122232425262728293031323334353637// solid. hpp#include &lt;iostream&gt;#include &lt;string&gt;#ifndef SOLID_EX#define SOLID_EXclass Service {public:  virtual void run() = 0;};class ServiceImpl : public Service {public:  ServiceImpl(const std::string&amp;);  virtual ~ServiceImpl() = default;  void run() override;private:  std::string _name;};class ServiceFactory {public:  virtual Service* makeSvc(const std::string&amp;) = 0;};class ServiceFactoryImpl : public ServiceFactory {public:  ServiceFactoryImpl();  virtual ~ServiceFactoryImpl() = default;  Service* makeSvc(const std::string&amp;);};#endifNote that, we have the Service and ServiceFactory implemented as pure abstract classes. And we have the implementations: 1234567891011121314151617181920// solid. cpp#include  solid. hpp void Service::run() {}ServiceImpl::ServiceImpl(const std::string&amp; name) {  this-&gt;_name = name;}void ServiceImpl::run() {  std::cout &lt;&lt; this-&gt;_name &lt;&lt; std::endl;}ServiceFactoryImpl::ServiceFactoryImpl() {}Service* ServiceFactoryImpl::makeSvc(const std::string&amp; name) {  ServiceImpl* serviceImpl = (ServiceImpl*)new ServiceImpl(name);  return serviceImpl;}We have the main function: 123456789// main. cpp#include  solid. hpp int main() {  ServiceFactory* serviceFactory = (ServiceFactory*)new ServiceFactoryImpl();  Service* service = serviceFactory-&gt;makeSvc( Hello, Factory! );  service-&gt;run();  return 0;}To run this code, you may make a run script like 123456cmake_minimum_required(VERSION 3. 0)project(solid_example CXX)include_directories((${PROJECT_SOURCE_DIR}))add_executable(${PROJECT_NAME} main. cpp solid. cpp)and run 1234$ cmake . $ cmake --build . $ . /Debug/solid_example. exeHello, Factory!ReferencesMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details"
    }, {
    "id": 52,
    "url": "https://wanted2.github.io/the-humble-object/",
    "title": "The Humble Object Pattern",
    "body": "2021/08/04 - Clean architecture is always the goal for any software design. The Humble Object1, [1] is a design pattern that is helpful for software testing. It is often characterized as the separability between testable and non-testable parts. The Presenter/View model is an example of a clear boundary between hard-to-test and easy-to-test components. The Humble Object [1] The idea is very simple: split the behaviors into two modules or classes. One of those modules is humble; it contains all the hard-to-test behaviors stripped down to their barest essence. The other module contains all the testable behaviors that were stripped out of the humble object.  For example, UIs are hard to unit test because it is very difficult to write tests that can see the screen and check that the appropriate elements are displayed there. However, most of the behavior of a GUI is, in fact, easy-to-test. Using the Humble Object pattern, we can separate these two kinds of behaviors into two different classes called the Presenter and the View. The Presenter/View model [1]   The View is the humble object that is hard to test. The code in this object is kept as simple as possible. It moves data into the UI but does not process that data.   The Presenter is the testable object. Its job is to accept data from the application and format it for presentation so that the View can simply move it to the screen. Anything and everything that appears on the screen, and that the application has some kind of control over, is represented in the &gt;View Model as a string, or a boolean, or an enum. Nothing is left for the View to do other than to load the data from the View Model into the screen. Thus the View is humble.  ExamplesWe take the Vue. JS code as an example. Vue. JS is actually an MVVM framework, but we can find the Humble Object pattern there. A humble view: Let’s see the following example: 123456789101112131415161718192021222324252627282930313233&lt;template&gt; &lt;div&gt;  &lt;div class= message &gt;   { { message } }  &lt;/div&gt;  Enter your username: &lt;input v-model= username &gt;  &lt;div   v-if= error    class= error   &gt;   Please enter a username with at least seven letters.   &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { name: 'Foo', data () {  return {   message: 'Welcome to the Vue. js cookbook',   username: ''  } }, computed: {  error () {   return this. username. trim(). length &lt; 7  } }}&lt;/script&gt;This view in &lt;template&gt;&lt;/template&gt; is humble: it only loads processed data, and it doesn’t process data!The test is straightforward now since we only test the presenter’s logic: 1234567891011121314151617181920212223242526272829303132333435363738import { shallowMount } from '@vue/test-utils'import Foo from '. /Foo'const factory = (values = {}) =&gt; { return shallowMount(Foo, {  data () {   return {    . . . values   }  } })}describe('Foo', () =&gt; { it('renders a welcome message', () =&gt; {  const wrapper = factory()  expect(wrapper. find('. message'). text()). toEqual( Welcome to the Vue. js cookbook ) }) it('renders an error when username is less than 7 characters', () =&gt; {  const wrapper = factory({ username: '' })  expect(wrapper. find('. error'). exists()). toBeTruthy() }) it('renders an error when username is whitespace', () =&gt; {  const wrapper = factory({ username: ' '. repeat(7) })  expect(wrapper. find('. error'). exists()). toBeTruthy() }) it('does not render an error when username is 7 characters or more', () =&gt; {  const wrapper = factory({ username: 'Lachlan' })  expect(wrapper. find('. error'). exists()). toBeFalsy() })})An arrogant view: The view in the previous example was humble: it only loads formatted data!But let’s see a view that is not humble: 1234567891011121314&lt;template&gt; &lt;div&gt;  &lt;div class= message &gt;   { { message. split(''). reverse(). join('') + '-'. repeat(Math. floor(Math. random()*100))} }  &lt;/div&gt;  Enter your username: &lt;input v-model= username &gt;  &lt;div   v-if= error    class= error   &gt;   Please enter a username with at least seven letters.   &lt;/div&gt; &lt;/div&gt;&lt;/template&gt;The code works, but the view is not humble now!It processes data inside &lt;template&gt;&lt;/template&gt; instead of only displaying the message. Due to the lack of humility, this code may be hard to test! ReferencesMartin, R. C. 2017. Clean Architecture - A Craftman’s Guide to Software Structure and Design. Prentice Hall. Details      Humble Object at XUnitPatterns. com &#8617;    "
    }, {
    "id": 53,
    "url": "https://wanted2.github.io/python38-sys-exit/",
    "title": "Termination in Python",
    "body": "2021/07/25 - Every program will come to its end! The termination of a Python process can be the result of “murder” by the system (SIGTERM/SIGKILL), or controlled by the program (using quit(), exit(), os. _exit() or sys. exit()), or by reducing budget. When writing code, programmers may need to determine whether to put a line of code to terminate the process. The termination process itself is complex. And before terminating, we should remember to make a report, an audit log to summarize the life of the Python process. What are kinds of termination in Python?The processes in Python can terminate by themselves or by the system. By the system: by Extinction and Starvation: A Python process can be terminated suddenly without notices. If the system (OS) issued a kill (SIGTERM/SIGKILL) command when the process violated some policies of the system, we call it termination by murder (i. e. , the system murdered the process). To “murder” a process, a Linux system can issue a command like: 1$ kill -SIGTERM PIDSIGTERM cares about termination processes, saving data to prevent losses, but SIGKILL doesn’t. When the system “murders” the processes by SIGKILL, there will be no saved data! Another form of sudden termination is starvation but it is not considered as termination. This happens when the system manager found some processes consuming too many resources, or those processes are not needed anymore. But the manager cannot “murder” the processes, then they decide to let them alive, but reducing the usage of resources. In Linux, the manager can use the tool cpulimit to limit CPU usage by process ID. To install it in Debian, 1$ sudo apt install cpulimitand to limit CPU usage of a process, say 1999, to less than 50% 1$ sudo cpulimit --pid 1999 --limit 50Note that this limit CPU usage during runtime, not at the beginning!Then by this starvation mechanism, we may found some very slow processes in the top command summaries, but they don’t end! By processes: by Add/Integration processes: Extinction and starvation patterns of terminations are in control of the system and out of control of the process itself. Other patterns in this section are about termination defined by the process. In this section, we will discuss two types of such terminations: addition and integration. In the termination by addition, a successful child process terminates, and results are returned to a parent process. Sometimes, calling cleanup handlers or flushing buffers are not needed, and then it is normal to use os. _exit() to terminate the child process. The computed results and resources consumed by the children will be merged into a more full-fledged member in parent processes. In the termination by integration, a successful process that completed its computation can be terminated. Computed results and used resources are returned to be used in other processes of the system. Then it is normal to use sys. exit([args]) in most of these cases. Some audithooks can be called here to ensure the integrity of the audit cycle. Termination processThis section describes the mindset in termination. The termination process has two different parts: (i) decide whether to terminate a process; (ii) implement the termination in code or by command line. For the first part, the information about the performance of the process is collected from related programs in the system such as syslog. Then a criteria database is established as a baseline. The scoring model is then created to evaluate the decisions: terminate or not. Termination rules set are matched with a scoring model to make the final decision. If the decision is uncertain, then further sensitive analysis may be needed to adjust the criteria and their weights. When termination is confirmed in the decision, it will be implemented. Manual closeout like in extinction and starvation can be carried out by system commands. Automatic termination by coding can be implemented by quit(), exit(), os. _exit() and sys. exit(). Note that quit() and exit() require site modules to be imported, and they should not be called in production. os. _exit() is used in child processes only. sys. exit() should be preferred in most cases. Successful processes should call 1sys. exit(0)Coming into the mindset should be: whether the cleanup of data, resources are carried out properly? Were related client processes notified about the termination? Did it determine which records and reports are needed to keep? Were closeups of supports and dependencies scheduled properly?More items are available and should be used in a checklist. The final reportA final report summarizes the lifetime of a process is important for further diagnosis and audits. If it logged its behavior in the code, then logs can be a good summary. Nonetheless, a good final report should include the following criteria:  Process performance: CPU usage and resources information can be monitored and logged.  Administrative performance: errors and exceptions, accesses, and other security events may happen during process lifetime.  Process structure: the structure of code can be logged for diagnosis.  Administrative files: any files and functions that are called or linked into this job should be traceable.  Techniques: planning, control, and errors/exception handling techniques should be logged. "
    }, {
    "id": 54,
    "url": "https://wanted2.github.io/python-38-sys-audit/",
    "title": "Code audit processes and the new feature of Python 3.8 - sys.audit",
    "body": "2021/07/17 - Auditing is actually a quality assurance feature. The purpose of audits is to identify and report problems, not to respond, prevent or act on them. If you are using Python 2. x or even Python 3. 7, this feature may not be available. Calls like sys. audit and sys. addaudithook are only available from version 3. 8. The new feature allows the audits to run at runtime level, or in other words, to keep track of changes in system calls and standard libraries while the application’s running. Two proposals that made this available are PEP 578[^4] and PEP 551[^3]. [^3]: PEP 551 - Security transparency in the Python runtime - Python. org[^4]: PEP 578 – Python Runtime Audit Hooks | Python. org Overview of technical auditsCode review vs. code audit: First of all, code audit differs from code reviews. First, while code reviews can be done by internal teams, each member reviews code of and focus on the specific part of the project, code audit is relevant to the whole project, a big picture and must be done objectively (often done by an outsider who is not a member of the project, or even from another company). Second, code reviews can be done by only reading and find suspicious pieces in the code, but code audit requires to see the product running in actions. Therefore, to support the audit, language tools, and utilities to ascertain the changes made by the programs during the run is helpful. General audit aspects and tools: There are several aspects to assess when auditing a software project. Code management: Most software projects adopt some kinds of code management tools like Git and Mercurial to reduce operation mistakes. Check your target projects if they have their own code repositories and governance policies for code management. Some items to include in the checklist:  Branch naming convention Workflow like Gitflow to determine the process of creating PR, merging, . etc.  Release management: Git tags systems may help to version different releases in project lifetime.  and any recommendations that help to improve code management. Architectures: Software products take many forms: desktop applications, mobile apps, websites, infrastructures, and even combinations to make a far complicated system. But whatever the forms they take, they will have architectural choices. Integrability: A product can be a mixture of many components: a website written with Django framework with PostgresSQL. First, please pay attention to the compatibilities among components: are all versions work well together? Are all of them well-tested together (system test or integration tests are sufficient)? Deployment and delivery: How is the product (system) deployed and delivered to the customer? Please pay attention to the deployment process. If the project has a continuous mechanism to deliver outcomes to the customer, it should make everything’s smoother. Especially if CI/CD processes have not been well-tested, then it should be good advice to construct such a pipeline. Container systems like Docker or Kubernetes are helpful. Then, it’s time to check whether the README file contains all the necessary elements:  instructions for configuration, instructions for installation, a user’s manual, a manifest file (with an attached list of files), information on copyrights and licenses, contact details for the distributors and developers, known bugs and malfunctions, a problem-solving section, a changelog (for developers). Maintenance and incident responses: The system may be down for many reasons: operating mistakes, system errors, human errors, and so on. Does the project has such a mechanism to report and respond to the errors?An error tracking tool like Bugsnag is good advice. Coding best practices: Using code analysis tools: Static code analyzers like linting tools are helpful to detect early bugs, bottlenecks, performance issues, security vulnerabilities, and threats connected with maintaining the application. Tips to improve code quality: Code reviews between team members (developers) help to detect early problems. Using githooks can enforce your local development. Standardize the configurations and formats among team members like IDE configurations can prevent several human errors. Finally, remember to share knowledge among the team. Python audit toolsProposals PEP 578 and PEP 551: Since code audit is inevitable in software development, the Python programming language also has its own tools to do the job. Remember that code audit differs from code reviews in the way that code audit reviews the running system!Then if it’s not running, this is not audit. In this post, we focus on the new features for runtime audit: proposals PEP 578 [^4] and PEP 551[^3]. These proposals were made available only from Python 3. 8. PEP 578 was only a part of PEP 551, which concerns the security transparency of Python programs: the lack of internal audits leads to the fact that many threats can bypass audits. Python up to 3. 7 in isolation could manage to audit all events in codes. However, no runtime events have been captured, and this led to a poor audit. Defenders have a need to audit specific uses of the Python programming language in order to detect abnormal or malicious usage. With PEP 578, the Python runtime gains the ability to provide this. The aim of this PEP is to assist system administrators with deploying a security-transparent version of the Python programming language that can integrate with their existing auditing and protection systems. sys. audit and sys. addaudithook: Runtime audithooks [^4] comes with two different APIs: CPython and standard way. You can use sys. audit/sys. addaudithooks in Python code and PySys_Audit/PySys_AddAuditHook in native C code PEP 5781. The backport to Python 3. 7 and older versions can be found in a third-party sysaudit2A list of auditable events can be found in 3. Some discussions on bypassing audit features in Python 3. 8 can be found in4. Native C interface: If you want native audits, you may need to re-compile the python binary. An example is the NetworkPromtHook example. We will write an example spython. c: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;Python. h&gt;#include &lt;opcode. h&gt;#include &lt;string. h&gt;#include &lt;locale. h&gt;static int my_hook(const char* event, PyObject* args, void* userData) {  /* Only care about 'socket. ' events */  if (strncmp(event,  socket.  , 7) != 0) {    return 0;  }  PyObject* msg = NULL;  /* So yeah, I'm very lazily using PyTuple_GET_ITEM here.    Not best practice! PyArg_ParseTuple is much better! */  if (strcmp(event,  socket. getaddrinfo ) == 0) {    msg = PyUnicode_FromFormat( WARNING: Attempt to resolve %S:%S ,      PyTuple_GET_ITEM(args, 0), PyTuple_GET_ITEM(args, 1));  }  else if (strcmp(event,  socket. connect ) == 0) {    PyObject* addro = PyTuple_GET_ITEM(args, 1);    msg = PyUnicode_FromFormat( WARNING: Attempt to connect %S:%S ,      PyTuple_GET_ITEM(addro, 0), PyTuple_GET_ITEM(addro, 1));  }  else {    msg = PyUnicode_FromFormat( WARNING: %s (event not handled) , event);  }  if (!msg) {    return -1;  }  fprintf(stderr,  %s. Continue [Y/n]\n , PyUnicode_AsUTF8(msg));  Py_DECREF(msg);  int ch = fgetc(stdin);  if (ch == 'n' || ch == 'N') {    exit(1);  }  while (ch != '\n') {    ch = fgetc(stdin);  }  return 0;}#ifdef MS_WINDOWSintwmain(int argc, wchar_t** argv){  PySys_AddAuditHook(my_hook, NULL);  return Py_Main(argc, argv);}#elseintmain(int argc, char** argv){  PySys_AddAuditHook(my_hook, NULL);  return _Py_UnixMain(argc, argv);}#endifIn Windows, this requires Visual Studio C, and compilation can be done: 123456789101112131415161718192021222324252627@setlocal@echo offif not defined PYTHONDIR echo PYTHONDIR must be set before building &amp;&amp; exit /B 1if exist  %PYTHONDIR%\PCbuild  (  set _PYTHONINCLUDE=-I %PYTHONDIR%\PC  -I %PYTHONDIR%\include   if  %VSCMD_ARG_TGT_ARCH%  ==  x86  (    set _PYTHONLIB=%PYTHONDIR%\PCbuild\win32  ) else (    set _PYTHONLIB=%PYTHONDIR%\PCbuild\amd64  )) else (  set _PYTHONINCLUDE=-I %PYTHONDIR%\include   set _PYTHONLIB=%PYTHONDIR%\libs)if exist  %_PYTHONLIB%\python_d. exe  (  set _MD=-MDd) else (  set _MD=-MD)@echo on@if not exist obj mkdir objcl -nologo %_MD% -c spython. c -Foobj\spython. obj -Iobj -Zi -O2 %_PYTHONINCLUDE%@if errorlevel 1 exit /B %ERRORLEVEL%link /nologo obj\spython. obj /out:spython. exe /debug:FULL /pdb:spython. pdb /libpath: %_PYTHONLIB% @if errorlevel 1 exit /B %ERRORLEVEL%Save this bat script as make. cmd and run it in a VC console to compile spython. exe. Then we can confirm the audit is in effect: Standard interface: To do audits in pure Python language, we can use sys. audit/sys. addaudithook as follows: 1234567891011121314151617181920import sysdef my_hook(event, args):  # Only care about 'socket. ' events  if not event. startswith( socket.  ):    return  if event ==  socket. getaddrinfo :    msg =  WARNING: Attempt to resolve {}:{} . format(args[0], args[1])  elif event ==  socket. connect :    addro = args[0]    msg =  WARNING: Attempt to connect {}:{} . format(addro[0], addro[1])  else:    msg =  WARNING: {} (event not handled) . format(event)  ch = input(msg +  . Continue [Y/n]\n )  if ch == 'n' or ch == 'N':    sys. exit(1)sys. addaudithook(my_hook), then confirm the effects similarly. ConclusionThis article introduced two important features in a software project. The first part showed an overview of code audit in practice. The second part discussed a new audit feature in the Python programming language: sys. audit/PySys_Audit. Although this feature is new in the Python programming language but the merit of capturing runtime events can be helpful. Benchmark in PEP 578 [^4] showed that adding this audit did not change the performance or cost then it is nice to have this in your project. References      sys - System-specific parameters and functions - Python 3. 9. 6 documentation &#8617;        sysaudit – sysaudit documentation &#8617;        Audit events table - Python 3. 9. 6 documentation &#8617;        Bypassing Python3. 8 Audit Hooks [Part 1] · daddycocoaman &#8617;    "
    }, {
    "id": 55,
    "url": "https://wanted2.github.io/python-chain-of-responsibility/",
    "title": "Pythonによる責任の連鎖",
    "body": "2021/07/15 - 責任の連鎖1はオブジェクト思考のデザインパターンで、送信者に対して複数の受信者を順次にデータを渡してあげます。受信側の処理を一連の部品に分けてして部品化いくため、独立性が高められ、コードの記述も簡略化されます。もちろん、処理が長くなるか、デバッグが困難になるというデメリットはあるけれども、連鎖を避けれない場合、ぜひなれたいパターンです。 責任の連鎖責任の連鎖: 責任の連鎖は一つのオブジェクト思考のデザインパターンで、送信者に対して複数の受信者を順次にデータを渡してあげます。例えば、あるリクエストに対して、複数の処理が必要です。  リクエスト情報を使って認証局に認証する 業務データベースに接続して、トランザクションを開始する 業務データを取得して、レスポンスを作る トランザクションを終了し、レスポンスを返す。もちろん、処理に不具合が発生する場合、エラー処理なども必要です。これらの処理は順序が決まっています。ですので、一つの流れを複数のハンドラーに分けて、実行時、最初のハンドラーを呼び出すだけで、連鎖の全体を順次に実行していきます。それが、責任の連鎖です。 メリットはいくつかあります。  Loose Coupling: 送信側と受信側の結び付けは弱めているので、処理の独立性が高まります。各部品（ハンドラー）には、自身の責任で対応しているので、部品化はしやすい。 Clean code: 受信側は連鎖を内部に記述しているため、連鎖全体を実行するのに、最初の部品を呼び出すだけで大丈夫ですので、記述が簡略になります。事例: 下記の連鎖を考えます。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from typing import Anyfrom abc import abstractmethodclass AbstractCaller:       抽象クラス       @abstractmethod  def __init__(self, name) -&gt; None:    pass  @abstractmethod  def next(self, caller) -&gt; Any:    pass  @abstractmethod  def __call__(self, x) -&gt; Any:    passclass Caller(AbstractCaller):  _caller = None  def __init__(self, name) -&gt; None:    super(). __init__(name)    self. _name = name  def next(self, caller: AbstractCaller) -&gt; Any:           次のコーラーをセットする           self. _caller = caller    return caller  @abstractmethod  def process(self, x):    pass  def __call__(self, x) -&gt; Any:    x = self. process(x)    print(f'{self. _name}: {x}')    if self. _caller is not None:      return self. _caller(x)    return xclass FirstCaller(Caller):  def process(self, x):    x += 1    return xclass SecondCaller(Caller):  def process(self, x):    x += 2    return xclass ThirdCaller(Caller):  def process(self, x):    x += 3    return xfirst = FirstCaller( First Stage's result )second = SecondCaller( Second Stage's result )third = ThirdCaller( Third Stage's result )# Chain of Responsibilitiesfirst \  . next(second) \  . next(third)print( Process 1:\n--------------- )print(f ---------------\nFinal result: {first(0)} )print( Process 2:\n--------------- )print(f ---------------\nFinal result: {second(0)} )連鎖の定義は下記になります。 123first \  . next(second) \  . next(third)firstの次にsecondを実行します。secondの次にthirdを実行します。連鎖の実行は単純に最初の要因を呼び出すだけです。 1first(0) # 連鎖の実行連鎖の要因とするfirst, second, thirdはCallerを継承しています。それぞれはCaller. __call__を継承しているので、processを実行して、値を修正してから連鎖の次の要因に渡します。 このようにすると、連鎖の各要因を実行する記述を隠蔽できるため、コードがすっきりになります。 課題独立性が高くて記述がすっきりでも弱点があります。連鎖が長くなると、実行は遅くなります。さらに、バグがある場合、連鎖の途中でどこの部品にバグがあるかを知りたいなら、最初から最後までデバッグしないといけないのです。 結論責任の連鎖を紹介しました。連鎖の操作がある場合、強力なパターンです。長い連鎖を部品化し、最初の部品を実行するだけで、連鎖全体を稼働できます。 参考文献      3. Data model — Python 3. 9. 6 documentation - Metaclasses &#8617;    "
    }, {
    "id": 56,
    "url": "https://wanted2.github.io/controlling-creative-activities/",
    "title": "Controlling Creative Activities",
    "body": "2021/07/11 -  Control is not necessarily the enemy of creativity; nor, popular myth to the contrary, do creative activity imply complete uncertainty. Source: Meredith et al. [1] Creative Activities like research and development projects, design projects, and similar processes depend strongly on the creativity of individuals and teams. Controlling such activities is quite sensitive: creative activities involve a high degree of uncertainty, but tight controlling inhibits the creativity. Such a dilemma can be controlled by looking carefully at processes, payoff, and risk management. Process ReviewProcess Review vs. Result Review: In creative activities, processes are often more certain than the results (Meredith et al. [1]). Therefore, controlling the processes is more realistic than controlling the outcomes. In contrast, outsource projects often certain enough then PMs only need to care about the results, and processes are not reviewed in those projects. The contract in an outsourcing request often takes the form:  In outsourcing, we provided a given input and desired outputs. The deliverable should be a software module that transforms the input into the outputs, and that’s all! Yes, that’s all, and any further requests can be filtered by the contract terms. However, creative projects don’t have such certain requirements. The outcomes can be undetermined or vaguely determined. Therefore, controlling creative activities often review the processes rather than the final deliverable: we know the process to produce the deliverables, but we cannot know what deliverables are at the contract time.  Even when we know the methods, we don’t know what the final results are. Phase-Gate Control: Because processes are more reliable to control in creative projects, there is a need to divide the process in these creative activities into measurable milestones. Assessments at each milestone should care about the progress so far, the potential value that can be achieved in the next step, and the desirability to change the research designs. That is the phase-gate control. Personnel ReassignmentFor such an uncertain type of project, it is valuable to reassign people: productive personnel are kept, while the rest is reassigned to other projects or other organizations. It is not difficult to perform this control, but there are several issues:    If the favored personnel doesn’t change, then there is a possibility to create an elite group: new personnel becomes demotivated when joining, while old personnel is already highly motivated for further achievements.     It is difficult to rank the middle quartile.  Control of Input ResourcesCreativity does not equal the generosity of resources. Resource controlling helps to enhance the efficiency of creative projects. Outcomes in R&amp;D projects often come in batches: sometimes, there are no visible results, but suddenly many outcomes can be delivered. ReferencesMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 57,
    "url": "https://wanted2.github.io/attendance-bot-microsoft-teams/",
    "title": "Hệ thống chấm công AttendanceBot tích hợp vào công cụ chat Microsoft Teams để quản lý và theo dõi nhân sự dự án",
    "body": "2021/07/10 - Quy trình thực thi dự án luôn đòi hỏi phân tích real-time các dữ liệu dự án như nhân sự và communication trong team và với khách hàng (Meredith et al. [1]). Việc theo dõi, phân tích, thậm chí lên cảnh báo này hỗ trợ các thành viên team được giữ “tỉnh táo” ở mức độ cao ví dụ như thông tin hôm nay nhân viên nào nghỉ và cần bổ sung nhân lực làm thay. AttendanceBot là một hệ thống chấm công thông minh sử dụng phân tích dữ liệu ngôn ngữ tự nhiên để đăng ký tự động các kỳ nghỉ cũng như thời gian in/out hàng ngày của thành viên, qua đó giảm tải cho quản lý dự án. Hệ thống tích hợp vào luồng quản lý dự án thông qua các ứng dụng chat như Slack, Microsoft Teams và Google Chat. Dịch vụ AttendanceBotChức năng chính: Quản lý thời gian lao động (timesheet). Nhân viên thực hiện chat với AttendanceBot bằng cụm từ quy định in để check in. Thời gian checkin được tính từ thời điểm này và được lưu trữ vào bảng timesheet của AttendanceBot. Bằng việc nhập lệnh out vào cửa sổ chat, nhân viên thực hiện checkout và cập nhật timesheet. Nhân viên có thể xem timesheet của chính mình trực tiếp trên công cụ chat bằng lệnh timesheet. Để tải timesheet của bản thân mình, nhân viên nhập lệnh timesheet report và AttendanceBot sẽ trả về đường link tới file timesheet CSV. Đồng thời hành vi check in/out của nhân viên cũng được thông báo trên channel của team, do đó PM có thể nhanh chóng nắm bắt nhân sự và nhận cảnh báo. Với quyền quản lý, PMs còn có thể truy cập và quản lý timesheet của các nhân viên. Quản lý kỳ nghỉ. Nhân viên có thể đăng ký lịch nghỉ dài hạn hoặc ngắn hạn, hoặc Work From Home (WFH) từ cửa sổ chat. PM và các quản lý cấp cao có thể nhận cảnh báo ngay lập tức với các kỳ nghỉ của nhân viên. Mẫu câu để đăng ký nghỉ có thể phong phú hơn từ khóa in/out, ví dụ, nhân viên có thể nhập WFH on next Monday, và AttendanceBot có thể nhận ra thời gian là thứ 2 tới ngày 12/7 và nội dung là Work from home. Quản lý phiên làm việc. Lên kế hoạch làm việc cho nhân viên trong cả tuần tiếp theo là một việc làm thường xuyên của PM. PM sẽ thực hiện lên kế hoạch trên công cụ lịch của AttendanceBot và từ đó, AttendanceBot sẽ lên schedule để gửi thông báo (notification) cho nhân viên về time shift. Khi gần tới phiên làm việc sẽ có cảnh báo/thông báo gửi về từng nhân viên để làm việc. Đơn giá: Báo giá của AttendanceBot có thể tìm thấy tại https://www. attendancebot. com/pricing/. Có 2 phiên bản là bản AttendanceBot và AttendanceBot Pro. Mức giá tính theo user và theo tháng là 4 đô và 6 đô tương ứng. Sử dụng cùng Microsoft Teams Một điểm hay của AttendanceBot là ngoài việc dùng trực tiếp dịch vụ tại trang chủ attendancebot. com thì các nhà phát triển chủ động một cách thông minh tích hợp dịch vụ dưới dạng chatbot tự động vào các nền tảng communication cho dự án nổi tiếng như Slack, Teams và Google Chat. Chính sự chủ động này đã khiến cho việc tích hợp AttendanceBot vào dự án đang chạy vô cùng dễ dàng cho người dùng. Để cài đặt bạn chỉ cần mở tab App và gõ tên app AttendanceBot vào khung tìm kiếm để lọc ra app. Sau đó theo các hướng dẫn để cài đặt vào thư mục dự án. Đăng ký lịch nghỉ: Sau khi cài đặt, AttendanceBot sẽ chủ động chat với bạn về cách sử dụng app. Có hai cách để tương tác với AttendanceBot:    Trong thư mục của team dự án, bạn có thể gửi tin lên @AttendanceBot Vacation from 12 December to 14 December.     Bạn mở một chat trực tiếp với AttendanceBot với nội dung Vacation from 12 December to 14 December.  Dù làm theo cách nào, thì AttendanceBot cũng sẽ chat lại cho bạn với nội dung như trên. Cú pháp xin nghỉ khá đa dạng nhưng phần lớn tuân theo một số kiểu câu: 12345&lt;Loại nghỉ&gt; from &lt;ngày bắt đầu&gt; to &lt;ngày kết thúc&gt;Ví dụ:Vacation from 12 December to 14 DecemberWFH from 12 December to 14 DecemberNhững mẫu câu về ngày nhất định như Vacation today hoặc WFH tommorrow cũng được bot hiểu. Check in/check out: Quản lý check in/out có thể được thực hiện khá đơn giản bằng các câu lệnh sau:  in: nhân viên bạn quản lý phải nhập câu lệnh này để checkin.  out: khi ra về nhân viên phải nhập lệnh này để checkout và cập nhật timesheet. Nếu bạn là quản lý trực tiếp của nhân viên, thông tin timesheet sẽ được thông báo để bạn quản lý và theo dõi.  timesheet: nhân viên có thể dùng lệnh này để tự xem timesheet của mình.  timesheet report: nhân viên có thể dùng lệnh này để tải CSV của timesheet về máy.  change manager: lần đầu đăng ký timesheet (in/out), nhân viên sẽ bị hỏi nhập thông tin quản lý trực tiếp. Trong quá trình dự án, nếu có thay đổi quản lý trực tiếp, nhân viên dùng lệnh này chat cập nhật quản lý. Chức năng này giúp luồng báo cáo của nhân viên tới quản lý dự án được tự động hóa và thông suốt.  my manager: nhân viên có thể xem tên quản lý trực tiếp của mình bằng lệnh này. Quản lý báo cáo: Source: AttendanceBot. com Công cụ hữu ích cuối cùng là dashboard để quản trị thông tin nhân sự. Với công cụ này, quản lý dự án có được cái nhìn tổng thể mà chi tiết (big picture) của dự án. Kết luậnAttendanceBot là công cụ tự động hóa (RPA) hữu ích, đồng thời cũng là công cụ có tích hợp trí tuệ nhân tạo. Mặc dù các xử lý ngôn ngữ tự nhiên bên trong khá đơn giản như quyết định bằng rule gắn với keyword (in/out) hay phân tích mẫu câu có sẵn template đơn giản như vacation from . . . to . . . , đây là một công cụ hữu ích và trong tương lai có thể kỳ vọng vào những bước tiến xa hơn. ReferencesMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 58,
    "url": "https://wanted2.github.io/student-syndrome/",
    "title": "The Student Syndrome: Hiệu Ứng Sinh Viên "Lười"",
    "body": "2021/07/04 - Hiệu ứng Sinh Viên “lười” là hiện tượng sinh viên hoặc nhân viên khi đến thời hạn nội bài, nộp hàng thì mới bắt đầu “chạy”, và nếu không kịp thì xin trì hoãn và yêu cầu quản lý dự án cũng như khách hàng đợi thêm một thời gian (1-2 ngày, có khi 1-2 tuần). Đây là một vấn đề không mới, nhưng là “bệnh nan y” mà quản lý dự án phải đối mặt theo cách này hay cách khác. Hậu quả của hiệu ứng này là khiến cho dự án “muộn” hơn so với kế hoạch và hầu hết trong các giáo trình quản lý dự án (như cuốn của Meredith et al. [1]) đều coi việc quản lý hiệu ứng này như một phần của quản trị rủi ro. Nguyên nhânCó nhiều nguyên nhân dẫn tới hiệu ứng Sinh Viên “lười” như sau:    Quản lý dự án yếu kém: PM thiếu tổ chức và thiếu trách nhiệm, trốn tránh việc giao tiếp thường xuyên với team dự án có thể là nguyên nhân dẫn đến hiệu ứng này. Khi team dự án bị “bỏ bê” nhiều, các bạn trở nên mất phương hướng, thụ động, ngại hành động, dẫn đến tiến độ chậm. Thế rồi, quản lý dự án đột nhiên xuất hiện và yêu cầu các bạn nộp bài để cho stakeholders xem, mà không hướng dẫn kịp vì bỏ bê lâu ngày, thì … toang!     Yêu cầu dự án không rõ ràng: Dự án chứa nhiều yêu cầu không rõ ràng để tạo ra một “big picture” có độ phân giải cao. Các bạn thử tưởng tượng khi xem một bức ảnh có độ phân giải thấp, thì những vùng khó nhìn trên ảnh sẽ có thể hiểu nổi không? Nếu không thể hiểu được thì bạn có xem vùng đó nữa không? Tất nhiên là không! và thành viên dự án cũng vậy. Khi có một nhiệm vụ không rõ ràng, các bạn sẽ có xu hướng tránh nhận nhiệm vụ đó. Và dự án trì trệ theo chiều hướng đó thôi!     Chạy song song nhiều dự án, thiếu tập trung: Cùng 1 team dự án phải care nhiều dự án quá cùng lúc dẫn tới dàn trải sức và kết cục sẽ phải ưu tiên 1 dự án nào đó hơn các dự án khác. Các dự án còn lại sẽ rơi vào “stagination mode” mà nói chung sẽ phải chờ!     Bản tính con người là “lười”: “Nhân chi sơ, tính bản thiện” nhưng ai cũng “lười”.  Hậu quả của hiệu ứng Sinh Viên “lười” là giảm chất lượng sản phẩm, khiến cho scope có nhiều chỗ không kịp làm, giảm niềm tin của stakeholders, team dự án thì lúc nào cũng rơi vào tình trạng “áp lực”, và rủi ro cho kế hoạch thì gia tăng. Thuốc trị   Định kỳ thường xuyên theo dõi, giám sát team dự án: Không bao giờ bỏ bê các bạn. Phải hỏi ngay khi có dấu hiệu không hiểu, phải chỉnh lại ngay khi có dấu hiệu “lạc lối”.     Định kỳ thường xuyên báo cáo và liên lạc với stakeholders: báo cáo tiến độ kịp thời và nhận feedback kịp thời khi có vấn đề.     Theo dõi sát sao thời gian rảnh của team (“slack”) và lên cảnh báo ngay khi có dấu hiệu thời gian rảnh nhiều hoặc khi tiến độ chậm lại.     Chia nhỏ đầu việc, đến mức có thể nắm được: Tránh giao 1 đầu việc to vì như vậy rủi ro chậm dự án là rất lớn. Lý do bởi vì thời gian để detect được hiệu ứng SInh Viên “lười” luôn không vượt quá thời gian công số của task. Vì vậy nếu để đầu việc công số lớn thì thời gian để phát hiện ra hiệu ứng cũng lớn theo. Do vậy, không nên để đầu việc nào có thời gian dài quá 1 ngày, hoặc vài giờ, tùy theo dự án.  ReferencesMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 59,
    "url": "https://wanted2.github.io/the-customer-interface/",
    "title": "The Customer Interface",
    "body": "2021/06/20 - Risks are quite often in project management [1]. Financial risk and schedule risk are among the most common risks in the project. One of the most popular situations is when the technical content turns to be more complex than planned. It adds a delay to the completion of the projects. In this post, we review one of the exemplar stories (cases) from the PM book of Meredith et al. [1, pp. 472], in which the unaddressed risks led to the loss for all sides in terms of costs and delays in schedules. The storyThe whole story can be found in [1]. The scene graph for this story is as follows. The organization. SPIS personnel: RB=Reggie Brown, RR=Roger Robert, JD=Jacqueline Doyle, RC=Rus Clemons. RB is the site manager, RR and JD are contract managers, and RC is the Vice President. NLP personnel: RJ=Rick James, LM=Lou Mayhew, SS=Sly Simmons, SG=Stand Goodsen, BJ=Bill Jones, MC=Mel Carter. SG is the on-site personnel but is not in charge of contracts. LM is the PIC for contracts. Here we brief the case: NLP ordered SPIS for an outage in 1989. SPIS’s site manager, RB speculated some delays, and it had become real. RB reported to SG as the technical personnel of NLP and asked for changes in contracts to change it to pay-as-you-go. SG replied as OK, but the real person in charge of contract management, LM, didn’t get any request! NLP had changed the procedures for purchasing from 1986, but SPIS and RB didn’t know about that. After the delays, bills were one million bucks higher than the original deal (500,000$). Key persons in NLP found that the price went higher without their notices due to the fact that SPIS reported to the wrong personnel (RB). In fact, they must ask LM for changes in contracts. SPIS and NLP arranged some meetings to resolve the issues, but finally, one thing to keep in mind is that NLP still has four outages remaining!If SPIS cannot handle well this customer relationship, they may lose the 4 outages that worth more. Lessons   QA to the right person: RB didn’t ask LM although, LM is the true PIC for contract, not SG.     Know the decision maker: LM or the boss is the right decision-maker, not SG.     Control the customer: SPIS wasn’t aware of changes in purchasing procedures of NLP.     Get money for work performed: I think SPIS should have divided work into smaller units and get paid for each done. In fact, they waited until everything of the big piece was done for purchasing.     Persevere  ReferencesMeredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 60,
    "url": "https://wanted2.github.io/crystal-ball-monte-carlo-risk-analysis/",
    "title": "Using Oracle CrystalBall for Monte Carlo Simulation based Risk Analysis",
    "body": "2021/06/17 - Uncertainty is everywhere in the lifecycle of a project. The duration of activities, resources to assign to actions, and schedules are all uncertain. Project managers can manage to reduce the risks associated with uncertainties, but they cannot be eliminated. Risk analysis helps to identify and sometimes visualize the risks, not to remove risks from projects. Oracle CrystalBall (OCB)1 is a simulation software for risk management. It utilizes Monte Carlo Simulation to consider every possibility that can happen when calculating and predicting the outcomes of their occurrences, thereby giving you the critical part of your model in which to concentrate. IntroductionRisk Analysis and Monte Carlo Simulation: Uncertainty is everywhere in the lifecycle of a project. The duration of activities, resources to assign to actions, and schedules are all uncertain. Project managers can manage to reduce the risks associated with uncertainties, but they cannot be eliminated. Risk analysis helps to identify and sometimes visualize the risks, not to remove risks from projects. To apply risk analysis, one must make assumptions about the probability distributions that characterize key parameters and variables associated with a decision and then use these to estimate the risk profiles or probability distributions of the outcomes of the decision. Monte Carlo Simulation (MCS) works upon an established assumption about distributions of risks and evaluating the risks by computing the values of multiple samples. Under statistical analysis, a range of values of interests can be selected with confidence. For example, let’s see when we have to estimate the duration of a task, which follows normal distributions with the mean is 5 days, and the standard deviation is about 1 day. Then MCS generates a set of 1,000 samples following the assumptions. We can find the worst cases and the best cases from simulated results, as the minimum values and the maximum values out of 1,000 samples. Oracle CrystalBall: Oracle CrystalBall (OCB) is a simulation software for risk management. It utilizes Monte Carlo Simulation to consider every possibility that can happen when calculating and predicting the outcomes of their occurrences, thereby giving you the critical part of your model in which to concentrate. With the Monte Carlo Simulation, your working processes are simplified, including the statistics and the necessary information at its side in split-view format. The software is an Excel add-in and useful for Excel professionals. Get a trial version: Because a license of OCB costs nearly $1,000, we experience the trial version. Please visit here and follow the instruction to download trial versions. To see the download link, you may need to register for an Oracle account. Oracle reviews your download request. In my case, it took less than 24 hours. Thanks to Oracle!The download came after the approval. That download content was: Installation: Read the guides and then double-click on crystalballsetup-x64. exe and follow the instructions for installation. The CrystalBall add-in is installed inside Excel and is available for every workbook. ExamplesProject Selection: Somewhere in the past, we have discussed about Discounted Cash Flow (or Net-Present Value). Problem: A company has been planning to develop a new product. The project has a hurdle rate $K=0. 12$ and prospective inflation rate $p=0. 03$. The most likely cash flows in ten years are as follows. Recall that the Discounted factor of $t$-th year is $d_t=\frac{1}{(1+K+p)^t}$. Note that the above table is the most likely number. Now let us assume that the expenditures in this example are fixed by contract with an outside vendor. Thus, there is no uncertainty about the outflows, but there are, uncertainties about the inflows. Assume that the estimated inflows are as shown in the below table and include a most likely estimate, a minimum (pessimistic) estimate, and a maximum (optimistic) estimate. Both the beta and the triangular statistical distributions are well suited for modeling variables with these three parameters, but fitting a beta distribution is complicated and not particularly intuitive. Therefore, we will assume that the triangular distribution will give us a reasonably good fit for the inflow variables.       Year   Minimum Inflow   Most Likely Inflow   Maximum Inflow         2008   $35,000   $50,000   $60,000       2009   95,000   120,000   136,000       2010   100,000   115,000   125,000       2011   88,000   105,000   116,000       2012   80,000   97,000   108,000       2013   75,000   90,000   100,000       2014   67,000   82,000   91,000       2015   51,000   65,000   73,000       2016   30,000   35,000   38,000         Total   $621,000   $759,000   $847,000   Register assumption and forecast variables: To do Monte Carlo simulation, we must register our assumptions on the uncertainties of inflows. Select cell B5, and then select the Define Assumption button from the ribbon menu of CrystalBall. Note that the project was assumed that the project generates no profit in the first three years. Therefore, we start from 2008 (or cell B5). Then choose the three-point estimates. Adjust the parameters of the triangular distribution according to the table in the previous section. Then click OK(O). Repeat the process for cells B6:B13. To run the simulation, we still need to select a forecasting goal: the total DCF the project will generate after 10 years, or the value of cell F15. Select cell F15 and the button Define Forecast from the ribbon menu. Input the name and the unit as follows. Simulation: Click button Start in the ribbon menu of CrystalBall to start MCS. The default number of simulations is 1,000. After done the simulation, we observe some results like: To find a robust estimation, we can see the median: 10,869 USD after 10 years is the expected DCF the project will generate. Not so much! References      Oracle Crystal Ball Downloads &#8617;    "
    }, {
    "id": 61,
    "url": "https://wanted2.github.io/microsoft-project-scheduling/",
    "title": "Planning Waterfall Project Schedule and Resources with Microsoft Project 2019",
    "body": "2021/06/13 - Scheduling is the conversion of a project action plan to actionable timetables [1]. Resource utilization and availability are needed to keep the schedule on time and sometimes help to shorten the runs. Networks techniques such as Activity-on-Arrow (PERT analysis) and Activity-on-Node (Critical Path Model, or CPM) help establish the schedule. Resource allocation considers labor, material, facilities, and equipment to achieve the action plan. Resource allocation requires adjusting the loading and leveling, while constrained planning can be solved using heuristic methods or optimization methods such as linear programming. While computer-aided scheduling and resource allocation are popular, Microsoft Project1 is among the best choices for project management software because it supports all the above techniques. Introduction: Scheduling: Given a Work-Breakdown Structure (WBS), project scheduling concerns the timeline of all activities and events that must be speculated to ensure the delivery to customers is on time and with high quality. Formal methods for this task establish a network of activities and events, which also highlights the critical events in the network. The critical activities and events delay the completion of a project if they are delayed. The order of tasks and dependencies between tasks can be seen in the schedule. The probability of various completion dates can be estimated then the schedule can be re-evaluated and reworked. A simple form of the network is the Gantt charts which can be seen in the above figure. The Gantt chart shows planned and actual progress for a number of tasks displayed as bars against a horizontal time scale. Program Evaluation and Review Technique (PERT) and Critical Path Model (CPM) are two different networking methods for estimate a project schedule. With preliminary knowledge from WBS, CPM constructs an Activity-on-Node (AON) network where each node represents an action. From a start node, with source nodes are predecessors, and target nodes are given tasks, we can construct a graph of tasks. Each task is assigned an expected time $t_e$. The longest path $\mathcal{L}$ in the network (with sum of time $T_e=\sum_{e\in\mathcal{L}} t_e$) show the shortest time needed to complete the project. $\mathcal{L}$ is the critical path and $T_e$ is the critical time of a project. The right-hand-side figure shows the CPM of 5 tasks with assigned resources (workers and equipment). Each activity can have some uncertainty in estimating durations; therefore, a stable predictor can be made using a three-point analysis: optimistic estimates ($a$), pessimistic estimates ($b$), and most likely estimates ($m$). The expected time can be estimated as follows. $t_e=\frac{a+4m+b}{6}$ The estimates $a,b,m$ can be found when the distribution of duration for an activity is known as a priori. Then the most likely time $m$ is the mode of the distribution. PMs select $a$ at the actual time required by the activity will be $a$ or greater about 99 percent of the time. Similarly, PMs select $b$ at the actual time required is $b$ or less for 99% of the time. The standard deviation of $t_e$ can be estimated as $\sigma_e=\frac{b-a}{6}$. For an activity, the slack is the difference between the latest possible start date (LS) and the earliest possible start date (ES), i. e. , slack = LS - ES. With these statistics, the PM can determine the uncertainty of the completion date. For example, the PM promised that a project finishes in $D$ days. Then we model the uncertainty by a normal distribution $Z=\frac{D-T_e}{\sigma_e}\sim \mathcal{N}(0, 1)$ For example, $D=50$ days, $T_e=39$ days, and $\sigma_e=6. 15$ days, then the likelihood that the PM can keep the promise is $\mathcal{N}^{-1}(Z)=\mathcal{N}^{-1}\left(\frac{50-39}{6. 15}\right)=\mathcal{N}^{-1}(1. 79)\approx 96\%$ In other words, the probability of lateness is only 4%. The risk assessment, another aspect of the schedules, will be addressed in another article. Resource allocation: CPM can be used for resources as well. However, to accelerate the speed, introducing more resources (or crashing with resources) can be useful. It is equal to trading time with cost, and this is the essence of resource allocation problem. Resource loading is the amount of a specific resource to finish an activity in a duration. For example, to finish a task, we need 2 labor-hour, then that is the resource loading. In reality, the resource loading can be uneven during the project lifetime. Resource leveling concerns making the loading even throughout the project time. It is done by shifting the tasks within their slack allowances. In other words, leveling helps to stabilize the workloads over time. One concern for the PMs is how many workers should be hired for a given workload?If the workload is uneven, then the number of workers and other resources must be sufficient to handle the peak. In other time, when the workload is much lower than the peak workload, the worker pool size can be adjusted to avoid waste. Microsoft Project: MSP is a multi-scale project management software that handles from small projects to large ones. It supports multiple views such as grid view and board view for better reporting. Statistical analyses like CPM and PERT are supported. Gantt charts are supported in both cloud and desktop solutions. In this practice, we use MSP for scheduling and resource allocation with analytics. We use the priority rule of As Soon as Possible (ASAP). Preparation: Sample data: In this practice, we use the sample project WBS is from a typical software development project. Please download the data from the following resource.  Software development project WBS (from University of Ohio): downloadThe sample project consists of 6 phases:  Phase 1: Requirements Definition Phase 2: Logical Design Phase 3: Physical Design Phase 4: Programming and Unit Testing Phase 5: System Testing Phase 6: InstallationFor simplicity, we only take a subset for example: 123456781	Requirements Definition (Phase 1) 1. 01	Requirements funding  1. 01. 01	Review project request  1. 01. 02	Establish preliminary justification  1. 01. 03	Fund Phase 1  1. 01. 04	Prioritize project  1. 01. 05	Establish project teamRegister for an evaluation version of Project: Unfortunately, Microsoft Project is non-free. Price for monthly users starts from 10$/user. Fortunately, they provide us an evaluation version for 30 days. Please visit https://www. microsoft. com/en-us/evalcenter/evaluate-project and choose Project Plan 3 and click Continue button.  Follow the step-by-step instructions to have an account set up. Visit https://portal. office. com/account/?ref=MeControl#subscriptions you can see as follows.  Visit https://portal. office. com/account/?ref=MeControl#installs and click Install Project button to get the download file. After downloading, please double-click and follow the guides to install. After installation, you may need to log in to your Microsoft account registered in the previous steps.  Run: From the Start screen of MSP, please select Waterfall project. Although Sprint project is available, we will explain it in another article.  Register tasks: In the Waterfall project, the default view is the Gantt chart. Now, we need to input by hands the WBS tasks. Note that the default unit for Waterfall projects works is days. Then workers will be awarded daily. Is this a good cost performance? In reality, it is better to make payrolls by precisely to hours and even by minutes. For example, worker A registers a task S for 1 working day. But in fact, nobody spends all 8 working hours for work!A study has shown that about 12% of working hours have been spent on other things like personal time. Estimating task durations by minutes or hours helps to reduce such uncertainty. MSP supports measurements by minutes, but hourly payment is good enough. Select Options &gt; Schedule &gt; Input worktime unit to minutes. To view the state of distributed tasks, you can switch the view Assigned workers and resources can be shown, too. Register resources: Please switch to Resource Sheet view.  There are two kinds of resources: noncurrent assets like equipment which price is fixed with one-time payments, and other resources which must be paid by hourly or monthly payments such as labors of office rental fees. , office rental fees aren’t counted in many cases, especially for remote workers. In this case, we have one worker and one PC (laptop) that are assigned to the tasks. For this case, we assign the price of 2,000 JPY per hour (tax included) to the payment. Overtime is paid by 2,500 JPY per hour. Note that when the tasks of the same person are run parallel, the total time is a sum!Then the working time is doubled, and any hours exceed 8 hours per day will be counted as overtime. To avoid payments for such concurrency caused overtime, the PMs should:  Break down the tasks smaller and make them run sequentially within the 8-hour frame.  Pay by hours or minutes for non-contract workers. If they are contract workers, PMs don’t need to care about the units but need to break down tasks. After registering the resources, switch back to the Gantt chart view, and assign the resources to tasks. Under the Report tab, there are many other views that are useful. Readers are recommended to explore by themselves.  View Critical Paths: Switch to the Gantt chart view, and from the View tab in the ribbon, choose Critical as the filter. We will observe the Critical Paths automatically as follows.  PERT Analysis: We use the macro from https://github. com/flametron/MSProject-2019-PERT. With the weight combinations $(1,4,1)$ we have the estimates of Duration: Conclusion: We explained several concepts in project scheduling and resource allocations. We also experienced with the Microsoft Project 2019 to handle scheduling problems and resource assignments. We applied some tips to reduce project costs. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details      Project Management Software - Microsoft Project &#8617;    "
    }, {
    "id": 62,
    "url": "https://wanted2.github.io/commitments-expenses-cash-flows/",
    "title": "Commitments, Expenses, and Cash Outflows - Three Perceptions of Project Costs",
    "body": "2021/06/13 - Different roles in a project can have different perceptions about cost [1], [2]. Commitments, expenses, and the organization’s cash flows are vital to consider about the cost. Losses incurred in the projects include finance expenses and taxes, too. Project managers can use their understanding of these perceptions to impact the cost of the project. Deferring and delaying are techniques the PMs can use to enhance their contributions to improve project expenditures. Three perceptions of project cost: Each key person in a project cares about different aspects of cost. To the project managers, commitments must be high to prove efficiency. However, accountants implemented their accounting systems that do not identify commitments. Accountants care more about expenses for labor cost, materials, and equipment. Senior and top managers care more about the cash outflows of the organization. If the project does not terminate on time, another tax term may start, and more money for taxes must be spent. Usually, a project should reduce all these factors, but the relationships of these costs often take the form in this figure. Commitment leads, cash outflow lags, and expenses fall in the middle. The bill: A typical bill of the project (profit and loss terms) is as follows.       Revenue (project sell price)   +$1,000,000       Cost of goods sold (project costs)   -$750,000       Gross-margin   +$ 250,000       Selling, general &amp; administrative expenses   -$180,000       Profit before interest and taxes   +$ 70,000       Financial expense   -$ 30,000       Profit before taxes   +$ 40,000       Taxes   -$ 20,000       Net profit   +$ 20,000   Project cost (cost of goods sold): In the exemplar bill, they sold the project at 1M dollars, but the actual cost of goods sold was about 75% of the selling price. Then the gross-margin (includes sales/GA costs, financial expenses, and taxes) was about $250,000. The net profit after tax was 20,000 dollars. While the cost for Sales and GA is almost fixed in many companies, the cost of goods sold can be reduced by the actions of PMs. An effective PM can reduce it by carefully making the choices:  Considering alternatives for design concepts and taking “trade-offs” can reduce the cost.  QA is expensive: QA or any safety factors can enhance the quality but also makes the product more expensive. Then PMs must be careful when thinking about doing excessive QA in their projects. Usually, after having safety at standard, it is possible to stop doing QA to prevent the cost go higher.  Control work: developers and technicians can require excessive time for development and testing. This only increases the cost of the project. PMs must control the cost of development by setting deadlines for any tasks and tests.  Cost of changes: changes are often in projects. The PMs must be aware of the contingency budget. Unexpected problems ultimately arise, at which time the funds are needed. Uses of this budget to finance a scope change is neither advantageous to the project manager nor to management.  Changes must be made under client or management approval Procurement of material, equipment, and services should be planned in the project schedule. The procurement that bases on incomplete requirements should be avoided.  Economy phenomena like inflation can impact the project negatively. The PMs must handle such impacts effectively. Finance expenses and taxes: Finance expenses include the order placements during the project’s life cycle, withheld funds, financial supports from vendors. Internal projects may not experience outside pressures like client’s payment considerations. However, they live with the pressure to shorten the lifetime and to pay back faster to the organization. In both cases, PMs need to care about delivering the outcomes faster to keep profits and reduce costs. In other words, it always means to stop early. Taxes are also another aspect for the top management to “kill” a project earlier. Sometimes, it can be an order to reduce the lifetime to a half. The reason behind the order can be the risk they feel about not meet a tax term deadline. In some cases, PMs can insist company’s top managers by changing the schedule to shift benefits and expenses from a tax term to the next. Defer when possible but don’t delay the termination: When thinking about the timing to place purchase orders, the PMs can use deferring techniques to avoid buying equipment and material when the price is high. The work of placing orders can be deferred until the price becomes low. However, this technique comes with risks: if the price does not go lower or if the item goes unavailable, it becomes a backfire to the project. Deferring order placements can be helpful, but delaying the termination of a project can be dangerous. Many organizations expect a project finishes on time to meet a tax deadline. Delaying the termination for only a few days can result in a new tax term starts, and lots of money will be spent. Indeed, a PM should insist the organization by shortening the lifetime of the project when it is possible and do not delay the end. Conclusion: An effective project manager can help the organization reducing project costs, expenses, and cash outflows a lot by the techniques described in this article. Financial expenses and taxes can be other factors for the top managers to shorten project lifetime. PMs must be aware of these risks and use techniques like deferring and delaying at the right time to mitigate. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. DetailsHamburger, D. H. 1986. Three perceptions of project cost. Project Management journal. (1986), 372–378. Details"
    }, {
    "id": 63,
    "url": "https://wanted2.github.io/design-structure-matrix/",
    "title": "The Design Structure Matrix and Information Flow",
    "body": "2021/06/12 - Tracking the information flow throughout the project is required for planning the coordinating flows. Traditional tools like the Gantt chart are good at tracking the interdependency between runs of concurrent tasks but fail to describe the causal dependencies between flows. This problem is magnified when multidisciplinary teams join the stage. Hence, the problem becomes increasingly serious while handling many domains at once. To cope with this issue, Design Structure Matrix (DSM), Domain Mapping Matrix (DMM), and Multiple Domain Matrix (MDM) have been proposed [1]. While DSM tracks the information flows, DMM can be used to show the person-task assignments. MDM combines both DSM and MDM in multiple domains, i. e. , coordination of not only tasks but also people. It can track all structures of the team, tasks, and assignments at once. Design structure matrix: Integration management is important because it makes the deliverables regarding customer needs but lacks coordination of flows. For instance, in the below figure, task 1 needs to gather information from task 3 before running. Such information flow does not appear in previous tools like the Gantt chart. While not aware of such flow, planning tasks, like which task should be done before a particular task, can result in a mess. With this schedule, then task 3 is completed after task 1. However, task 1 requires information flow from task 3, thus task 1 must be revisited and reworked after task 3’s completed. Source: dsmweb. org Concurrency: Concurrent engineering helps to enhance the performance but makes the system is hard to design due to the complexity of dependencies and reworks. Let’s see the example. The PM can plan to let task 2, task 3, and task 4 run concurrently. What happens here is that there are two situations needing reworks. The first one was already explained in the first section: task 1 must be updated according to information from task 3. The second one is task 4 that needs information from task 5, but task 5 also completes after task 2 running concurrently with task 4. Let’s see whether we can eliminate the reworks. The first solution is to move the mark X above the diagonal: for example, let task 5 inputs directly to task 6, and instead of task 3, task 6 inputs to task 1. The second solution is to add more activities into the concurrency: for example, add task 5 and task 1 to the concurrency, and all first 5 tasks run concurrently. Domain Mapping Matrix: Instead of working in only one domain, sometimes we may need to map two different domains, such as assignments between people and tasks. Domain Mapping Matrix shows such assignments in a matrix form. For example, by watching the above DMM, one can find who is the Person In Charge (PIC) of a task in the project. Multiple Domain Matrix: Single-class tracking can be done with DSM in a domain like tasks. Mapping different domains are done using DMM, then attributions of tasks to identities can be done, too. But when we have multiple classes (domains), the problem becomes complex. Multiple Domain Matrix helps to solve the multi-class tracking problem.  MDM is helpful here because it can demonstrate all inter-class and intra-class relationships at once. Not only the hierarchy of the tasks but also of people in the team can be seen. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 64,
    "url": "https://wanted2.github.io/matrix/",
    "title": "The Matrix Organization",
    "body": "2021/06/10 - Pure project organization emphasizes the role of project managers, while functional organization emphasizes the importance of functional departments such as accounting, finance, marketing, R&amp;D, . etc. Both have their own advantages and disadvantages. The combination is the Matrix Organization [1] which takes the strengths of both predecessors in a “balanced” form between project managers and functional ones. The Matrix organizations: Formation: Matrix Organization can be represented as tabular data, where each row for a PM and each column for a functional department. The president directly manages the departments, and the PM deal directly with the clients. PMs have flexibilities to handle the clients, while they also need to deal with the functional managers for many aspects of their projects: human resources, technology, resources, schedules, . etc. PMs decide where, when and what the project members achieve, but the functional managers select those people for the project and which technology is used in the project. Each cell represents the effort the PMs need to borrow from the departments for the aspects of their projects. For example, PM1 needs a half man-month from the R&amp;D department, but PM2 does not need to borrow effort from R&amp;D. Advantages: Since its flexibility, PMs are the center of the organization while functional departments are the supporters. Responses to the clients are rapid, and knowledge sharing among the organization and across projects/departments is possible. Disadvantages: First, it is the conflicts between PMs and functional managers when dealing with resources. The workers always feel like they have two bosses, and that is another conflict. Second, it is a myth about who is in charge of the project. Since PMs control only people and tasks, functional managers control everything else like resources, schedules. Third, people in these projects tend to resist the termination of the projects. The first and second disadvantages often result in political fighting inside the organization. Imbalanced organizations: Pure project organization is like its name, purely project-oriented. Functional departments, if any, are just small offices, even with single members. Since there is no functional department, each project must carry out responsibilities with all functional aspects such as budgets and costs (accounting), finance, technology, and everything else. The project managers are then powerful in their projects. They decide every aspect of the projects, deal directly with clients, and only report to the top management (president or the program managers). Even if there are functional managers in the organizations, PMs do not care about them. The pros include the freedom of each project and the PMs, less time spending on negotiating resources with functional departments. But the disadvantages are also many. Since each PM need to decide everything in the project, the burden is heavy. Since each project is independent, less collaboration and knowledge sharing between projects is expected. In short, PMs here are quite soloers. Another imbalanced organization is the Functional Organization. In this formation, it is usually no PM, and no project, of course. The organization is of many departments, and there can be some internal projects in each department. But as each department has its technical focus, projects that require collaborations across departments are rare. Dealing with clients is slow because of the deep structure of management. In other words, processes in this form are slow. Conclusion: Imbalanced options like pure project organizations and functional organizations are different extremes of the project spectrum. The Matrix Organization (TMO) takes a more balanced form and comes with many strengths. However, a source of conflicts is the political fighting between PMs and FMs, and the myth of finding who is actually in charge of the project should care are problems. Pure project options may fit more small companies like startups, while functional ones fit big organizations like universities and corporations. TMO is the middle and fits mid-size ventures. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 65,
    "url": "https://wanted2.github.io/the-fisher-method/",
    "title": "The Fisher's method",
    "body": "2021/06/08 - Conciliating the conflicts with allies is an important step to maintaining a good alliance. Key persons in the organization may be stubborn, especially the functional managers. When both the functional managers and the project managers are technical people, cooperation between them often results in conflicts. Fisher’s method (1983, [1]) is an effective way, often named by the “principled negotiation” or win-win method, applying in situations where allies disagree in some aspects of the project. Deal with the problem, not with the person: Separating the people from the issues allows the parties to address the issues without damaging their relationship. It also helps them to get a clearer view of the substantive problem. Three sources of problems in parties involved in negotiation:    The multi-view problem: each party has its viewpoints, with different perceptions of the facts. The parties should not assume that their worst confusions become the actions of the other party. Nor should one side blame the other for the problem. One should put itself in the stand of others.     The emotion problem: Getting the task done is a frustrating process, sometimes. When their interest is threatened, people often react with confusion or anger. When seeing a man with anger expression, we may dismiss the man’s feeling, but it is likely to provoke more intense emotional responses. Dealing with emotions requires a calm mind. Symbolic gestures such as apologies or an expression of sympathy can help to defuse strong emotions.     The communication problem: A party may not listen to what another one’s speaking. The listeners should pay full attention to the speakers. However, misunderstanding can happen even the listeners hear carefully.  Dealing carefully with these issues may start by preventing them from arising. Allies should think about their counterparts as partners, not as adversaries. Focus on the interest, not the positions:  Your position is something you have decided upon. Your interests are what caused you to so decide. Fisher and Ury [1] Then interests are the root cause to solve the conflicts. Fisher argued that negotiations must focus on such interests. The first step is to identify the interests by asking the reasons for the positions to hold. For example, when the PM asks the functional managers that “We need to finish the task by June 25th”, and the functional manager responds that “We are busy until the end of June”. This conversation is no end without expected output. The PM starts with a position way then the functional manager also ends with a position argument. Removing the position may be like starting with “Can we talk about the schedule?”. The scheduling problem touches on the mutual interests of PM and functional manager, and it works. Once the parties have identified their interests, they must discuss them together and in a positive way. Generate more options for mutual gain: More options allow parties to achieve safer decisions. However, there are four problems for multi-party scenarios to have mutual gain:  Parties rely strongly on only one option and fail to consider alternatives.  Parties have the intent to narrow the set of options to find limited answers.  Parties have a bias on finding the options which bring advantages for them (bias on win-lose terms).  Parties do not feel the responsibility to find the solution and decide to let others do so. To generate meaningful options, parties should come together and brainstorm for creative proposals. Evaluation should be separated and start from the most useful proposals. Use Objective Criteria: When interests come in opposing directions, using objective criteria brings better resolution. These criteria ground parties into the same standard and resolve the differences. This leads to an agreement or a situation in which parties can agree. Scientific findings, professional standards, or legal precedent are possible sources of objective criteria. One way to test for objectivity is to ask if both sides would agree to be bound by those standards.  For example, children may fairly divide a piece of cake by having one child cut it while the other choose their pieces. Some notes on using objective criteria:  First, each issue should be approached as a shared search for objective criteria.  Second, each party must keep an open mind and reasonability.  Third, while they should be reasonable, negotiators must never give in to pressure, threats, or bribes. References: Fisher, R. and Ury, W. 1983. Getting to Yes: Negotiating Agreement Without Giving In. New York: Penguin Books. Details"
    }, {
    "id": 66,
    "url": "https://wanted2.github.io/never-let-the-boss-surprised/",
    "title": ""Never Let The Boss Be Surprised!"",
    "body": "2021/06/07 - A prime law in projects is that “Never Let The Boss Be Surprised!” [1]. As such a phrase, team members must be honest to the Project Manager (PM), and the PM must also be honest to the senior managers in a parent organization. Never hide the troubles nor the risks from the PM! Introduction: On the need of a man with high self-esteem: At the beginning of a project, the PM must select the best personnel for the team by cooperating with the functional managers. Besides personal characteristics such as technical skills, political sensitivity, problem or goal orientation, another aspect which is usually expected to be found in the team member is high self-esteem. That translates into 日本語 as “自尊心” or into Vietnamese as “lòng tự tôn. ” In practice, there are two signs of lack of self-esteem:    The members worry about their own errors and try to hide the troubles from the eyes of the PM. They are threatened by their own mistakes, though everyone can have mistakes!     The members try to point out the mistakes of others. They try to take advantage of others’ troubles and cover their own.  A man who is high self-esteem will never hide badness nor laugh at other mistakes. The man faces troubles as challenges and helps others to improve. Then a man without self-esteem always makes the boss surprised! And “Bad News Travels Fast”: You are the PM in a company amongst other PMs. You try to hide the unexpectedness in your project spilling out, but ultimately, the “bad news travels fast” when it happens. Then the boss (senior manager) won’t be surprised because of the news, but because you hide the news. Not only the team members must be honest to their boss, but also the boss (PM) must be honest to the senior manager and the top management. Under the corporate culture, not so many things can be a secret. The right actions should include timely reports, being self-esteem, and being straightforward. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details"
    }, {
    "id": 67,
    "url": "https://wanted2.github.io/weighted-score-model/",
    "title": "Weighted Score Models",
    "body": "2021/06/06 - In an attempt to overcome some of the disadvantages of profitability models, with one example, the Discounted Cash Flow we have discussed last time, which particularly their focus on a single decision criterion, a number of evaluation/selection models that use multiple criteria to evaluate a project has been developed. Such models vary widely in their complexity and information requirements. Weighted Factor Scoring Model is such one flexible method we discuss today. Weighted Score Model (WSM): The main disadvantage of a profitability model like the DCF is the limitation to a single decision criterion at a time. A solution to integrate multiple criteria into the selection process is to make a project evaluation form, in which each row presents a different criterion of the project with a specific score. A project council is established with experts to give a score on each criterion. Unweighted Binary Factor Model gives 0-1 score $b_i\in{0,1}$ to the project for each criterion. Unweighted Score Factor Model gives a numeric score $s_i\in\mathbb{R}$ with normally at a scale of 5 for each criterion. The total score of a project is given by$S=\sum_{i=1}^ns_i,$where $n$ is the number of criteria. A sample evaluation form can be as follows (taken from [1]).  Project _________________________  Rater ____________ Date _____________       Criterion   Qualifies   Does Not Qualify         No increase in energy requirements   x           Potential market size, dollars   x           Potential market share, percent   x           No new facility required   x           No new technical expertise required       x       No decrease in quality of final product   x           Ability to manage the project with current personnel       x       No requirement for reorganization   x           Impact on workforce safety   x           Impact on environmental standards   x           Profitability               Rate of return more than 15% aftertax   x           Estimated annual profits more than $250,000   x           Time-to-break-even less than 3 years   x           Need for external consultants       x       Consistency with the current line of business       x       Impact on company image               With customers   x           With our industry       x       Totals   12   5   A project with more qualified points is more likely to be accepted. And the Weighted Score Factor Model, each criterion is associated with a weight value $w_i$. The final score for a project is then $S_w = \sum_{i=1}^nw_is_i. $ This model gives flexibility to the evaluation method because the importance of each criterion is modeled by $w_i$ and then differently. The list of criteria and weight values can be chosen by experts based on their experiences. Problem and Discussion: The following problem is taken from the PM text [1]:  Use a weighted score model to choose between three methods $(A, B, C)$ of financing the acquisition of a major competitor. The relative weights for each criterion are shown in the following table as the scores for each location on each criterion. A score of 1 represents unfavorable, 2 satisfactory, and 3 favorable.       Category   Weight   A   B   C         Consulting costs   20   1   2   3       Acquisition time   20   2   3   1       Disruption   10   2   1   3       Cultural differences   10   3   3   2       Skill redundancies   10   2   1   1       Implementation risks   25   1   2   3       Infrastructure   10   2   2   2   In practice, MS Excel is enough to get this done, but we usually use some statistical tools like the Oracle CrystalBall software1. In this case, a higher score means a better method to implement. Let’s compute the score for each method: $S_A = 20 \times 1 + 20 \times 2 + 10 \times 2 + 10 \times 3 + 10 \times 2 + 25 \times 1 + 10 \times 2 = 175$ $S_B = 20 \times 2 + 20 \times 3 + 10 \times 1 + 10 \times 3 + 10 \times 1 + 25 \times 2 + 10 \times 2 = 220$ $S_A = 20 \times 3 + 20 \times 1 + 10 \times 3 + 10 \times 2 + 10 \times 1 + 25 \times 3 + 10 \times 2 = 235$ Method C has the highest final score and is then acceptable. Choosing the most probable weight values may be a deep problem, then to aggregate them into the best decision to the next stages, adjusting the value by trial-and-error may be approached. References: Meredith, J. R. , Shafer, S. M. and Mantel Jr, S. J. 2017. Project management: a strategic managerial approach. John Wiley &amp; Sons. Details      Oracle Crystal Ball Downloads &#8617;    "
    }, {
    "id": 68,
    "url": "https://wanted2.github.io/discounted-cash-flow/",
    "title": "Discounted Cash Flow and The Hiring of a Postdoc",
    "body": "2021/06/01 - In the winter of 20xx, a professor was asked to evaluate a postdoc profile for hiring. The offer is about 4. 2 million per year (currency is not revealed). An average postdoc lifetime is about 5 years (and then moving towards tenure or perish). The inflation rate is about 3% per year. The sponsors of the professor’s project can give 50 million for five years. What is the hurdle rate that makes the profile (together with the project) is acceptable? Assumptions:    Assume that the profile is trusted that the holder can produce sufficient works in a year worths 4. 2M.     Although 50M flow to the project, other stuff also needs money. Hence, the professor needs to pay themselves. Then let’s say about 21M are secured for paying the postdoc.  Discounted Cash Flow: Also referred to as the net present value (NPV) method, the discounted cash flow method determines the net present value of all cash flows by discounting them by the required rate of return (also known as the hurdle rate, cutoff rate, and similar terms) as follows: $\mbox{DCF} = I_0 + \sum_{t=1}^T\frac{F_0}{(1+h+p)^t},$where,  $I_0&lt;0$ is the initial investment, and in this case, $I_0=-21$M, which is a negative value $T=5$ is the total number of phases.  $F_0$ is the net cash flow in phase $t$ $h$ is the hurdle rate, i. e. , the percentage of gross-income which must be returned to the investors (project’s sponsors) $p=0. 03$ is the inflation rate. Now, by substitution in to their case, $\mbox{DCF} = -21 + \sum_{t=1}^5\frac{F_0}{(1+h+0. 03)^t}$. If the DCF is non-negative then the postdoc is deemed acceptable, else rejected. When $h=15$% then how much the postdoc must produce every year?: Let’s say the sponsors require 15% of the gross-income made by the work of the postdoc. Then which $F_0$ must be to make the DCF non-negative? We would solve the equation: $DCF =-21 + F_0\sum_{t=1}^5\frac{1}{(1+0. 15+0. 03)^t} \geq 0$or,$-21+F_0\times 3. 127 \geq 0$ Then the money the postdoc is expected to make each year in the following 5 years is $F_0\geq \frac{21}{3. 217}=6. 7$M. In other words, if the sponsors require 15% of gross-income, the postdoc must make more 2. 5M than the cash they are paid. When $h=5$% then how much the postdoc must produce every year?: We would solve the equation: $DCF =-21 + F_0\sum_{t=1}^5\frac{1}{(1+0. 05+0. 03)^t} \geq 0$or,$-21+F_0\times 3. 993 \geq 0$ Then the money the postdoc is expected to make each year in the following 5 years is $F_0\geq \frac{21}{3. 993}=5. 2$M. In other words, if the sponsors require only 5% of gross-income, the postdoc must make 1M than the cash they are paid every year. What if the offer is only 3. 5M in gross-income and the initial investment is only 17. 5M?: In case of hurdle rate is 5%, then the value the postdoc must make is $\frac{17. 5}{3. 993}=4. 4M$. The expectation is low, and matching the ability of the postdoc (their profile can make 4. 2M per year as evaluated), then lower salary lower duty, straight life! In case of hurdle rate is 15%, then the value the postdoc must make is $\frac{17. 5}{3. 127}=5. 6M$. The expectation is higher than the ability of the postdoc (their profile can make 4. 2M per year as evaluated). Conclusion: what’s the strategy?: The best deal for their postdoc is to negotiate with the professor and the sponsors to have:  hurdle rate is 5%; and annual income is only 3. 5 million. Otherwise, overtime is frequent, and their life will be destroyed. "
    }, {
    "id": 69,
    "url": "https://wanted2.github.io/mOCR-mlkit-androidx-example/",
    "title": "mOCR: A real-time application of OCR with Google MLKit and Android CameraX",
    "body": "2021/05/31 - Google MLKit1 is a software solution for Machine Learning problems in mobile devices (Android and iOS). It supports computer visions and natural language applications. The library is optimized for mobiles and is convenient. In this example, we build a mobile OCR solution with Android CameraX API2. The demo code can be seen at my Github. Introduction: We build a real-time demo shown in the left figure.    When a user tap on the screen, a bounding box surrounding the location being tapped is the detection area from which text characters are recognized.   The recognized text is detected in real-time and displayed at the bottom sheet.  CameraX API supports invocations of image analysis whenever an image is captured. Setup: Google MLKit setup: Since Text Recognition API does not support local models, we have to make the application downloading the model weights through Play services (i. e. , Firebase). This can be done by using the presets in build. gradle: 1  implementation 'com. google. android. gms:play-services-mlkit-text-recognition:16. 2. 0'CameraX setup: We also need to add presets for CameraX. With this API, all events such as start capturing, handling when an image’s available, stop capturing are bound automatically to the cameraX lifecycle. The lifecycle is initialized with a camera selector, an image analyzer, and a Preview. 1234  def camerax_version =  1. 1. 0-alpha04   implementation  androidx. camera:camera-core:${camerax_version}   implementation  androidx. camera:camera-camera2:${camerax_version}   implementation  androidx. camera:camera-lifecycle:${camerax_version} Camera lifecycle: Initializing the lifecycle: CameraX API binds all events in a camera into a lifecycle. To preview captured image sequence, we need to add to the main layout a PreviewView component: 1234  &lt;androidx. camera. view. PreviewView    android:id= @+id/fs_cam_preview     android:layout_width= match_parent     android:layout_height= match_parent  /&gt;The captured images will be displayed here, and in the code of the main activity, we retrieve the view: 1mContentView = findViewById(R. id. fs_cam_preview);For concurrency, there is a com. google. common. util. concurrent. ListenableFuture class to wrap up the ProcessCameraProvider to multi-processing: 1private ListenableFuture&lt;ProcessCameraProvider&gt; cameraProviderListenableFuture;The process provider is initialized as follows (in onCreate()): 123456789    cameraProviderListenableFuture = ProcessCameraProvider. getInstance(this);    cameraProviderListenableFuture. addListener(() -&gt; {      try {        ProcessCameraProvider processCameraProvider = cameraProviderListenableFuture. get();        bindPreview(processCameraProvider);      } catch (ExecutionException | InterruptedException e) {        Logger. e(e,  ERROR:  );      }    }, ContextCompat. getMainExecutor(this));Heavyweight resources such as image analysis, previewing, and camera selection can be bind to the lifecycle owner in the bindPreview function: 12345678  private void bindPreview(ProcessCameraProvider processCameraProvider) {    Preview preview = new Preview. Builder(). build();    CameraSelector cameraSelector = new CameraSelector. Builder()        . requireLensFacing(CameraSelector. LENS_FACING_BACK)        . build();    preview. setSurfaceProvider(mContentView. getSurfaceProvider());    Camera camera = processCameraProvider. bindToLifecycle(this, cameraSelector, imageAnalysis, preview);  }For ease of use, we initialized the back camera with CameraSelector. LENS_FACING_BACK. And the preview is bound to a view mContentView. With the ProcessCameraProvider, the heavyweight resources are managed efficiently by the lifecycle owner. Using the CameraX API, we pay less for managing the lifecycles and with more reliable processing. Image Analyzer: A useful tool in CameraX API is the ImageAnalyzer interface, which is used to define custom Image Processing pipelines. Here, whenever an image capture is returned in the form of ImageProxy instances, we need to get the image, crop the region of interest and do real-time text recognition. Such a pipeline can be done in ImageAnalysis. Analyzer. analyze’s body. To keep track of the location user touched in the screen, we have a posisition: 1private Point position;And the custom image analysis pipeline: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657  private class MLKitAnalyzer implements ImageAnalysis. Analyzer {    TextRecognizer textRecognizer;    public MLKitAnalyzer() {      textRecognizer = TextRecognition. getClient(TextRecognizerOptions. DEFAULT_OPTIONS);      Logger. i( Loaded text recognition models! );    }    @Override    public void analyze(@NonNull ImageProxy image) {      @SuppressLint( UnsafeOptInUsageError ) Image image1 = image. getImage();      Logger. i(position. toString());      Logger. i( Canvas size:   + overlayView. getWidth() +  ,   + overlayView. getHeight());      int width = overlayView. getWidth();      int height = overlayView. getHeight();      int mx = Math. min(width, height) / 4;      int my = mx / 2;      int left = position. x - mx;      int top = position. y - my;      int right = position. x + mx;      int bottom = position. y + my;      if (image1 != null) {        ImageConvertUtils convertUtils = ImageConvertUtils. getInstance();        InputImage inputImage = InputImage. fromMediaImage(image1, image. getImageInfo(). getRotationDegrees());        Bitmap bitmap;        try {          bitmap = convertUtils. convertToUpRightBitmap(inputImage);        } catch (MlKitException e) {          Logger. e(e,  ERROR );          return;        }        Rect rect = new Rect(Math. max(0, left * bitmap. getWidth() / width),            Math. max(0, top * bitmap. getHeight() / height),            right * bitmap. getWidth() / width,            bottom * bitmap. getHeight() / height);        Bitmap crop = Bitmap. createBitmap(bitmap, rect. left, rect. top,            Math. min(bitmap. getWidth(), rect. right) - rect. left,            Math. min(bitmap. getHeight(), rect. bottom) - rect. top);        inputImage = InputImage. fromBitmap(crop, 0);        Logger. i( Image size:   + inputImage. getWidth() +  ,   + inputImage. getHeight());        Task&lt;Text&gt; results = textRecognizer. process(inputImage)            . addOnSuccessListener(text -&gt; {//              overlayView. setLatestText(text);              overlayView. setPosistion(position);              Logger. i( Found text:  + text. getText());              overlayView. invalidate();              TextView txtDetected = sheetView. findViewById(R. id. txt_detected);              txtDetected. setText(text. getText());            })            . addOnFailureListener(e -&gt; {              Logger. e(e,  ERROR );            })            . addOnCompleteListener(task -&gt; image. close());      }    }  }The recognizer is initialized in the constructor, and it will take some seconds for the first time since the model needs to be downloaded from Firebase. After the text blocks are detected, we will display the results in a bottom sheet. A green rectangle with an aspect ratio of 2:1 is drawn around the touched location after passing the position to the OverlayView class. Google MLKit: The Machine Learning pipeline running in the Image Analyzer is real-time, and the precision is quite good. By limiting the search region to a 256 x 128 rectangle with the center is the touched location seems to be helpful to reduce the false alarms (noisy text). It can be a good interaction since apps like searching for words in English often need to focus only on one compound word. For example, foreign tourists (people who don’t use English), when looking for unknown words, may only need to adjust the rectangle to the word they don’t know. Conclusion: I used Google MLKit and Android CameraX API to build a lightweight mobile OCR application, the mOCR. The result is a real-time recognition achieved by CameraX and MLKit. The accuracy is good with a focused design in human user interaction. A promising application should look into dictionaries for signs in metro stations or airports, and it is helpful in the Olympics. The next step is to add the dictionary lookup to mOCR. References:       ML Kit - Google Developers &#8617;        CameraX - Android デベロッパー - Android Developers &#8617;    "
    }, {
    "id": 70,
    "url": "https://wanted2.github.io/ai-intellience-stupidity/",
    "title": "Máy móc thông minh hay ngu ngốc: Hiện trạng và kỳ vọng năm 2021",
    "body": "2021/05/29 - Ảnh bởi Alex Knight trên tuần san Unsplash  Biết, nhưng nói không biết … ấy là biết. (Lão Tử)  Ta dại, ta về nơi vắng vẻ;Người khôn, người đến chốn lao xao. (Ngạn ngữ cổ Việt Nam)  Machines are still very, very stupid. The smartest AI systems today have less common sense than a house cat. (Yann Lecun, Turing Award 2019 winner) Cái sự “lập lờ” giữa ngu-khôn và cái gọi là “common sense” (kiến thức sống): Các cụ ta ngày xưa “khiêm tốn” nhận mình ngu bằng câu “Ta dại, ta về nơi vắng vẻ. Người khôn, người đến chỗ lao xao”. Tuy nhiên, thực ra câu này hàm ý mỉa mai sự đời, tức là chúng chửi các cụ ngu thì các cụ “chuồn” về nơi thanh tịnh các cụ sống, còn kệ chúng nó xô bồ bon chen. Thế các cụ có thực sự là “ngu” không?Chắc là không, mà câu này chỉ hàm ý mỉa mai, vì các cụ đã trải qua hết các đắng cay ngọt bùi của sự đời, kinh nghiệm còn thiếu gì mà các cụ phải nhận dại. Cái sự “lập lờ” (ambiguity) này không chỉ có trong văn hóa Việt Nam mà có trong phần lớn văn hóa của người Á Đông. Lão Tử ở Trung Quốc có câu “Biết, nhưng nói là không biết … ấy là biết”. Điều này thể hiện một bộ phận không nhỏ trong xã hội đương thời của Lão Tử có hiện tượng “biết, nhưng cứ nói là không biết” (known but pretend to be unknown), và Lão Tử hàm ý cũng mỉa mai, có lẽ những người đó mới thực sự là người “biết”. Suy nghĩ của người Á Đông là như vậy, chỉ cần nhìn thấy được sự lập lờ để né tránh, còn chấp nhận cho nó tồn tại. Tuy nhiên, người phương Tây thì thường có khuynh hướng làm rõ ràng ra. Như giáo sư Yann Lecun, người nhận giải Turing năm 2019, một giải tương đương giải Nobel cho ngành khoa học máy tính, đã thẳng thắn phê “kém, ngu” cho ứng dụng máy móc hiện tại1. Tôi xin tạm dịch nguyên văn lời phát biểu của ông trong lễ nhận giải cách đây 2 năm có lẻ:  Máy móc (hiện tại năm 2019) vẫn rất, rất ngu. Hệ thống AI thông minh nhất ngày nay còn yếu kiến thức sống hơn cả một chú mèo nhà. Cái kiến thức sống, tiếng Anh là common sense mà giáo sư Lecun nói đến thực ra cũng phải là điều xa lạ với trí tuệ con người. Trong mảng video game, khi chơi game Montezuma’s Revenge, việc gặp phải một cánh cửa bị khóa thì bước tiếp theo cần làm theo kiến thức sống thông thường (common sense) đó chính là nếu cửa không mở -&gt; đi tìm chìa khóa để mở. Nhưng cái suy luận đơn giản này (reasoning) lại không có trong AI nếu không có người chỉ dẫn. Hoặc như trong ứng dụng xe tự hành, một kiểm chứng gần đây cho thấy nếu đặt một ký hiệu phù hợp trên đường, người ta có thể khiến xe tự hành đi vào làn không cho phép1. Con người với hệ suy luận đơn giản có thể biết để không bao giờ đi vào làn không cho phép cho dù có bất cứ ám hiệu nào đặt trên đường. Như vậy, AI vẫn còn rất nhiều việc phía trước, với những nhiệm vụ cấp thấp như định hướng cho xe tự hành hoặc đưa quyết định về hành động tiếp theo. Còn những suy luận cấp cao như nhìn nhận phân biệt ngu-khôn trong những nền văn hóa Á Đông, có lẽ còn ở một tương lai xa hơn. Trí tuệ nhân tạo vẫn cần giải quyết vấn đề common sense (kiến thức sống) trước khi tiến tới những vấn đề sâu xa hơn. Ví dụ như đơn giản trong bài toán game, mà để mở được cửa là một bài kiểm tra đưa ra quyết định có nên di chuyển nhà máy rời khỏi thành phố để về nông thôn mở trang trại hay không, thì có lẽ AI sẽ không thể tái hiện được hết các khả năng. Lý do cũng giống hệt như việc phân biệt ngu-khôn ở trên, giả dụ ở lại thành phố là ngu, còn mang về nông thôn là khôn, nhưng nó sẽ dẫn đến tương lai là người ta sẽ tìm cách hạ thu nhập cơ bản ở nông thôn xuống, thế là quyết định trước được đánh nhãn là khôn, giờ lại trở thành ngu. AI hiện trạng năm 2021, dù của bất cứ tập đoàn công ty nào trên thế giới, thiếu sự hỗ trợ sau lưng của con người thì vẫn chỉ là một cỗ máy vô hại, dễ bị đánh lừa. Bài kiểm tra Turing và cách phân biệt giữa AI ngu dốt với AI thông minh: Bài kiểm tra Turing1 là một phần không thể thiếu của lịch sử AI. Alan Turing là một nhà khoa học máy tính vĩ đại trong lịch sử với kỳ công phá giải mật mã của phát xít Đức. Sau thể chiến II, ông đã dành khá nhiều công sức để phát triển lên cỗ máy tính thông minh, mà đó chính là nền móng của trí tuệ nhân tạo hiện đại (chủ yếu thuật toán). Đương nhiên với tư cách một nhà khoa học máy tính và cũng là một kỹ sư hệ thống, Turing không chỉ phát triển (develop) thuật toán và mã chương trình, ông còn phải thiết kế về mặt quản lý chất lượng sản phẩm, và đương nhiên một câu hỏi đã xoáy vào tâm trí ông khi nghĩ đến sự an toàn của người dùng khi sử dụng AI: Làm thế nào để đánh giá sự thông minh của AI? Sau một thời gian dày công đào sâu vấn đề, ông đi đến kết luận trong luận văn nổi tiếng “Computing Machinery and Intellignce” [1], đó là cái cần kiểm tra chính là “AI có thực sự đang suy nghĩ?”. Các bạn đều biết, việc không suy nghĩ mà vẫn trả lời được bài kiểm tra chỉ có 2 khả năng: một là may mắn, hai là ngu nhưng copy giỏi. Khả năng hai tức là cỗ máy không suy nghĩ, nhưng nó đơn giản copy lại câu trả lời từ đâu đó. Turing thì vẫn là người phương Tây, nên có thể vì thế ông không chấp nhận sự “lập lờ” ngu-khôn mà người Á Đông thường chấp nhận (tức là ngu nhưng bắt chước giỏi thì cứ tính là khôn vì vẫn được việc, và lại rẻ nữa chứ, vì mức giá của công việc AI ở phương Đông hiện tại chắc chỉ bằng 1/10 tới 1/5 của Silicon Valley), và sâu hơn, ông không tính cái trường hợp hai là trí tuệ thông minh. Tất nhiên, với suy nghĩ của phương Tây, Turing cũng đúng. Vì ngày nay chúng ta đang phải đối phó với nguy cơ DeepFake1 chẳng hạn. Đó chính ví dụ rõ ràng của trường hợp 2, mà Turing muốn loại. Và bài kiểm tra Turing chính là để AI thực hiện phỏng vấn với con người và con người sẽ tìm cách hỏi đáp theo một kịch bản mở (chứ không phải lời thoại soạn trước), để thử thách AI. Sau đó, dựa vào sự xác nhận của con người đã tương tác với AI để đánh giá độ hoàn hảo của AI. Và quay lại vấn đề ở mục 1. , vấn đề thử thách nhất với AI trong màn phỏng vấn này chính là làm sao đánh giá được những quyết định của chính AI đưa ra là ngu hay khôn, là biết hay chưa biết về lâu dài. Vì những quyết định đó sẽ ảnh hưởng tới lợi ích về sau, như vị trí làm việc tương lai, đãi ngộ, …Còn về phía người thực hiện phỏng vấn, thì họ cũng như nhà tuyển dụng, chuyện ngu hay khôn, biết hay không cũng không quan trọng bằng việc tìm ra AI tốt có khả năng tự suy nghĩ, chứ không phải ngu nhưng bắt chước giỏi. Đó chính là ý nguyện của Alan Turing! Kết luận: Qua bài tiểu luận nhỏ, hy vọng đã có thể đưa đến các bạn một cái nhìn đúng về sự thông minh của AI hiện tại. Thực ra cái quan trọng nhất của công việc AI, là đảm bảo di chúc của Alan Turing được thực hiện, tức là, AI phải có khả năng tự suy nghĩ, cái đó thì thông qua conversation là bài kiểm tra Turing (mà thực ra là phỏng vấn). Một việc chắc chắn không thể tránh khỏi đó là hiện tại các phát triển AI chỉ mang tính cấp thấp như nhận dạng, tracking, phát hiện, cảnh báo có điều kiện, … trong tương lai, cùng với Deep Learning (thuật toán học dựa trên hoạt động của não người, để cho mô hình AI học từ hàng ngàn, hàng triệu thậm chí hàng tỷ nơ rôn não), thì chúng ta sẽ nhanh chóng đưa AI xử lý hết các nhiệm vụ cấp thấp, để tiến tới những nhiệm vụ cấp cao hơn, như quyết định có lợi cho tương lai, phân biệt ngu-khôn, biết-chưa biết, … References: Turing, A. 1950. Computing machinery and intelligence. Mind. 49, 236 (1950), 433. Details      Không chỉ là video khiêu dâm bóng ma Deepfake đang khiến cả thế giới lo sợ vì ngày càng khó kiểm soát &#8617; &#8617;2 &#8617;3 &#8617;4    "
    }, {
    "id": 71,
    "url": "https://wanted2.github.io/on-limitation-mediapipe/",
    "title": "On Limitation of MediaPipe Holistic Face Detection Module",
    "body": "2021/05/22 - Source 1, Source 2, Credit: Microsoft Google AI announced MediaPipe Holistic1 as a simultaneous face, hand, and pose inference engine for on-device AI. As we knew, on-device AI works in a specialized environment such as Edge devices (Arduino, Raspberry Pi, Jetson Nano) and mobile devices (Android/iOS/…). These environments are characterized by limited computing power (except Jetson Nano, all are low-end CPUs), often no Internet (wifi modules maybe not embedded). Then MediaPipe is a great offer. It provides a consistent interface for working with deep learning models and computer vision models in various programming languages (Java, Swift, Python, Javascript, …). The solution is also end-to-end, so the code can be done by calling ready-to-use functions. But wait, such a big deal?This post provides a fairer view of the MediaPipe library for on-device face detection. The result is that, although MediaPipe is fast, however, the accuracy is limited for crowded scenes. It works best when there is only one person in the frame. Lesson 1: MediaPipe works poorly in multi-faces scenario: Let’s start with the famous selfie photo made by Microsoft Lumina 730.  My code is as follows. 1234567891011121314151617181920212223242526272829import cv2import mediapipe as mpfrom time import timefile_list = ['selfie. jpg', 'selfie-3. jpg', 'selfie-small. jpg']mp_face_detection = mp. solutions. face_detectionmp_drawing = mp. solutions. drawing_utils# For static images:with mp_face_detection. FaceDetection(  min_detection_confidence=0. 5) as face_detection: for idx, file in enumerate(file_list):  print(f'Processing {file} . . . ')  image = cv2. imread(file)  # Convert the BGR image to RGB and process it with MediaPipe Face Detection.   t1 = time()  results = face_detection. process(cv2. cvtColor(image, cv2. COLOR_BGR2RGB))  t2 = time()  print(f'Processed {file} in {t2-t1:. 5f} second(s)')  # Draw face detections of each face.   if not results. detections:   continue  annotated_image = image. copy()  for detection in results. detections:   print('Nose tip:')   print(mp_face_detection. get_key_point(     detection, mp_face_detection. FaceKeyPoint. NOSE_TIP))   mp_drawing. draw_detection(annotated_image, detection)  cv2. imwrite('. /annotated_image' + str(idx) + '. png', annotated_image)I downloaded the selfie photo and named it selfie. jpg. Since I speculated that MediaPipe does not work in such a scenario (More than 1,000 faces in one photo), I cropped two different versions: one with less than 100 faces (selfie-small. jpg) and one with only 3 big faces (selfie-3. jpg).      Original photo       Cropped image with less than 100 faces       Cropped image with 3 big faces  Results is as follows. 123456789101112131415161718192021python run_mediapipe. pyProcessing selfie. jpg . . . INFO: Created TensorFlow Lite XNNPACK delegate for CPU. INFO: Replacing 162 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 2 partitions. Processed selfie. jpg in 0. 02069 second(s)Processing selfie-3. jpg . . . Processed selfie-3. jpg in 0. 00496 second(s)Nose tip:x: 0. 62226236y: 0. 37454954Nose tip:x: 0. 23528881y: 0. 65063083Nose tip:x: 0. 43768775y: 0. 32625142Processing selfie-small. jpg . . . Processed selfie-small. jpg in 0. 00852 second(s)What this means is that, MediaPipe did not detect any faces in the original scenario (1,000 faces) and second scenario (about 100 faces). But when there are 3 big faces, the result is promising (no miss in the 3 faces I wanted to detect). Then for on-device AI, we cannot expect too much. The limits of them which I obtained from this example:    Miss all faces in crowded scenes.     Miss all small faces: even with selfie-3. jpg, only big faces (area is more than 10% of the whole image) can be detected well. Other small faces are missed.  With low budgets, we cannot expect too much. One bright side of MediaPipe from this result is that because small faces are often missed, then false positives (spoofers) is not a serious problem. For critical applications such as face authentication, making wrong decisions (false positives) can lead to spoofers getting in the system, but if MediaPipe misses too many small faces, our small spoofers are omitted hopefully in return. Lesson 2: MediaPipe is fast: For the original image, MediaPipe took about 20 ms on average. For the small image (100 faces), it took about 10-15 ms. And for the smallest image, it took about 7-8 ms. Since MediaPipe is not good at crowded scenes, then we can speculate that it is used in scenarios with a few faces. In such scenarios, then 7-8ms/image, or 125 FPS is not bad. Conclusion: MediaPipe is fast but works poorly in crowded scenes. From this observation, one recommendation is to only use it in one-person or few-people scenarios. The source code can be found at my Github. References:       Google AI Blog: MediaPipe Holistic – Simultaneous Face, Hand and Pose Prediction, on Device &#8617;    "
    }, {
    "id": 72,
    "url": "https://wanted2.github.io/understanding-object-detection-nms/",
    "title": "Understanding Object Detection Algorithms: the Non-maximum-suppression (NMS)",
    "body": "2021/05/20 - The non-maximum-suppression (NMS) is one of the most common parts of object detection methods. The idea is to remove the overlapping bounding boxes and keep only the separated boxes in the detection result. Given a threshold and starting from a particular box, any other boxes which have the overlappings with the reference larger than the threshold will be opted out. The process is repeated until all boxes are visited. Basic algorithms and speed-up: Let the threshold be $\theta=0. 33$ and start from the red box. Then we can remove the dark and the blue ones because they have overlappings with the reference larger than 0. 33. Here, we chose the overlap is the rate between intersection and union of the boxes. It is illustrated as follows.  As illustrated above, if the reference box (green one) has more than $t$ of the area of the target box (dark box), then the target box is overlapped with the reference and that the target should be removed.  Algorithm parameters: threshold $\theta \in (0, 1]$ A list of detected bounding boxes $L$.  Procedure $NMS(L, \theta)$:  $\quad$1. Sort the box $L$, for example, by the bottom right coordinate values. $\quad$2. Initialize the picked boxes list $P\leftarrow \phi$ $\quad$3. Loop until $L$ become empty:  $\qquad$3. 1 Choose the last box in the sorted list, push the index to the picked indices $P$  $\qquad$3. 2 Find all overlapping boxes with the reference (the last box in $L$), and remove all of them from the list $L$  $\qquad$3. 3 Remove the reference box from $L$ To implement an efficient (fast) NMS, it is best to avoid using nested for loops. For example, in some reference implementations, step 3. 2 can be implemented as a for loop. However, if we use numpy matrix operations such as numpy. where or numpy. multiply, we can perform faster the NMS. An example is from Adrian Rosebrock’s implementation which resulted in a 100x faster NMS. A careful implementation of NMS is ready to use is the tf. image. non_max_suppression. I also used it in the inference of YOLOv5. Discussions: Why sorting the boxes before running nms?: It is quite trivial, and often omitted issue in NMS. Let’s see the first example we explained. What if we don’t start from the red box but the dark box?What happens is as follows.  The result has changed, according to the changes in the choice of the reference. Therefore, we need to sort the box list before running NMS. Should we prune low-confidence boxes before or after running NMS?: In my implementation of inference of YOLOv5, I prune low-confidence boxes before NMS. Because NMS takes time, pruning low-probability candidates beforehand is preferred. After NMS, if the candidates is still too many, then we can merge boxes again using another overlapping measurements such as the IoU. In this implementation, the NMS often takes about 10-20 ms to opt out some hundreds boxes. Conclusion: We discussed about the vital part of object detection to remove redundancies in detected results: the non-maximum-suppression algorithm. To enhance the accuracy with NMS, some improvements such as the Soft NMS [1] should be the idea. References: Bodla, N. , Singh, B. , Chellappa, R. and Davis, L. S. 2017. Improving Object Detection With One Line of Code. CoRR. abs/1704. 04503, (2017). Details"
    }, {
    "id": 73,
    "url": "https://wanted2.github.io/animal-vision-series-part-1/",
    "title": "Animal Vision Part 1: Course Introduction",
    "body": "2021/03/06 - The purpose of this learning-by-doing course is to get familiar with a cross-discipline topic: Computer Vision for Agriculture. Specifically, we introduce Animal Vision (AniV), targeting canine animals. Learners get through several useful applications of Computer Vision (CV) into Animal Farming:  Object Recognition, Detection and Segmentation (including class, instance and panoptic segmentation [1]) Methods to adapt to new domains in farming such as Transfer Learning and Domain Adaptation.  Image enhancement methods such as de-raining, super-resolution, . etc.  Applications such as Dog counting, animal crowd density estimation, anomaly detection. The objective of this course is to build a proper Animal Vision system below. Introduction: Automation is changing the world with growing conventional technologies such as Information Tech, Software, Hardware, and frontier domains such as Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), Internet of Things (IoT), Computational Linguistics (CL), and Computer Vision (CV). Cloud platforms like Amazon Web Services (AWS) provide convenient solutions to handle many industrial problems at hand, such as Equipment Defect Detection using AWS Lookout for Equipment1. For Agriculture Automation, the uses of CV are also growing2 [2] in two sectors in agriculture: crops and farming.       Applications   Sub-tasks   Description   Technologies         Crop Management   Crop Monitoring   AI engine mounted into a UAV can detect crops and monitor their growth for human diagnostic. Human experts and farmers can perform further investigations and change the plan of feeding crops.    UAV/Drones, object detection/classification/segmentation           Yield Prediction   Using deep learning with satellite imagery, we can gather various information like soil conditions, nitrogen levels, moisture, seasonal weather, historical yield information of crops for precise farming.    Time series forecasting, fruit object counting       Food Safety   Environment Management   From historical information of crop environment, such as soil conditions, nitrogen levels, moisture, seasonal weather, spotting the outliers to judge the quality of the crops and fruits.    Time series anomaly detection           Spraying pesticides   We use AI-enabled drones to monitor the infected crops and spray the pesticides to prevent crops from insects and pests. The computer vision allows drones to precisely detect the infected crops and spray the pesticides accordingly.    Image anomaly detection, UAV/Drones, object detection/classification/segmentation           Automatic quality grading and sorting   Using the deep learning techniques, we calculate the percentage of infection. The grading and sorting of the fruit image helping farmers to reduce the crop damages due to storage.    Image classification, image anomaly detection, object detection/classification/segmentation           Weight estimation   CV enables automatic weight detection to ensure that animals grow well before being sent to the slaughterhouse.    Image weight estimation, UAV/Drones, object detection/classification/segmentation       Farming Management   Livestock Management   Automatic counting of animals in the farms helps the farmers better manage their hounds. When an individual has a sickness, the AI-enabled system can alert the farmers to separate that individual from the rest.    Animal counting, density estimation, image anomaly detection, UAV/Drones, object detection/classification/segmentation   The applications in this field include Forestry Management, which is similar to Crop Management but has some other issues such as burning prevention. Our targets include Farming Management (and weight estimation). System: A system for farming management:    A camera captures an image and sends it to the server. In the server, a Global Anomaly Detector (GAD) detects whether if there is a non-targeted class in the image. For example, if the monitor scenario only accepts dogs and sheep, the system must raise an alert (5) when a wolf or a human thief appears.     If GAD detects only accepted classes, (4) a 1-class detector/segmentation model (or limited-domain model, LDM) detects the (7) annotations (bounding boxes and polygons). Here, we find other benefits of GAD: it reduces the load and mistakes of LDM. Why? Because in the course, we see that 1-class models and 1-vs-rest learning mechanisms often result in over-confidence problem: the 1-class model makes many false-positives (FPs) on the rest classes. Therefore, using a GAD reduces the chance of such over-confidence by early detection of intrusions.     For each detection of LDM, it is passed to a wide-domain classifier (WDC) such as an ImageNet pre-trained model. GAD does early intrusion detection, WDC performs late detection of intrusions. Because a 1-class model can give many FPs, GAD is insufficient. Since WDC learned in 1000 classes, a larger knowledge base than LDM, it is less over-confident. WDC firmly prevents non-target classes from being intruded into the Analytics Database.     If the detection surpasses RAD, it has less chance to be a false alarm. We want to obtain further analytics information: (10) Regional Anomaly Detector (RAD) which finds if the instance region has sickness or not; (11) Fine-Grained Classifier (FGC) returns the detailed classes such as breed names of the instances (an Egyptian dog rather than a dog); and (12) Weight Estimation Model (WEM) returns the instance weight for further diagnostics.     The above information is inserted by timestamp to the Analytics Database to display in the Dashboard to the farm owners.  In this course, we learn the CV and ML technologies to implement this system. Technologies: Image classification: As said before, Global Anomaly Detector (GAD) for early intrusion detection, as well as the Wide-Domain Classifier for late intrusion detection, are implemented by Wide-domain models such as pre-trained ImageNet models. This technology is well known today, especially after 2012 with the boost of the Deep Learning era. Object detection and segmentation: We apply these classes of technology to 1-class models and RAD models above. Image Anomaly Detection: This technology aims at delivering the infected instances in an image. The deliverables can be bounding boxes, polygons, or binary labels (sickness/noon-sickness). GAD and RAD use this technology. Gathering training data can be a problem due to the frequency of rare events. Machine Learning: Data augmentation, balancing training data, domain adaptation, and transfer learning can be the core set of ML technologies we need to learn. Image Processing: Some worst cases in practice require further care about image quality:  Bad weather such as raining and snowing can result in low detection quality. De-raining models can be useful.  Camera movement can be complement by calibration.  Low-resolution imagery requires up-scaling or super-resolution. The models can be run on the input images before fed to GAD. Attribute Detection: For farming, they feed most animal breeds for meat. Therefore, attributes which affect the quality of food are inevitable. They are animal health conditions such as weight and sickness. Fine-Grained Categorization: FGC helps to identify the breeds and origins of the animals. This technology contributes to quality insurance to keep traceability of food origin. About this course: In this course, we implement the Animal Vision System (AVS) using frontier technologies. Tools:       Language   English       OS   Ubuntu, macOS, Windows x64       Programming Language   Python (C/C++)       Deep Learning frameworks   PyTorch       Computer Vision and Machine Learning libraries   opencv-contrib-python, scikit-learn, scipy, scikit-image   Posts:  Introduction (this post) Introduction about Computer Vision (using scikit-image and opencv)     Image filters, edge detection, calibration, artifact removals, . etc.     Object detection, classification and segmentation     Image features   Conventional object detection methods like the Viola-Jones cascade method.    Convolutional neural networks, convolutional/pooling/upscaling/dilated operations, etc.     Advanced problems in image enhancement (1)     Generative Adversarial Networks, Auto-Encoders    Advanced problems in image enhancement (2)     Haze removal, artifact removal, reconstruction   Super-resolution, de-raining methods.     Machine learning (1)     The role of data augmentation   Common learning problems: over-fitting/under-fitting   Over-confidence problems: Open-world recognition    Machine learning (2)     Learning in new domains   Transfer learning vs. Domain Adaptation    Machine learning (3)     One-vs-rest learning   Limited-domain learners (experts) vs. Wide-domain learners (generalists) and over-confidence problems.     Machine learning (4)     Anomaly Detection    Animal Vision (1)     Object counting   Density Estimation    Animal Vision (2)     Attribute detection: weight estimation and anomalous individual detection    Conclusion     The whole system   References: Kirillov, A. , He, K. , Girshick, R. , Rother, C. and Dollar, P. 2019. Panoptic Segmentation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Jun. 2019). DetailsTian, H. , Wang, T. , Liu, Y. , Qiao, X. and Li, Y. 2020. Computer vision technology in agricultural automation –A review. Information Processing in Agriculture. 7, 1 (2020), 1–19. DOI:https://doi. org/10. 1016/j. inpa. 2019. 09. 006. Details      Amazon Lookout for Equipment - Amazon Web Services &#8617;        Application of Computer Vision in Precision Agriculture \&amp; Farming - by Vikram Singh Bisen - VSINGHBISEN - Medium &#8617;    "
    }, {
    "id": 74,
    "url": "https://wanted2.github.io/aws-vpc-aurora-postgresql-odoo-crm-deployment/",
    "title": "Deploy Odoo to AWS EC2 and Aurora with best practices",
    "body": "2021/02/15 - IntroductionWhat is Odoo?: Odoo is an open-source software platform and eco-system for business apps that cover all your company needs: CRM, e-commerce, accounting, inventory, point-of-sale, project management, so on. The suite of open source apps is very easy-to-use and fully integrated: the customers deploy the base Odoo distribution in the Odoo cloud with few clicks or their on-premises server/cloud, and integration of the apps is without pains. Because every app needed for business as CRM and point-of-sale are available in the eco-system, installing can be done in some single steps. Which Odoo edition should I use?: Comparisons: Odoo has two different editions: Odoo Enterprise and Community. The enterprise edition provides better support and a richer set of features. With community edition, the company pays less for a usable solution but with lack of some useful features:  No mobile app.  No integration to commercial platforms like Amazon and eBay.  No digital sign, subscription, gift programs, VoIP supports.  No custom supports such as field services, AI/IoT solutions, payroll OCR, scheduling, maintenance.  No social and marketing automation. Odoo Enterprise: There are several aspects to care about the pricing of Odoo Enterprise to your organization:  The number of users: Odoo license is applied to the number of employees. Note, employees who report their timesheets are counted as users. New customers obtain a 15% discount for each user at the time of this post.  The number of apps: Only CRM app is coming with the base Odoo Enterprise. The price of each additional app ranges from 4 to 8 dollars.  Where do you host Odoo?: There are 3 hosting types: Cloud hosting, on-premises, and Odoo. sh platform. The first two options add no extra costs. You pay by yourself to the Cloud provider (AWS, IBM Cloud) or server costs. For the third option, the Odoo. sh platform, you can find a simulation below with a 50-employee company, using 50GB of data every month, 1 staging environment, and they pay about 207. 20 dollars monthly. I follow the rule-of-thumb as discussed here. 1 worker would be equal to 25 users. Therefore with 50 employees, we would like to have 3 workers. Source: https://www. odoo. sh/pricing  How do you implement Odoo?: After purchasing the license to Odoo Enterprise, you can implement it in 3 ways: by yourself, using Odoo Success Packs, or by asking help from a partner company. The first option charges you nothing. The second one is available only if Odoo has some branches in your neighborhood. It charges at least 975 dollars (after a 15% discount for new customers). The final option is to check the list of Odoo partner companies in your region and asking them for installation and maintenance supports. Every option to install and maintain a distribution of Odoo Enterprise requires at least 1000$/year, except the first one. If you put Odoo Enterprise in your server, Odoo Success Packs are not applied. Let’s get some example of pricing model for Odoo Enterprise in a medium-sized company: Example 1: Company A has 50 employees with only 15 users (Human Resource teams, accountants, sales) use Odoo. They use 37 apps (excluding CRM) and Odoo. sh platform with Odoo Success Packs Standard. It requires 1 Odoo Cloud Worker, with 100GB data storage and 1 staging environment. This plan costs A 92 dollars per month.       Item   Quantity   Price   Cost ($)         Employees usage   15   8$/user   120       Discount on employees usage   12   -2$/user   -30       App usage   37 apps   8. 22$/app   304       Hosting (Odoo. sh)   1   92   92       Implementation (Success Packs Standard)   1   333. 33   333. 33       Discount on Success Packs Standard   1   -50   -50         Total           769. 33/month   Source: https://www. odoo. sh/pricing Example 2: Company B has 50 employees with only 15 users (Human Resource teams, accountants, sales) use Odoo. They use 37 apps (excluding CRM). They choose to buy the Enterprise pack but buy no Odoo. sh or Success Packs.       Item   Quantity   Price   Cost ($)         Employees usage   15   8$/user   120       Discount on employees usage   15   -2$/user   -30       App usage   37 apps   8. 22$/app   304       Hosting (Odoo. sh)   0   207. 20   0       Implementation (Success Packs)   0   333. 33   0       Discount on Success Packs   0   -50   0       Customer hosting service   1   200   200       Development and maintenance   1   200   200         Total           794. 00/month   Source: https://www. odoo. sh/pricing The development and maintenance which company B pay for is to the developers who implement and maintain the Odoo distribution in the company. In usual cases, there are one or two people in charge of this stuff. Note, this is the lowest price the author found. Comparing to example 1, company B pays a little bit in total, and the processes rely more on human workers and are more prone to human errors. Huh, a ready-to-use solution is cheaper than a self-service solution, right? Odoo Community: The pricing model becomes affordable, but you have to implement and find hosting services yourself. Then, a sufficient background on the Odoo Community edition and deployments are required. You only pay for hosting services and people who implement Odoo on your business. Note that, although the community edition does not come with some custom apps, there is a third-party community with many alternatives for free and paid: https://apps. odoo. com/. Furthermore, when the app you need in the given third-party community, you can start adding an idea to the community since the development of Odoo is fully open-source. (However, it might be better to have ready-to-use solutions). The process is highly dependent on human and is prone to human errors. Compared to the second example, this solution costs less, but the customer hosting price and Development/Maintenance costs might not be changed. So will similar quality, it only reduces about 60% for Odoo apps. And because you need specialized personnel to implement custom Odoo apps, the cost for Development/Maintenance can increase. Deploying Odoo to Amazon Web Service (AWS)After understanding the pricing model and the merits and demerits of each edition, we go to the deployment with Odoo Community to AWS EC2, Aurora for better understanding. Setup VPC in a defense-in-depth style: Setup SSH Bastion server for further investigation: Setup Aurora instance: Deploy Odoo 14. 0: ConclusionIn the first section, we compare available solutions for deployments of Odoo (both paid and free editions). Surprisingly, we found that paid solution, the Odoo Enterprise can be cheaper than using the free solutions if we count all development and maintenance fees. And because this is a reliable and stable solution, Odoo Enterprise with full support from Odoo Cloud Platform is more advantageous with less effort (you and your people can sleep well). From the view of business, the winner is determined. In the second part, although using the community edition does not offer any real benefit in business, we still explored the way to deploy by ourselves to AWS. In conclusion, ready-to-use solutions like Odoo Enterprise with Odoo Cloud seem to be the winner. If you like this, please give :+1: "
    }, {
    "id": 75,
    "url": "https://wanted2.github.io/hybrid-cloud-vmware-on-aws/",
    "title": "On Hybrid Clouds: VMware Cloud on AWS",
    "body": "2021/02/14 - Note: this post is about building Customer Data Center (CDC) for enterprise, not about applications for end-users. For many years, enterprise customers have been deployed their infrastructure into VMware cloud with powerful and secure virtualization technologies like vSphere for computing resources (VMs, . etc. ), vSAN (storage) and NSX (networking). While new and leading public cloud solutions like Amazon Web Services (AWS) offer lower cost tiers, with greater opportunities in building innovative solutions, transforming from the existing VMware cloud to AWS is a trade-off between reliability (on existing solutions) with better business offers. Building a hybrid cloud using VMware Cloud on AWS is a way to achieve both goals: maintaining customers’ trust while migrating to novel solutions. Introduction: Migration to AWSEnterprise customers refer to the companies who have more than 500 personnel each (See the report 1, pdf). These customers have some characteristics in common: they have adopted a stable and reliable infrastructure for years and they are refraining from making a sudden changes in what is already running well. For example, a company who already have their infrastructure running on VMware Cloud and the system have established and have been running well for 20 years, then now it would be difficult t persuade the executives to change to AWS and stop all what already been running well so far. The problems arise when persuading customer to migrate to a new platform like AWS:  The cost of destroying what have been running well: Our infrastructure in VMware have been running well for 20 years, now if we stop everything, we need to investigate on re-training our staffs to be familiar with new systems. The cost of migrating our products to new platform is also high and the waste should be considerable. This is a common customer claim.  The gain of new business opportunities in new platform: Does new platform offer us genuine solutions? Is the gain better than the cost?Solving these trade-offs is a hurdle for migration to new businesses. VMware has offered a great solution to this problem: hybrid clouds with VMware Cloud on AWS. In short, this means the customers deploy their data center to AWS Cloud using VMware Cloud technologies. All of the resources will be allocated in AWS but the management will be done using VMware Cloud solutions such as vSphere, vSAN and NSX23. VMware Cloud on AWSBenefits:  Access to a public cloud environment that is consistent with an on-premises environment and can be operated with the same tools and skill sets as customers’ on-premises VMware environments, allowing migration, operations, and integration with customers’ on-premises environments Ability to add or remove resources on demand within minutes and use resources with hourly pay-as-you-go pricing, enabling agility and flexibility with customers’ VMware environments Access to AWS public cloud services, including databases, data analytics services, and emerging technologies such as artificial intelligence/machine learning (AI/ML) Delivered as a completely managed service by VMware, with pay-as-you-go pricing and no up-front commitmentsLearning resources: Tutorials:    Migrating MS SQL Server to AWS using VMware Cloud     Managing Oracle Database using VMware Cloud on AWS     Performance Characterization of Microsoft SQL Server Using VMware Cloud on AWS     Optimize Virtual Machine Configurations in VMware Cloud on AWS for Enterprise Applications Workload     DNS Strategies for VMware Cloud on AWS     Using an On-Premises DHCP server     Understanding Integrations with AWS Services  Official guides:  User guides AWS guidesOthers:  Series on how-to VMC on AWS VMware Cloud on AWS Tech Zone Introducing the HashiCorp Terraform Provider for VMware Cloud on AWSConclusionSmall business customers are likely to accept the solutions using new platforms like AWS because they don’t possess an existing solution their own. However, medium and large enterprises will need to consider the waste when migrating to new business platforms. Therefore, a solution like VMware Cloud on AWS is a big deal. Enterprises will not need to stop their existing infrastructures, they only move the underlying resources which they rarely need to interact with to AWS, while they still use the same tools (vSphere, vSAN, NSX) for management in VMware Cloud. References      The Business Value of Running Applications on VMware Cloud on AWS in VMware Hybrid Cloud Environments &#8617;        VMware Cloud on AWS: Tech Deep Dive Webinar &#8617;        VMware Cloud on AWS: Deployment, Migration, and Configuration of Oracle Workloads &#8617;    "
    }, {
    "id": 76,
    "url": "https://wanted2.github.io/oracle-cloud-infra/",
    "title": "About Oracle Cloud Infrastructure (OCI)",
    "body": "2021/02/12 - I’ve created this diagram to summarize the services in Oracle Cloud Infrastructure (OCI). I usually use AWS for building applications but apparently, OCI is often used in enterprise solutions. For example, Zoom also chose OCI to deploy their infra. Unsurprisingly, I found that the structure of services in OCI is similar to AWS in several foundational categories: Management, Security, Identity, Database, Compute, Storage, . etc. However, in terms of Artifical Intelligence and IoT applications, it feels like OCI has less showcases and developers who use OCI will be likely to have more tedious work to work. For example, building a chatbot is a relatively affordable task for IBM Watson users or AWS ML users. Quite recently, OCI has announced some of their tutorials in Fraud Detection, and released their Oracle Machine Learning (OML) solution. Anyway, all cloud platforms have their own upsides and downsides. Developers would learn to use and making their own choice. For me, for building AI/IoT applications, I still feel good with Amazon ;) "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});